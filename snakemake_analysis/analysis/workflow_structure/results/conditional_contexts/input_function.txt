repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if "archive" in genome']
    (47, 'def get_archive_input(wildcards):')
    (48, '    genome = genomes[wildcards.genome]')
    (49, '    if "archive" in genome:')
    (50, '        return f"resources/archives/{wildcards.genome}"')
    (51, '    else:')
    (52, '        return []')
    (53, '')
    (54, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if "bam-url" in benchmark']
    (64, 'def get_bwa_input(wildcards):')
    (65, '    benchmark = get_benchmark(wildcards.benchmark)')
    (66, '    if "bam-url" in benchmark:')
    (67, '        return expand(')
    (68, '            "resources/reads/{benchmark}.{read}.fq",')
    (69, '            read=[1, 2],')
    (70, '            benchmark=wildcards.benchmark,')
    (71, '        )')
    (72, '    else:')
    (73, '        return benchmark["fastqs"]')
    (74, '')
    (75, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if upper']
    (81, '    def label(name):')
    (82, '        lower, upper = get_cov_interval(name)')
    (83, '        if upper:')
    (84, '            return f"{lower}-{upper-1}"')
    (85, '        return f"\\xe2\\x89\\xa5{lower}"')
    (86, '')
    (87, '    return {name: label(name) for name in coverages}')
    (88, '')
    (89, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if isinstance(truth, dict)']
    (90, 'def get_truth_url(wildcards, input):')
    (91, '    genome = genomes[wildcards.genome]')
    (92, '    truth = genome["truth"][get_genome_build()]')
    (93, '    if isinstance(truth, dict):')
    (94, '        truth = truth[wildcards.truthset]')
    (95, '    if input.archive:')
    (96, '        return f"{input.archive}/{truth}"')
    (97, '    else:')
    (98, '        return truth')
    (99, '')
    (100, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if input.archive']
    (114, 'def get_confidence_bed_cmd(wildcards, input):')
    (115, '    genome = genomes[wildcards.genome]')
    (116, '    bed = genome["confidence-regions"][get_genome_build()]')
    (117, '')
    (118, '    if input.archive:')
    (119, '        return f"cat {input.archive}/{bed}"')
    (120, '    else:')
    (121, '        return f"curl --insecure -L {bed}"')
    (122, '')
    (123, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if f.endswith(runinfo_suffix)']
    (139, 'def get_happy_prefix(wildcards, output):')
    (140, '    runinfo_suffix = ".runinfo.json"')
    (141, '    for f in output:')
    (142, '        if f.endswith(runinfo_suffix):')
    (143, '            return f[: -len(runinfo_suffix)]')
    (144, '')
    (145, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if upper']
    (146, 'def get_cov_label(wildcards):')
    (147, '    lower, upper = get_cov_interval(wildcards.cov)')
    (148, '    if upper:')
    (149, '        return f"{lower}:{upper}"')
    (150, '    return f"{lower}:inf"')
    (151, '')
    (152, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if greater']
    (153, 'def get_cov_interval(name):')
    (154, '    threshold = coverages[name]')
    (155, '    upper_bound = None')
    (156, '')
    (157, '    greater = [cov for cov in coverages.values() if cov > threshold]')
    (158, '    if greater:')
    (159, '        upper_bound = min(greater)')
    (160, '')
    (161, '    return threshold, upper_bound')
    (162, '')
    (163, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if "rename-contigs" in callset']
    (164, 'def get_callset(wildcards):')
    (165, '    callset = config["variant-calls"][wildcards.callset]')
    (166, '    if "rename-contigs" in callset:')
    (167, '        return "results/normalized-variants/{callset}.replaced-contigs.bcf"')
    (168, '    else:')
    (169, '        return get_raw_callset(wildcards)')
    (170, '')
    (171, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if is_local_file(target_bed)']
    (181, 'def get_target_bed_input(wildcards):')
    (182, '    target_bed = get_benchmark(wildcards.benchmark)["target-regions"]')
    (183, '    if is_local_file(target_bed):')
    (184, '        return target_bed')
    (185, '    else:')
    (186, '        return []')
    (187, '')
    (188, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if is_local_file(target_bed)']
    (189, 'def get_target_bed_statement(wildcards):')
    (190, '    target_bed = get_benchmark(wildcards.benchmark)["target-regions"]')
    (191, '')
    (192, '    if is_local_file(target_bed):')
    (193, '        return f"cat {target_bed}"')
    (194, '    else:')
    (195, '        return f"curl --insecure -L {target_bed}"')
    (196, '')
    (197, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if "target-regions" in benchmark']
    (198, 'def get_target_regions(wildcards):')
    (199, '    benchmark = get_benchmark(wildcards.benchmark)')
    (200, '    if "target-regions" in benchmark:')
    (201, '        return "resources/regions/{benchmark}/target-regions.bed"')
    (202, '    else:')
    (203, '        return []')
    (204, '')
    (205, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if input.target']
    (206, 'def get_target_regions_intersect_statement(wildcards, input):')
    (207, '    if input.target:')
    (208, '        return f"bedtools intersect -a /dev/stdin -b {input.target} |"')
    (209, '    else:')
    (210, '        return ""')
    (211, '')
    (212, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if benchmark["grch37"] and not config.get("grch37")']
    (213, 'def get_liftover_statement(wildcards, input, output):')
    (214, '    benchmark = get_benchmark(wildcards.benchmark)')
    (215, '')
    (216, '    if benchmark["grch37"] and not config.get("grch37"):')
    (217, '        return f"| liftOver /dev/stdin {input.liftover} {output} /dev/null"')
    (218, '    else:')
    (219, '        return f"> {output}"')
    (220, '')
    (221, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if isinstance(truthset, str)']
    (238, 'def get_benchmark_truth(wildcards):')
    (239, '    genome_name = get_benchmark(wildcards.benchmark)["genome"]')
    (240, '    genome = genomes[genome_name]')
    (241, '')
    (242, '    truthset = genome["truth"][get_genome_build()]')
    (243, '    if isinstance(truthset, str):')
    (244, '        return f"resources/variants/{genome_name}/all.truth.bcf"')
    (245, '    else:')
    (246, '        return f"resources/variants/{genome_name}.merged.truth.bcf"')
    (247, '')
    (248, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if entries["benchmark"] == benchmar']
    (322, 'def get_benchmark_callsets(benchmark):')
    (323, '    return [')
    (324, '        callset')
    (325, '        for callset, entries in config["variant-calls"].items()')
    (326, '        if entries["benchmark"] == benchmark')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if benchmarks[entries["benchmark"]]["genome"] == genom']
    (337, 'def get_genome_callsets(genome):')
    (338, '    return sorted(')
    (339, '        callset')
    (340, '        for callset, entries in config["variant-calls"].items()')
    (341, '        if benchmarks[entries["benchmark"]]["genome"] == genome')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if labelval(label, callset']
    (359, '        def labelval(label, callset):')
    (360, '            return config["variant-calls"][callset].get("labels", dict()).get(label)')
    (361, '')
    (362, '        return {')
    (363, '            callset: labelval(label, callset)')
    (364, '            for callset in callsets')
    (365, '            if labelval(label, callset)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if "variant-calls" in config', 'wildcard_constraints']
    (388, 'def get_collect_precision_recall_labels(wildcards):')
    (389, '    callsets = get_benchmark_callsets(wildcards.benchmark)')
    (390, '    return get_callset_label_entries(callsets)')
    (391, '')
    (392, '')
    (393, 'if "variant-calls" in config:')
    (394, '')
    (395, '    wildcard_constraints:')
    (396, '        callset="|".join(config["variant-calls"]),')
    (397, '        classification="|".join(["fp", "fn"]),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/sd-divergence, file=workflow/rules/regions.smk
context_key: ['if str(wc.anno).startswith("gene-conversion-")']
    (29, 'def get_annotation_file(wc):')
    (30, '    if str(wc.anno).startswith("gene-conversion-"):')
    (31, '        sm_hap = str(wc.anno).strip("gene-conversion-")')
    (32, '        sm, h = sm_hap.split("_")')
    (33, '        return gc_df.loc[(sm, h)].file')
    (34, '    else:')
    (35, '        return config["annotation_files"][wc.anno]')
    (36, '')
    (37, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/sd-divergence, file=workflow/rules/regions.smk
context_key: ['if a1 == a2 or a1 in ["SD", "sd"]']
    (110, 'def allowed_anno_combinations(wc):')
    (111, '    file_fmt = rules.size_of_combinations_of_annotation_files.output.tbl')
    (112, '    out = []')
    (113, '    for sm in tbl.index:')
    (114, '        for h in [1, 2]:')
    (115, '            anno = list(config["annotation_files"].keys()) + [')
    (116, '                f"gene-conversion-{sm}_{h}"')
    (117, '            ]')
    (118, '            for a1 in anno:')
    (119, '                for a2 in anno:')
    (120, '                    f = (file_fmt).format(h=h, sm=sm, anno1=a1, anno2=a2)')
    (121, '                    if a1 == a2 or a1 in ["SD", "sd"]:')
    (122, '                        out.append(f)')
    (123, '                    else:')
    (124, '                        continue')
    (125, '    return out')
    (126, '')
    (127, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if pd.isnull(group)']
    (25, '    def _group_or_sample(row):')
    (26, '        group = row.get("group", None)')
    (27, '        if pd.isnull(group):')
    (28, '            return row["sample"]')
    (29, '        return group')
    (30, '')
    (31, '    samples["group"] = [_group_or_sample(row) for _, row in samples.iterrows()]')
    (32, '    validate(samples, schema="../schemas/samples.schema.yaml")')
    (33, '    return samples')
    (34, '')
    (35, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if samples.loc[wildcards.sample]["platform"] == "nanopore"']
    (69, 'def pairhmm_mode(wildcards):')
    (70, '    if samples.loc[wildcards.sample]["platform"] == "nanopore":')
    (71, '        mode = "homopolymer"')
    (72, '    else:')
    (73, '        mode = "exact"')
    (74, '    return mode')
    (75, '')
    (76, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if scenario == "nanopore_only"']
    (77, 'def get_group_candidates(wildcards):')
    (78, '    sample = wildcards.sample')
    (79, '    group = samples.loc[sample]["group"]')
    (80, '    wildcards.group = group')
    (81, '    scenario = scenario_name(wildcards)')
    (82, '    if scenario == "nanopore_only":')
    (83, '        sample = list(')
    (84, '            samples.query(f"group == \\\'{group}\\\' & platform == \\\'nanopore\\\'")["sample"]')
    (85, '        )[0]')
    (86, '        return f"results/calling/candidate-calls/{sample}.{{scatteritem}}.bcf"')
    (87, '    elif scenario == "illumina_only":')
    (88, '        sample = list(')
    (89, '            samples.query(f"group == \\\'{group}\\\' & platform == \\\'illumina\\\'")["sample"]')
    (90, '        )[0]')
    (91, '        return f"results/calling/candidate-calls/{sample}.{{scatteritem}}.bcf"')
    (92, '    elif scenario == "nanopore_with_illumina_support":')
    (93, '        sample = list(')
    (94, '            samples.query(f"group == \\\'{group}\\\' & platform == \\\'nanopore\\\'")["sample"]')
    (95, '        )[0]')
    (96, '        return f"results/calling/candidate-calls/{sample}.{{scatteritem}}.bcf"')
    (97, '    else:')
    (98, '        raise ValueError(f"Unknown scenario: {scenario}")')
    (99, '')
    (100, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if group_size == 1', 'if scenario == "nanopore_only"']
    (101, 'def observation_string(wildcards, input):')
    (102, '    group_size = len(samples.query(f"group == \\\'{wildcards.group}\\\'"))')
    (103, '    scenario = scenario_name(wildcards)')
    (104, '    if group_size == 1:')
    (105, '        if scenario == "nanopore_only":')
    (106, '            return f"nanopore={input.obs[0]}"')
    (107, '        elif scenario == "illumina_only":')
    (108, '            return f"illumina={input.obs[0]}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if group_size == 1', 'elif group_size == 2']
    (101, 'def observation_string(wildcards, input):')
    (102, '    group_size = len(samples.query(f"group == \\\'{wildcards.group}\\\'"))')
    (103, '    scenario = scenario_name(wildcards)')
    (104, '    if group_size == 1:')
    (105, '        if scenario == "nanopore_only":')
    (106, '            return f"nanopore={input.obs[0]}"')
    (107, '        elif scenario == "illumina_only":')
    (108, '            return f"illumina={input.obs[0]}"')
    (109, '    elif group_size == 2:')
    (110, '        return f"nanopore={input.obs[0]} illumina={input.obs[1]}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if group_size == 1', 'else']
    (101, 'def observation_string(wildcards, input):')
    (102, '    group_size = len(samples.query(f"group == \\\'{wildcards.group}\\\'"))')
    (103, '    scenario = scenario_name(wildcards)')
    (104, '    if group_size == 1:')
    (105, '        if scenario == "nanopore_only":')
    (106, '            return f"nanopore={input.obs[0]}"')
    (107, '        elif scenario == "illumina_only":')
    (108, '            return f"illumina={input.obs[0]}"')
    (109, '    elif group_size == 2:')
    (110, '        return f"nanopore={input.obs[0]} illumina={input.obs[1]}"')
    (111, '    else:')
    (112, '        raise ValueError(')
    (113, '            f"Too many samples ({group_size}) for this group ({wildcards.group}) and scenario ({scenario})"')
    (114, '        )')
    (115, '')
    (116, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if len(s) == 0']
    (117, 'def get_observations(wildcards):')
    (118, '    s = samples.query(f"group == \\\'{wildcards.group}\\\'")')
    (119, '    if len(s) == 0:')
    (120, '        raise ValueError(f"No samples for group {wildcards.group}")')
    (121, '')
    (122, '    observations = []')
    (123, '')
    (124, '    has_nanopore = len(s.query("platform == \\\'nanopore\\\'")["sample"]) > 0')
    (125, '    has_illumina = len(s.query("platform == \\\'illumina\\\'")["sample"]) > 0')
    (126, '')
    (127, '    if has_nanopore:')
    (128, '        for sample_nanopore in list(s.query("platform == \\\'nanopore\\\'")["sample"]):')
    (129, '            observations.append(')
    (130, '                f"results/calling/calls/observations/{sample_nanopore}.{{scatteritem}}.bcf"')
    (131, '            )')
    (132, '')
    (133, '    if has_illumina:')
    (134, '        for sample_illumina in list(s.query("platform == \\\'illumina\\\'")["sample"]):')
    (135, '            observations.append(')
    (136, '                f"results/calling/calls/observations/{sample_illumina}.{{scatteritem}}.bcf"')
    (137, '            )')
    (138, '')
    (139, '    return observations')
    (140, '')
    (141, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if num_samples_in_group == 1', 'if "illumina" in set(s["platform"])']
    (142, 'def scenario_name(wildcards):')
    (143, '    s = samples.query(f"group == \\\'{wildcards.group}\\\'")')
    (144, '    num_samples_in_group = len(s)')
    (145, '    if num_samples_in_group == 1:')
    (146, '        if "illumina" in set(s["platform"]):')
    (147, '            return "illumina_only"')
    (148, '        elif "nanopore" in set(s["platform"]):')
    (149, '            return "nanopore_only"')
    (150, '        else:')
    (151, '            platforms = ", ".join(set(s["platform"]))')
    (152, '            raise ValueError(')
    (153, '                f"Single sample scenario not defined for platforms {platforms}"')
    (154, '            )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if num_samples_in_group == 1', 'elif num_samples_in_group == 2', 'if len(set(s["platform"]) - {"illumina", "nanopore"}) == 0']
    (142, 'def scenario_name(wildcards):')
    (143, '    s = samples.query(f"group == \\\'{wildcards.group}\\\'")')
    (144, '    num_samples_in_group = len(s)')
    (145, '    if num_samples_in_group == 1:')
    (146, '        if "illumina" in set(s["platform"]):')
    (147, '            return "illumina_only"')
    (148, '        elif "nanopore" in set(s["platform"]):')
    (149, '            return "nanopore_only"')
    (150, '        else:')
    (151, '            platforms = ", ".join(set(s["platform"]))')
    (152, '            raise ValueError(')
    (153, '                f"Single sample scenario not defined for platforms {platforms}"')
    (154, '            )')
    (155, '    elif num_samples_in_group == 2:')
    (156, '        if len(set(s["platform"]) - {"illumina", "nanopore"}) == 0:')
    (157, '            return "nanopore_with_illumina_support"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if num_samples_in_group == 1', 'elif num_samples_in_group == 2', 'else']
    (142, 'def scenario_name(wildcards):')
    (143, '    s = samples.query(f"group == \\\'{wildcards.group}\\\'")')
    (144, '    num_samples_in_group = len(s)')
    (145, '    if num_samples_in_group == 1:')
    (146, '        if "illumina" in set(s["platform"]):')
    (147, '            return "illumina_only"')
    (148, '        elif "nanopore" in set(s["platform"]):')
    (149, '            return "nanopore_only"')
    (150, '        else:')
    (151, '            platforms = ", ".join(set(s["platform"]))')
    (152, '            raise ValueError(')
    (153, '                f"Single sample scenario not defined for platforms {platforms}"')
    (154, '            )')
    (155, '    elif num_samples_in_group == 2:')
    (156, '        if len(set(s["platform"]) - {"illumina", "nanopore"}) == 0:')
    (157, '            return "nanopore_with_illumina_support"')
    (158, '        else:')
    (159, '            raise ValueError(')
    (160, '                "Need both illumina and nanopore samples for this scenario"')
    (161, '            )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if num_samples_in_group == 1', 'else']
    (142, 'def scenario_name(wildcards):')
    (143, '    s = samples.query(f"group == \\\'{wildcards.group}\\\'")')
    (144, '    num_samples_in_group = len(s)')
    (145, '    if num_samples_in_group == 1:')
    (146, '        if "illumina" in set(s["platform"]):')
    (147, '            return "illumina_only"')
    (148, '        elif "nanopore" in set(s["platform"]):')
    (149, '            return "nanopore_only"')
    (150, '        else:')
    (151, '            platforms = ", ".join(set(s["platform"]))')
    (152, '            raise ValueError(')
    (153, '                f"Single sample scenario not defined for platforms {platforms}"')
    (154, '            )')
    (155, '    elif num_samples_in_group == 2:')
    (156, '        if len(set(s["platform"]) - {"illumina", "nanopore"}) == 0:')
    (157, '            return "nanopore_with_illumina_support"')
    (158, '        else:')
    (159, '            raise ValueError(')
    (160, '                "Need both illumina and nanopore samples for this scenario"')
    (161, '            )')
    (162, '    else:')
    (163, '        raise ValueError(')
    (164, '            "Scenarios with more than two samples per group currently not supported"')
    (165, '        )')
    (166, '')
    (167, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if scenario == "nanopore_only"']
    (168, 'def get_scenario(wildcards):')
    (169, '    scenario = scenario_name(wildcards)')
    (170, '    if scenario == "nanopore_only":')
    (171, '        return "resources/scenarios/nanopore_circle_scenario.yaml"')
    (172, '    elif scenario == "illumina_only":')
    (173, '        return "resources/scenarios/illumina_circle_scenario.yaml"')
    (174, '    elif scenario == "nanopore_with_illumina_support":')
    (175, '        return "resources/scenarios/nanopore_illumina_joint_circle_scenario.yaml"')
    (176, '    else:')
    (177, '        raise ValueError(f"Unknown scenario: {scenario}")')
    (178, '')
    (179, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if samples.loc[wildcards.sample]["platform"] == "nanopore"']
    (180, 'def get_minimap2_mapping_params(wildcards):')
    (181, '    if samples.loc[wildcards.sample]["platform"] == "nanopore":')
    (182, '        return "-x map-ont"')
    (183, '    elif samples.loc[wildcards.sample]["platform"] == "illumina":')
    (184, '        return "-x sr"')
    (185, '    else:')
    (186, '        return ""')
    (187, '')
    (188, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if units.loc[wildcards.sample]["fq2"].any()']
    (189, 'def get_minimap2_input(wildcards):')
    (190, '    if units.loc[wildcards.sample]["fq2"].any():')
    (191, '        return [')
    (192, '            "results/calling/merged/{sample}_R1.fastq.gz",')
    (193, '            "results/calling/merged/{sample}_R2.fastq.gz",')
    (194, '        ]')
    (195, '    else:')
    (196, '        return ["results/calling/merged/{sample}_single.fastq.gz"]')
    (197, '')
    (198, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if wildcards.read == "single"']
    (199, 'def get_fastqs(wildcards):')
    (200, '    if wildcards.read == "single":')
    (201, '        fq1 = units.loc[wildcards.sample]["fq1"]')
    (202, '        return list(fq1)')
    (203, '    elif wildcards.read == "R1":')
    (204, '        fq1 = units.loc[wildcards.sample]["fq1"]')
    (205, '        return list(fq1)')
    (206, '    elif wildcards.read == "R2":')
    (207, '        fq2 = units.loc[wildcards.sample]["fq2"]')
    (208, '        return list(fq2)')
    (209, '')
    (210, '')
    (211, "# black wasn\\'t happy about the inline version of this in the params section")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cyrcular-calling, file=workflow/rules/common.smk
context_key: ['if self.target != other.target']
    (244, '    def merge(self, other: "Region", min_overlap: int = 0) -> "Region":')
    (245, '        if self.target != other.target:')
    (246, '            raise ValueError("targets do not match")')
    (247, '        if self.overlap(other) < min_overlap:')
    (248, '            raise ValueError("ranges do not overlap")')
    (249, '        return Region(')
    (250, '            self.target, min(self.start, other.start), max(self.end, other.end)')
    (251, '        )')
    (252, '')
    (253, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ovewh/Climaso, file=workflow/Snakefile
context_key: ['Parameters', 'if model in ["NorESM2-LM", "NorESM2-MM"]']
    (65, 'def get_paths(w, variable,experiment, grid_label=None, activity=None, control=False):')
    (66, '    """')
    (67, '    Get CMIP6 model paths in database based on the lookup tables.')
    (68, '')
    (69, '    Parameters:')
    (70, '    -----------')
    (71, '        w : snake.wildcards')
    (72, '                a named tuple that contains the snakemake wildcards')
    (73, '')
    (74, '    """')
    (75, '    model = w.model')
    (76, '    if model in ["NorESM2-LM", "NorESM2-MM"]:')
    (77, "        root_path = f\\'{ROOT_PATH_NORESM}/{CMIP_VER}\\'")
    (78, '        look_fnames = LOOK_FNAMES_NORESM')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ovewh/Climaso, file=workflow/Snakefile
context_key: ['Parameters', 'if activity']
    (65, 'def get_paths(w, variable,experiment, grid_label=None, activity=None, control=False):')
    (66, '    """')
    (67, '    Get CMIP6 model paths in database based on the lookup tables.')
    (68, '')
    (69, '    Parameters:')
    (70, '    -----------')
    (71, '        w : snake.wildcards')
    (72, '                a named tuple that contains the snakemake wildcards')
    (73, '')
    (74, '    """')
    (75, '    model = w.model')
    (76, '    if model in ["NorESM2-LM", "NorESM2-MM"]:')
    (77, "        root_path = f\\'{ROOT_PATH_NORESM}/{CMIP_VER}\\'")
    (78, '        look_fnames = LOOK_FNAMES_NORESM')
    (79, '    else:')
    (80, "        root_path = f\\'{ROOT_PATH}/{CMIP_VER}\\'")
    (81, '        look_fnames = LOOK_FNAMES')
    (82, '    if activity:')
    (83, '        activity= activity')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ovewh/Climaso, file=workflow/Snakefile
context_key: ['Parameters', 'else', 'if experiment not in list(look_fnames[activity][model].keys())']
    (65, 'def get_paths(w, variable,experiment, grid_label=None, activity=None, control=False):')
    (66, '    """')
    (67, '    Get CMIP6 model paths in database based on the lookup tables.')
    (68, '')
    (69, '    Parameters:')
    (70, '    -----------')
    (71, '        w : snake.wildcards')
    (72, '                a named tuple that contains the snakemake wildcards')
    (73, '')
    (74, '    """')
    (75, '    model = w.model')
    (76, '    if model in ["NorESM2-LM", "NorESM2-MM"]:')
    (77, "        root_path = f\\'{ROOT_PATH_NORESM}/{CMIP_VER}\\'")
    (78, '        look_fnames = LOOK_FNAMES_NORESM')
    (79, '    else:')
    (80, "        root_path = f\\'{ROOT_PATH}/{CMIP_VER}\\'")
    (81, '        look_fnames = LOOK_FNAMES')
    (82, '    if activity:')
    (83, '        activity= activity')
    (84, '    else:')
    (85, '        activity = LOOK_EXP[experiment]')
    (86, '        ')
    (87, '        if experiment not in list(look_fnames[activity][model].keys()):')
    (88, "            activity = config.get(\\'default_activity\\',\\'CMIP\\')")
    (89, '        ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ovewh/Climaso, file=workflow/Snakefile
context_key: ['Parameters', 'if control']
    (65, 'def get_paths(w, variable,experiment, grid_label=None, activity=None, control=False):')
    (66, '    """')
    (67, '    Get CMIP6 model paths in database based on the lookup tables.')
    (68, '')
    (69, '    Parameters:')
    (70, '    -----------')
    (71, '        w : snake.wildcards')
    (72, '                a named tuple that contains the snakemake wildcards')
    (73, '')
    (74, '    """')
    (75, '    model = w.model')
    (76, '    if model in ["NorESM2-LM", "NorESM2-MM"]:')
    (77, "        root_path = f\\'{ROOT_PATH_NORESM}/{CMIP_VER}\\'")
    (78, '        look_fnames = LOOK_FNAMES_NORESM')
    (79, '    else:')
    (80, "        root_path = f\\'{ROOT_PATH}/{CMIP_VER}\\'")
    (81, '        look_fnames = LOOK_FNAMES')
    (82, '    if activity:')
    (83, '        activity= activity')
    (84, '    else:')
    (85, '        activity = LOOK_EXP[experiment]')
    (86, '        ')
    (87, '        if experiment not in list(look_fnames[activity][model].keys()):')
    (88, "            activity = config.get(\\'default_activity\\',\\'CMIP\\')")
    (89, '        ')
    (90, '    if control:')
    (91, "        prefered_variant=config[\\'model_specific_variant\\'][\\'control\\'].get(model, config[\\'variant_default\\'])")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ovewh/Climaso, file=workflow/Snakefile
context_key: ['Parameters', 'if prefered_variant in available_variants']
    (65, 'def get_paths(w, variable,experiment, grid_label=None, activity=None, control=False):')
    (66, '    """')
    (67, '    Get CMIP6 model paths in database based on the lookup tables.')
    (68, '')
    (69, '    Parameters:')
    (70, '    -----------')
    (71, '        w : snake.wildcards')
    (72, '                a named tuple that contains the snakemake wildcards')
    (73, '')
    (74, '    """')
    (75, '    model = w.model')
    (76, '    if model in ["NorESM2-LM", "NorESM2-MM"]:')
    (77, "        root_path = f\\'{ROOT_PATH_NORESM}/{CMIP_VER}\\'")
    (78, '        look_fnames = LOOK_FNAMES_NORESM')
    (79, '    else:')
    (80, "        root_path = f\\'{ROOT_PATH}/{CMIP_VER}\\'")
    (81, '        look_fnames = LOOK_FNAMES')
    (82, '    if activity:')
    (83, '        activity= activity')
    (84, '    else:')
    (85, '        activity = LOOK_EXP[experiment]')
    (86, '        ')
    (87, '        if experiment not in list(look_fnames[activity][model].keys()):')
    (88, "            activity = config.get(\\'default_activity\\',\\'CMIP\\')")
    (89, '        ')
    (90, '    if control:')
    (91, "        prefered_variant=config[\\'model_specific_variant\\'][\\'control\\'].get(model, config[\\'variant_default\\'])")
    (92, '    else:')
    (93, "        prefered_variant = config[\\'model_specific_variant\\'][\\'experiment\\'].get(model, config[\\'variant_default\\'])")
    (94, '    table_id = TABLE_IDS.get(variable,DEFAULT_TABLE_ID)')
    (95, '    institution = LOOK_INSTITU[model]')
    (96, '    try:')
    (97, '        file_endings = look_fnames[activity][model][experiment]')
    (98, '    except:')
    (99, '        raise KeyError(f"File ending is not defined for this combination of {activity}, {model} and {experiment} " +')
    (100, '                        "please update config/lookup_file_endings.yaml accordingly")')
    (101, '    available_variants = list(file_endings.keys())')
    (102, '    if prefered_variant in available_variants:')
    (103, '        variant = prefered_variant')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ovewh/Climaso, file=workflow/Snakefile
context_key: ['Parameters', 'if grid_label == None']
    (65, 'def get_paths(w, variable,experiment, grid_label=None, activity=None, control=False):')
    (66, '    """')
    (67, '    Get CMIP6 model paths in database based on the lookup tables.')
    (68, '')
    (69, '    Parameters:')
    (70, '    -----------')
    (71, '        w : snake.wildcards')
    (72, '                a named tuple that contains the snakemake wildcards')
    (73, '')
    (74, '    """')
    (75, '    model = w.model')
    (76, '    if model in ["NorESM2-LM", "NorESM2-MM"]:')
    (77, "        root_path = f\\'{ROOT_PATH_NORESM}/{CMIP_VER}\\'")
    (78, '        look_fnames = LOOK_FNAMES_NORESM')
    (79, '    else:')
    (80, "        root_path = f\\'{ROOT_PATH}/{CMIP_VER}\\'")
    (81, '        look_fnames = LOOK_FNAMES')
    (82, '    if activity:')
    (83, '        activity= activity')
    (84, '    else:')
    (85, '        activity = LOOK_EXP[experiment]')
    (86, '        ')
    (87, '        if experiment not in list(look_fnames[activity][model].keys()):')
    (88, "            activity = config.get(\\'default_activity\\',\\'CMIP\\')")
    (89, '        ')
    (90, '    if control:')
    (91, "        prefered_variant=config[\\'model_specific_variant\\'][\\'control\\'].get(model, config[\\'variant_default\\'])")
    (92, '    else:')
    (93, "        prefered_variant = config[\\'model_specific_variant\\'][\\'experiment\\'].get(model, config[\\'variant_default\\'])")
    (94, '    table_id = TABLE_IDS.get(variable,DEFAULT_TABLE_ID)')
    (95, '    institution = LOOK_INSTITU[model]')
    (96, '    try:')
    (97, '        file_endings = look_fnames[activity][model][experiment]')
    (98, '    except:')
    (99, '        raise KeyError(f"File ending is not defined for this combination of {activity}, {model} and {experiment} " +')
    (100, '                        "please update config/lookup_file_endings.yaml accordingly")')
    (101, '    available_variants = list(file_endings.keys())')
    (102, '    if prefered_variant in available_variants:')
    (103, '        variant = prefered_variant')
    (104, '    else:')
    (105, '        print("did not find prefered variant {} for {} in [{}]".format(prefered_variant, model,\\\' \\\'.join(available_variants)))')
    (106, '        print("selecting variant {}".format(available_variants[0]))')
    (107, '        variant = available_variants[0]')
    (108, '    if grid_label == None:')
    (109, "        grid_label = look_fnames[activity][model][experiment][variant][table_id][\\'gl\\'][0]")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ovewh/Climaso, file=workflow/Snakefile
context_key: ['Parameters', 'if os.path.exists(check_path)==False']
    (65, 'def get_paths(w, variable,experiment, grid_label=None, activity=None, control=False):')
    (66, '    """')
    (67, '    Get CMIP6 model paths in database based on the lookup tables.')
    (68, '')
    (69, '    Parameters:')
    (70, '    -----------')
    (71, '        w : snake.wildcards')
    (72, '                a named tuple that contains the snakemake wildcards')
    (73, '')
    (74, '    """')
    (75, '    model = w.model')
    (76, '    if model in ["NorESM2-LM", "NorESM2-MM"]:')
    (77, "        root_path = f\\'{ROOT_PATH_NORESM}/{CMIP_VER}\\'")
    (78, '        look_fnames = LOOK_FNAMES_NORESM')
    (79, '    else:')
    (80, "        root_path = f\\'{ROOT_PATH}/{CMIP_VER}\\'")
    (81, '        look_fnames = LOOK_FNAMES')
    (82, '    if activity:')
    (83, '        activity= activity')
    (84, '    else:')
    (85, '        activity = LOOK_EXP[experiment]')
    (86, '        ')
    (87, '        if experiment not in list(look_fnames[activity][model].keys()):')
    (88, "            activity = config.get(\\'default_activity\\',\\'CMIP\\')")
    (89, '        ')
    (90, '    if control:')
    (91, "        prefered_variant=config[\\'model_specific_variant\\'][\\'control\\'].get(model, config[\\'variant_default\\'])")
    (92, '    else:')
    (93, "        prefered_variant = config[\\'model_specific_variant\\'][\\'experiment\\'].get(model, config[\\'variant_default\\'])")
    (94, '    table_id = TABLE_IDS.get(variable,DEFAULT_TABLE_ID)')
    (95, '    institution = LOOK_INSTITU[model]')
    (96, '    try:')
    (97, '        file_endings = look_fnames[activity][model][experiment]')
    (98, '    except:')
    (99, '        raise KeyError(f"File ending is not defined for this combination of {activity}, {model} and {experiment} " +')
    (100, '                        "please update config/lookup_file_endings.yaml accordingly")')
    (101, '    available_variants = list(file_endings.keys())')
    (102, '    if prefered_variant in available_variants:')
    (103, '        variant = prefered_variant')
    (104, '    else:')
    (105, '        print("did not find prefered variant {} for {} in [{}]".format(prefered_variant, model,\\\' \\\'.join(available_variants)))')
    (106, '        print("selecting variant {}".format(available_variants[0]))')
    (107, '        variant = available_variants[0]')
    (108, '    if grid_label == None:')
    (109, "        grid_label = look_fnames[activity][model][experiment][variant][table_id][\\'gl\\'][0]")
    (110, "    check_path = f\\'{root_path}/{activity}/{institution}/{model}/{experiment}/{variant}/{table_id}/{variable}/{grid_label}\\'")
    (111, '    if os.path.exists(check_path)==False:')
    (112, "        grid_labels = [\\'gr\\',\\'gn\\', \\'gl\\',\\'grz\\', \\'gr1\\']")
    (113, '        i = 0')
    (114, '        while os.path.exists(check_path)==False and i < len(grid_labels):')
    (115, '            grid_label = grid_labels[i]')
    (116, "            check_path = f\\'{root_path}/{activity}/{institution}/{model}/{experiment}/{variant}/{table_id}/{variable}/{grid_label}\\'")
    (117, '            i += 1 ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ovewh/Climaso, file=workflow/Snakefile
context_key: ['Parameters', 'if control']
    (65, 'def get_paths(w, variable,experiment, grid_label=None, activity=None, control=False):')
    (66, '    """')
    (67, '    Get CMIP6 model paths in database based on the lookup tables.')
    (68, '')
    (69, '    Parameters:')
    (70, '    -----------')
    (71, '        w : snake.wildcards')
    (72, '                a named tuple that contains the snakemake wildcards')
    (73, '')
    (74, '    """')
    (75, '    model = w.model')
    (76, '    if model in ["NorESM2-LM", "NorESM2-MM"]:')
    (77, "        root_path = f\\'{ROOT_PATH_NORESM}/{CMIP_VER}\\'")
    (78, '        look_fnames = LOOK_FNAMES_NORESM')
    (79, '    else:')
    (80, "        root_path = f\\'{ROOT_PATH}/{CMIP_VER}\\'")
    (81, '        look_fnames = LOOK_FNAMES')
    (82, '    if activity:')
    (83, '        activity= activity')
    (84, '    else:')
    (85, '        activity = LOOK_EXP[experiment]')
    (86, '        ')
    (87, '        if experiment not in list(look_fnames[activity][model].keys()):')
    (88, "            activity = config.get(\\'default_activity\\',\\'CMIP\\')")
    (89, '        ')
    (90, '    if control:')
    (91, "        prefered_variant=config[\\'model_specific_variant\\'][\\'control\\'].get(model, config[\\'variant_default\\'])")
    (92, '    else:')
    (93, "        prefered_variant = config[\\'model_specific_variant\\'][\\'experiment\\'].get(model, config[\\'variant_default\\'])")
    (94, '    table_id = TABLE_IDS.get(variable,DEFAULT_TABLE_ID)')
    (95, '    institution = LOOK_INSTITU[model]')
    (96, '    try:')
    (97, '        file_endings = look_fnames[activity][model][experiment]')
    (98, '    except:')
    (99, '        raise KeyError(f"File ending is not defined for this combination of {activity}, {model} and {experiment} " +')
    (100, '                        "please update config/lookup_file_endings.yaml accordingly")')
    (101, '    available_variants = list(file_endings.keys())')
    (102, '    if prefered_variant in available_variants:')
    (103, '        variant = prefered_variant')
    (104, '    else:')
    (105, '        print("did not find prefered variant {} for {} in [{}]".format(prefered_variant, model,\\\' \\\'.join(available_variants)))')
    (106, '        print("selecting variant {}".format(available_variants[0]))')
    (107, '        variant = available_variants[0]')
    (108, '    if grid_label == None:')
    (109, "        grid_label = look_fnames[activity][model][experiment][variant][table_id][\\'gl\\'][0]")
    (110, "    check_path = f\\'{root_path}/{activity}/{institution}/{model}/{experiment}/{variant}/{table_id}/{variable}/{grid_label}\\'")
    (111, '    if os.path.exists(check_path)==False:')
    (112, "        grid_labels = [\\'gr\\',\\'gn\\', \\'gl\\',\\'grz\\', \\'gr1\\']")
    (113, '        i = 0')
    (114, '        while os.path.exists(check_path)==False and i < len(grid_labels):')
    (115, '            grid_label = grid_labels[i]')
    (116, "            check_path = f\\'{root_path}/{activity}/{institution}/{model}/{experiment}/{variant}/{table_id}/{variable}/{grid_label}\\'")
    (117, '            i += 1 ')
    (118, '    if control:')
    (119, "        version = config[\\'version\\'][\\'version_control\\'].get(w.model, \\'latest\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ovewh/Climaso, file=workflow/Snakefile
context_key: ['Parameters', 'else', "if version != \\'latest\\'"]
    (65, 'def get_paths(w, variable,experiment, grid_label=None, activity=None, control=False):')
    (66, '    """')
    (67, '    Get CMIP6 model paths in database based on the lookup tables.')
    (68, '')
    (69, '    Parameters:')
    (70, '    -----------')
    (71, '        w : snake.wildcards')
    (72, '                a named tuple that contains the snakemake wildcards')
    (73, '')
    (74, '    """')
    (75, '    model = w.model')
    (76, '    if model in ["NorESM2-LM", "NorESM2-MM"]:')
    (77, "        root_path = f\\'{ROOT_PATH_NORESM}/{CMIP_VER}\\'")
    (78, '        look_fnames = LOOK_FNAMES_NORESM')
    (79, '    else:')
    (80, "        root_path = f\\'{ROOT_PATH}/{CMIP_VER}\\'")
    (81, '        look_fnames = LOOK_FNAMES')
    (82, '    if activity:')
    (83, '        activity= activity')
    (84, '    else:')
    (85, '        activity = LOOK_EXP[experiment]')
    (86, '        ')
    (87, '        if experiment not in list(look_fnames[activity][model].keys()):')
    (88, "            activity = config.get(\\'default_activity\\',\\'CMIP\\')")
    (89, '        ')
    (90, '    if control:')
    (91, "        prefered_variant=config[\\'model_specific_variant\\'][\\'control\\'].get(model, config[\\'variant_default\\'])")
    (92, '    else:')
    (93, "        prefered_variant = config[\\'model_specific_variant\\'][\\'experiment\\'].get(model, config[\\'variant_default\\'])")
    (94, '    table_id = TABLE_IDS.get(variable,DEFAULT_TABLE_ID)')
    (95, '    institution = LOOK_INSTITU[model]')
    (96, '    try:')
    (97, '        file_endings = look_fnames[activity][model][experiment]')
    (98, '    except:')
    (99, '        raise KeyError(f"File ending is not defined for this combination of {activity}, {model} and {experiment} " +')
    (100, '                        "please update config/lookup_file_endings.yaml accordingly")')
    (101, '    available_variants = list(file_endings.keys())')
    (102, '    if prefered_variant in available_variants:')
    (103, '        variant = prefered_variant')
    (104, '    else:')
    (105, '        print("did not find prefered variant {} for {} in [{}]".format(prefered_variant, model,\\\' \\\'.join(available_variants)))')
    (106, '        print("selecting variant {}".format(available_variants[0]))')
    (107, '        variant = available_variants[0]')
    (108, '    if grid_label == None:')
    (109, "        grid_label = look_fnames[activity][model][experiment][variant][table_id][\\'gl\\'][0]")
    (110, "    check_path = f\\'{root_path}/{activity}/{institution}/{model}/{experiment}/{variant}/{table_id}/{variable}/{grid_label}\\'")
    (111, '    if os.path.exists(check_path)==False:')
    (112, "        grid_labels = [\\'gr\\',\\'gn\\', \\'gl\\',\\'grz\\', \\'gr1\\']")
    (113, '        i = 0')
    (114, '        while os.path.exists(check_path)==False and i < len(grid_labels):')
    (115, '            grid_label = grid_labels[i]')
    (116, "            check_path = f\\'{root_path}/{activity}/{institution}/{model}/{experiment}/{variant}/{table_id}/{variable}/{grid_label}\\'")
    (117, '            i += 1 ')
    (118, '    if control:')
    (119, "        version = config[\\'version\\'][\\'version_control\\'].get(w.model, \\'latest\\')")
    (120, '    else:')
    (121, "        version = config[\\'version\\'][\\'version_exp\\'].get(experiment, \\'latest\\')")
    (122, "        if version != \\'latest\\':")
    (123, "            version = version.get(w.model, \\'latest\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ovewh/Climaso, file=workflow/Snakefile
context_key: ['Parameters', 'if os.path.exists(check_path)']
    (65, 'def get_paths(w, variable,experiment, grid_label=None, activity=None, control=False):')
    (66, '    """')
    (67, '    Get CMIP6 model paths in database based on the lookup tables.')
    (68, '')
    (69, '    Parameters:')
    (70, '    -----------')
    (71, '        w : snake.wildcards')
    (72, '                a named tuple that contains the snakemake wildcards')
    (73, '')
    (74, '    """')
    (75, '    model = w.model')
    (76, '    if model in ["NorESM2-LM", "NorESM2-MM"]:')
    (77, "        root_path = f\\'{ROOT_PATH_NORESM}/{CMIP_VER}\\'")
    (78, '        look_fnames = LOOK_FNAMES_NORESM')
    (79, '    else:')
    (80, "        root_path = f\\'{ROOT_PATH}/{CMIP_VER}\\'")
    (81, '        look_fnames = LOOK_FNAMES')
    (82, '    if activity:')
    (83, '        activity= activity')
    (84, '    else:')
    (85, '        activity = LOOK_EXP[experiment]')
    (86, '        ')
    (87, '        if experiment not in list(look_fnames[activity][model].keys()):')
    (88, "            activity = config.get(\\'default_activity\\',\\'CMIP\\')")
    (89, '        ')
    (90, '    if control:')
    (91, "        prefered_variant=config[\\'model_specific_variant\\'][\\'control\\'].get(model, config[\\'variant_default\\'])")
    (92, '    else:')
    (93, "        prefered_variant = config[\\'model_specific_variant\\'][\\'experiment\\'].get(model, config[\\'variant_default\\'])")
    (94, '    table_id = TABLE_IDS.get(variable,DEFAULT_TABLE_ID)')
    (95, '    institution = LOOK_INSTITU[model]')
    (96, '    try:')
    (97, '        file_endings = look_fnames[activity][model][experiment]')
    (98, '    except:')
    (99, '        raise KeyError(f"File ending is not defined for this combination of {activity}, {model} and {experiment} " +')
    (100, '                        "please update config/lookup_file_endings.yaml accordingly")')
    (101, '    available_variants = list(file_endings.keys())')
    (102, '    if prefered_variant in available_variants:')
    (103, '        variant = prefered_variant')
    (104, '    else:')
    (105, '        print("did not find prefered variant {} for {} in [{}]".format(prefered_variant, model,\\\' \\\'.join(available_variants)))')
    (106, '        print("selecting variant {}".format(available_variants[0]))')
    (107, '        variant = available_variants[0]')
    (108, '    if grid_label == None:')
    (109, "        grid_label = look_fnames[activity][model][experiment][variant][table_id][\\'gl\\'][0]")
    (110, "    check_path = f\\'{root_path}/{activity}/{institution}/{model}/{experiment}/{variant}/{table_id}/{variable}/{grid_label}\\'")
    (111, '    if os.path.exists(check_path)==False:')
    (112, "        grid_labels = [\\'gr\\',\\'gn\\', \\'gl\\',\\'grz\\', \\'gr1\\']")
    (113, '        i = 0')
    (114, '        while os.path.exists(check_path)==False and i < len(grid_labels):')
    (115, '            grid_label = grid_labels[i]')
    (116, "            check_path = f\\'{root_path}/{activity}/{institution}/{model}/{experiment}/{variant}/{table_id}/{variable}/{grid_label}\\'")
    (117, '            i += 1 ')
    (118, '    if control:')
    (119, "        version = config[\\'version\\'][\\'version_control\\'].get(w.model, \\'latest\\')")
    (120, '    else:')
    (121, "        version = config[\\'version\\'][\\'version_exp\\'].get(experiment, \\'latest\\')")
    (122, "        if version != \\'latest\\':")
    (123, "            version = version.get(w.model, \\'latest\\')")
    (124, "    fname = f\\'{variable}_{table_id}_{model}_{experiment}_{variant}_{grid_label}\\'")
    (125, '    if os.path.exists(check_path):')
    (126, '        fn = glob_wildcards(')
    (127, "            f\\'{root_path}/{activity}/{institution}/{model}/{experiment}/{variant}/{table_id}/{variable}/{grid_label}/{version}/{fname}_{{file_endings}}\\')")
    (128, '        if not fn.file_endings:')
    (129, '            fn = glob_wildcards(')
    (130, "            f\\'{root_path}/{activity}/{institution}/{model}/{experiment}/{variant}/{table_id}/{variable}/{grid_label}/latest/{fname}_{{file_endings}}\\')")
    (131, "            version=\\'latest\\'")
    (132, "            print(f\\'falling back to latest version... for {model}, {variable}, \\')")
    (133, '        paths = expand(')
    (134, "            f\\'{root_path}/{activity}/{institution}/{model}/{experiment}/{variant}/{table_id}/{variable}/{grid_label}/{version}/{fname}_{{file_endings}}\\'")
    (135, '            ,file_endings=fn.file_endings)')
    (136, '        paths = sorted(paths)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ovewh/Climaso, file=workflow/Snakefile
context_key: ['if grid_label == None']
    (143, 'def get_control_path(w, variable, grid_label=None):')
    (144, '    if grid_label == None:')
    (145, "        grid_label = config[\\'default_grid_label\\']")
    (146, "    if w.get(\\'experiment\\', None) == \\'piControl\\':")
    (147, "        paths = get_paths(w, variable,\\'piControl\\', grid_label,activity=\\'CMIP\\', control=False)")
    (148, "    elif w.get(\\'experiment\\',None) == \\'histSST\\':")
    (149, "        paths = get_paths(w, variable, \\'histSST-piAer\\', grid_label, activity=\\'AerChemMIP\\',control=False)")
    (150, '    else:')
    (151, '        try:')
    (152, "            paths = get_paths(w, variable,\\'piClim-control\\', grid_label,activity=\\'RFMIP\\', control=True)")
    (153, '        except KeyError:')
    (154, "            paths = get_paths(w, variable,\\'piClim-control\\', grid_label,activity=\\'AerChemMIP\\', control=True)")
    (155, '    if not paths:')
    (156, '        print(paths)')
    (157, '        import sys; sys.exit()')
    (158, '    return paths')
    (159, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/align_kmers.smk
context_key: ['if not config["settings"]["align_kmers"]["plot_manhattan"]']
    (147, 'def aggregate_input_align_kmers(wildcards):')
    (148, '    checkpoint_output = checkpoints.fetch_significant_kmers.get(**wildcards).output[0]')
    (149, '    if not config["settings"]["align_kmers"]["plot_manhattan"]:')
    (150, '        return expand("results/align_kmers/{phenos_filt}/{phenos_filt}_kmers_alignment.sorted.bam.bai",')
    (151, '               phenos_filt=glob_wildcards(os.path.join(checkpoint_output, "{phenos_filt}_kmers_list.txt")).phenos_filt)')
    (152, '    else:')
    (153, '        return expand(')
    (154, '                    [')
    (155, '                        "results/align_kmers/{phenos_filt}/{phenos_filt}_kmers_alignment.sorted.bam.bai",')
    (156, '                        "results/plots/manhattan/align_kmers/{phenos_filt}/{phenos_filt}_kmers_alignment.manhattan_plot.pdf",')
    (157, '                    ], phenos_filt=glob_wildcards(os.path.join(checkpoint_output, "{phenos_filt}_kmers_list.txt")).phenos_filt')
    (158, '                    )')
    (159, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/common.smk
context_key: ['if ( len(fastqs) == 0 or len(fastqs) == 1 )', 'if is_sra_se(wildcards.sample, wildcards.library)']
    (149, 'def get_individual_fastq(wildcards):')
    (150, '    """Get individual raw FASTQ files from library sheet, based on a read (end) wildcard"""')
    (151, '    fastqs = samples.loc[(wildcards.sample, wildcards.library), ["fq1", "fq2"]].dropna()')
    (152, '    if ( len(fastqs) == 0 or len(fastqs) == 1 ):')
    (153, '        if is_sra_se(wildcards.sample, wildcards.library):')
    (154, '            return expand("resources/ref/sra-se-reads/{accession}.fastq",')
    (155, '                              accession=samples.loc[ (wildcards.sample, wildcards.library), "sra" ])')
    (156, '        elif is_sra_pe(wildcards.sample, wildcards.library):')
    (157, '            return expand("resources/ref/sra-pe-reads/{accession}_1.fastq",')
    (158, '                              accession=samples.loc[ (wildcards.sample, wildcards.library), "sra" ])')
    (159, '        else:')
    (160, '            return samples.loc[ (wildcards.sample, wildcards.library), "fq1" ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/common.smk
context_key: ['if ( len(fastqs) == 0 or len(fastqs) == 1 )', 'elif len(fastqs) == 2', 'if is_sra_pe(wildcards.sample, wildcards.library)']
    (149, 'def get_individual_fastq(wildcards):')
    (150, '    """Get individual raw FASTQ files from library sheet, based on a read (end) wildcard"""')
    (151, '    fastqs = samples.loc[(wildcards.sample, wildcards.library), ["fq1", "fq2"]].dropna()')
    (152, '    if ( len(fastqs) == 0 or len(fastqs) == 1 ):')
    (153, '        if is_sra_se(wildcards.sample, wildcards.library):')
    (154, '            return expand("resources/ref/sra-se-reads/{accession}.fastq",')
    (155, '                              accession=samples.loc[ (wildcards.sample, wildcards.library), "sra" ])')
    (156, '        elif is_sra_pe(wildcards.sample, wildcards.library):')
    (157, '            return expand("resources/ref/sra-pe-reads/{accession}_1.fastq",')
    (158, '                              accession=samples.loc[ (wildcards.sample, wildcards.library), "sra" ])')
    (159, '        else:')
    (160, '            return samples.loc[ (wildcards.sample, wildcards.library), "fq1" ]')
    (161, '    elif len(fastqs) == 2:')
    (162, '        if is_sra_pe(wildcards.sample, wildcards.library):')
    (163, '            return expand("resources/ref/sra-pe-reads/{accession}_2.fastq",')
    (164, '                          accession=samples.loc[ (wildcards.sample, wildcards.library), "sra" ])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/common.smk
context_key: ['if ( len(fastqs) == 0 or len(fastqs) == 1 )', 'elif len(fastqs) == 2', 'else']
    (149, 'def get_individual_fastq(wildcards):')
    (150, '    """Get individual raw FASTQ files from library sheet, based on a read (end) wildcard"""')
    (151, '    fastqs = samples.loc[(wildcards.sample, wildcards.library), ["fq1", "fq2"]].dropna()')
    (152, '    if ( len(fastqs) == 0 or len(fastqs) == 1 ):')
    (153, '        if is_sra_se(wildcards.sample, wildcards.library):')
    (154, '            return expand("resources/ref/sra-se-reads/{accession}.fastq",')
    (155, '                              accession=samples.loc[ (wildcards.sample, wildcards.library), "sra" ])')
    (156, '        elif is_sra_pe(wildcards.sample, wildcards.library):')
    (157, '            return expand("resources/ref/sra-pe-reads/{accession}_1.fastq",')
    (158, '                              accession=samples.loc[ (wildcards.sample, wildcards.library), "sra" ])')
    (159, '        else:')
    (160, '            return samples.loc[ (wildcards.sample, wildcards.library), "fq1" ]')
    (161, '    elif len(fastqs) == 2:')
    (162, '        if is_sra_pe(wildcards.sample, wildcards.library):')
    (163, '            return expand("resources/ref/sra-pe-reads/{accession}_2.fastq",')
    (164, '                          accession=samples.loc[ (wildcards.sample, wildcards.library), "sra" ])')
    (165, '        else:')
    (166, '            return samples.loc[ (wildcards.sample, wildcards.library), "fq2" ]')
    (167, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/common.smk
context_key: ['if is_sra_se(wildcards.sample, wildcards.library)']
    (168, 'def get_fastqs(wildcards):')
    (169, '    """Get raw FASTQ files from library sheet."""')
    (170, '    fastqs = samples.loc[(wildcards.sample, wildcards.library), ["fq1", "fq2"]].dropna()')
    (171, '    if is_sra_se(wildcards.sample, wildcards.library):')
    (172, '        return expand("resources/ref/sra-se-reads/{accession}.fastq",')
    (173, '                          accession=samples.loc[ (wildcards.sample, wildcards.library), "sra" ])')
    (174, '    elif is_sra_pe(wildcards.sample, wildcards.library):')
    (175, '        return expand(["resources/ref/sra-pe-reads/{accession}_1.fastq", "resources/ref/sra-pe-reads/{accession}_2.fastq"],')
    (176, '                          accession=samples.loc[ (wildcards.sample, wildcards.library), "sra" ])')
    (177, '    elif len(fastqs) == 1:')
    (178, '        return samples.loc[ (wildcards.sample, wildcards.library), "fq1" ]')
    (179, '    else:')
    (180, '        u = samples.loc[ (wildcards.sample, wildcards.library), ["fq1", "fq2"] ].dropna()')
    (181, '        return [ f"{u.fq1}", f"{u.fq2}" ]')
    (182, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/common.smk
context_key: ['if not sra_only', 'if fastqs["fq1"].str.endswith(\\\'gz\\\')']
    (183, 'def ends_with_gz(wildcards):')
    (184, '    fastqs = samples.loc[(wildcards.sample, wildcards.library), ["fq1", "fq2"]].dropna()')
    (185, '    if not sra_only:')
    (186, '        if fastqs["fq1"].str.endswith(\\\'gz\\\'):')
    (187, '            return True')
    (188, '        else:')
    (189, '            return False')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/common.smk
context_key: ['if not sra_only', 'else']
    (183, 'def ends_with_gz(wildcards):')
    (184, '    fastqs = samples.loc[(wildcards.sample, wildcards.library), ["fq1", "fq2"]].dropna()')
    (185, '    if not sra_only:')
    (186, '        if fastqs["fq1"].str.endswith(\\\'gz\\\'):')
    (187, '            return True')
    (188, '        else:')
    (189, '            return False')
    (190, '    else:')
    (191, '        return False')
    (192, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=friendsofstrandseq/mosaicatcher-pipeline, file=workflow/rules/common.smk
context_key: ['if genecore is False']
    (56, '    def __init__(')
    (57, '        self,')
    (58, '        input_path,')
    (59, '        output_path,')
    (60, '        check_sm_tag=False,')
    (61, '        bam=True,')
    (62, '        genecore=False,')
    (63, '        genecore_path=str,')
    (64, '    ):')
    (65, '        if genecore is False:')
    (66, '            df_config_files = self.handle_input_data(thisdir=input_path, bam=bam)')
    (67, '        elif genecore is True:')
    (68, '            df_config_files, d_master = self.handle_input_data_genecore(')
    (69, '                thisdir=genecore_path')
    (70, '            )')
    (71, '            self.d_master = d_master')
    (72, '')
    (73, '        os.makedirs(os.path.dirname(output_path), exist_ok=True)')
    (74, '        df_config_files.to_csv(output_path, sep="\\\\t", index=False)')
    (75, '        self.df_config_files = df_config_files')
    (76, '')
    (77, '    @staticmethod')
    (78, '    def handle_input_data_genecore(thisdir):')
    (79, '        """_summary_')
    (80, '        Args:')
    (81, '            thisdir (_type_): _description_')
    (82, '            exclude_list (_type_, optional): _description_. Defaults to list.')
    (83, '        Returns:')
    (84, '            _type_: _description_')
    (85, '        """')
    (86, '        complete_df_list = list()')
    (87, '')
    (88, '        # List of folders/files to not consider (restrict to samples only)')
    (89, '        l = [')
    (90, '            e')
    (91, '            for e in os.listdir(')
    (92, '                "{genecore_prefix}/{date_folder}".format(')
    (93, '                    genecore_prefix=config["genecore_prefix"],')
    (94, '                    date_folder=config["genecore_date_folder"],')
    (95, '                )')
    (96, '            )')
    (97, '            if e.endswith(".gz")')
    (98, '        ]')
    (99, '')
    (100, '        # Create a list of  files to process for each sample')
    (101, '        d_master = collections.defaultdict(dict)')
    (102, '        sub_l = list()')
    (103, '        for j, e in enumerate(l):')
    (104, '            sub_l.append(e)')
    (105, '            if (j + 1) % 192 == 0:')
    (106, '                common_element = findstem(sub_l)')
    (107, '                l_elems = common_element.split("lane1")')
    (108, '                prefix = l_elems[0]')
    (109, '                technician_name = l_elems[0].split("_")[-2]')
    (110, '                sample = l_elems[1].split("x")[0]')
    (111, '                index = l_elems[1].split("x")[1].split("PE")[0][-1]')
    (112, '                pe_index = common_element[-1]')
    (113, '                sub_l = list()')
    (114, '')
    (115, '                d_master[sample]["prefix"] = prefix')
    (116, '                d_master[sample]["technician_name"] = technician_name')
    (117, '                d_master[sample]["index"] = index')
    (118, '                d_master[sample]["pe_index"] = pe_index')
    (119, '                d_master[sample]["common_element"] = common_element')
    (120, '')
    (121, '        samples_to_process = (')
    (122, '            config["samples_to_process"]')
    (123, '            if len(config["samples_to_process"]) > 0')
    (124, '            else list(d_master.keys())')
    (125, '        )')
    (126, '')
    (127, '        config["data_location"] = "{data_location}/{genecore_date_folder}".format(')
    (128, '            data_location=config["data_location"],')
    (129, '            genecore_date_folder=config["genecore_date_folder"],')
    (130, '        )')
    (131, '')
    (132, '        genecore_list = [')
    (133, '            expand(')
    (134, '                "{data_location}/{sample}/fastq/{sample}x0{index}PE20{cell_nb}.{pair}.fastq.gz",')
    (135, '                data_location=config["data_location"],')
    (136, '                sample=sample,')
    (137, '                index=d_master[sample]["index"],')
    (138, '                cell_nb=list(')
    (139, '                    range(')
    (140, '                        (int(d_master[sample]["pe_index"]) * 100) + 1,')
    (141, '                        (int(d_master[sample]["pe_index"]) * 100) + 97,')
    (142, '                    )')
    (143, '                ),')
    (144, '                pair=["1", "2"],')
    (145, '            )')
    (146, '            for sample in d_master')
    (147, '            if sample in samples_to_process')
    (148, '        ]')
    (149, '        genecore_list = [sub_e for e in genecore_list for sub_e in e]')
    (150, '')
    (151, '        complete_df_list = list()')
    (152, '')
    (153, '        for sample in d_master:')
    (154, '            df = pd.DataFrame(')
    (155, '                [')
    (156, '                    {"File": os.path.basename(f), "Folder": os.path.dirname(f)}')
    (157, '                    for f in genecore_list')
    (158, '                    if sample in f')
    (159, '                ]')
    (160, '            )')
    (161, '            if df.shape[0] > 0:')
    (162, '                df["File"] = df["File"].str.replace(".fastq.gz", "", regex=True)')
    (163, '                df["Sample"] = sample')
    (164, '                df["Pair"] = df["File"].apply(lambda r: r.split(".")[1])')
    (165, '                df["Cell"] = df["File"].apply(lambda r: r.split(".")[0])')
    (166, '                df["Full_path"] = df[["Folder", "File"]].apply(')
    (167, '                    lambda r: f"{r[\\\'Folder\\\']}/{r[\\\'File\\\']}.fastq.gz", axis=1')
    (168, '                )')
    (169, '                df["Genecore_path"] = df["File"].apply(')
    (170, '                    lambda r: f"{config[\\\'genecore_prefix\\\']}/{config[\\\'genecore_date_folder\\\']}/{d_master[sample][\\\'prefix\\\']}lane1/{r.replace(\\\'.\\\', \\\'_\\\')}_sequence.txt.gz"')
    (171, '                )')
    (172, '                df["Genecore_file"] = df["File"].apply(')
    (173, '                    lambda r: f"{d_master[sample][\\\'prefix\\\']}lane1{r.replace(\\\'.\\\', \\\'_\\\')}"')
    (174, '                )')
    (175, '                df["Genecore_file"] = df["Genecore_file"].apply(')
    (176, '                    lambda r: "_".join(r.split("_")[:-1])')
    (177, '                )')
    (178, '')
    (179, '                # Concat dataframes for each sample & output')
    (180, '                complete_df_list.append(df)')
    (181, '')
    (182, '        complete_df = pd.concat(complete_df_list)')
    (183, '')
    (184, '        complete_df = complete_df.sort_values(by=["Cell", "File"]).reset_index(')
    (185, '            drop=True')
    (186, '        )')
    (187, '        pd.options.display.max_colwidth = 200')
    (188, '        print(complete_df)')
    (189, '        # exit()')
    (190, '        return complete_df, d_master')
    (191, '')
    (192, '    @staticmethod')
    (193, '    def handle_input_data(thisdir, exclude_list=list, bam=bool):')
    (194, '        """_summary_')
    (195, '        Args:')
    (196, '            thisdir (_type_): _description_')
    (197, '            exclude_list (_type_, optional): _description_. Defaults to list.')
    (198, '        Returns:')
    (199, '            _type_: _description_')
    (200, '        """')
    (201, '        # Extension & folder based on bam boolean input')
    (202, '        ext = ".bam" if bam is True else ".fastq.gz"')
    (203, '        folder = "bam" if bam is True else "fastq"')
    (204, '        complete_df_list = list()')
    (205, '        # List of folders/files to not consider (restrict to samples only)')
    (206, '        exclude = [')
    (207, '            "._.DS_Store",')
    (208, '            ".DS_Store",')
    (209, '            "all",')
    (210, '            "ashleys_counts",')
    (211, '            "bam",')
    (212, '            "cell_selection",')
    (213, '            "config",')
    (214, '            "counts",')
    (215, '            "fastq",')
    (216, '            "fastqc",')
    (217, '            "haplotag",')
    (218, '            "log",')
    (219, '            "merged_bam",')
    (220, '            "mosaiclassifier",')
    (221, '            "normalizations",')
    (222, '            "ploidy",')
    (223, '            "plots",')
    (224, '            "predictions",')
    (225, '            "segmentation",')
    (226, '            "snv_calls",')
    (227, '            "stats",')
    (228, '            "strandphaser",')
    (229, '        ]')
    (230, '')
    (231, '        for sample in [e for e in os.listdir(thisdir) if e not in exclude]:')
    (232, '            # Create a list of  files to process for each sample')
    (233, '            l_files_all = [')
    (234, '                f')
    (235, '                for f in os.listdir(')
    (236, '                    "{thisdir}/{sample}/{folder}/".format(')
    (237, '                        thisdir=thisdir, sample=sample, folder=folder')
    (238, '                    )')
    (239, '                )')
    (240, '                if f.endswith(ext)')
    (241, '            ]')
    (242, '')
    (243, '            # Dataframe creation')
    (244, '            df = pd.DataFrame([{"File": f} for f in l_files_all])')
    (245, '            df["File"] = df["File"].str.replace(ext, "", regex=True)')
    (246, '            df["Folder"] = thisdir')
    (247, '            df["Sample"] = sample')
    (248, '            df["Cell"] = df["File"].apply(lambda r: r.split(".")[0])')
    (249, '            df["Full_path"] = "{thisdir}/{sample}/{folder}/".format(')
    (250, '                thisdir=thisdir, sample=sample, folder=folder')
    (251, '            )')
    (252, '            df["Full_path"] = df["Full_path"] + df["File"] + ext')
    (253, '')
    (254, '            complete_df_list.append(df)')
    (255, '')
    (256, '        # Concat dataframes for each sample & output')
    (257, '        complete_df = pd.concat(complete_df_list)')
    (258, '        complete_df = complete_df.sort_values(by=["Cell", "File"]).reset_index(')
    (259, '            drop=True')
    (260, '        )')
    (261, '        return complete_df')
    (262, '')
    (263, '')
    (264, '# GENECORE')
    (265, '')
    (266, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=friendsofstrandseq/mosaicatcher-pipeline, file=workflow/rules/common.smk
context_key: ['if stem not in arr[k]']
    (267, 'def findstem(arr):')
    (268, '')
    (269, '    # Determine size of the array')
    (270, '    n = len(arr)')
    (271, '')
    (272, '    # Take first word from array')
    (273, '    # as reference')
    (274, '    s = arr[0]')
    (275, '    l = len(s)')
    (276, '')
    (277, '    res = ""')
    (278, '')
    (279, '    for i in range(l):')
    (280, '        for j in range(i + 1, l + 1):')
    (281, '')
    (282, '            # generating all possible substrings')
    (283, '            # of our reference string arr[0] i.e s')
    (284, '            stem = s[i:j]')
    (285, '            k = 1')
    (286, '            for k in range(1, n):')
    (287, '')
    (288, '                # Check if the generated stem is')
    (289, '                # common to all words')
    (290, '                if stem not in arr[k]:')
    (291, '                    break')
    (292, '')
    (293, '            # If current substring is present in')
    (294, '            # all strings and its length is greater')
    (295, '            # than current result')
    (296, '            if k + 1 == n and len(res) < len(stem):')
    (297, '                res = stem')
    (298, '')
    (299, '    return res')
    (300, '')
    (301, '')
    (302, '# Create configuration file with samples')
    (303, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=friendsofstrandseq/mosaicatcher-pipeline, file=workflow/rules/common.smk
context_key: ['else config["plottype_counts"][0']
    (267, 'def findstem(arr):')
    (268, '')
    (269, '    # Determine size of the array')
    (270, '    n = len(arr)')
    (271, '')
    (272, '    # Take first word from array')
    (273, '    # as reference')
    (274, '    s = arr[0]')
    (275, '    l = len(s)')
    (276, '')
    (277, '    res = ""')
    (278, '')
    (279, '    for i in range(l):')
    (280, '        for j in range(i + 1, l + 1):')
    (281, '')
    (282, '            # generating all possible substrings')
    (283, '            # of our reference string arr[0] i.e s')
    (284, '            stem = s[i:j]')
    (285, '            k = 1')
    (286, '            for k in range(1, n):')
    (287, '')
    (288, '                # Check if the generated stem is')
    (289, '                # common to all words')
    (290, '                if stem not in arr[k]:')
    (291, '                    break')
    (292, '')
    (293, '            # If current substring is present in')
    (294, '            # all strings and its length is greater')
    (295, '            # than current result')
    (296, '            if k + 1 == n and len(res) < len(stem):')
    (297, '                res = stem')
    (298, '')
    (299, '    return res')
    (300, '')
    (301, '')
    (302, '# Create configuration file with samples')
    (303, '')
    (304, 'c = HandleInput(')
    (305, '    input_path=config["data_location"],')
    (306, '    genecore_path="{genecore_prefix}/{genecore_date_folder}".format(')
    (307, '        genecore_prefix=config["genecore_prefix"],')
    (308, '        genecore_date_folder=config["genecore_date_folder"],')
    (309, '    ),')
    (310, '    output_path="{data_location}/config/config_df_ashleys.tsv".format(')
    (311, '        data_location=config["data_location"]')
    (312, '    ),')
    (313, '    check_sm_tag=False,')
    (314, '    bam=False,')
    (315, '    genecore=config["genecore"],')
    (316, ')')
    (317, '# df_config_files = c.df_config_files')
    (318, 'if config["genecore"] is True:')
    (319, '    d_master = c.d_master')
    (320, '')
    (321, '# Read config file previously produced')
    (322, 'df_config_files = c.df_config_files')
    (323, 'df_config_files["Selected"] = True')
    (324, '')
    (325, '# List of available samples')
    (326, 'samples = list(sorted(list(df_config_files.Sample.unique().tolist())))')
    (327, '')
    (328, '')
    (329, '')
    (330, '')
    (331, '# Creation of dicts to be used in the rules')
    (332, 'dict_cells_nb_per_sample = (')
    (333, '    df_config_files.loc[df_config_files["Selected"] == True]')
    (334, '    .groupby("Sample")["Cell"]')
    (335, '    .nunique()')
    (336, '    .to_dict()')
    (337, ')')
    (338, '')
    (339, 'allbams_per_sample = df_config_files.groupby("Sample")["Cell"].apply(list).to_dict()')
    (340, 'cell_per_sample = (')
    (341, '    df_config_files.loc[df_config_files["Selected"] == True]')
    (342, '    .groupby("Sample")["Cell"]')
    (343, '    .unique()')
    (344, '    .apply(list)')
    (345, '    .to_dict()')
    (346, ')')
    (347, 'bam_per_sample_local = (')
    (348, '    df_config_files.loc[df_config_files["Selected"] == True]')
    (349, '    .groupby("Sample")["Cell"]')
    (350, '    .unique()')
    (351, '    .apply(list)')
    (352, '    .to_dict()')
    (353, ')')
    (354, 'bam_per_sample = (')
    (355, '    df_config_files.loc[df_config_files["Selected"] == True]')
    (356, '    .groupby("Sample")["Cell"]')
    (357, '    .unique()')
    (358, '    .apply(list)')
    (359, '    .to_dict()')
    (360, ')')
    (361, '')
    (362, 'plottype_counts = (')
    (363, '    config["plottype_counts"]')
    (364, '    if config["GC_analysis"] is True')
    (365, '    else config["plottype_counts"][0]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/musta, file=workflow/rules/common.smk
context_key: ['if not os.path.isabs(filepath)']
    (52, 'def expand_filepath(filepath):')
    (53, '    filepath = os.path.expandvars(os.path.expanduser(filepath))')
    (54, '    if not os.path.isabs(filepath):')
    (55, '        raise FileNotFoundError(')
    (56, '            errno.ENOENT,')
    (57, '            os.strerror(errno.ENOENT) + " (path must be absolute)",')
    (58, '            filepath,')
    (59, '        )')
    (60, '    return filepath')
    (61, '')
    (62, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/musta, file=workflow/rules/common.smk
context_key: ['if does not exists, create path and return it. If any errors, retur']
    (68, 'def tmp_path(path=""):')
    (69, '    """')
    (70, '    if does not exists, create path and return it. If any errors, return')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/musta, file=workflow/rules/common.smk
context_key: ['if path', 'try']
    (68, 'def tmp_path(path=""):')
    (69, '    """')
    (70, '    if does not exists, create path and return it. If any errors, return')
    (71, '    default path')
    (72, '    :param path: path')
    (73, '    :return: path')
    (74, '    """')
    (75, '    default_path = os.getenv("TMPDIR", config.get("paths").get("tmp_dir"))')
    (76, '    if path:')
    (77, '        try:')
    (78, '            os.makedirs(path)')
    (79, '        except OSError as e:')
    (80, '            if e.errno != errno.EEXIST:')
    (81, '                return default_path')
    (82, '        return path')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/musta, file=workflow/rules/common.smk
context_key: ['if path']
    (68, 'def tmp_path(path=""):')
    (69, '    """')
    (70, '    if does not exists, create path and return it. If any errors, return')
    (71, '    default path')
    (72, '    :param path: path')
    (73, '    :return: path')
    (74, '    """')
    (75, '    default_path = os.getenv("TMPDIR", config.get("paths").get("tmp_dir"))')
    (76, '    if path:')
    (77, '        try:')
    (78, '            os.makedirs(path)')
    (79, '        except OSError as e:')
    (80, '            if e.errno != errno.EEXIST:')
    (81, '                return default_path')
    (82, '        return path')
    (83, '    return default_path')
    (84, '')
    (85, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/musta, file=workflow/rules/common.smk
context_key: ['if n >= prefix[s]']
    (103, '    def bytes2human(n):')
    (104, '        # http://code.activestate.com/recipes/578019')
    (105, '        # >>> bytes2human(10000)')
    (106, "        # \\'9.8K\\'")
    (107, '        # >>> bytes2human(100001221)')
    (108, "        # \\'95.4M\\'")
    (109, '        symbols = ("K", "M", "G", "T", "P", "E", "Z", "Y")')
    (110, '        prefix = {}')
    (111, '        for i, s in enumerate(symbols):')
    (112, '            prefix[s] = 1 << (i + 1) * 10')
    (113, '        for s in reversed(symbols):')
    (114, '            if n >= prefix[s]:')
    (115, '                value = float(n) / prefix[s]')
    (116, '                return "%.0f%s" % (value, s)')
    (117, '        return "%sB" % n')
    (118, '')
    (119, '    def preserve(resource, percentage, stock):')
    (120, '        preserved = resource - max(resource * percentage // 100, stock)')
    (121, '        return preserved if preserved != 0 else stock')
    (122, '')
    (123, '    # def preserve(resource, percentage, stock):')
    (124, '    #     return resource - max(resource * percentage // 100, stock)')
    (125, '')
    (126, '    params_template = "\\\'-Xms{} -Xmx{} -XX:ParallelGCThreads={} " "-Djava.io.tmpdir={}\\\'"')
    (127, '')
    (128, '    mem_min = 1024**3 * 2  # 2GB')
    (129, '')
    (130, '    mem_size = preserve(total_physical_mem_size(), percentage_to_preserve, stock_mem)')
    (131, '    cpu_nums = preserve(cpu_count(), percentage_to_preserve, stock_cpu)')
    (132, '    tmpdir = tmp_path(tmp_dir)')
    (133, '')
    (134, '    return params_template.format(')
    (135, '        bytes2human(mem_min).lower(),')
    (136, '        bytes2human(max(mem_size, mem_min)).lower(),')
    (137, '        min(cpu_nums, multiply_by),')
    (138, '        tmpdir,')
    (139, '    )')
    (140, '')
    (141, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/musta, file=workflow/rules/common.smk
context_key: ['if not samples_master[wildcards.sample]["normal_bam"]']
    (176, 'def select_filtered(wildcards):')
    (177, '    with open(config["samples"], "r") as file:')
    (178, '        samples_master = yaml.load(file, Loader=yaml.FullLoader)')
    (179, '    if not samples_master[wildcards.sample]["normal_bam"]:')
    (180, '        return rules.filter_mutect_tumoronly.output.vcf')
    (181, '    else:')
    (182, '        return rules.filter_mutect.output.vcf')
    (183, '')
    (184, '')
    (185, '## functions for pipeline starting from vcf')
    (186, '')
    (187, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/musta, file=workflow/rules/common.smk
context_key: ['try', 'if force and os.path.exists(path)']
    (238, 'def ensure_dir(path, force=False):')
    (239, '    try:')
    (240, '        if force and os.path.exists(path):')
    (241, '            shutil.rmtree(path)')
    (242, '        os.makedirs(path)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/musta, file=workflow/rules/common.smk
context_key: ['try', 'if not os.path.isdir(path)']
    (238, 'def ensure_dir(path, force=False):')
    (239, '    try:')
    (240, '        if force and os.path.exists(path):')
    (241, '            shutil.rmtree(path)')
    (242, '        os.makedirs(path)')
    (243, '    except OSError:')
    (244, '        if not os.path.isdir(path):')
    (245, '            raise')
    (246, '')
    (247, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/musta, file=workflow/rules/common.smk
context_key: ['try', 'if delete and os.path.exists(path)']
    (248, 'def exist_dir(path, delete=False):')
    (249, '    try:')
    (250, '        if delete and os.path.exists(path):')
    (251, '            shutil.rmtree(path)')
    (252, '    # os.makedirs(path)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/musta, file=workflow/rules/common.smk
context_key: ['try', 'if not os.path.isdir(path)']
    (248, 'def exist_dir(path, delete=False):')
    (249, '    try:')
    (250, '        if delete and os.path.exists(path):')
    (251, '            shutil.rmtree(path)')
    (252, '    # os.makedirs(path)')
    (253, '    except OSError:')
    (254, '        if not os.path.isdir(path):')
    (255, '            raise')
    (256, '')
    (257, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=wharvey31/denovo_sv_validation, file=workflow/rules/common.smk
context_key: ['if wildcards.val_type not in alnsource_pattern_dict']
    (48, 'def get_aln_source(wildcards, alnsource_pattern_dict):')
    (49, '    """')
    (50, '    Get an alignment source (BAM, CRAM) for a sample.')
    (51, '    """')
    (52, '')
    (53, '    if wildcards.val_type not in alnsource_pattern_dict:')
    (54, '        raise RuntimeError(')
    (55, '            \\\'Cannot find alignment source "{}" in alignment source pattern dictionary\\\'.format(')
    (56, '                wildcards.val_type')
    (57, '            )')
    (58, '        )')
    (59, '')
    (60, '    alnsource_pattern = alnsource_pattern_dict[wildcards.val_type]')
    (61, '')
    (62, '    if "{sample}" not in alnsource_pattern:')
    (63, '        raise RuntimeError(')
    (64, '            "{{sample}} not in alignment source pattern: {}".format(wildcards.val_type)')
    (65, '        )')
    (66, '')
    (67, '    return alnsource_pattern.format(sample=wildcards.parent)')
    (68, '')
    (69, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=wharvey31/denovo_sv_validation, file=workflow/rules/common.smk
context_key: ['if "{source}" not in bed_pattern']
    (70, 'def get_variant_input(wildcards, bed_pattern, allow_missing=False):')
    (71, '    """')
    (72, '    Get an input file name if it exists.')
    (73, '    """')
    (74, '')
    (75, '    if "{source}" not in bed_pattern:')
    (76, '        raise RuntimeError("{source} not in BED pattern")')
    (77, '')
    (78, '    if "{caller}" not in bed_pattern:')
    (79, '        raise RuntimeError("{caller} not in BED pattern")')
    (80, '')
    (81, '    if "{sample}" not in bed_pattern:')
    (82, '        raise RuntimeError("{sample} not in BED pattern")')
    (83, '')
    (84, '    if "{svtype}" not in bed_pattern:')
    (85, '        raise RuntimeError("{svtype} not in BED pattern")')
    (86, '')
    (87, '    bed_file_name = bed_pattern.format(**wildcards)')
    (88, '')
    (89, '    if not os.path.isfile(bed_file_name):')
    (90, '        if allow_missing:')
    (91, '            return []')
    (92, '')
    (93, '        raise RuntimeError("Missing BED file: {}".format(bed_file_name))')
    (94, '')
    (95, '    return bed_file_name')
    (96, '')
    (97, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=wharvey31/denovo_sv_validation, file=workflow/rules/common.smk
context_key: ['if wildcards.vartype == "indel"']
    (98, 'def gather_setdef(wildcards):')
    (99, '    if wildcards.vartype == "indel":')
    (100, '        return expand(')
    (101, '            "temp/tables/sample/{{sample}}/{{parent}}_{{val_type}}/{set_def}/{{vartype}}_{{svtype}}/{{parent}}_{{val_type}}.tsv.gz",')
    (102, '            set_def=SET_DEF_INDEL.keys(),')
    (103, '        )')
    (104, '    else:')
    (105, '        return expand(')
    (106, '            "temp/tables/sample/{{sample}}/{{parent}}_{{val_type}}/{set_def}/{{vartype}}_{{svtype}}/{{parent}}_{{val_type}}.tsv.gz",')
    (107, '            set_def=SET_DEF_SV.keys(),')
    (108, '        )')
    (109, '')
    (110, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=wharvey31/denovo_sv_validation, file=workflow/rules/common.smk
context_key: ['if len(len_list) == 0']
    (136, 'def step_miner(len_list):')
    (137, '    """')
    (138, '    Take a sorted list and split it into two sets (low and high) for each possible split (index = 1, 2, 3, ..., len(len_list)).')
    (139, '    For each set (low and high), compute the root-mean-squared error between the mean of the set and each element. For the split with')
    (140, '    the lowest error, the low and high lists are returned (tuple of two lists, low list is the first element, high list is the second).')
    (141, '')
    (142, "    This StepMiner algorithm is borrowed from Debashis Sahoo\\'s thesis:")
    (143, '    http://genedesk.ucsd.edu/home/dsahoo-thesis.pdf')
    (144, '    """')
    (145, '')
    (146, '    min_index = None')
    (147, '    min_error = None')
    (148, '')
    (149, '    n_list = len(len_list)')
    (150, '')
    (151, '    if len(len_list) == 0:')
    (152, '        return [], [], 0, 0')
    (153, '')
    (154, '    if len(len_list) == 1:')
    (155, '        return len_list, [], 0, 0')
    (156, '')
    (157, '    # Get split with the least error')
    (158, '    for index in range(1, len(len_list)):')
    (159, '')
    (160, '        len_low = len_list[:index]')
    (161, '        len_high = len_list[index:]')
    (162, '')
    (163, '        mean_low = np.mean(len_low)')
    (164, '        mean_high = np.mean(len_high)')
    (165, '')
    (166, '        error = np.sum(np.abs(len_low - mean_low) ** 2) + np.sum(')
    (167, '            np.abs(len_high - mean_high) ** 2')
    (168, '        )')
    (169, '')
    (170, '        if error > 0:')
    (171, '            error = np.log2(error / n_list)')
    (172, '')
    (173, '        if index == 1 or error < min_error:')
    (174, '            min_error = error')
    (175, '            min_index = index')
    (176, '')
    (177, '    # Return splits')
    (178, '    return len_list[:min_index], len_list[min_index:], min_index, min_error')
    (179, '')
    (180, '')
    (181, '#')
    (182, '# Alignment summary record')
    (183, '#')
    (184, '')
    (185, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=wharvey31/denovo_sv_validation, file=workflow/rules/common.smk
context_key: ['if n > 0']
    (186, 'def align_summary_haploid(len_list):')
    (187, '')
    (188, '    # Get stats')
    (189, '    n = len(len_list)')
    (190, '')
    (191, '    if n > 0:')
    (192, '        mean = np.mean(len_list)')
    (193, '        min = np.min(len_list)')
    (194, '        max = np.max(len_list)')
    (195, '    else:')
    (196, '        mean = 0')
    (197, '        min = 0')
    (198, '        max = 0')
    (199, '')
    (200, '    # Return series')
    (201, '    return pd.Series(')
    (202, '        [n, mean, min, max, ",".join(["{:d}".format(val) for val in len_list])],')
    (203, '        index=["N", "MEAN", "MIN", "MAX", "LENS"],')
    (204, '    )')
    (205, '')
    (206, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=wharvey31/denovo_sv_validation, file=workflow/rules/common.smk
context_key: ['if n_low > 0']
    (207, 'def align_summary_diploid(len_list):')
    (208, '')
    (209, '    len_low, len_high, min_index, min_error = step_miner(sorted(len_list))')
    (210, '')
    (211, '    # Get stats')
    (212, '    n_low = len(len_low)')
    (213, '    n_high = len(len_high)')
    (214, '')
    (215, '    if n_low > 0:')
    (216, '        mean_low = np.mean(len_low)')
    (217, '        min_low = np.min(len_low)')
    (218, '        max_low = np.max(len_low)')
    (219, '    else:')
    (220, '        mean_low = 0')
    (221, '        min_low = 0')
    (222, '        max_low = 0')
    (223, '')
    (224, '    if n_high > 0:')
    (225, '        mean_high = np.mean(len_high)')
    (226, '        min_high = np.min(len_high)')
    (227, '        max_high = np.max(len_high)')
    (228, '    else:')
    (229, '        mean_high = 0')
    (230, '        min_high = 0')
    (231, '        max_high = 0')
    (232, '')
    (233, '    if n_low > 0 and n_high > 0:')
    (234, '        separation = min_high - max_low')
    (235, '    else:')
    (236, '        separation = 0')
    (237, '')
    (238, '    # Return series')
    (239, '    return pd.Series(')
    (240, '        [')
    (241, '            n_low,')
    (242, '            mean_low,')
    (243, '            min_low,')
    (244, '            max_low,')
    (245, '            n_high,')
    (246, '            mean_high,')
    (247, '            min_high,')
    (248, '            max_high,')
    (249, '            separation,')
    (250, '            ",".join(["{:d}".format(val) for val in len_low]),')
    (251, '            ",".join(["{:d}".format(val) for val in len_high]),')
    (252, '        ],')
    (253, '        index=[')
    (254, '            "N_LO",')
    (255, '            "MEAN_LO",')
    (256, '            "MIN_LO",')
    (257, '            "MAX_LO",')
    (258, '            "N_HI",')
    (259, '            "MEAN_HI",')
    (260, '            "MIN_HI",')
    (261, '            "MAX_HI",')
    (262, '            "DIST_LH",')
    (263, '            "LENS_LO",')
    (264, '            "LENS_HI",')
    (265, '        ],')
    (266, '    )')
    (267, '')
    (268, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=wharvey31/denovo_sv_validation, file=workflow/rules/common.smk
context_key: ['if match_obj is None']
    (274, 'def validate_summary(df, strategy="size50_2_4"):')
    (275, '    """')
    (276, '    Run validation on. Generates a "VAL" colmun with:')
    (277, '    * VALID:')
    (278, '    * NOTVALID:')
    (279, '    * NOCALL:')
    (280, '    * NODATA:')
    (281, '    """')
    (282, '')
    (283, '    # Get parameters')
    (284, '    match_obj = re.match(r"^size(\\\\d+)_(\\\\d+)_(\\\\d+)$", strategy)')
    (285, '')
    (286, '    if match_obj is None:')
    (287, '        raise RuntimeError("No implementation for strategy: " + strategy)')
    (288, '')
    (289, '    val_threshold = np.float32(match_obj[1]) / 100')
    (290, '')
    (291, '    min_support = np.int32(match_obj[2])')
    (292, '    min_call_depth = np.int32(match_obj[3])')
    (293, '')
    (294, '    # Subset df to needed columns')
    (295, '    df = df[')
    (296, '        [')
    (297, '            "ID",')
    (298, '            "SAMPLE",')
    (299, '            "CALLER",')
    (300, '            "ALNSAMPLE",')
    (301, '            "ALNSOURCE",')
    (302, '            "SVTYPE",')
    (303, '            "SVLEN",')
    (304, '            "LENS_HI",')
    (305, '            "LENS_LO",')
    (306, '            "HAS_ALN",')
    (307, '            "WINDOW_SIZE",')
    (308, '        ]')
    (309, '    ].copy()')
    (310, '')
    (311, '    # Get lengths and SV length differences')
    (312, '    df["LEN"] = df.apply(')
    (313, '        lambda row: (row["LENS_LO"].split(",") if not pd.isnull(row["LENS_LO"]) else [])')
    (314, '        + (row["LENS_HI"].split(",") if not pd.isnull(row["LENS_HI"]) else []),')
    (315, '        axis=1,')
    (316, '    )')
    (317, '')
    (318, '    df["LEN_DIFF"] = df.apply(')
    (319, '        lambda row: [')
    (320, '            (')
    (321, '                int(val)')
    (322, '                - row["WINDOW_SIZE"]')
    (323, '                - (row["SVLEN"] if row["SVTYPE"] == "INS" else 0)')
    (324, '            )')
    (325, '            for val in row["LEN"]')
    (326, '        ],')
    (327, '        axis=1,')
    (328, '    )')
    (329, '')
    (330, '    # Count support')
    (331, '    df["SUPPORT_COUNT"] = df.apply(')
    (332, '        lambda row: np.sum(')
    (333, '            np.abs([np.int32(element) / row["SVLEN"] for element in row["LEN_DIFF"]])')
    (334, '            < val_threshold')
    (335, '        ),')
    (336, '        axis=1,')
    (337, '    )')
    (338, '')
    (339, '    # Call validation status')
    (340, '    df["VAL"] = df.apply(')
    (341, '        lambda row: ("VALID" if (row["SUPPORT_COUNT"] > min_support) else "NOTVALID")')
    (342, '        if (len(row["LEN"]) >= min_call_depth)')
    (343, '        else "NOCALL",')
    (344, '        axis=1,')
    (345, '    )')
    (346, '')
    (347, '    df["VAL"] = df.apply(lambda row: row["VAL"] if row["HAS_ALN"] else "NODATA", axis=1)')
    (348, '')
    (349, '    # Clean up')
    (350, '    del df["LENS_HI"]')
    (351, '    del df["LENS_LO"]')
    (352, '    del df["LEN"]')
    (353, '')
    (354, '    df["LEN_DIFF"] = df["LEN_DIFF"].apply(')
    (355, '        lambda vals: ",".join([str(val) for val in vals])')
    (356, '    )')
    (357, '')
    (358, '    return df')
    (359, '')
    (360, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=wharvey31/denovo_sv_validation, file=workflow/rules/common.smk
context_key: ['if x != Non']
    (405, 'def determine_combined_set(wildcards):')
    (406, '    val_types = [')
    (407, '        x')
    (408, '        for x in [')
    (409, '            config.get("ASM"),')
    (410, '            config.get("READS"),')
    (411, '            config.get("SVPOP"),')
    (412, '            config.get("CALLABLE"),')
    (413, '        ]')
    (414, '        if x != None')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=wharvey31/denovo_sv_validation, file=workflow/rules/common.smk
context_key: ['if "INS" in wildcards.ids']
    (445, 'def find_region(wildcards):')
    (446, '    split_id = wildcards.ids.split("-")')
    (447, '    start = max(0, int(split_id[1]) - 1000)')
    (448, '    if "INS" in wildcards.ids:')
    (449, '        end = int(split_id[1]) + 1001')
    (450, '    else:')
    (451, '        end = int(split_id[1]) + int(split_id[3]) + 1000')
    (452, '    return f"{split_id[0]}:{start}-{end}"')
    (453, '')
    (454, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nismod/open-gira, file=workflow/rules/storm_workflow_global_variables.smk
context_key: ['Returns', 'if len(config["specific_boxes"]) != 0']
    (29, 'def all_boxes() -> List[str]:')
    (30, '    """')
    (31, '    Generate a list of box IDs for the cyclones workflow')
    (32, '')
    (33, '    Returns:')
    (34, '        list[str]')
    (35, '    """')
    (36, '')
    (37, '    if len(config["specific_boxes"]) != 0:')
    (38, '        return [f"box_{num}" for num in config["specific_boxes"]]')
    (39, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nismod/open-gira, file=workflow/rules/storm_workflow_global_variables.smk
context_key: ['if len(STORM_BASINS) == 0']
    (29, 'def all_boxes() -> List[str]:')
    (30, '    """')
    (31, '    Generate a list of box IDs for the cyclones workflow')
    (32, '')
    (33, '    Returns:')
    (34, '        list[str]')
    (35, '    """')
    (36, '')
    (37, '    if len(config["specific_boxes"]) != 0:')
    (38, '        return [f"box_{num}" for num in config["specific_boxes"]]')
    (39, '')
    (40, '    else:')
    (41, '        return [')
    (42, '            f"box_{int(idx)}"')
    (43, '            for idx in range(')
    (44, '                0, int((180 - -180) * (90 - -90) / float(config["box_width_height"]) ** 2)')
    (45, '            )')
    (46, '        ]')
    (47, '')
    (48, '')
    (49, '#### POWER/STORMS WORKFLOW ####')
    (50, '')
    (51, '# list of ISO A3 country codes')
    (52, 'COUNTRY_CODES = country_codes()')
    (53, '')
    (54, '# list of IDs of form "box_<int>"')
    (55, 'ALL_BOXES = all_boxes()')
    (56, '')
    (57, 'CONNECTOR_OUT = (')
    (58, '    expand(')
    (59, '        os.path.join(')
    (60, '            config["output_dir"],')
    (61, '            "power_processed",')
    (62, '            "all_boxes",')
    (63, '            "{box_id}",')
    (64, '            "connector_{box_id}.json",')
    (65, '        ),')
    (66, '        box_id=ALL_BOXES,')
    (67, '    )')
    (68, ')')
    (69, '')
    (70, 'STORM_BASINS = config["storm_basins"]')
    (71, 'if len(STORM_BASINS) == 0:')
    (72, '    print("Inputting all storm basins")')
    (73, '    # east pacific, north atlantic, north indian, south india, south pacific, west pacific')
    (74, '    STORM_BASINS =  ("EP", "NA", "NI", "SI", "SP", "WP")')
    (75, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nismod/open-gira, file=workflow/rules/storm_workflow_global_variables.smk
context_key: ['if not SAMPLES']
    (29, 'def all_boxes() -> List[str]:')
    (30, '    """')
    (31, '    Generate a list of box IDs for the cyclones workflow')
    (32, '')
    (33, '    Returns:')
    (34, '        list[str]')
    (35, '    """')
    (36, '')
    (37, '    if len(config["specific_boxes"]) != 0:')
    (38, '        return [f"box_{num}" for num in config["specific_boxes"]]')
    (39, '')
    (40, '    else:')
    (41, '        return [')
    (42, '            f"box_{int(idx)}"')
    (43, '            for idx in range(')
    (44, '                0, int((180 - -180) * (90 - -90) / float(config["box_width_height"]) ** 2)')
    (45, '            )')
    (46, '        ]')
    (47, '')
    (48, '')
    (49, '#### POWER/STORMS WORKFLOW ####')
    (50, '')
    (51, '# list of ISO A3 country codes')
    (52, 'COUNTRY_CODES = country_codes()')
    (53, '')
    (54, '# list of IDs of form "box_<int>"')
    (55, 'ALL_BOXES = all_boxes()')
    (56, '')
    (57, 'CONNECTOR_OUT = (')
    (58, '    expand(')
    (59, '        os.path.join(')
    (60, '            config["output_dir"],')
    (61, '            "power_processed",')
    (62, '            "all_boxes",')
    (63, '            "{box_id}",')
    (64, '            "connector_{box_id}.json",')
    (65, '        ),')
    (66, '        box_id=ALL_BOXES,')
    (67, '    )')
    (68, ')')
    (69, '')
    (70, 'STORM_BASINS = config["storm_basins"]')
    (71, 'if len(STORM_BASINS) == 0:')
    (72, '    print("Inputting all storm basins")')
    (73, '    # east pacific, north atlantic, north indian, south india, south pacific, west pacific')
    (74, '    STORM_BASINS =  ("EP", "NA", "NI", "SI", "SP", "WP")')
    (75, '')
    (76, 'SAMPLES = config["storm_files_sample_set"]')
    (77, 'if not SAMPLES:')
    (78, "    # empty list interpreted as \\'run with all available samples\\'")
    (79, '    SAMPLES = list(range(0, 10))')
    (80, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nismod/open-gira, file=workflow/rules/storm_workflow_global_variables.smk
context_key: ['if STORMS == "None"']
    (29, 'def all_boxes() -> List[str]:')
    (30, '    """')
    (31, '    Generate a list of box IDs for the cyclones workflow')
    (32, '')
    (33, '    Returns:')
    (34, '        list[str]')
    (35, '    """')
    (36, '')
    (37, '    if len(config["specific_boxes"]) != 0:')
    (38, '        return [f"box_{num}" for num in config["specific_boxes"]]')
    (39, '')
    (40, '    else:')
    (41, '        return [')
    (42, '            f"box_{int(idx)}"')
    (43, '            for idx in range(')
    (44, '                0, int((180 - -180) * (90 - -90) / float(config["box_width_height"]) ** 2)')
    (45, '            )')
    (46, '        ]')
    (47, '')
    (48, '')
    (49, '#### POWER/STORMS WORKFLOW ####')
    (50, '')
    (51, '# list of ISO A3 country codes')
    (52, 'COUNTRY_CODES = country_codes()')
    (53, '')
    (54, '# list of IDs of form "box_<int>"')
    (55, 'ALL_BOXES = all_boxes()')
    (56, '')
    (57, 'CONNECTOR_OUT = (')
    (58, '    expand(')
    (59, '        os.path.join(')
    (60, '            config["output_dir"],')
    (61, '            "power_processed",')
    (62, '            "all_boxes",')
    (63, '            "{box_id}",')
    (64, '            "connector_{box_id}.json",')
    (65, '        ),')
    (66, '        box_id=ALL_BOXES,')
    (67, '    )')
    (68, ')')
    (69, '')
    (70, 'STORM_BASINS = config["storm_basins"]')
    (71, 'if len(STORM_BASINS) == 0:')
    (72, '    print("Inputting all storm basins")')
    (73, '    # east pacific, north atlantic, north indian, south india, south pacific, west pacific')
    (74, '    STORM_BASINS =  ("EP", "NA", "NI", "SI", "SP", "WP")')
    (75, '')
    (76, 'SAMPLES = config["storm_files_sample_set"]')
    (77, 'if not SAMPLES:')
    (78, "    # empty list interpreted as \\'run with all available samples\\'")
    (79, '    SAMPLES = list(range(0, 10))')
    (80, '')
    (81, 'STORMS = config["specific_storm_analysis"]')
    (82, 'if STORMS == "None":')
    (83, '    STORMS = None')
    (84, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nismod/open-gira, file=workflow/rules/storm_workflow_global_variables.smk
context_key: ['if wildcards.STORM_MODEL == "constant"']
    (160, 'def get_storm_file(wildcards):')
    (161, '    """Helper to get storm events file, given:')
    (162, '    - OUTPUT_DIR')
    (163, '    - STORM_BASIN')
    (164, '    - STORM_MODEL (global climate model)')
    (165, '    - SAMPLE (0-9)')
    (166, '    """')
    (167, '    if wildcards.STORM_MODEL == "constant":')
    (168, '        fname = f"{wildcards.OUTPUT_DIR}/input/storm-ibtracs/events/constant/{wildcards.STORM_BASIN}/STORM_DATA_IBTRACS_{wildcards.STORM_BASIN}_1000_YEARS_{wildcards.SAMPLE}.txt"')
    (169, '    else:')
    (170, '        fname = f"{wildcards.OUTPUT_DIR}/input/storm-ibtracs/events/{wildcards.STORM_MODEL}/{wildcards.STORM_BASIN}/STORM_DATA_{wildcards.STORM_MODEL}_{wildcards.STORM_BASIN}_1000_YEARS_{wildcards.SAMPLE}_IBTRACSDELTA.txt"')
    (171, '    return fname')
    (172, '')
    (173, '# check wind speed thresholds for damage are correctly ordered')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if pd.isnull(group)']
    (43, 'def _group_or_sample(row):')
    (44, '    group = row.get("group", None)')
    (45, '    if pd.isnull(group):')
    (46, '        return row["sample_name"]')
    (47, '    return group')
    (48, '')
    (49, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if "groups" in config']
    (43, 'def _group_or_sample(row):')
    (44, '    group = row.get("group", None)')
    (45, '    if pd.isnull(group):')
    (46, '        return row["sample_name"]')
    (47, '    return group')
    (48, '')
    (49, '')
    (50, 'samples["group"] = [_group_or_sample(row) for _, row in samples.iterrows()]')
    (51, 'validate(samples, schema="../schemas/samples.schema.yaml")')
    (52, '')
    (53, '')
    (54, 'groups = samples["group"].unique()')
    (55, '')
    (56, 'if "groups" in config:')
    (57, '    group_annotation = (')
    (58, '        pd.read_csv(config["groups"], sep="\\\\t", dtype={"group": str})')
    (59, '        .set_index("group")')
    (60, '        .sort_index()')
    (61, '    )')
    (62, '    group_annotation = group_annotation.loc[groups]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if "umis" not in units.columns']
    (43, 'def _group_or_sample(row):')
    (44, '    group = row.get("group", None)')
    (45, '    if pd.isnull(group):')
    (46, '        return row["sample_name"]')
    (47, '    return group')
    (48, '')
    (49, '')
    (50, 'samples["group"] = [_group_or_sample(row) for _, row in samples.iterrows()]')
    (51, 'validate(samples, schema="../schemas/samples.schema.yaml")')
    (52, '')
    (53, '')
    (54, 'groups = samples["group"].unique()')
    (55, '')
    (56, 'if "groups" in config:')
    (57, '    group_annotation = (')
    (58, '        pd.read_csv(config["groups"], sep="\\\\t", dtype={"group": str})')
    (59, '        .set_index("group")')
    (60, '        .sort_index()')
    (61, '    )')
    (62, '    group_annotation = group_annotation.loc[groups]')
    (63, 'else:')
    (64, '    group_annotation = pd.DataFrame({"group": groups}).set_index("group")')
    (65, '')
    (66, 'units = (')
    (67, '    pd.read_csv(')
    (68, '        config["units"],')
    (69, '        sep="\\\\t",')
    (70, '        dtype={"sample_name": str, "unit_name": str},')
    (71, '        comment="#",')
    (72, '    )')
    (73, '    .set_index(["sample_name", "unit_name"], drop=False)')
    (74, '    .sort_index()')
    (75, ')')
    (76, '')
    (77, 'if "umis" not in units.columns:')
    (78, '    units["umis"] = pd.NA')
    (79, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['else Non']
    (43, 'def _group_or_sample(row):')
    (44, '    group = row.get("group", None)')
    (45, '    if pd.isnull(group):')
    (46, '        return row["sample_name"]')
    (47, '    return group')
    (48, '')
    (49, '')
    (50, 'samples["group"] = [_group_or_sample(row) for _, row in samples.iterrows()]')
    (51, 'validate(samples, schema="../schemas/samples.schema.yaml")')
    (52, '')
    (53, '')
    (54, 'groups = samples["group"].unique()')
    (55, '')
    (56, 'if "groups" in config:')
    (57, '    group_annotation = (')
    (58, '        pd.read_csv(config["groups"], sep="\\\\t", dtype={"group": str})')
    (59, '        .set_index("group")')
    (60, '        .sort_index()')
    (61, '    )')
    (62, '    group_annotation = group_annotation.loc[groups]')
    (63, 'else:')
    (64, '    group_annotation = pd.DataFrame({"group": groups}).set_index("group")')
    (65, '')
    (66, 'units = (')
    (67, '    pd.read_csv(')
    (68, '        config["units"],')
    (69, '        sep="\\\\t",')
    (70, '        dtype={"sample_name": str, "unit_name": str},')
    (71, '        comment="#",')
    (72, '    )')
    (73, '    .set_index(["sample_name", "unit_name"], drop=False)')
    (74, '    .sort_index()')
    (75, ')')
    (76, '')
    (77, 'if "umis" not in units.columns:')
    (78, '    units["umis"] = pd.NA')
    (79, '')
    (80, 'validate(units, schema="../schemas/units.schema.yaml")')
    (81, '')
    (82, 'primer_panels = (')
    (83, '    (')
    (84, '        pd.read_csv(')
    (85, '            config["primers"]["trimming"]["tsv"],')
    (86, '            sep="\\\\t",')
    (87, '            dtype={"panel": str, "fa1": str, "fa2": str},')
    (88, '            comment="#",')
    (89, '        )')
    (90, '        .set_index(["panel"], drop=False)')
    (91, '        .sort_index()')
    (92, '    )')
    (93, '    if config["primers"]["trimming"].get("tsv", "")')
    (94, '    else None')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if wildcards.by == "odds"']
    (145, '    def inner(wildcards):')
    (146, '        if wildcards.by == "odds":')
    (147, '            pattern = "results/calls/{{{{group}}}}.{{{{event}}}}.{{scatteritem}}.filtered_odds.{ext}"')
    (148, '        elif wildcards.by == "ann":')
    (149, '            pattern = "results/calls/{{{{group}}}}.{{{{event}}}}.{{scatteritem}}.filtered_ann.{ext}"')
    (150, '        else:')
    (151, '            raise ValueError(')
    (152, '                "Unexpected wildcard value for \\\'by\\\': {}".format(wildcards.by)')
    (153, '            )')
    (154, '        return gather.calling(pattern.format(ext=ext))')
    (155, '')
    (156, '    return inner')
    (157, '')
    (158, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if not is_activated("benchmarking") and query["filter"]']
    (159, 'def get_control_fdr_input(wildcards):')
    (160, '    query = get_fdr_control_params(wildcards)')
    (161, '    if not is_activated("benchmarking") and query["filter"]:')
    (162, '        by = "ann" if query["local"] else "odds"')
    (163, '        return "results/calls/{{group}}.{{event}}.filtered_{by}.bcf".format(by=by)')
    (164, '    else:')
    (165, '        return "results/calls/{group}.bcf"')
    (166, '')
    (167, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if is_activated("calc_consensus_reads")']
    (168, 'def get_recalibrate_quality_input(wildcards, bai=False):')
    (169, '    ext = "bai" if bai else "bam"')
    (170, '    if is_activated("calc_consensus_reads"):')
    (171, '        return "results/consensus/{}.{}".format(wildcards.sample, ext)')
    (172, '    elif is_activated("primers/trimming"):')
    (173, '        return "results/trimmed/{sample}.trimmed.{ext}".format(')
    (174, '            sample=wildcards.sample, ext=ext')
    (175, '        )')
    (176, '    elif is_activated("remove_duplicates"):')
    (177, '        return "results/dedup/{}.{}".format(wildcards.sample, ext)')
    (178, '    else:')
    (179, '        return "results/mapped/{}.{}".format(wildcards.sample, ext)')
    (180, '')
    (181, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if pd.isna(unit["fq1"])']
    (182, 'def get_cutadapt_input(wildcards):')
    (183, '    unit = units.loc[wildcards.sample].loc[wildcards.unit]')
    (184, '')
    (185, '    if pd.isna(unit["fq1"]):')
    (186, '        return get_sra_reads(wildcards.sample, wildcards.unit, ["fq1", "fq2"])')
    (187, '')
    (188, '    if unit["fq1"].endswith("gz"):')
    (189, '        ending = ".gz"')
    (190, '    else:')
    (191, '        ending = ""')
    (192, '')
    (193, '    if pd.isna(unit["fq2"]):')
    (194, '        # single end local sample')
    (195, '        return "pipe/cutadapt/{S}/{U}.fq1.fastq{E}".format(')
    (196, '            S=unit.sample_name, U=unit.unit_name, E=ending')
    (197, '        )')
    (198, '    else:')
    (199, '        # paired end local sample')
    (200, '        return expand(')
    (201, '            "pipe/cutadapt/{S}/{U}.{{read}}.fastq{E}".format(')
    (202, '                S=unit.sample_name, U=unit.unit_name, E=ending')
    (203, '            ),')
    (204, '            read=["fq1", "fq2"],')
    (205, '        )')
    (206, '')
    (207, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if pd.isna(pattern)']
    (215, 'def get_raw_reads(sample, unit, fq):')
    (216, '    pattern = units.loc[sample].loc[unit, fq]')
    (217, '    if pd.isna(pattern):')
    (218, '        return get_sra_reads(sample, unit, fq)')
    (219, '')
    (220, '    if "*" in pattern:')
    (221, '        files = sorted(glob.glob(units.loc[sample].loc[unit, fq]))')
    (222, '        if not files:')
    (223, '            raise ValueError(')
    (224, '                "No raw fastq files found for unit pattern {} (sample {}). "')
    (225, '                "Please check the your sample sheet.".format(unit, sample)')
    (226, '            )')
    (227, '    else:')
    (228, '        files = [pattern]')
    (229, '    return files')
    (230, '')
    (231, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['try', 'if isinstance(adapters, str)']
    (240, 'def get_cutadapt_adapters(wildcards):')
    (241, '    unit = units.loc[wildcards.sample].loc[wildcards.unit]')
    (242, '    try:')
    (243, '        adapters = unit["adapters"]')
    (244, '        if isinstance(adapters, str):')
    (245, '            return adapters')
    (246, '        return ""')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if is_paired_end(wildcards.sample)']
    (271, 'def get_map_reads_input(wildcards):')
    (272, '    if is_paired_end(wildcards.sample):')
    (273, '        return [')
    (274, '            "results/merged/{sample}_R1.fastq.gz",')
    (275, '            "results/merged/{sample}_R2.fastq.gz",')
    (276, '        ]')
    (277, '    return "results/merged/{sample}_single.fastq.gz"')
    (278, '')
    (279, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if controls']
    (293, 'def get_group_sample_aliases(wildcards, controls=True):')
    (294, '    if controls:')
    (295, '        return samples.loc[samples["group"] == wildcards.group]["alias"]')
    (296, '    return samples.loc[')
    (297, '        (samples["group"] == wildcards.group) & (samples["control"] == "no")')
    (298, '    ]["alias"]')
    (299, '')
    (300, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if is_activated("primers/trimming")']
    (301, 'def get_consensus_input(wildcards):')
    (302, '    if is_activated("primers/trimming"):')
    (303, '        return "results/trimmed/{}.trimmed.bam".format(wildcards.sample)')
    (304, '    elif is_activated("remove_duplicates"):')
    (305, '        return "results/dedup/{}.bam".format(wildcards.sample)')
    (306, '    else:')
    (307, '        return "results/mapped/{}.bam".format(wildcards.sample)')
    (308, '')
    (309, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if is_activated("remove_duplicates")']
    (310, 'def get_trimming_input(wildcards):')
    (311, '    if is_activated("remove_duplicates"):')
    (312, '        return "results/dedup/{}.bam".format(wildcards.sample)')
    (313, '    else:')
    (314, '        return "results/mapped/{}.bam".format(wildcards.sample)')
    (315, '')
    (316, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if isinstance(primer_panels, pd.DataFrame)', 'if not pd.isna(primer_panels.loc[wc.panel, "fa2"])']
    (317, 'def get_primer_bed(wc):')
    (318, '    if isinstance(primer_panels, pd.DataFrame):')
    (319, '        if not pd.isna(primer_panels.loc[wc.panel, "fa2"]):')
    (320, '            return "results/primers/{}_primers.bedpe".format(wc.panel)')
    (321, '        else:')
    (322, '            return "results/primers/{}_primers.bed".format(wc.panel)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if isinstance(primer_panels, pd.DataFrame)', 'else', 'if config["primers"]["trimming"].get("primers_fa2", "")']
    (317, 'def get_primer_bed(wc):')
    (318, '    if isinstance(primer_panels, pd.DataFrame):')
    (319, '        if not pd.isna(primer_panels.loc[wc.panel, "fa2"]):')
    (320, '            return "results/primers/{}_primers.bedpe".format(wc.panel)')
    (321, '        else:')
    (322, '            return "results/primers/{}_primers.bed".format(wc.panel)')
    (323, '    else:')
    (324, '        if config["primers"]["trimming"].get("primers_fa2", ""):')
    (325, '            return "results/primers/uniform_primers.bedpe"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if isinstance(primer_panels, pd.DataFrame)', 'else', 'else']
    (317, 'def get_primer_bed(wc):')
    (318, '    if isinstance(primer_panels, pd.DataFrame):')
    (319, '        if not pd.isna(primer_panels.loc[wc.panel, "fa2"]):')
    (320, '            return "results/primers/{}_primers.bedpe".format(wc.panel)')
    (321, '        else:')
    (322, '            return "results/primers/{}_primers.bed".format(wc.panel)')
    (323, '    else:')
    (324, '        if config["primers"]["trimming"].get("primers_fa2", ""):')
    (325, '            return "results/primers/uniform_primers.bedpe"')
    (326, '        else:')
    (327, '            return "results/primers/uniform_primers.bed"')
    (328, '')
    (329, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if isinstance(primer_panels, pd.DataFrame)', 'if not pd.isna(primer_panels.loc[panel, "fa2"])']
    (330, 'def get_sample_primer_fastas(sample):')
    (331, '    if isinstance(primer_panels, pd.DataFrame):')
    (332, '        panel = samples.loc[sample, "panel"]')
    (333, '        if not pd.isna(primer_panels.loc[panel, "fa2"]):')
    (334, '            return [')
    (335, '                primer_panels.loc[panel, "fa1"],')
    (336, '                primer_panels.loc[panel, "fa2"],')
    (337, '            ]')
    (338, '        return primer_panels.loc[panel, "fa1"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if isinstance(primer_panels, pd.DataFrame)', 'else', 'if config["primers"]["trimming"].get("primers_fa2", "")']
    (330, 'def get_sample_primer_fastas(sample):')
    (331, '    if isinstance(primer_panels, pd.DataFrame):')
    (332, '        panel = samples.loc[sample, "panel"]')
    (333, '        if not pd.isna(primer_panels.loc[panel, "fa2"]):')
    (334, '            return [')
    (335, '                primer_panels.loc[panel, "fa1"],')
    (336, '                primer_panels.loc[panel, "fa2"],')
    (337, '            ]')
    (338, '        return primer_panels.loc[panel, "fa1"]')
    (339, '    else:')
    (340, '        if config["primers"]["trimming"].get("primers_fa2", ""):')
    (341, '            return [')
    (342, '                config["primers"]["trimming"]["primers_fa1"],')
    (343, '                config["primers"]["trimming"]["primers_fa2"],')
    (344, '            ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if isinstance(primer_panels, pd.DataFrame)', 'else']
    (330, 'def get_sample_primer_fastas(sample):')
    (331, '    if isinstance(primer_panels, pd.DataFrame):')
    (332, '        panel = samples.loc[sample, "panel"]')
    (333, '        if not pd.isna(primer_panels.loc[panel, "fa2"]):')
    (334, '            return [')
    (335, '                primer_panels.loc[panel, "fa1"],')
    (336, '                primer_panels.loc[panel, "fa2"],')
    (337, '            ]')
    (338, '        return primer_panels.loc[panel, "fa1"]')
    (339, '    else:')
    (340, '        if config["primers"]["trimming"].get("primers_fa2", ""):')
    (341, '            return [')
    (342, '                config["primers"]["trimming"]["primers_fa1"],')
    (343, '                config["primers"]["trimming"]["primers_fa2"],')
    (344, '            ]')
    (345, '        return config["primers"]["trimming"]["primers_fa1"]')
    (346, '')
    (347, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if panel == "uniform"', 'if config["primers"]["trimming"].get("primers_fa2", "")']
    (348, 'def get_panel_primer_input(panel):')
    (349, '    if panel == "uniform":')
    (350, '        if config["primers"]["trimming"].get("primers_fa2", ""):')
    (351, '            return [')
    (352, '                config["primers"]["trimming"]["primers_fa1"],')
    (353, '                config["primers"]["trimming"]["primers_fa2"],')
    (354, '            ]')
    (355, '        return config["primers"]["trimming"]["primers_fa1"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if panel == "uniform"', 'else', 'if not pd.isna(panel["fa2"])']
    (348, 'def get_panel_primer_input(panel):')
    (349, '    if panel == "uniform":')
    (350, '        if config["primers"]["trimming"].get("primers_fa2", ""):')
    (351, '            return [')
    (352, '                config["primers"]["trimming"]["primers_fa1"],')
    (353, '                config["primers"]["trimming"]["primers_fa2"],')
    (354, '            ]')
    (355, '        return config["primers"]["trimming"]["primers_fa1"]')
    (356, '    else:')
    (357, '        panel = primer_panels.loc[panel]')
    (358, '        if not pd.isna(panel["fa2"]):')
    (359, '            return [panel["fa1"], panel["fa2"]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if panel == "uniform"', 'else']
    (348, 'def get_panel_primer_input(panel):')
    (349, '    if panel == "uniform":')
    (350, '        if config["primers"]["trimming"].get("primers_fa2", ""):')
    (351, '            return [')
    (352, '                config["primers"]["trimming"]["primers_fa1"],')
    (353, '                config["primers"]["trimming"]["primers_fa2"],')
    (354, '            ]')
    (355, '        return config["primers"]["trimming"]["primers_fa1"]')
    (356, '    else:')
    (357, '        panel = primer_panels.loc[panel]')
    (358, '        if not pd.isna(panel["fa2"]):')
    (359, '            return [panel["fa1"], panel["fa2"]]')
    (360, '        return panel["fa1"]')
    (361, '')
    (362, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if isinstance(primer_panels, pd.DataFrame)']
    (369, 'def get_primer_regions(wc):')
    (370, '    if isinstance(primer_panels, pd.DataFrame):')
    (371, '        return "results/primers/{}_primer_regions.tsv".format(')
    (372, '            samples.loc[wc.sample, "panel"]')
    (373, '        )')
    (374, '    return "results/primers/uniform_primer_regions.tsv"')
    (375, '')
    (376, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if units.loc[wc.sample]["umis"].isnull().any()']
    (377, 'def get_markduplicates_extra(wc):')
    (378, '    c = config["params"]["picard"]["MarkDuplicates"]')
    (379, '')
    (380, '    if units.loc[wc.sample]["umis"].isnull().any():')
    (381, '        b = ""')
    (382, '    else:')
    (383, '        b = "--BARCODE_TAG RX"')
    (384, '')
    (385, '    if is_activated("calc_consensus_reads"):')
    (386, '        d = "--TAG_DUPLICATE_SET_MEMBERS true"')
    (387, '    else:')
    (388, '        d = ""')
    (389, '')
    (390, '    return f"{c} {b} {d}"')
    (391, '')
    (392, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if is_activated("primers/trimming") and not group_is_paired_end(wildcards.group)']
    (393, 'def get_group_bams(wildcards, bai=False):')
    (394, '    ext = "bai" if bai else "bam"')
    (395, '    if is_activated("primers/trimming") and not group_is_paired_end(wildcards.group):')
    (396, '        WorkflowError("Primer trimming is only available for paired end data.")')
    (397, '    return expand(')
    (398, '        "results/recal/{sample}.{ext}",')
    (399, '        sample=get_group_samples(wildcards.group),')
    (400, '        ext=ext,')
    (401, '    )')
    (402, '')
    (403, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if wildcards.read_type == "se"']
    (442, 'def get_processed_consensus_input(wildcards):')
    (443, '    if wildcards.read_type == "se":')
    (444, '        return "results/consensus/fastq/{}.se.fq".format(wildcards.sample)')
    (445, '    return [')
    (446, '        "results/consensus/fastq/{}.1.fq".format(wildcards.sample),')
    (447, '        "results/consensus/fastq/{}.2.fq".format(wildcards.sample),')
    (448, '    ]')
    (449, '')
    (450, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if is_activated("mutational_burden")']
    (480, 'def get_mutational_burden_targets():')
    (481, '    mutational_burden_targets = []')
    (482, '    if is_activated("mutational_burden"):')
    (483, '        for group in groups:')
    (484, '            mutational_burden_targets.extend(')
    (485, '                expand(')
    (486, '                    "results/plots/mutational-burden/{group}.{alias}.{mode}.mutational-burden.svg",')
    (487, '                    group=group,')
    (488, '                    mode=config["mutational_burden"].get("mode", "curve"),')
    (489, '                    alias=get_group_tumor_aliases(group),')
    (490, '                )')
    (491, '            )')
    (492, '    return mutational_burden_targets')
    (493, '')
    (494, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if is_activated("annotations/vcfs")']
    (506, 'def get_selected_annotations():')
    (507, '    selection = ".annotated"')
    (508, '    if is_activated("annotations/vcfs"):')
    (509, '        selection += ".db-annotated"')
    (510, '    if is_activated("annotations/dgidb"):')
    (511, '        selection += ".dgidb"')
    (512, '    return selection')
    (513, '')
    (514, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if filter']
    (534, 'def get_candidate_calls():')
    (535, '    filter = config["calling"]["filter"].get("candidates")')
    (536, '    if filter:')
    (537, '        return "results/candidate-calls/{group}.{caller}.{scatteritem}.filtered.bcf"')
    (538, '    else:')
    (539, '        return "results/candidate-calls/{group}.{caller}.{scatteritem}.bcf"')
    (540, '')
    (541, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if wildcards.batch == "all"']
    (542, 'def get_report_batch(wildcards):')
    (543, '    if wildcards.batch == "all":')
    (544, '        _groups = groups')
    (545, '    else:')
    (546, '        _groups = samples.loc[')
    (547, '            samples[config["report"]["stratify"]["by-column"]] == wildcards.batch,')
    (548, '            "group",')
    (549, '        ].unique()')
    (550, '    if not any(_groups):')
    (551, '        raise ValueError("No samples found. Is your sample sheet empty?")')
    (552, '    return _groups')
    (553, '')
    (554, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if is_activated("report/stratify")']
    (555, 'def get_report_batches():')
    (556, '    if is_activated("report/stratify"):')
    (557, '        yield "all"')
    (558, '        yield from samples[config["report"]["stratify"]["by-column"]].unique()')
    (559, '    else:')
    (560, '        yield "all"')
    (561, '')
    (562, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if n']
    (574, 'def get_vep_threads():')
    (575, '    n = len(samples)')
    (576, '    if n:')
    (577, '        return max(workflow.cores / n, 1)')
    (578, '    else:')
    (579, '        return 1')
    (580, '')
    (581, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if plugin in config["annotations"]["vep"]["plugins"]', 'if plugin == "REVEL"']
    (582, 'def get_plugin_aux(plugin, index=False):')
    (583, '    if plugin in config["annotations"]["vep"]["plugins"]:')
    (584, '        if plugin == "REVEL":')
    (585, '            suffix = ".tbi" if index else ""')
    (586, '            return "resources/revel_scores.tsv.gz{suffix}".format(suffix=suffix)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if plugin in config["annotations"]["vep"]["plugins"]']
    (582, 'def get_plugin_aux(plugin, index=False):')
    (583, '    if plugin in config["annotations"]["vep"]["plugins"]:')
    (584, '        if plugin == "REVEL":')
    (585, '            suffix = ".tbi" if index else ""')
    (586, '            return "resources/revel_scores.tsv.gz{suffix}".format(suffix=suffix)')
    (587, '    return []')
    (588, '')
    (589, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if query.get("local", config["calling"]["fdr-control"].get("local", False)']
    (590, 'def get_fdr_control_params(wildcards):')
    (591, '    query = config["calling"]["fdr-control"]["events"][wildcards.event]')
    (592, '    threshold = query.get(')
    (593, '        "threshold", config["calling"]["fdr-control"].get("threshold", 0.05)')
    (594, '    )')
    (595, '    events = query["varlociraptor"]')
    (596, '    local = (')
    (597, '        "--local"')
    (598, '        if query.get("local", config["calling"]["fdr-control"].get("local", False))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['else "']
    (590, 'def get_fdr_control_params(wildcards):')
    (591, '    query = config["calling"]["fdr-control"]["events"][wildcards.event]')
    (592, '    threshold = query.get(')
    (593, '        "threshold", config["calling"]["fdr-control"].get("threshold", 0.05)')
    (594, '    )')
    (595, '    events = query["varlociraptor"]')
    (596, '    local = (')
    (597, '        "--local"')
    (598, '        if query.get("local", config["calling"]["fdr-control"].get("local", False))')
    (599, '        else ""')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if wildcards.caller == "delly"']
    (609, 'def get_fixed_candidate_calls(wildcards):')
    (610, '    if wildcards.caller == "delly":')
    (611, '        return "results/candidate-calls/{group}.delly.no_bnds.bcf"')
    (612, '    else:')
    (613, '        return "results/candidate-calls/{group}.{caller}.bcf"')
    (614, '')
    (615, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if input.predefined']
    (616, 'def get_filter_targets(wildcards, input):')
    (617, '    if input.predefined:')
    (618, '        return " | bedtools intersect -a /dev/stdin -b {input.predefined} ".format(')
    (619, '            input=input')
    (620, '        )')
    (621, '    else:')
    (622, '        return ""')
    (623, '')
    (624, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if isinstance(filter, str)']
    (625, 'def get_filter_expression(filter_name):')
    (626, '    filter = config["calling"]["filter"][filter_name]')
    (627, '    if isinstance(filter, str):')
    (628, '        return filter')
    (629, '    else:')
    (630, '        return config["calling"]["filter"][filter_name]["expression"]')
    (631, '')
    (632, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if isinstance(filter, str)']
    (633, 'def get_filter_aux_entries(filter_name):')
    (634, '    filter = config["calling"]["filter"][filter_name]')
    (635, '    if isinstance(filter, str):')
    (636, '        return {}')
    (637, '    else:')
    (638, '        aux = config["calling"]["filter"][filter_name].get("aux-files", {})')
    (639, '        return aux  # [f"--aux {name} {path}" for name, path in aux.items()]')
    (640, '')
    (641, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if annotations']
    (698, 'def get_annotation_pipes(wildcards, input):')
    (699, '    if annotations:')
    (700, '        return "| {}".format(')
    (701, '            " | ".join(')
    (702, '                [')
    (703, '                    f"SnpSift annotate -name \\\'{prefix}_\\\' {repr(path)} /dev/stdin"')
    (704, '                    for (prefix, _), path in zip(annotations, input.annotations)')
    (705, '                ]')
    (706, '            )')
    (707, '        )')
    (708, '    else:')
    (709, '        return ""')
    (710, '')
    (711, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if wildcards.format == "vcf"']
    (717, 'def get_tabix_params(wildcards):')
    (718, '    if wildcards.format == "vcf":')
    (719, '        return "-p vcf"')
    (720, '    if wildcards.format == "txt":')
    (721, '        return "-s 1 -b 2 -e 2"')
    (722, '    raise ValueError("Invalid format for tabix: {}".format(wildcards.format))')
    (723, '')
    (724, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if "REVEL" in config["annotations"]["vep"]["plugins"]']
    (751, '    def append_items(items, field_func, header_func):')
    (752, '        for item in items:')
    (753, '            parts.append(field_func(item))')
    (754, '            header.append(header_func(item))')
    (755, '')
    (756, '    annotation_fields = [')
    (757, '        "SYMBOL",')
    (758, '        "Gene",')
    (759, '        "Feature",')
    (760, '        "IMPACT",')
    (761, '        "HGVSp",')
    (762, '        "HGVSg",')
    (763, '        "Consequence",')
    (764, '        "CANONICAL",')
    (765, '    ]')
    (766, '')
    (767, '    if "REVEL" in config["annotations"]["vep"]["plugins"]:')
    (768, '        annotation_fields.append("REVEL")')
    (769, '')
    (770, '    annotation_fields.extend(')
    (771, '        [')
    (772, '            field')
    (773, '            for field in config_output.get("annotation_fields", [])')
    (774, '            if field not in annotation_fields')
    (775, '        ]')
    (776, '    )')
    (777, '')
    (778, '    append_items(annotation_fields, "ANN[\\\'{}\\\']".format, str.lower)')
    (779, '    append_items(["CLIN_SIG"], "ANN[\\\'{}\\\']".format, lambda x: "clinical significance")')
    (780, '')
    (781, '    samples = get_group_sample_aliases(wildcards)')
    (782, '')
    (783, '    def append_format_field(field, name):')
    (784, '        append_items(')
    (785, '            samples, f"FORMAT[\\\'{field}\\\'][\\\'{{}}\\\']".format, f"{{}}: {name}".format')
    (786, '        )')
    (787, '')
    (788, '    if config_output.get("event_prob", False):')
    (789, '        events = list(scenario["events"].keys())')
    (790, '        events += ["artifact", "absent"]')
    (791, '        append_items(events, lambda x: f"INFO[\\\'PROB_{x.upper()}\\\']", "prob: {}".format)')
    (792, '    append_format_field("AF", "allele frequency")')
    (793, '    append_format_field("DP", "read depth")')
    (794, '')
    (795, '    if config_output.get("observations", False):')
    (796, '        append_format_field("OBS", "observations")')
    (797, '    return {"expr": join_items(parts), "header": join_items(header)}')
    (798, '')
    (799, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if isinstance(get_panel_primer_input(wc.panel), list)']
    (816, 'def get_filter_params(wc):')
    (817, '    if isinstance(get_panel_primer_input(wc.panel), list):')
    (818, '        return "-b -f 2"')
    (819, '    return "-b -F 4"')
    (820, '')
    (821, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if not isinstance(get_sample_primer_fastas(wc.sample), list)']
    (822, 'def get_single_primer_flag(wc):')
    (823, '    if not isinstance(get_sample_primer_fastas(wc.sample), list):')
    (824, '        return "--first-of-pair"')
    (825, '    return ""')
    (826, '')
    (827, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if isinstance(primers, list)']
    (828, 'def format_bowtie_primers(wc, primers):')
    (829, '    if isinstance(primers, list):')
    (830, '        return "-1 {r1} -2 {r2}".format(r1=primers[0], r2=primers[1])')
    (831, '    return primers')
    (832, '')
    (833, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if input.variant_oncoprints']
    (865, 'def get_variant_oncoprint_tables(wildcards, input):')
    (866, '    if input.variant_oncoprints:')
    (867, '        oncoprint_dir = input.variant_oncoprints')
    (868, '        valid = re.compile(r"^[^/]+\\\\.tsv$")')
    (869, '        tables = [f for f in os.listdir(oncoprint_dir) if valid.match(f)]')
    (870, '        assert all(table.endswith(".tsv") for table in tables)')
    (871, '        genes = [gene_table[:-4] for gene_table in tables]')
    (872, '        return list(')
    (873, '            zip(genes, expand(f"{oncoprint_dir}/{{oncoprint}}", oncoprint=tables))')
    (874, '        )')
    (875, '    else:')
    (876, '        return []')
    (877, '')
    (878, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if "labels" in event']
    (879, 'def get_datavzrd_report_labels(wildcards):')
    (880, '    event = config["calling"]["fdr-control"]["events"][wildcards.event]')
    (881, '    labels = {"batch": wildcards.batch}')
    (882, '    if "labels" in event:')
    (883, '        labels.update({key: str(value) for key, value in event["labels"].items()})')
    (884, '    else:')
    (885, '        labels["callset"] = wildcards.event.replace("_", " ")')
    (886, '    return labels')
    (887, '')
    (888, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if len(get_report_batch(wildcards)) > 1']
    (923, 'def get_variant_oncoprints(wildcards):')
    (924, '    if len(get_report_batch(wildcards)) > 1:')
    (925, '        return "results/tables/oncoprints/{wildcards.batch}.{wildcards.event}/variant-oncoprints"')
    (926, '    else:')
    (927, '        return []')
    (928, '')
    (929, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if len(get_report_batch(wildcards)) > 1']
    (931, '    def inner(wildcards):')
    (932, '        if len(get_report_batch(wildcards)) > 1:')
    (933, '            oncoprint_path = (')
    (934, '                f"results/tables/oncoprints/{wildcards.batch}.{wildcards.event}"')
    (935, '            )')
    (936, '            if oncoprint_type == "gene":')
    (937, '                return f"{oncoprint_path}/gene-oncoprint.tsv"')
    (938, '            elif oncoprint_type == "variant":')
    (939, '                return f"{oncoprint_path}/variant-oncoprints"')
    (940, '            else:')
    (941, '                raise ValueError(f"bug: unsupported oncoprint type {oncoprint_type}")')
    (942, '        else:')
    (943, '            return []')
    (944, '')
    (945, '    return inner')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nikostr/dna-seq-deepvariant-glnexus-variant-calling, file=workflow/rules/common.smk
context_key: ['if len(fastqs) == 2']
    (29, 'def get_fastq(wildcards):')
    (30, '    """Get fastq files of given sample-unit."""')
    (31, '    fastqs = samples.loc[(wildcards.sample, wildcards.unit), ["fq1", "fq2"]].dropna()')
    (32, '    if len(fastqs) == 2:')
    (33, '        return {"sample": [fastqs.fq1, fastqs.fq2]}')
    (34, '    return {"sample": [fastqs.fq1]}')
    (35, '')
    (36, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nikostr/dna-seq-deepvariant-glnexus-variant-calling, file=workflow/rules/common.smk
context_key: ['if not is_single_end(**wildcards)']
    (42, 'def get_trimmed_reads(wildcards):')
    (43, '    """Get trimmed reads of given sample-unit."""')
    (44, '    if not is_single_end(**wildcards):')
    (45, '        # paired-end sample')
    (46, '        return expand(')
    (47, '            "results/trimmed/{sample}-{unit}.{group}.fastq.gz",')
    (48, '            group=[1, 2],')
    (49, '            **wildcards')
    (50, '        )')
    (51, '    # single end sample')
    (52, '    return "results/trimmed/{sample}-{unit}.fastq.gz".format(**wildcards)')
    (53, '')
    (54, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=elsamah/ExomeSeq_wNorm, file=workflow/rules/common.smk
context_key: ['if pd.isna(control) or pd.isnull(control)']
    (16, 'def has_a_control(wildcards):')
    (17, '    # Assumes samples are in [SAMPLE]_[CELLTYPE]_[ANTIBODY] format')
    (18, '    control = samples.loc[ (wildcards.sample), "control" ]')
    (19, '    if pd.isna(control) or pd.isnull(control):')
    (20, '        return False')
    (21, '    else:')
    (22, '        return control != "_".join([wildcards.sample])')
    (23, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=elsamah/ExomeSeq_wNorm, file=workflow/rules/common.smk
context_key: ['if pd.isna(control) or pd.isnull(control)']
    (24, 'def get_sample_control(wildcards, retself=True):')
    (25, '    # Assumes control is a [SAMPLE]_[CELLTYPE]_[ANTIBODY] format')
    (26, '    control = samples.loc[ (wildcards.sample), "control" ]')
    (27, '    if pd.isna(control) or pd.isnull(control):')
    (28, '        control = [ wildcards.sample ] if retself else ""')
    (29, '    else:')
    (30, '        control = control.split("_")')
    (31, '    return "_".join(control)')
    (32, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=elsamah/ExomeSeq_wNorm, file=workflow/rules/common.smk
context_key: ["if type == \\'snv\\'"]
    (76, "def get_maf_output(wildcards, type=\\'indel\\'):")
    (77, '    res = []')
    (78, "    if type == \\'snv\\':")
    (79, '        for v in snv_vcfs.itertuples():')
    (80, '            res.append(')
    (81, '                "results/MAF_38_final/{}/{}/{}.maf".format(')
    (82, '                    type, wildcards.sample, v.snv')
    (83, '                )')
    (84, '            )')
    (85, "    elif type == \\'indel\\':")
    (86, '        for v in indel_vcfs.itertuples():')
    (87, '            res.append(')
    (88, '                "results/MAF_38_final/{}/{}/{}.maf".format(')
    (89, '                    type, wildcards.sample, v.indel')
    (90, '                )')
    (91, '            )')
    (92, '    return res')
    (93, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/rna-seq-star-deseq2, file=workflow/rules/common.smk
context_key: ['if pd.isna(unit["fq1"])']
    (37, 'def get_cutadapt_input(wildcards):')
    (38, '    unit = units.loc[wildcards.sample].loc[wildcards.unit]')
    (39, '')
    (40, '    if pd.isna(unit["fq1"]):')
    (41, '        # SRA sample (always paired-end for now)')
    (42, '        accession = unit["sra"]')
    (43, '        return expand("sra/{accession}_{read}.fastq", accession=accession, read=[1, 2])')
    (44, '')
    (45, '    if unit["fq1"].endswith("gz"):')
    (46, '        ending = ".gz"')
    (47, '    else:')
    (48, '        ending = ""')
    (49, '')
    (50, '    if pd.isna(unit["fq2"]):')
    (51, '        # single end local sample')
    (52, '        return "pipe/cutadapt/{S}/{U}.fq1.fastq{E}".format(')
    (53, '            S=unit.sample_name, U=unit.unit_name, E=ending')
    (54, '        )')
    (55, '    else:')
    (56, '        # paired end local sample')
    (57, '        return expand(')
    (58, '            "pipe/cutadapt/{S}/{U}.{{read}}.fastq{E}".format(')
    (59, '                S=unit.sample_name, U=unit.unit_name, E=ending')
    (60, '            ),')
    (61, '            read=["fq1", "fq2"],')
    (62, '        )')
    (63, '')
    (64, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/rna-seq-star-deseq2, file=workflow/rules/common.smk
context_key: ['if "strandedness" in units.columns']
    (127, 'def get_strandedness(units):')
    (128, '    if "strandedness" in units.columns:')
    (129, '        return units["strandedness"].tolist()')
    (130, '    else:')
    (131, '        strand_list = ["none"]')
    (132, '        return strand_list * units.shape[0]')
    (133, '')
    (134, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if len(GENOMES) == 1']
    (14, 'def write_log():')
    (15, '    ## reference genome')
    (16, '    if len(GENOMES) == 1:')
    (17, '        LOGGER.info(f"REFERENCE GENOME: 1 genome is specified:")')
    (18, '    elif len(GENOMES) < 4:')
    (19, '        LOGGER.info(f"REFERENCE GENOME: {len(GENOMES)} genomes are specified:")')
    (20, '    else:')
    (21, '        LOGGER.info(f"REFERENCE GENOME: {len(GENOMES)} genomes are specified.")')
    (22, '')
    (23, '    ## get all chromosome names and store them in the dict config[chromosomes][genome][all] for later use')
    (24, '    for i, genome in enumerate(GENOMES):')
    (25, '        if i < 4:')
    (26, '            if len(recursive_get(["chromosome", genome], [])) > 1:')
    (27, '                LOGGER.info(f"  - {genome}:")')
    (28, '            else:')
    (29, '                LOGGER.info(f"  - {genome}")')
    (30, '            name = recursive_get(["chromosome", genome, "name"], "")')
    (31, '            if name:')
    (32, '                all_sex_chr = recursive_get(["chromosome", genome, "all_sex_chr"], "")')
    (33, '                LOGGER.info(')
    (34, '                    f"    - Detected genome as \\\'{name}\\\': Applying default chromosome names for sex and mt chromosomes"')
    (35, '                )')
    (36, '            sex_chr = recursive_get(["chromosome", genome, "sex_chr"], "")')
    (37, '            if sex_chr:')
    (38, '                LOGGER.info(f"    - Sex chromosome (X): {to_list(sex_chr)}")')
    (39, '            autosomes = recursive_get(["chromosome", genome, "autosomes"], "")')
    (40, '            if autosomes:')
    (41, '                if len(autosomes) > 10:')
    (42, '                    LOGGER.info(')
    (43, '                        f"    - Autosomes: {autosomes[:4] + [\\\'...\\\'] + autosomes[-4:]}"')
    (44, '                    )')
    (45, '                else:')
    (46, '                    LOGGER.info(f"    - Autosomes: {autosomes}")')
    (47, '        elif i == 4:')
    (48, '            LOGGER.info(f"    - ...")')
    (49, '        else:')
    (50, '            break')
    (51, '')
    (52, '    # ----------------------------------------------------------------------------------------------------------')
    (53, '    ## sample file')
    (54, '    LOGGER.info(f"SAMPLES:")')
    (55, '    if PAIRED_END:')
    (56, '        if SAMPLE_FILE == "yaml":')
    (57, '            LOGGER.info(f"  - Samples (YAML input) in paired-end format:")')
    (58, '        else:')
    (59, '            LOGGER.info(f"  - Sample file (\\\'{SAMPLE_FILE}\\\') in paired-end format:")')
    (60, '')
    (61, '        LOGGER.info(f"    - {len(SAMPLES)} SAMPLES")')
    (62, '        LOGGER.info(f"    - {len([l for s in SAMPLES.values() for l in s])} libraries")')
    (63, '        tmp = [')
    (64, '            i["Data2"] for s in SAMPLES.values() for l in s.values() for i in l.values()')
    (65, '        ]')
    (66, '        LOGGER.info(')
    (67, '            f"    - {len([x for x in tmp if str(x) != \\\'nan\\\'])} paired-end fastq files"')
    (68, '        )')
    (69, '        nb = len([x for x in tmp if str(x) == "nan"])')
    (70, '        if nb:')
    (71, '            LOGGER.info(f"    - {nb} single-end fastq files")')
    (72, '    else:')
    (73, '        if SAMPLE_FILE == "yaml":')
    (74, '            LOGGER.info(f"  - Samples (YAML input) in single-end format:")')
    (75, '        else:')
    (76, '            LOGGER.info(f"  - Sample file (\\\'{SAMPLE_FILE}\\\') in single-end format:")')
    (77, '        LOGGER.info(f"    - {len(SAMPLES)} SAMPLES")')
    (78, '        LOGGER.info(f"    - {len([l for s in SAMPLES.values() for l in s])} libraries")')
    (79, '        LOGGER.info(')
    (80, '            f"    - {len([i for s in SAMPLES.values() for l in s.values() for i in l])} single-end fastq files"')
    (81, '        )')
    (82, '')
    (83, '    if len(EXTERNAL_SAMPLES) > 0:')
    (84, '        if EXTERNAL_SAMPLE_FILE == "yaml":')
    (85, '            LOGGER.info(f"  - External samples (YAML input; only stats are computed):")')
    (86, '        else:')
    (87, '            LOGGER.info(')
    (88, '                f"  - External sample file (\\\'{EXTERNAL_SAMPLE_FILE}\\\'; only stats are computed):"')
    (89, '            )')
    (90, '')
    (91, '        for i, genome in enumerate(EXTERNAL_SAMPLES):')
    (92, '            if i < 4:')
    (93, '                LOGGER.info(')
    (94, '                    f"    - {len(EXTERNAL_SAMPLES[genome])} bam files {to_list(genome)}"')
    (95, '                )')
    (96, '            elif i == 4:')
    (97, '                LOGGER.info(f"    - ...")')
    (98, '            else:')
    (99, '                break')
    (100, '')
    (101, '    # ----------------------------------------------------------------------------------------------------------')
    (102, '    # print a summary of the workflow')
    (103, '    LOGGER.info("MAPPING WORKFLOW:")')
    (104, '    if run_subsampling:')
    (105, '        if subsampling_number < 1:')
    (106, '            LOGGER.info(f"  - Subsampling {100 * subsampling_number}% of the reads")')
    (107, '        else:')
    (108, '            LOGGER.info(f"  - Subsampling {subsampling_number} reads per fastq file")')
    (109, '')
    (110, '    if run_adapter_removal:')
    (111, '        if COLLAPSE:')
    (112, '            LOGGER.info(')
    (113, '                f"  - Removing adapters with AdapterRemoval and collapsing paired-end reads"')
    (114, '            )')
    (115, '        else:')
    (116, '            LOGGER.info(f"  - Removing adapters with AdapterRemoval")')
    (117, '')
    (118, '    LOGGER.info(f"  - Mapping with {mapper}")')
    (119, '')
    (120, '    LOGGER.info(f"  - Sorting bam file")')
    (121, '')
    (122, '    if run_filtering:')
    (123, '        if save_low_qual:')
    (124, '            LOGGER.info(')
    (125, '                f"  - Filtering and keeping separately low quality/unmapped reads"')
    (126, '            )')
    (127, '        else:')
    (128, '            LOGGER.info(f"  - Filtering and removing low quality/unmapped reads")')
    (129, '')
    (130, '    if remove_duplicates != "False":')
    (131, '        if remove_duplicates == "markduplicates":')
    (132, '            LOGGER.info(f"  - Removing duplicates with MarkDuplicates")')
    (133, '        elif remove_duplicates == "dedup":')
    (134, '            LOGGER.info(f"  - Removing duplicates with DeDup")')
    (135, '        else:')
    (136, '            LOGGER.info(f"  - Removing duplicates with {remove_duplicates}")')
    (137, '')
    (138, '    if run_damage_rescale:')
    (139, '        LOGGER.info(f"  - Rescaling damage with MapDamage2")')
    (140, '')
    (141, '    if run_realign:')
    (142, '        LOGGER.info(f"  - Realigning indels with GATK v3.8")')
    (143, '')
    (144, '    if run_compute_md:')
    (145, '        LOGGER.info(f"  - Recomputing md flag")')
    (146, '')
    (147, '    # ----------------------------------------------------------------------------------------------------------')
    (148, '    LOGGER.info("FINAL BAM FILES:")')
    (149, '    LOGGER.info(')
    (150, '        f"  - BAM FILES ({len(final_bam)} files in folder \\\'{os.path.dirname(final_bam[0])}\\\'):"')
    (151, '    )')
    (152, '    for i, file in enumerate(final_bam):')
    (153, '        if i < 4:')
    (154, '            LOGGER.info(f"    - {file}")')
    (155, '        elif i == 4:')
    (156, '            LOGGER.info(f"    - ...")')
    (157, '        else:')
    (158, '            break')
    (159, '')
    (160, '    if save_low_qual:')
    (161, '        LOGGER.info(')
    (162, '            f"  - LOW QUALITY AND UNMAPPED BAM FILES ({len(final_bam_low_qual)} files in folder \\\'{os.path.dirname(final_bam_low_qual[0])}\\\'):"')
    (163, '        )')
    (164, '        for i, file in enumerate(final_bam_low_qual):')
    (165, '            if i < 4:')
    (166, '                LOGGER.info(f"    - {file}")')
    (167, '            elif i == 4:')
    (168, '                LOGGER.info(f"    - ...")')
    (169, '            else:')
    (170, '                break')
    (171, '')
    (172, '    # ----------------------------------------------------------------------------------------------------------')
    (173, '    LOGGER.info("ANALYSES:")')
    (174, '    if len(run_sex_inference) > 0:')
    (175, '        if len(genome) > 0:')
    (176, '            LOGGER.info(f"  - Inferring sex {run_sex_inference}")')
    (177, '        else:')
    (178, '            LOGGER.info(f"  - Inferring sex")')
    (179, '')
    (180, '    if len(run_depth) > 0:')
    (181, '        if len(genome) > 0:')
    (182, '            LOGGER.info(f"  - Computing depth per given chromosomes {run_depth}")')
    (183, '        else:')
    (184, '            LOGGER.info(f"  - Computing depth per given chromosomes ")')
    (185, '')
    (186, '    if run_damage != "False":')
    (187, '        if run_damage == "mapDamage":')
    (188, '            LOGGER.info(')
    (189, '                f"  - Inferring damage and read length with MapDamage2 on all alignments"')
    (190, '            )')
    (191, '')
    (192, '        fraction = recursive_get(["damage", "bamdamage_fraction"], 0)')
    (193, '        if fraction == 0:')
    (194, '            LOGGER.info(')
    (195, '                f"  - Inferring damage and read length with bamdamge on all alignments"')
    (196, '            )')
    (197, '        elif fraction < 1:')
    (198, '            LOGGER.info(')
    (199, '                f"  - Inferring damage and read length with bamdamge on {100 * fraction}% of the alignments"')
    (200, '            )')
    (201, '        else:')
    (202, '            LOGGER.info(')
    (203, '                f"  - Inferring damage and read length with bamdamge on {fraction} alignments"')
    (204, '            )')
    (205, '')
    (206, '    if len(run_imputation) > 0:')
    (207, '        if len(genome) > 1:')
    (208, '            LOGGER.info(f"  - Imputing {run_imputation}")')
    (209, '        else:')
    (210, '            LOGGER.info(f"  - Imputing")')
    (211, '')
    (212, '    # ----------------------------------------------------------------------------------------------------------')
    (213, '    # print a summary of the stats')
    (214, '    LOGGER.info("REPORTS:")')
    (215, '    LOGGER.info(f"  - Mapping statistics tables:")')
    (216, '    for i, file in enumerate(stat_csv):')
    (217, '        LOGGER.info(f"    - {file}")')
    (218, '')
    (219, '    if run_qualimap:')
    (220, '        LOGGER.info(')
    (221, '            f"  - Qualimap report: {len(qualimap_files)} report(s) on final bam files:"')
    (222, '        )')
    (223, '        for i, file in enumerate(qualimap_files):')
    (224, '            if i < 4:')
    (225, '                LOGGER.info(f"    - {file}")')
    (226, '            elif i == 4:')
    (227, '                LOGGER.info(f"    - ...")')
    (228, '            else:')
    (229, '                break')
    (230, '')
    (231, '    if run_multiqc:')
    (232, '        LOGGER.info(')
    (233, '            f"  - Mulitqc report: {len(multiqc_files)} HTML report(s) combining statistics per genome:"')
    (234, '        )')
    (235, '        for i, file in enumerate(multiqc_files):')
    (236, '            if i < 4:')
    (237, '                LOGGER.info(f"    - {file}")')
    (238, '            elif i == 4:')
    (239, '                LOGGER.info(f"    - ...")')
    (240, '            else:')
    (241, '                break')
    (242, '')
    (243, '    LOGGER.info(')
    (244, '        f"  => Snakemake report: run \\\'snakemake --report report.html\\\' after finalization of the run to get the report"')
    (245, '    )')
    (246, '')
    (247, '')
    (248, '##########################################################################################################')
    (249, '## read sample file')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if file == ""']
    (250, 'def read_sample_file():')
    (251, '    file = recursive_get(["sample_file"], "config/SAMPLES.tsv")')
    (252, '    if file == "":')
    (253, '        return {}, False, False, ""')
    (254, '')
    (255, '    if isinstance(file, dict):  ## yaml input')
    (256, '        SAMPLES = file')
    (257, '        input = "yaml"')
    (258, '')
    (259, '    else:  ## sample file')
    (260, '        ## read the file')
    (261, '        delim = recursive_get(["delim"], "\\\\s+")')
    (262, '        db = pd.read_csv(file, sep=delim, comment="#", dtype=str)')
    (263, '        input = file')
    (264, '')
    (265, '        ## check number of columns and column names')
    (266, '        if len(db.columns) == 6:')
    (267, '            colnames = ["ID", "Data", "MAPQ", "LB", "PL", "SM"]')
    (268, '            if not set(colnames).issubset(db.columns):')
    (269, '                LOGGER.error(')
    (270, '                    f"ERROR: The column names in the sample file are wrong! Expected are for single-end reads {colnames}!"')
    (271, '                )')
    (272, '                sys.exit(1)')
    (273, '')
    (274, '        elif len(db.columns) == 7:')
    (275, '            colnames = ["ID", "Data1", "Data2", "MAPQ", "LB", "PL", "SM"]')
    (276, '            if not set(colnames).issubset(db.columns):')
    (277, '                LOGGER.error(')
    (278, '                    f"ERROR: The column names in the sample file are wrong! Expected are for paired-end reads {colnames}!"')
    (279, '                )')
    (280, '                sys.exit(1)')
    (281, '')
    (282, '        else:')
    (283, '            LOGGER.error(')
    (284, '                f"ERROR: The number of columns in the sample file is wrong ({len(db.columns)} columns)!"')
    (285, '            )')
    (286, '            sys.exit(1)')
    (287, '')
    (288, '        ## --------------------------------------------------------------------------------------------------')
    (289, '        ## check if all IDs per LB and SM are unique')
    (290, '        all_fastq = db.groupby(["ID", "LB", "SM"])["ID"].agg(["count"]).reset_index()')
    (291, '        if max(all_fastq["count"]) > 1:')
    (292, '            LOGGER.error(')
    (293, '                f"ERROR: The ID\\\'s {all_fastq[\\\'ID\\\']} are not uniq within library and sample!"')
    (294, '            )')
    (295, '            sys.exit(1)')
    (296, '')
    (297, '        ## --------------------------------------------------------------------------------------------------')
    (298, '        ## dataframe to nested dict')
    (299, '        SAMPLES = {}')
    (300, '        cols = [e for e in list(db.columns) if e not in ("SM", "LB", "ID")]')
    (301, '        for index, row in db.iterrows():')
    (302, '            ## if key not present add new dict')
    (303, '            if row["SM"] not in SAMPLES:')
    (304, '                SAMPLES[row["SM"]] = {}')
    (305, '            if row["LB"] not in SAMPLES[row["SM"]]:')
    (306, '                SAMPLES[row["SM"]][row["LB"]] = {}')
    (307, '            if row["ID"] not in SAMPLES[row["SM"]][row["LB"]]:')
    (308, '                SAMPLES[row["SM"]][row["LB"]][row["ID"]] = {}')
    (309, '')
    (310, '            ## add all remaining columns to this dict')
    (311, '            for col in cols:')
    (312, '                SAMPLES[row["SM"]][row["LB"]][row["ID"]][col] = row[col]')
    (313, '')
    (314, '    #    ## from dataframe to nested dict')
    (315, '    #    from collections import defaultdict')
    (316, '    #    d = defaultdict(dict)')
    (317, '    #    cols = [e for e in list(db.columns) if e not in ("SM", "LB", "ID")]')
    (318, '    #    for row in db.itertuples(index=False):')
    (319, '    #        for col in cols:')
    (320, '    #            d[row.SM][row.LB][row.ID][col] = row[col]')
    (321, '')
    (322, '    #    SAMPLES = dict(d)')
    (323, '')
    (324, '    ## do we have paired-end data or single-end data')
    (325, '    PAIRED_END = "Data1" in [')
    (326, '        name')
    (327, '        for sm in SAMPLES.values()')
    (328, '        for lb in sm.values()')
    (329, '        for id in lb.values()')
    (330, '        for name in id')
    (331, '    ]')
    (332, '')
    (333, '    ## test all fastq files')
    (334, '    if PAIRED_END:')
    (335, '        ## forward reads')
    (336, '        for fq in [')
    (337, '            id["Data1"]')
    (338, '            for sm in SAMPLES.values()')
    (339, '            for lb in sm.values()')
    (340, '            for id in lb.values()')
    (341, '        ]:')
    (342, '            if not os.path.isfile(fq):')
    (343, '                LOGGER.error(f"ERROR: Fastq file \\\'{fq}\\\' does not exist!")')
    (344, '                sys.exit(1)')
    (345, '')
    (346, '        ## reverse reads (may be NaN)')
    (347, '        for fq in [')
    (348, '            id["Data2"]')
    (349, '            for sm in SAMPLES.values()')
    (350, '            for lb in sm.values()')
    (351, '            for id in lb.values()')
    (352, '        ]:')
    (353, '            if fq == fq and not os.path.isfile(fq):')
    (354, '                LOGGER.error(f"ERROR: Fastq file \\\'{fq}\\\' does not exist!")')
    (355, '                sys.exit(1)')
    (356, '')
    (357, '    else:  ## single end')
    (358, '        for fq in [')
    (359, '            id["Data"]')
    (360, '            for sm in SAMPLES.values()')
    (361, '            for lb in sm.values()')
    (362, '            for id in lb.values()')
    (363, '        ]:')
    (364, '            if not os.path.isfile(fq):')
    (365, '                LOGGER.error(f"ERROR: Fastq file \\\'{fq}\\\' does not exist!")')
    (366, '                sys.exit(1)')
    (367, '')
    (368, '    ## test if collapsing is correctly set')
    (369, '    COLLAPSE = "--collapse" in recursive_get(')
    (370, '        ["adapterremoval", "params"], "--minlength 30 --trimns --trimqualities"')
    (371, '    )')
    (372, '    if not PAIRED_END and COLLAPSE:')
    (373, '        LOGGER.error(')
    (374, '            f"ERROR: config[adapterremoval][params] contains the parameter \\\'--collapse\\\', which is not compatible with SE data!"')
    (375, '        )')
    (376, '        sys.exit(1)')
    (377, '')
    (378, '    return SAMPLES, PAIRED_END, COLLAPSE, input')
    (379, '')
    (380, '')
    (381, '##########################################################################################')
    (382, '## read config[external_sample]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if external_sample == ""']
    (383, 'def get_external_samples():')
    (384, '    external_sample = recursive_get(["external_sample"], "")')
    (385, '    if external_sample == "":')
    (386, '        return {}, ""')
    (387, '')
    (388, '    if isinstance(external_sample, dict):')
    (389, '        samples_stats = external_sample')
    (390, '        EXTERNAL_SAMPLE_FILE = "yaml"')
    (391, '')
    (392, '    elif os.path.isfile(external_sample):')
    (393, '        delim = recursive_get(["delim"], "\\\\s+")')
    (394, '        db_stats = pd.read_csv(external_sample, sep=delim, comment="#", dtype=str)')
    (395, '        EXTERNAL_SAMPLE_FILE = external_sample')
    (396, '        colnames = ["SM", "Bam", "Genome"]')
    (397, '        if not set(colnames).issubset(db_stats.columns):')
    (398, '            LOGGER.error(')
    (399, '                f"ERROR: The column names in the bam file (given by paramter config[external_sample]) are wrong! Expected are {colnames}!"')
    (400, '            )')
    (401, '            sys.exit(1)')
    (402, '')
    (403, '        ## from dataframe to nested dict')
    (404, '        from collections import defaultdict')
    (405, '')
    (406, '        d = defaultdict(dict)')
    (407, '        for row in db_stats.itertuples(index=False):')
    (408, '            d[row.Genome][row.SM] = row.Bam')
    (409, '        samples_stats = dict(d)')
    (410, '')
    (411, '    else:')
    (412, '        LOGGER.error(')
    (413, '            f"ERROR: The argument of parameter config[external_sample] is not valide \\\'{external_sample}\\\'!"')
    (414, '        )')
    (415, '        sys.exit(1)')
    (416, '')
    (417, '    ## test for each GENOMES separetly')
    (418, '    for genome in list(samples_stats):')
    (419, '        ## does the GENOMES name exist?')
    (420, '        if genome not in GENOMES:')
    (421, '            LOGGER.error(')
    (422, '                f"ERROR: Genome name config[external_sample] does not exist ({genome})!"')
    (423, '            )')
    (424, '            sys.exit(1)')
    (425, '')
    (426, '        ## test if there are duplicated sample names')
    (427, '        if len(list(samples_stats[genome])) != len(set(list(samples_stats[genome]))):')
    (428, '            LOGGER.error(')
    (429, '                f"ERROR: Parameter config[external_sample][{genome}] contains duplicated sample names!"')
    (430, '            )')
    (431, '            sys.exit(1)')
    (432, '')
    (433, '        ## test each bam file')
    (434, '        for bam in list(samples_stats[genome].values()):')
    (435, '            if not os.path.isfile(bam):')
    (436, '                LOGGER.error(')
    (437, '                    f"ERROR: Bam file config[external_sample][genome][{s}][bam] does not exist ({bam})!"')
    (438, '                )')
    (439, '                sys.exit(1)')
    (440, '')
    (441, '    return samples_stats, EXTERNAL_SAMPLE_FILE')
    (442, '')
    (443, '')
    (444, '##########################################################################################')
    (445, '## all functions for main snakemake file')
    (446, '')
    (447, '## get recursively the argument of the given keys in a nested dict (run eval())')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if len(keys) == 1']
    (448, 'def recursive_get(keys, def_value, my_dict=config):')
    (449, '    assert type(keys) is list')
    (450, '    first = keys[0]')
    (451, '    if len(keys) == 1:')
    (452, '        value = my_dict.get(first, def_value)')
    (453, '    else:')
    (454, '        value = recursive_get(keys[1:], def_value, my_dict=my_dict.get(first, {}))')
    (455, '    return eval_param(value)')
    (456, '    #return value')
    (457, '')
    (458, '')
    (459, "## same as above, but the argument is tested if it is present in the \\'available_args\\'")
    (460, "## first argument of \\'available_args\\' is the default")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if arg not in available_args']
    (461, 'def recursive_get_and_test(key, available_args, my_dict=config):')
    (462, '    arg = str(recursive_get(key, available_args[0], my_dict))')
    (463, '    if arg not in available_args:')
    (464, '        LOGGER.error(')
    (465, '            f"ERROR: The parameter config[{\\\'][\\\'.join(key)}] has no valid argument (currently \\\'{arg}\\\'; available {available_args})!"')
    (466, '        )')
    (467, '        sys.exit(1)')
    (468, '    return arg')
    (469, '')
    (470, '')
    (471, '## update the argument in a nested dict (and create leaves if needed)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if len(keys) == 1']
    (472, 'def update_value(keys, value, my_dict=config):')
    (473, '    key = keys[0]')
    (474, '    if len(keys) == 1:')
    (475, '        new_dict = {}')
    (476, '        new_dict[key] = value')
    (477, '        return new_dict')
    (478, '    else:')
    (479, '        if key in my_dict:')
    (480, '            new_dict = update_value(keys[1:], value, my_dict[key])')
    (481, '            my_dict[key].update(new_dict)')
    (482, '            return my_dict')
    (483, '        else:')
    (484, '            my_dict[key] = {}')
    (485, '            new_dict = update_value(keys[1:], value, my_dict[key])')
    (486, '            my_dict[key].update(new_dict)')
    (487, '            return my_dict')
    (488, '')
    (489, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['try', 'if type(x) is str']
    (510, 'def eval_elem(x):')
    (511, '    try:')
    (512, '        if type(x) is str:')
    (513, '            return eval(x, globals=None, locals=None)')
    (514, '        else:')
    (515, '            return x')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if type(x) is list']
    (520, 'def eval_param(x):')
    (521, '    if type(x) is list:')
    (522, '        return list(map(eval_elem, x))')
    (523, '    elif type(x) is str:')
    (524, '        return eval_elem(x)')
    (525, '    else:')
    (526, '        return x')
    (527, '')
    (528, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if type(x) is list']
    (529, 'def to_str(x):')
    (530, '    if type(x) is list:')
    (531, '        #return [str(i) for i in x]')
    (532, '        return list(map(str, x))')
    (533, '    elif type(x) is int:')
    (534, '        return str(x)')
    (535, '    elif type(x) is float:')
    (536, '        return str(x)')
    (537, '    else:')
    (538, '        return x')
    (539, '')
    (540, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if type(x) is list']
    (541, 'def to_list(x):')
    (542, '    if type(x) is list:')
    (543, '        return x')
    (544, '    return list(to_str(x).split(" "))')
    (545, '')
    (546, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if not os.path.isfile(fasta)']
    (603, 'def set_chromosome_names(genome):')
    (604, '    ## test if fasta is valid')
    (605, '    fasta = recursive_get(["genome", genome], "")')
    (606, '    if not os.path.isfile(fasta):')
    (607, '        LOGGER.error(')
    (608, '            f"ERROR: Reference genome config[{genome}] does not exist ({fasta})!"')
    (609, '        )')
    (610, '')
    (611, '    ## get all chromosome names from the reference genome')
    (612, '    if pathlib.Path(f"{fasta}.fai").exists():')
    (613, '        allChr = list(')
    (614, '            map(str, pd.read_csv(f"{fasta}.fai", header=None, sep="\\\\t")[0].tolist())')
    (615, '        )')
    (616, '    elif pathlib.Path(')
    (617, '        f"{RESULT_DIR}/00_reference/{genome}/{genome}.fasta.fai"')
    (618, '    ).exists():')
    (619, '        fasta = f"{RESULT_DIR}/00_reference/{genome}/{genome}.fasta"')
    (620, '        allChr = list(')
    (621, '            map(str, pd.read_csv(f"{fasta}.fai", header=None, sep="\\\\t")[0].tolist())')
    (622, '        )')
    (623, '    else:')
    (624, '        cmd = f"grep \\\'^>\\\' {fasta} | cut -c2- | awk \\\'{{print $1}}\\\'"')
    (625, '        allChr = list(')
    (626, '            map(str, subprocess.check_output(cmd, shell=True, text=True).split())')
    (627, '        )')
    (628, '    config = update_value(["chromosome", genome, "all"], allChr)')
    (629, '')
    (630, '')
    (631, '## return a list of the chromosome names which do not match')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if list(set(names) - set(allChr))']
    (632, 'def valid_chromosome_names(genome, names):')
    (633, '    allChr = to_str(recursive_get(["chromosome", genome, "all"], []))')
    (634, '    if list(set(names) - set(allChr)):')
    (635, '        return list(set(names) - set(allChr))')
    (636, '    else:')
    (637, '        return []')
    (638, '')
    (639, '')
    (640, '## check the chromosome names if they are valid')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if sorted(allChr) == sorted(hg19)']
    (641, 'def set_sex_inference(genome):')
    (642, '    ## get the chromosome names of the given genome')
    (643, '    allChr = get_chromosome_names(genome)')
    (644, '')
    (645, '    ## try to recognize if it is the human hg19 or GRCh38 genome. If so apply default chromosome names')
    (646, '    hg19 = list(map(str, list(range(1, 23)) + ["X", "Y", "MT"]))')
    (647, '    GRCh38 = [f"str{x}" for x in list(range(1, 23)) + ["X", "Y", "M"]]')
    (648, '    if sorted(allChr) == sorted(hg19):')
    (649, '        name = "hg19"')
    (650, '        detectedSexChrom = ["X", "Y", "MT"]')
    (651, '        detectedAutosomes = list(set(allChr) - set(detectedSexChrom))')
    (652, '    elif sorted(allChr) == sorted(GRCh38):')
    (653, '        name = "GRCh38"')
    (654, '        detectedSexChrom = ["chrX", "chrY", "chrM"]')
    (655, '        detectedAutosomes = list(set(allChr) - set(detectedSexChrom))')
    (656, '    else:')
    (657, '        name = ""')
    (658, '        detectedSexChrom = []')
    (659, '        detectedAutosomes = []')
    (660, '')
    (661, '    ## write the sex and autosome names')
    (662, '    config = update_value(["chromosome", genome, "name"], name)')
    (663, '    config = update_value(["chromosome", genome, "all_sex_chr"], detectedSexChrom)')
    (664, '    config = update_value(["chromosome", genome, "sex_chr"], detectedSexChrom[0])')
    (665, '    config = update_value(["chromosome", genome, "autosomes"], detectedAutosomes)')
    (666, '')
    (667, '    # check if the chromosomes specified in sex determination exist')
    (668, '    ## X chromosome')
    (669, '    sex_chr = to_str(recursive_get(["sex_inference", genome, "sex_chr"], []))')
    (670, '    if len(sex_chr):')
    (671, '        if valid_chromosome_names(genome, sex_chr):')
    (672, '            LOGGER.error(')
    (673, '                f"ERROR: Sex chromosome specified in config[sex_inference][{genome}][sex_chr] ({sex_chr}) does not exist in the reference genome."')
    (674, '            )')
    (675, '            os._exit(1)')
    (676, '        config = update_value(["chromosome", genome, "sex_chr"], sex_chr)')
    (677, '')
    (678, '    # autosomes')
    (679, '    autosomes = to_str(recursive_get(')
    (680, '            ["sex_inference", genome, "autosomes"],')
    (681, '            [],')
    (682, '        )')
    (683, '    )')
    (684, '')
    (685, '    if len(autosomes):')
    (686, '        if valid_chromosome_names(genome, autosomes):')
    (687, '            LOGGER.error(')
    (688, '                f"ERROR: In config[sex_inference][{genome}][autosomes], the following chromosome names are not recognized: {valid_chromosome_names(genome, autosomes)}!"')
    (689, '            )')
    (690, '            os._exit(1)')
    (691, '        config = update_value(["chromosome", genome, "autosomes"], autosomes)')
    (692, '')
    (693, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if valid_chromosome_names(genome, depth)']
    (694, 'def read_depth(genome):')
    (695, '    # check if chromosomes for which DoC was requested exist')
    (696, '    depth = to_str(recursive_get(["depth", genome, "chromosomes"], ""))')
    (697, '    if valid_chromosome_names(genome, depth):')
    (698, '        LOGGER.error(')
    (699, '            f"ERROR: config[depth][{genome}][chromosomes] contains unrecognized chromosome names ({valid_chromosome_names(genome, depth)})!"')
    (700, '        )')
    (701, '        os._exit(1)')
    (702, '    config = update_value(["chromosome", genome, "depth"], depth)')
    (703, '')
    (704, '')
    (705, '## convert string to boolean')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if type(v) is list']
    (711, 'def str2list(v):')
    (712, '    if type(v) is list:')
    (713, '        return [str(x) for x in v]')
    (714, '    else:')
    (715, '        return [str(v)]')
    (716, '')
    (717, '')
    (718, '## get incremental memory allocation when jobs fail')
    (719, "## \\'startStr\\' is the first memory allocation in GB")
    (720, '## input is in GB; output in MB; default is 2GB, but can be changed by a rule')
    (721, '')
    (722, '## get incremental memory allocation when jobs fail')
    (723, "## \\'start\\' is the first memory allocation in GB (default 4GB)")
    (724, '## input is in GB; output is in MB;')
    (725, '## global variable memory_increment_ratio defines by how much (ratio) the memory is increased if not defined specifically')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if type(moduleList) is not list']
    (726, 'def get_memory_alloc(module, attempt, default=2):')
    (727, '    moduleList = module')
    (728, '    if type(moduleList) is not list:')
    (729, '        moduleList = [module]')
    (730, '    mem_start = int(recursive_get(moduleList + ["mem"], default))')
    (731, '    mem_incre = int(')
    (732, '        recursive_get(')
    (733, '            moduleList + ["mem_increment"], memory_increment_ratio * mem_start')
    (734, '        )')
    (735, '    )')
    (736, '    return int(1024 * ((attempt - 1) * mem_incre + mem_start))')
    (737, '')
    (738, '')
    (739, "## in this second version the \\'mem\\' is added to the word of the last element")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if type(moduleList) is not list']
    (740, 'def get_memory_alloc2(module, attempt, default=2):')
    (741, '    moduleList = module')
    (742, '    if type(moduleList) is not list:')
    (743, '        moduleList = [moduleList]')
    (744, '    mem_start = int(recursive_get(moduleList[:-1] + [moduleList[-1] + "_mem"], default))')
    (745, '    mem_incre = int(')
    (746, '        recursive_get(')
    (747, '            moduleList[:-1] + [moduleList[-1] + "_mem_increment"],')
    (748, '            memory_increment_ratio * mem_start,')
    (749, '        )')
    (750, '    )')
    (751, '    return int(1024 * ((attempt - 1) * mem_incre + mem_start))')
    (752, '')
    (753, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if type(moduleList) is not list']
    (768, 'def get_runtime_alloc(module, attempt, default=12):')
    (769, '    moduleList = module')
    (770, '    if type(moduleList) is not list:')
    (771, '        moduleList = [module]')
    (772, '    time_start = int(recursive_get(moduleList + ["time"], default))')
    (773, '    time_incre = int(')
    (774, '        recursive_get(')
    (775, '            moduleList + ["time_increment"], runtime_increment_ratio * time_start')
    (776, '        )')
    (777, '    )')
    (778, '    return int(60 * ((attempt - 1) * time_incre + time_start))')
    (779, '')
    (780, '')
    (781, "## in this second version the \\'time\\' is added to the word of the last element")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if type(moduleList) is not list']
    (782, 'def get_runtime_alloc2(module, attempt, default=12):')
    (783, '    moduleList = module')
    (784, '    if type(moduleList) is not list:')
    (785, '        moduleList = [module]')
    (786, '    time_start = int(')
    (787, '        recursive_get(moduleList[:-1] + [moduleList[-1] + "_time"], default)')
    (788, '    )')
    (789, '    time_incre = int(')
    (790, '        recursive_get(')
    (791, '            moduleList[:-1] + [moduleList[-1] + "_time_increment"],')
    (792, '            runtime_increment_ratio * time_start,')
    (793, '        )')
    (794, '    )')
    (795, '    return int(60 * ((attempt - 1) * time_incre + time_start))')
    (796, '')
    (797, '')
    (798, '## get the number of threads of the given parameter')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if type(moduleList) is not list']
    (799, 'def get_threads(module, default=1):')
    (800, '    moduleList = module')
    (801, '    if type(moduleList) is not list:')
    (802, '        moduleList = [module]')
    (803, '    return int(recursive_get(moduleList + ["threads"], default))')
    (804, '')
    (805, '')
    (806, "## in this second version the \\'threads\\' is added to the word of the last element")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if type(moduleList) is not list']
    (807, 'def get_threads2(module, default=1):')
    (808, '    moduleList = module')
    (809, '    if type(moduleList) is not list:')
    (810, '        moduleList = [module]')
    (811, '    return int(recursive_get(moduleList[:-1] + [moduleList[-1] + "_threads"], default))')
    (812, '')
    (813, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if bin[-4']
    (821, 'def get_picard_bin():')
    (822, '    bin = recursive_get(["software", "picard_jar"], "picard.jar")')
    (823, '    if bin[-4:] == ".jar":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/scripts/utils.py
context_key: ['if bin[-4']
    (829, 'def get_gatk_bin():')
    (830, '    bin = recursive_get(["software", "gatk3_jar"], "GenomeAnalysisTK.jar")')
    (831, '    if bin[-4:] == ".jar":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/imputation_glimpse.smk
context_key: ['if return_str']
    (55, 'def get_num_chunks(wildcards, return_str=False):')
    (56, '    chunk_file = checkpoints.glimpse_chunk.get(**wildcards).output[0]')
    (57, '    myCommand = f"wc -l {chunk_file}"')
    (58, '    proc = subprocess.Popen(myCommand, stdout=subprocess.PIPE, shell=True)')
    (59, '    (out, err) = proc.communicate()')
    (60, '    n_chunks = int(out.decode().split()[0]) + 1')
    (61, '    if return_str:')
    (62, '        n_chunks = str(n_chunks)')
    (63, '    return n_chunks')
    (64, '')
    (65, '# -----------------------------------------------------------------------------#')
    (66, '')
    (67, '')
    (68, '# rules definitions')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if "_R1" == wc.id[-3']
    (4, 'def get_fastq_of_ID(wc):')
    (5, '    # print(f"get_fastq_of_ID: {wc}")')
    (6, '    if "_R1" == wc.id[-3:]:')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['elif "_R2" == wc.id[-3']
    (4, 'def get_fastq_of_ID(wc):')
    (5, '    # print(f"get_fastq_of_ID: {wc}")')
    (6, '    if "_R1" == wc.id[-3:]:')
    (7, '        filename = SAMPLES[wc.sm][wc.lb][wc.id[:-3]]["Data1"]')
    (8, '    elif "_R2" == wc.id[-3:]:')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['elif PAIRED_END']
    (4, 'def get_fastq_of_ID(wc):')
    (5, '    # print(f"get_fastq_of_ID: {wc}")')
    (6, '    if "_R1" == wc.id[-3:]:')
    (7, '        filename = SAMPLES[wc.sm][wc.lb][wc.id[:-3]]["Data1"]')
    (8, '    elif "_R2" == wc.id[-3:]:')
    (9, '        filename = SAMPLES[wc.sm][wc.lb][wc.id[:-3]]["Data2"]')
    (10, '    elif PAIRED_END:')
    (11, '        # elif PAIRED_END != 0:  ## SE library in a paired-end sample file')
    (12, '        filename = SAMPLES[wc.sm][wc.lb][wc.id]["Data1"]')
    (13, '    else:')
    (14, '        filename = SAMPLES[wc.sm][wc.lb][wc.id]["Data"]')
    (15, '    return filename')
    (16, '')
    (17, '')
    (18, '## get the fastq file(s) used for mapping (output is a list)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if rem_adapt == ""']
    (19, 'def get_fastq_4_mapping(wc, rem_adapt=""):')
    (20, '    # print(f"get_fastq_4_mapping: {wc}")')
    (21, '    if rem_adapt == "":  ## if not set take the global setting')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if rem_adapt', 'if not PAIRED_END']
    (19, 'def get_fastq_4_mapping(wc, rem_adapt=""):')
    (20, '    # print(f"get_fastq_4_mapping: {wc}")')
    (21, '    if rem_adapt == "":  ## if not set take the global setting')
    (22, '        rem_adapt = run_adapter_removal')
    (23, '')
    (24, '    if rem_adapt:')
    (25, '        folder = f"{wc.folder}/01_fastq/01_trimmed/01_adapter_removal/{wc.sm}/{wc.lb}"')
    (26, '        if not PAIRED_END:')
    (27, '            filename = [f"{folder}/{wc.id}.fastq.gz"]')
    (28, '        elif COLLAPSE:')
    (29, '            filename = [rules.adapter_removal_collapse.output.R]')
    (30, '        else:')
    (31, '            # if str(SAMPLES[wc.sm][wc.lb][wc.id]["Data2"]) == "nan":')
    (32, '            data2 = recursive_get(')
    (33, '                [wc.sm, wc.lb, wc.id, "Data2"],')
    (34, '                "nan",')
    (35, '                my_dict=SAMPLES,')
    (36, '            )')
    (37, '            if data2 != data2:')
    (38, '                # single-end files')
    (39, '                filename = [f"{folder}/{wc.id}.fastq.gz"]')
    (40, '            else:')
    (41, '                # paired-end files, not collapsing')
    (42, '                filename = [')
    (43, '                    f"{folder}/{wc.id}_R1.fastq.gz",')
    (44, '                    f"{folder}/{wc.id}_R2.fastq.gz",')
    (45, '                ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if rem_adapt', 'else', 'if not PAIRED_END']
    (19, 'def get_fastq_4_mapping(wc, rem_adapt=""):')
    (20, '    # print(f"get_fastq_4_mapping: {wc}")')
    (21, '    if rem_adapt == "":  ## if not set take the global setting')
    (22, '        rem_adapt = run_adapter_removal')
    (23, '')
    (24, '    if rem_adapt:')
    (25, '        folder = f"{wc.folder}/01_fastq/01_trimmed/01_adapter_removal/{wc.sm}/{wc.lb}"')
    (26, '        if not PAIRED_END:')
    (27, '            filename = [f"{folder}/{wc.id}.fastq.gz"]')
    (28, '        elif COLLAPSE:')
    (29, '            filename = [rules.adapter_removal_collapse.output.R]')
    (30, '        else:')
    (31, '            # if str(SAMPLES[wc.sm][wc.lb][wc.id]["Data2"]) == "nan":')
    (32, '            data2 = recursive_get(')
    (33, '                [wc.sm, wc.lb, wc.id, "Data2"],')
    (34, '                "nan",')
    (35, '                my_dict=SAMPLES,')
    (36, '            )')
    (37, '            if data2 != data2:')
    (38, '                # single-end files')
    (39, '                filename = [f"{folder}/{wc.id}.fastq.gz"]')
    (40, '            else:')
    (41, '                # paired-end files, not collapsing')
    (42, '                filename = [')
    (43, '                    f"{folder}/{wc.id}_R1.fastq.gz",')
    (44, '                    f"{folder}/{wc.id}_R2.fastq.gz",')
    (45, '                ]')
    (46, '    else:')
    (47, '        folder = f"{wc.folder}/01_fastq/00_reads/01_files_orig/{wc.sm}/{wc.lb}"')
    (48, '        if not PAIRED_END:')
    (49, '            filename = [f"{folder}/{wc.id}.fastq.gz"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if rem_adapt', 'else', 'else']
    (19, 'def get_fastq_4_mapping(wc, rem_adapt=""):')
    (20, '    # print(f"get_fastq_4_mapping: {wc}")')
    (21, '    if rem_adapt == "":  ## if not set take the global setting')
    (22, '        rem_adapt = run_adapter_removal')
    (23, '')
    (24, '    if rem_adapt:')
    (25, '        folder = f"{wc.folder}/01_fastq/01_trimmed/01_adapter_removal/{wc.sm}/{wc.lb}"')
    (26, '        if not PAIRED_END:')
    (27, '            filename = [f"{folder}/{wc.id}.fastq.gz"]')
    (28, '        elif COLLAPSE:')
    (29, '            filename = [rules.adapter_removal_collapse.output.R]')
    (30, '        else:')
    (31, '            # if str(SAMPLES[wc.sm][wc.lb][wc.id]["Data2"]) == "nan":')
    (32, '            data2 = recursive_get(')
    (33, '                [wc.sm, wc.lb, wc.id, "Data2"],')
    (34, '                "nan",')
    (35, '                my_dict=SAMPLES,')
    (36, '            )')
    (37, '            if data2 != data2:')
    (38, '                # single-end files')
    (39, '                filename = [f"{folder}/{wc.id}.fastq.gz"]')
    (40, '            else:')
    (41, '                # paired-end files, not collapsing')
    (42, '                filename = [')
    (43, '                    f"{folder}/{wc.id}_R1.fastq.gz",')
    (44, '                    f"{folder}/{wc.id}_R2.fastq.gz",')
    (45, '                ]')
    (46, '    else:')
    (47, '        folder = f"{wc.folder}/01_fastq/00_reads/01_files_orig/{wc.sm}/{wc.lb}"')
    (48, '        if not PAIRED_END:')
    (49, '            filename = [f"{folder}/{wc.id}.fastq.gz"]')
    (50, '        else:')
    (51, '            data2 = recursive_get(')
    (52, '                [wc.sm, wc.lb, wc.id, "Data2"],')
    (53, '                "nan",')
    (54, '                my_dict=SAMPLES,')
    (55, '            )')
    (56, '            # checking a single-end file')
    (57, '            if data2 != data2:')
    (58, '                filename = [f"{folder}/{wc.id}.fastq.gz"]')
    (59, '            else:')
    (60, '                filename = [')
    (61, '                    f"{folder}/{wc.id}_R1.fastq.gz",')
    (62, '                    f"{folder}/{wc.id}_R2.fastq.gz",')
    (63, '                ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if rem_adapt']
    (19, 'def get_fastq_4_mapping(wc, rem_adapt=""):')
    (20, '    # print(f"get_fastq_4_mapping: {wc}")')
    (21, '    if rem_adapt == "":  ## if not set take the global setting')
    (22, '        rem_adapt = run_adapter_removal')
    (23, '')
    (24, '    if rem_adapt:')
    (25, '        folder = f"{wc.folder}/01_fastq/01_trimmed/01_adapter_removal/{wc.sm}/{wc.lb}"')
    (26, '        if not PAIRED_END:')
    (27, '            filename = [f"{folder}/{wc.id}.fastq.gz"]')
    (28, '        elif COLLAPSE:')
    (29, '            filename = [rules.adapter_removal_collapse.output.R]')
    (30, '        else:')
    (31, '            # if str(SAMPLES[wc.sm][wc.lb][wc.id]["Data2"]) == "nan":')
    (32, '            data2 = recursive_get(')
    (33, '                [wc.sm, wc.lb, wc.id, "Data2"],')
    (34, '                "nan",')
    (35, '                my_dict=SAMPLES,')
    (36, '            )')
    (37, '            if data2 != data2:')
    (38, '                # single-end files')
    (39, '                filename = [f"{folder}/{wc.id}.fastq.gz"]')
    (40, '            else:')
    (41, '                # paired-end files, not collapsing')
    (42, '                filename = [')
    (43, '                    f"{folder}/{wc.id}_R1.fastq.gz",')
    (44, '                    f"{folder}/{wc.id}_R2.fastq.gz",')
    (45, '                ]')
    (46, '    else:')
    (47, '        folder = f"{wc.folder}/01_fastq/00_reads/01_files_orig/{wc.sm}/{wc.lb}"')
    (48, '        if not PAIRED_END:')
    (49, '            filename = [f"{folder}/{wc.id}.fastq.gz"]')
    (50, '        else:')
    (51, '            data2 = recursive_get(')
    (52, '                [wc.sm, wc.lb, wc.id, "Data2"],')
    (53, '                "nan",')
    (54, '                my_dict=SAMPLES,')
    (55, '            )')
    (56, '            # checking a single-end file')
    (57, '            if data2 != data2:')
    (58, '                filename = [f"{folder}/{wc.id}.fastq.gz"]')
    (59, '            else:')
    (60, '                filename = [')
    (61, '                    f"{folder}/{wc.id}_R1.fastq.gz",')
    (62, '                    f"{folder}/{wc.id}_R2.fastq.gz",')
    (63, '                ]')
    (64, '    return filename')
    (65, '')
    (66, '')
    (67, '## fastqc may be run on the original or trimmed fastq files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if "trim" in wc.type']
    (68, 'def inputs_fastqc(wc):')
    (69, '    if "trim" in wc.type:')
    (70, '        return get_fastq_4_mapping(wc, True)')
    (71, '    else:')
    (72, '        return get_fastq_4_mapping(wc, False)')
    (73, '')
    (74, '')
    (75, '## get the bam file used for sorting')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if mapper == "bwa_aln"']
    (76, 'def get_bam_4_sorting(wc):')
    (77, '    # print(f"get_bam_for_sorting: {wc}")')
    (78, '    if mapper == "bwa_aln":')
    (79, '        if (')
    (80, '            not COLLAPSE')
    (81, '            # and str(SAMPLES[wc.sm][wc.lb][wc.id]["Data2"]) != "nan"')
    (82, '            and str(')
    (83, '                recursive_get(')
    (84, '                    [wc.sm, wc.lb, wc.id, "Data2"],')
    (85, '                    "nan",')
    (86, '                    my_dict=SAMPLES,')
    (87, '                )')
    (88, '            )')
    (89, '            != "nan"')
    (90, '        ):')
    (91, '            folder = "02_bwa_sampe"')
    (92, '        else:')
    (93, '            folder = "02_bwa_samse"')
    (94, '    elif mapper == "bwa_mem":')
    (95, '        folder = "02_bwa_mem"')
    (96, '    elif mapper == "bowtie2":')
    (97, '        folder = "02_bowtie2"')
    (98, '    else:')
    (99, '        LOGGER.error(')
    (100, '            f"ERROR: The parameter config[mapping][mapper] is not correctly specified: {mapper} is unknown!"')
    (101, '        )')
    (102, '        os._exit(1)')
    (103, '    return f"{wc.folder}/01_fastq/02_mapped/{folder}/{wc.sm}/{wc.lb}/{wc.id}.{wc.genome}.bam"')
    (104, '')
    (105, '')
    (106, '## get the final bam file of at the FASTQ level')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if run_filtering']
    (107, 'def get_final_bam_FASTQ(wc):')
    (108, '    if run_filtering:')
    (109, '        file = f"{wc.folder}/01_fastq/03_filtered/01_bam_filter/{wc.sm}/{wc.lb}/{wc.id}.{wc.genome}.bam"')
    (110, '    else:')
    (111, '        file = f"{wc.folder}/01_fastq/02_mapped/03_bam_sort/{wc.sm}/{wc.lb}/{wc.id}.{wc.genome}.bam"')
    (112, '    # print(f"get_bam_4_final_fastq: {file}")')
    (113, '    return file')
    (114, '')
    (115, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if remove_duplicates == "markduplicates"']
    (165, 'def get_bam_4_damage(wc):')
    (166, '    if remove_duplicates == "markduplicates":')
    (167, '        bam = f"{wc.folder}/02_library/01_duplicated/01_markduplicates/{wc.sm}/{wc.lb}.{wc.genome}.bam"')
    (168, '    elif remove_duplicates == "dedup":')
    (169, '        bam = f"{wc.folder}/02_library/01_duplicated/01_dedup/{wc.sm}/{wc.lb}.{wc.genome}.bam"')
    (170, '    else:')
    (171, '        bam = get_bam_4_markduplicates(wc)')
    (172, '    return bam')
    (173, '')
    (174, '')
    (175, '## get the final bam files at the library level')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if run_damage_rescale']
    (176, 'def get_final_bam_LB(wc):')
    (177, '    if run_damage_rescale:')
    (178, '        file = f"{wc.folder}/02_library/02_rescaled/01_mapDamage/{wc.sm}/{wc.lb}.{wc.genome}.bam"')
    (179, '    else:')
    (180, '        file = get_bam_4_damage(wc)')
    (181, '    return file')
    (182, '')
    (183, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if run_realign']
    (232, 'def get_bam_4_samtools_calmd(wc):')
    (233, '    if run_realign:')
    (234, '        return f"{wc.folder}/03_sample/01_realigned/01_realign/{wc.sm}.{wc.genome}.bam"')
    (235, '    else:')
    (236, '        return get_bam_4_realign(wc)')
    (237, '')
    (238, '')
    (239, '## get the final bam files at the sample level')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if is_external_sample(wc.sm, wc.genome)']
    (240, 'def get_bam_4_final_bam(wc):')
    (241, '    if is_external_sample(wc.sm, wc.genome):')
    (242, '        return EXTERNAL_SAMPLES[wc.genome][wc.sm]')
    (243, '    if run_compute_md:')
    (244, '        return f"{wc.folder}/03_sample/02_md_flag/01_md_flag/{wc.sm}.{wc.genome}.bam"')
    (245, '    return get_bam_4_samtools_calmd(wc)')
    (246, '')
    (247, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if run_damage == "bamdamage"']
    (256, 'def get_damage_output():')
    (257, '    if run_damage == "bamdamage":')
    (258, '        files = [')
    (259, '            f"{RESULT_DIR}/04_stats/01_sparse_stats/02_library/04_bamdamage/{sm}/{lb}.{genome}.{type}.{ext}"')
    (260, '            for sm in SAMPLES')
    (261, '            for lb in SAMPLES[sm]')
    (262, '            for type in ["dam", "length"]')
    (263, '            for ext in ["pdf", "svg"]')
    (264, '            for genome in GENOMES')
    (265, '        ]')
    (266, '    elif run_damage == "mapDamage":')
    (267, '        files = [')
    (268, '            f"{{RESULT_DIR}}/04_stats/01_sparse_stats/02_library/04_mapDamage/{sm}/{lb}.{genome}_results_mapDamage/Fragmisincorporation_plot.pdf"')
    (269, '            for sm in SAMPLES')
    (270, '            for lb in SAMPLES[sm]')
    (271, '            for genome in GENOMES')
    (272, '        ]')
    (273, '    else:  ## if False')
    (274, '        files = []')
    (275, '    return files')
    (276, '')
    (277, '')
    (278, '##########################################################################################')
    (279, '## STATS')
    (280, '## get all individual stat table files to concatenate')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if wc.level == "FASTQ"']
    (281, 'def path_stats_by_level(wc):')
    (282, '    if wc.level == "FASTQ":')
    (283, '        paths = [')
    (284, '            f"{wc.folder}/04_stats/02_separate_tables/{wc.genome}/{sm}/{lb}/{id}/fastq_stats.csv"')
    (285, '            for sm in SAMPLES')
    (286, '            for lb in SAMPLES[sm]')
    (287, '            for id in SAMPLES[sm][lb]')
    (288, '        ]')
    (289, '    elif wc.level == "LB":')
    (290, '        paths = [')
    (291, '            f"{wc.folder}/04_stats/02_separate_tables/{wc.genome}/{sm}/{lb}/library_stats.csv"')
    (292, '            for sm in SAMPLES')
    (293, '            for lb in SAMPLES[sm]')
    (294, '        ]')
    (295, '    elif wc.level == "SM":')
    (296, '        paths = [')
    (297, '            f"{wc.folder}/04_stats/02_separate_tables/{wc.genome}/{sm}/sample_stats.csv"')
    (298, '            for sm in SAMPLES')
    (299, '        ]')
    (300, '    else:')
    (301, '        LOGGER.error(')
    (302, '            f"ERROR: def path_stats_by_level({wc.level}): should never happen!"')
    (303, '        )')
    (304, '        os._exit(1)')
    (305, '    return paths')
    (306, '')
    (307, '')
    (308, '## get the corresponding bam file (generic, across all levels):')
    (309, '##  - if final bam file: get the corresponding final bam file at the different levels')
    (310, '##  - otherwise retake the path')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if paths[1] == "04_final_fastq"']
    (311, 'def get_bam_file(wc):')
    (312, '    paths = list(pathlib.Path(wc.file).parts)')
    (313, '    if paths[1] == "04_final_fastq":  ## fastq')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['elif paths[1] == "03_final_library"']
    (311, 'def get_bam_file(wc):')
    (312, '    paths = list(pathlib.Path(wc.file).parts)')
    (313, '    if paths[1] == "04_final_fastq":  ## fastq')
    (314, '        file = get_final_bam_FASTQ(')
    (315, '            Wildcards(')
    (316, '                fromdict={')
    (317, '                    "folder": wc.folder,')
    (318, '                    "sm": paths[3],')
    (319, '                    "lb": paths[4],')
    (320, '                    "id": paths[5],')
    (321, '                    "genome": wc.genome,')
    (322, '                }')
    (323, '            )')
    (324, '        )')
    (325, '    elif paths[1] == "03_final_library":  ## library')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if str2bool']
    (343, 'def get_sex_file(wc):')
    (344, '    if str2bool(')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if save_low_qua']
    (372, 'def get_final_bam_low_qual_files():')
    (373, '    final_bam_low_qual = [')
    (374, '        f"{RESULT_DIR}/03_sample/03_final_sample/01_bam_low_qual/{sm}.{genome}.bam"')
    (375, '        for sm in SAMPLES')
    (376, '        for genome in GENOMES')
    (377, '        if save_low_qual')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if run_adapter_remova']
    (404, 'def get_fastqc_files():')
    (405, '    fastqc = [')
    (406, '        f"{RESULT_DIR}/04_stats/01_sparse_stats/01_fastq/00_reads/01_files_orig/{sm}/{lb}/{id}_fastqc.zip"')
    (407, '        for sm in SAMPLES')
    (408, '        for lb in SAMPLES[sm]')
    (409, '        for id in SAMPLES[sm][lb]')
    (410, '    ] + [')
    (411, '        f"{RESULT_DIR}/04_stats/01_sparse_stats/01_fastq/01_trimmed/01_adapter_removal/{sm}/{lb}/{id}_fastqc.zip"')
    (412, '        for sm in SAMPLES')
    (413, '        for lb in SAMPLES[sm]')
    (414, '        for id in SAMPLES[sm][lb]')
    (415, '        if run_adapter_removal')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if run_qualima']
    (475, 'def get_qualimap_files():')
    (476, '    qualimap_files = [')
    (477, '        f"{RESULT_DIR}/04_stats/01_sparse_stats/03_sample/03_final_sample/01_bam/{sm}.{genome}_qualimap"')
    (478, '        for genome in GENOMES')
    (479, '        for sm in SAMPLES')
    (480, '        if run_qualimap')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if str2bool(recursive_get(["sex_inference", genome, "run"], "False")']
    (485, 'def get_sex_files():')
    (486, '    sex_files = []')
    (487, '    for genome in GENOMES:')
    (488, '        ext = (')
    (489, '            "sex"')
    (490, '            if str2bool(recursive_get(["sex_inference", genome, "run"], "False"))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['else "nosex']
    (485, 'def get_sex_files():')
    (486, '    sex_files = []')
    (487, '    for genome in GENOMES:')
    (488, '        ext = (')
    (489, '            "sex"')
    (490, '            if str2bool(recursive_get(["sex_inference", genome, "run"], "False"))')
    (491, '            else "nosex"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if run_multiq']
    (501, 'def get_multiqc_files():')
    (502, '    multiqc_files = [')
    (503, '        f"{RESULT_DIR}/04_stats/02_separate_tables/{genome}/multiqc_mapache.html"')
    (504, '        for genome in GENOMES')
    (505, '        if run_multiqc')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if str2bool(recursive_get_and_test(["imputation", genome, "run"], ["False", "True"]))']
    (510, 'def get_imputation_files():')
    (511, '    files = []')
    (512, '    for genome in GENOMES:')
    (513, '        if str2bool(recursive_get_and_test(["imputation", genome, "run"], ["False", "True"])):')
    (514, '            files = files + [')
    (515, '                f"{RESULT_DIR}/03_sample/04_imputed/07_glimpse_sampled/{sm}.{genome}_gp{GP}.{ext}"')
    (516, '                for sm in SAMPLES')
    (517, '                for GP in str2list(recursive_get(["imputation", genome, "gp_filter"], "[0.8]"))')
    (518, '                for ext in ["bcf", "bcf.csi"]')
    (519, '            ] + [')
    (520, '                f"{RESULT_DIR}/03_sample/04_imputed/07_glimpse_sampled/unphased/{sm}.{genome}_gp.txt"')
    (521, '                for sm in SAMPLES')
    (522, '            ]')
    (523, '    return files')
    (524, '')
    (525, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if str2bool(recursive_get_and_test(["imputation", genome, "run"], ["False", "True"]))']
    (526, 'def get_imputation_files_external():')
    (527, '    samples_ = [sm for gen in EXTERNAL_SAMPLES for sm in EXTERNAL_SAMPLES[gen]]')
    (528, '    files = []')
    (529, '    for genome in GENOMES:')
    (530, '        if str2bool(recursive_get_and_test(["imputation", genome, "run"], ["False", "True"])):')
    (531, '            files = files + [')
    (532, '                f"{RESULT_DIR}/03_sample/04_imputed/07_glimpse_sampled/{sm}.{genome}_gp{GP}.{ext}"')
    (533, '                for sm in samples_')
    (534, '                for GP in str2list(recursive_get(["imputation", genome, "gp_filter"], "[0.8]"))')
    (535, '                for ext in ["bcf", "bcf.csi"]')
    (536, '            ] + [')
    (537, '                f"{RESULT_DIR}/03_sample/04_imputed/07_glimpse_sampled/unphased/{sm}.{genome}_gp.txt"')
    (538, '                for sm in samples_')
    (539, '            ]')
    (540, '    return files')
    (541, '')
    (542, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if str2bool(recursive_get_and_test(["imputation", genome, "run"], ["False", "True"]))']
    (543, 'def get_imputation_plots():')
    (544, '    files = []')
    (545, '    for genome in GENOMES:')
    (546, '        if str2bool(recursive_get_and_test(["imputation", genome, "run"], ["False", "True"])):')
    (547, '            files = files + [')
    (548, '                f"{RESULT_DIR}/03_sample/04_imputed/07_glimpse_sampled/unphased/{sm}.{genome}_gp.svg"')
    (549, '                for sm in SAMPLES')
    (550, '            ]')
    (551, '    return files')
    (552, '')
    (553, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/common.smk
context_key: ['if str2bool(recursive_get_and_test(["imputation", genome, "run"], ["False", "True"]))']
    (554, 'def get_imputation_plots_external():')
    (555, '    samples_ = [sm for gen in EXTERNAL_SAMPLES for sm in EXTERNAL_SAMPLES[gen]]')
    (556, '    files = []')
    (557, '    for genome in GENOMES:')
    (558, '        if str2bool(recursive_get_and_test(["imputation", genome, "run"], ["False", "True"])):')
    (559, '            files = files + [')
    (560, '                f"{RESULT_DIR}/03_sample/04_imputed/07_glimpse_sampled/unphased/{sm}.{genome}_gp.svg"')
    (561, '                for sm in samples_')
    (562, '    ]')
    (563, '    return files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=EichlerLab/ONT_pipelines, file=workflow/rules/common.smk
context_key: ['if os.path.isfile(manifest_df.at[wildcards.sample, "SUMMARY"])']
    (53, 'def find_summary_fofn(wildcards):')
    (54, '    if os.path.isfile(manifest_df.at[wildcards.sample, "SUMMARY"]):')
    (55, '        return "-f " + manifest_df.at[wildcards.sample, "SUMMARY"]')
    (56, '    else:')
    (57, '        return ""')
    (58, '')
    (59, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dima, file=workflow/rules/common.smk
context_key: ['if not os.path.isabs(filepath)']
    (23, 'def expand_filepath(filepath):')
    (24, '    filepath = os.path.expandvars(os.path.expanduser(filepath))')
    (25, '    if not os.path.isabs(filepath):')
    (26, '        raise FileNotFoundError(')
    (27, '            errno.ENOENT,')
    (28, '            os.strerror(errno.ENOENT) + " (path must be absolute)",')
    (29, '            filepath,')
    (30, '        )')
    (31, '    return filepath')
    (32, '')
    (33, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dima, file=workflow/rules/common.smk
context_key: ['if units.loc[wildcards.unit, ["fq2"]].isna().all()']
    (34, 'def get_fastq(wildcards, units):')
    (35, '    if units.loc[wildcards.unit, ["fq2"]].isna().all():')
    (36, '        return expand_filepath(units.loc[wildcards.unit, ["fq1"]].dropna()[0])')
    (37, '    else:')
    (38, '        return expand_filepath(')
    (39, '            units.loc[wildcards.unit, ["fq1"]].dropna()[0]')
    (40, '        ), expand_filepath(units.loc[wildcards.unit, ["fq2"]].dropna()[0])')
    (41, '')
    (42, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dima, file=workflow/rules/common.smk
context_key: ['if units.loc[wildcards.unit, ["fq2"]].isna().all()']
    (47, 'def get_r2_fastq(wildcards, units):')
    (48, '    if units.loc[wildcards.unit, ["fq2"]].isna().all():')
    (49, '        return expand_filepath(units.loc[wildcards.unit, ["fq1"]].dropna()[0])')
    (50, '    else:')
    (51, '        return expand_filepath(')
    (52, '            units.loc[wildcards.unit, ["fq1"]].dropna()[0]')
    (53, '        ), expand_filepath(units.loc[wildcards.unit, ["fq2"]].dropna()[0])')
    (54, '')
    (55, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dima, file=workflow/rules/common.smk
context_key: ['if units.loc[wildcards.unit, ["fq2"]].isna().all()']
    (56, 'def get_trimmed_reads(wildcards, units):')
    (57, '    if units.loc[wildcards.unit, ["fq2"]].isna().all():')
    (58, '        return rules.post_rename_fastq_se.output.r1')
    (59, '    else:')
    (60, '        return (')
    (61, '            rules.post_rename_fastq_pe.output.r1,')
    (62, '            rules.post_rename_fastq_pe.output.r2,')
    (63, '        )')
    (64, '')
    (65, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dima, file=workflow/rules/common.smk
context_key: ['if read_type == "se"', 'if conservative_cpu_count(reserve_cores=2, max_cores=99) > 2']
    (80, 'def threads_calculator(read_type="pe"):')
    (81, '    if read_type == "se":')
    (82, '        if conservative_cpu_count(reserve_cores=2, max_cores=99) > 2:')
    (83, '            return conservative_cpu_count(reserve_cores=2, max_cores=99) / 2')
    (84, '        else:')
    (85, '            return 1')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dima, file=workflow/rules/common.smk
context_key: ['if read_type == "se"', 'else', 'if conservative_cpu_count(reserve_cores=2, max_cores=99) > 4']
    (80, 'def threads_calculator(read_type="pe"):')
    (81, '    if read_type == "se":')
    (82, '        if conservative_cpu_count(reserve_cores=2, max_cores=99) > 2:')
    (83, '            return conservative_cpu_count(reserve_cores=2, max_cores=99) / 2')
    (84, '        else:')
    (85, '            return 1')
    (86, '    else:')
    (87, '        if conservative_cpu_count(reserve_cores=2, max_cores=99) > 4:')
    (88, '            return conservative_cpu_count(reserve_cores=2, max_cores=99) / 4')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dima, file=workflow/rules/common.smk
context_key: ['if read_type == "se"', 'else', 'else']
    (80, 'def threads_calculator(read_type="pe"):')
    (81, '    if read_type == "se":')
    (82, '        if conservative_cpu_count(reserve_cores=2, max_cores=99) > 2:')
    (83, '            return conservative_cpu_count(reserve_cores=2, max_cores=99) / 2')
    (84, '        else:')
    (85, '            return 1')
    (86, '    else:')
    (87, '        if conservative_cpu_count(reserve_cores=2, max_cores=99) > 4:')
    (88, '            return conservative_cpu_count(reserve_cores=2, max_cores=99) / 4')
    (89, '        else:')
    (90, '            return 1')
    (91, '')
    (92, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dima, file=workflow/rules/common.smk
context_key: ['if not os.path.isabs(filepath)']
    (93, 'def expand_filepath(filepath):')
    (94, '    filepath = os.path.expandvars(os.path.expanduser(filepath))')
    (95, '    if not os.path.isabs(filepath):')
    (96, '        raise FileNotFoundError(')
    (97, '            errno.ENOENT,')
    (98, '            os.strerror(errno.ENOENT) + " (path must be absolute)",')
    (99, '            filepath,')
    (100, '        )')
    (101, '    return filepath')
    (102, '')
    (103, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dima, file=workflow/rules/common.smk
context_key: ['if does not exists, create path and return it. If any errors, retur']
    (114, 'def tmp_path(path=""):')
    (115, '    """')
    (116, '    if does not exists, create path and return it. If any errors, return')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dima, file=workflow/rules/common.smk
context_key: ['if path', 'try']
    (114, 'def tmp_path(path=""):')
    (115, '    """')
    (116, '    if does not exists, create path and return it. If any errors, return')
    (117, '    default path')
    (118, '    :param path: path')
    (119, '    :return: path')
    (120, '    """')
    (121, '    default_path = os.getenv("TMPDIR", config.get("paths").get("tmp_dir"))')
    (122, '    if path:')
    (123, '        try:')
    (124, '            os.makedirs(path)')
    (125, '        except OSError as e:')
    (126, '            if e.errno != errno.EEXIST:')
    (127, '                return default_path')
    (128, '        return path')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dima, file=workflow/rules/common.smk
context_key: ['if path']
    (114, 'def tmp_path(path=""):')
    (115, '    """')
    (116, '    if does not exists, create path and return it. If any errors, return')
    (117, '    default path')
    (118, '    :param path: path')
    (119, '    :return: path')
    (120, '    """')
    (121, '    default_path = os.getenv("TMPDIR", config.get("paths").get("tmp_dir"))')
    (122, '    if path:')
    (123, '        try:')
    (124, '            os.makedirs(path)')
    (125, '        except OSError as e:')
    (126, '            if e.errno != errno.EEXIST:')
    (127, '                return default_path')
    (128, '        return path')
    (129, '    return default_path')
    (130, '')
    (131, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dima, file=workflow/rules/common.smk
context_key: ['if n >= prefix[s]']
    (149, '    def bytes2human(n):')
    (150, '        # http://code.activestate.com/recipes/578019')
    (151, '        # >>> bytes2human(10000)')
    (152, "        # \\'9.8K\\'")
    (153, '        # >>> bytes2human(100001221)')
    (154, "        # \\'95.4M\\'")
    (155, '        symbols = ("K", "M", "G", "T", "P", "E", "Z", "Y")')
    (156, '        prefix = {}')
    (157, '        for i, s in enumerate(symbols):')
    (158, '            prefix[s] = 1 << (i + 1) * 10')
    (159, '        for s in reversed(symbols):')
    (160, '            if n >= prefix[s]:')
    (161, '                value = float(n) / prefix[s]')
    (162, '                return "%.0f%s" % (value, s)')
    (163, '        return "%sB" % n')
    (164, '')
    (165, '    def preserve(resource, percentage, stock):')
    (166, '        preserved = resource - max(resource * percentage // 100, stock)')
    (167, '        return preserved if preserved != 0 else stock')
    (168, '')
    (169, '    # def preserve(resource, percentage, stock):')
    (170, '    #     return resource - max(resource * percentage // 100, stock)')
    (171, '')
    (172, '    params_template = "\\\'-Xms{} -Xmx{} -XX:ParallelGCThreads={} " "-Djava.io.tmpdir={}\\\'"')
    (173, '')
    (174, '    mem_min = 1024**3 * 2  # 2GB')
    (175, '')
    (176, '    mem_size = preserve(total_physical_mem_size(), percentage_to_preserve, stock_mem)')
    (177, '    cpu_nums = preserve(cpu_count(), percentage_to_preserve, stock_cpu)')
    (178, '    tmpdir = tmp_path(tmp_dir)')
    (179, '')
    (180, '    return params_template.format(')
    (181, '        bytes2human(mem_min).lower(),')
    (182, '        bytes2human(max(mem_size // cpu_nums * multiply_by, mem_min)).lower(),')
    (183, '        min(cpu_nums, multiply_by),')
    (184, '        tmpdir,')
    (185, '    )')
    (186, '')
    (187, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dima, file=workflow/rules/common.smk
context_key: ['if len(known_sites) == 0']
    (209, 'def get_known_sites(known_sites=["dbsnp", "mills", "ph1_indel"]):')
    (210, '    known_variants = config.get("resources").get("known_variants")')
    (211, '    ks = []')
    (212, '    if len(known_sites) == 0:')
    (213, '        known_sites = known_variants.keys()')
    (214, '    for k, v in known_variants.items():')
    (215, '        if k in known_sites:')
    (216, '            ks.append("--known-sites {} ".format(v))')
    (217, '    return "".join(ks)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hzi-bifo/phylogeny-of-single-cells, file=workflow/rules/common.smk
context_key: ['if wildcards.read_type == "se"']
    (137, 'def get_processed_consensus_input(wildcards):')
    (138, '    if wildcards.read_type == "se":')
    (139, '        return "results/consensus/fastq/{}.se.fq".format(wildcards.sample)')
    (140, '    return [')
    (141, '        "results/consensus/fastq/{}.1.fq".format(wildcards.sample),')
    (142, '        "results/consensus/fastq/{}.2.fq".format(wildcards.sample),')
    (143, '    ]')
    (144, '')
    (145, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hzi-bifo/phylogeny-of-single-cells, file=workflow/rules/common.smk
context_key: ['try', 'if isinstance(cutadapt, str)']
    (174, 'def get_cutadapt_parameters(wildcards):')
    (175, '    unit = units.loc[wildcards.sample].loc[wildcards.unit]')
    (176, '    try:')
    (177, '        cutadapt = unit["cutadapt"]')
    (178, '        if isinstance(cutadapt, str):')
    (179, '            return cutadapt')
    (180, '        return ""')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=friendsofstrandseq/ashleys-qc-pipeline, file=workflow/rules/common.smk
context_key: ['if genecore is False']
    (6, '    def __init__(')
    (7, '        self,')
    (8, '        input_path,')
    (9, '        output_path,')
    (10, '        check_sm_tag=False,')
    (11, '        bam=True,')
    (12, '        genecore=False,')
    (13, '        genecore_path=str,')
    (14, '    ):')
    (15, '        if genecore is False:')
    (16, '            df_config_files = self.handle_input_data(thisdir=input_path, bam=bam)')
    (17, '        elif genecore is True:')
    (18, '            df_config_files, d_master = self.handle_input_data_genecore(')
    (19, '                thisdir=genecore_path')
    (20, '            )')
    (21, '            self.d_master = d_master')
    (22, '')
    (23, '        os.makedirs(os.path.dirname(output_path), exist_ok=True)')
    (24, '        df_config_files.to_csv(output_path, sep="\\\\t", index=False)')
    (25, '        self.df_config_files = df_config_files')
    (26, '')
    (27, '    @staticmethod')
    (28, '    def handle_input_data_genecore(thisdir):')
    (29, '        """_summary_')
    (30, '')
    (31, '        Args:')
    (32, '            thisdir (_type_): _description_')
    (33, '            exclude_list (_type_, optional): _description_. Defaults to list.')
    (34, '')
    (35, '        Returns:')
    (36, '            _type_: _description_')
    (37, '        """')
    (38, '        complete_df_list = list()')
    (39, '')
    (40, '        # List of folders/files to not consider (restrict to samples only)')
    (41, '        l = [')
    (42, '            e')
    (43, '            for e in os.listdir(')
    (44, '                "{genecore_prefix}/{date_folder}".format(')
    (45, '                    genecore_prefix=config["genecore_prefix"],')
    (46, '                    date_folder=config["genecore_date_folder"],')
    (47, '                )')
    (48, '            )')
    (49, '            if e.endswith(".gz")')
    (50, '        ]')
    (51, '')
    (52, '        # Create a list of  files to process for each sample')
    (53, '        d_master = collections.defaultdict(dict)')
    (54, '        sub_l = list()')
    (55, '        for j, e in enumerate(l):')
    (56, '            sub_l.append(e)')
    (57, '            if (j + 1) % 192 == 0:')
    (58, '                common_element = findstem(sub_l)')
    (59, '                l_elems = common_element.split("lane1")')
    (60, '                prefix = l_elems[0]')
    (61, '                technician_name = l_elems[0].split("_")[-2]')
    (62, '                sample = l_elems[1].split("x")[0]')
    (63, '                index = l_elems[1].split("x")[1].split("PE")[0][-1]')
    (64, '                pe_index = common_element[-1]')
    (65, '                sub_l = list()')
    (66, '')
    (67, '                d_master[sample]["prefix"] = prefix')
    (68, '                d_master[sample]["technician_name"] = technician_name')
    (69, '                d_master[sample]["index"] = index')
    (70, '                d_master[sample]["pe_index"] = pe_index')
    (71, '                d_master[sample]["common_element"] = common_element')
    (72, '')
    (73, '        samples_to_process = (')
    (74, '            config["samples_to_process"]')
    (75, '            if len(config["samples_to_process"]) > 0')
    (76, '            else list(d_master.keys())')
    (77, '        )')
    (78, '')
    (79, '        config["data_location"] = "{data_location}/{genecore_date_folder}".format(')
    (80, '            data_location=config["data_location"],')
    (81, '            genecore_date_folder=config["genecore_date_folder"],')
    (82, '        )')
    (83, '')
    (84, '        genecore_list = [')
    (85, '            expand(')
    (86, '                "{data_location}/{sample}/fastq/{sample}x0{index}PE20{cell_nb}.{pair}.fastq.gz",')
    (87, '                data_location=config["data_location"],')
    (88, '                sample=sample,')
    (89, '                index=d_master[sample]["index"],')
    (90, '                cell_nb=list(')
    (91, '                    range(')
    (92, '                        (int(d_master[sample]["pe_index"]) * 100) + 1,')
    (93, '                        (int(d_master[sample]["pe_index"]) * 100) + 97,')
    (94, '                    )')
    (95, '                ),')
    (96, '                pair=["1", "2"],')
    (97, '            )')
    (98, '            for sample in d_master')
    (99, '            if sample in samples_to_process')
    (100, '        ]')
    (101, '        genecore_list = [sub_e for e in genecore_list for sub_e in e]')
    (102, '')
    (103, '        complete_df_list = list()')
    (104, '')
    (105, '        for sample in d_master:')
    (106, '            df = pd.DataFrame(')
    (107, '                [')
    (108, '                    {"File": os.path.basename(f), "Folder": os.path.dirname(f)}')
    (109, '                    for f in genecore_list')
    (110, '                    if sample in f')
    (111, '                ]')
    (112, '            )')
    (113, '            if df.shape[0] > 0:')
    (114, '                df["File"] = df["File"].str.replace(".fastq.gz", "", regex=True)')
    (115, '                df["Sample"] = sample')
    (116, '                df["Pair"] = df["File"].apply(lambda r: r.split(".")[1])')
    (117, '                df["Cell"] = df["File"].apply(lambda r: r.split(".")[0])')
    (118, '                df["Full_path"] = df[["Folder", "File"]].apply(')
    (119, '                    lambda r: f"{r[\\\'Folder\\\']}/{r[\\\'File\\\']}.fastq.gz", axis=1')
    (120, '                )')
    (121, '                df["Genecore_path"] = df["File"].apply(')
    (122, '                    lambda r: f"{config[\\\'genecore_prefix\\\']}/{config[\\\'genecore_date_folder\\\']}/{d_master[sample][\\\'prefix\\\']}lane1/{r.replace(\\\'.\\\', \\\'_\\\')}_sequence.txt.gz"')
    (123, '                )')
    (124, '                df["Genecore_file"] = df["File"].apply(')
    (125, '                    lambda r: f"{d_master[sample][\\\'prefix\\\']}lane1{r.replace(\\\'.\\\', \\\'_\\\')}"')
    (126, '                )')
    (127, '                df["Genecore_file"] = df["Genecore_file"].apply(')
    (128, '                    lambda r: "_".join(r.split("_")[:-1])')
    (129, '                )')
    (130, '')
    (131, '                # Concat dataframes for each sample & output')
    (132, '                complete_df_list.append(df)')
    (133, '')
    (134, '        complete_df = pd.concat(complete_df_list)')
    (135, '')
    (136, '        complete_df = complete_df.sort_values(by=["Cell", "File"]).reset_index(')
    (137, '            drop=True')
    (138, '        )')
    (139, '        pd.options.display.max_colwidth = 200')
    (140, '        print(complete_df)')
    (141, '        # exit()')
    (142, '        return complete_df, d_master')
    (143, '')
    (144, '    @staticmethod')
    (145, '    def handle_input_data(thisdir, exclude_list=list, bam=bool):')
    (146, '        """_summary_')
    (147, '')
    (148, '        Args:')
    (149, '            thisdir (_type_): _description_')
    (150, '            exclude_list (_type_, optional): _description_. Defaults to list.')
    (151, '')
    (152, '        Returns:')
    (153, '            _type_: _description_')
    (154, '        """')
    (155, '        # Extension & folder based on bam boolean input')
    (156, '        ext = ".bam" if bam is True else ".fastq.gz"')
    (157, '        folder = "bam" if bam is True else "fastq"')
    (158, '        complete_df_list = list()')
    (159, '        # List of folders/files to not consider (restrict to samples only)')
    (160, '        exclude = [')
    (161, '            "._.DS_Store",')
    (162, '            ".DS_Store",')
    (163, '            "all",')
    (164, '            "ashleys_counts",')
    (165, '            "bam",')
    (166, '            "cell_selection",')
    (167, '            "config",')
    (168, '            "counts",')
    (169, '            "fastq",')
    (170, '            "fastqc",')
    (171, '            "haplotag",')
    (172, '            "log",')
    (173, '            "merged_bam",')
    (174, '            "mosaiclassifier",')
    (175, '            "normalizations",')
    (176, '            "ploidy",')
    (177, '            "plots",')
    (178, '            "predictions",')
    (179, '            "segmentation",')
    (180, '            "snv_calls",')
    (181, '            "stats",')
    (182, '            "strandphaser",')
    (183, '        ]')
    (184, '')
    (185, '        for sample in [e for e in os.listdir(thisdir) if e not in exclude]:')
    (186, '            # Create a list of  files to process for each sample')
    (187, '            l_files_all = [')
    (188, '                f')
    (189, '                for f in os.listdir(')
    (190, '                    "{thisdir}/{sample}/{folder}/".format(')
    (191, '                        thisdir=thisdir, sample=sample, folder=folder')
    (192, '                    )')
    (193, '                )')
    (194, '                if f.endswith(ext)')
    (195, '            ]')
    (196, '')
    (197, '            # Dataframe creation')
    (198, '            df = pd.DataFrame([{"File": f} for f in l_files_all])')
    (199, '            df["File"] = df["File"].str.replace(ext, "", regex=True)')
    (200, '            df["Folder"] = thisdir')
    (201, '            df["Sample"] = sample')
    (202, '            df["Cell"] = df["File"].apply(lambda r: r.split(".")[0])')
    (203, '            df["Full_path"] = "{thisdir}/{sample}/{folder}/".format(')
    (204, '                thisdir=thisdir, sample=sample, folder=folder')
    (205, '            )')
    (206, '            df["Full_path"] = df["Full_path"] + df["File"] + ext')
    (207, '')
    (208, '            complete_df_list.append(df)')
    (209, '')
    (210, '        # Concat dataframes for each sample & output')
    (211, '        complete_df = pd.concat(complete_df_list)')
    (212, '        complete_df = complete_df.sort_values(by=["Cell", "File"]).reset_index(')
    (213, '            drop=True')
    (214, '        )')
    (215, '        return complete_df')
    (216, '')
    (217, '')
    (218, '# GENECORE')
    (219, '')
    (220, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=friendsofstrandseq/ashleys-qc-pipeline, file=workflow/rules/common.smk
context_key: ['if stem not in arr[k]']
    (221, 'def findstem(arr):')
    (222, '')
    (223, '    # Determine size of the array')
    (224, '    n = len(arr)')
    (225, '')
    (226, '    # Take first word from array')
    (227, '    # as reference')
    (228, '    s = arr[0]')
    (229, '    l = len(s)')
    (230, '')
    (231, '    res = ""')
    (232, '')
    (233, '    for i in range(l):')
    (234, '        for j in range(i + 1, l + 1):')
    (235, '')
    (236, '            # generating all possible substrings')
    (237, '            # of our reference string arr[0] i.e s')
    (238, '            stem = s[i:j]')
    (239, '            k = 1')
    (240, '            for k in range(1, n):')
    (241, '')
    (242, '                # Check if the generated stem is')
    (243, '                # common to all words')
    (244, '                if stem not in arr[k]:')
    (245, '                    break')
    (246, '')
    (247, '            # If current substring is present in')
    (248, '            # all strings and its length is greater')
    (249, '            # than current result')
    (250, '            if k + 1 == n and len(res) < len(stem):')
    (251, '                res = stem')
    (252, '')
    (253, '    return res')
    (254, '')
    (255, '')
    (256, '# Create configuration file with samples')
    (257, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=friendsofstrandseq/ashleys-qc-pipeline, file=workflow/rules/common.smk
context_key: ['else config["plottype_counts"][0']
    (221, 'def findstem(arr):')
    (222, '')
    (223, '    # Determine size of the array')
    (224, '    n = len(arr)')
    (225, '')
    (226, '    # Take first word from array')
    (227, '    # as reference')
    (228, '    s = arr[0]')
    (229, '    l = len(s)')
    (230, '')
    (231, '    res = ""')
    (232, '')
    (233, '    for i in range(l):')
    (234, '        for j in range(i + 1, l + 1):')
    (235, '')
    (236, '            # generating all possible substrings')
    (237, '            # of our reference string arr[0] i.e s')
    (238, '            stem = s[i:j]')
    (239, '            k = 1')
    (240, '            for k in range(1, n):')
    (241, '')
    (242, '                # Check if the generated stem is')
    (243, '                # common to all words')
    (244, '                if stem not in arr[k]:')
    (245, '                    break')
    (246, '')
    (247, '            # If current substring is present in')
    (248, '            # all strings and its length is greater')
    (249, '            # than current result')
    (250, '            if k + 1 == n and len(res) < len(stem):')
    (251, '                res = stem')
    (252, '')
    (253, '    return res')
    (254, '')
    (255, '')
    (256, '# Create configuration file with samples')
    (257, '')
    (258, 'c = HandleInput(')
    (259, '    input_path=config["data_location"],')
    (260, '    genecore_path="{genecore_prefix}/{genecore_date_folder}".format(')
    (261, '        genecore_prefix=config["genecore_prefix"],')
    (262, '        genecore_date_folder=config["genecore_date_folder"],')
    (263, '    ),')
    (264, '    output_path="{data_location}/config/config_df_ashleys.tsv".format(')
    (265, '        data_location=config["data_location"]')
    (266, '    ),')
    (267, '    check_sm_tag=False,')
    (268, '    bam=False,')
    (269, '    genecore=config["genecore"],')
    (270, ')')
    (271, 'df_config_files = c.df_config_files')
    (272, 'if config["genecore"] is True:')
    (273, '    d_master = c.d_master')
    (274, '')
    (275, '')
    (276, 'samples = list(sorted(list(df_config_files.Sample.unique().tolist())))')
    (277, '')
    (278, '# genecore_mapping = df_config_files.groupby("Genecore_file")["Cell"].unique().apply(lambda r: r[0]).to_dict()')
    (279, '')
    (280, '# Dictionnary of libraries available per sample')
    (281, 'cell_per_sample = (')
    (282, '    df_config_files.groupby("Sample")["Cell"].unique().apply(list).to_dict()')
    (283, ')')
    (284, '')
    (285, '# Plottype options for QC count plot')
    (286, 'plottype_counts = (')
    (287, '    config["plottype_counts"]')
    (288, '    if config["GC_analysis"] is True')
    (289, '    else config["plottype_counts"][0]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=scripts/io.py
context_key: ['if path and path[-1] == "/"']
    (92, 'def path_check(path, is_file):')
    (93, '    if path and path[-1] == "/":')
    (94, '        path = path[0:-1]')
    (95, '    if exists(path):')
    (96, '        return path')
    (97, '    else:')
    (98, '        if is_file:')
    (99, '            print(f"{bcolors.FAIL}CRITICAL: File {path} does not exist, exiting if this is an essential file..")')
    (100, '            return ""')
    (101, '        else:')
    (102, '            print(f"{bcolors.OKBLUE}INFO: Directory {path} does not exist, creating and hoping for the best now..")')
    (103, '            if path:')
    (104, '                os.makedirs(path, exist_ok=True)')
    (105, '            return path')
    (106, '')
    (107, '# set static vars from config file here')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=rules/common.smk
context_key: ['if wildcards.tool == "humann"']
    (23, 'def dga_counts(wildcards):')
    (24, '')
    (25, '    humann  = [ os.path.join(RESULTDIR, "03-CountData", "humann", f"genefamilies_{UNITS}_combined.tsv")   ]')
    (26, '    megan   = [ os.path.join(RESULTDIR, "03-CountData", "megan", "megan_combined.csv")    ]')
    (27, '')
    (28, '    if wildcards.tool == "humann": return humann')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=rules/common.smk
context_key: ['if TRIM == "true"']
    (32, 'def merge_input(wildcards):')
    (33, '    if TRIM == "true":')
    (34, '        return  {   "R1" : os.path.join(RESULTDIR, "01-QualityControl", "trimmed_pe", "{wildcards.sample}_1.fastq.gz".format(wildcards=wildcards)),')
    (35, '                    "R2" : os.path.join(RESULTDIR, "01-QualityControl", "trimmed_pe", "{wildcards.sample}_2.fastq.gz".format(wildcards=wildcards))    }')
    (36, '    else:')
    (37, '        return  {   "R1" : os.path.join(READDIR, "{wildcards.sample}_1".format(wildcards=wildcards), EXT),')
    (38, '                    "R2" : os.path.join(READDIR, "{wildcards.sample}_2".format(wildcards=wildcards), EXT)   }')
    (39, '')
    (40, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=rules/common.smk
context_key: ['if REFERENCE != ""']
    (41, 'def get_humann_reads(wildcards):')
    (42, '    if REFERENCE != "":')
    (43, '        return os.path.join(RESULTDIR, "02-Decontamination", "{wildcards.sample}_unmapped.fastq.gz".format(wildcards=wildcards))')
    (44, '    else:')
    (45, '        return os.path.join(TEMPDIR, "concat_reads", "{wildcards.sample}_concat.fq.gz".format(wildcards=wildcards))')
    (46, '')
    (47, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=rules/common.smk
context_key: ['if MODE == "paired"']
    (48, 'def get_bowtie_reads(wildcards):')
    (49, '    if MODE == "paired":')
    (50, '        return os.path.join(TEMPDIR, "concat_reads", "{wildcards.sample}_concat.fq.gz".format(wildcards=wildcards))')
    (51, '    elif TRIM == "true":')
    (52, '        return os.path.join(RESULTDIR, "01-QualityControl", "trimmed_se", "{wildcards.sample}.fastq.gz".format(wildcards=wildcards))')
    (53, '    else:')
    (54, '        return os.path.join(READDIR, "{wildcards.sample}".format(wildcards=wildcards), EXT)')
    (55, '')
    (56, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=rules/common.smk
context_key: ['if REFERENCE != ""']
    (64, 'def get_diamond_reads(wildcards):')
    (65, '    if REFERENCE != "":')
    (66, '        return os.path.join(RESULTDIR, "02-Decontamination", "{wildcards.sample}_unmapped.fastq.gz".format(wildcards=wildcards))')
    (67, '    else:')
    (68, '        return os.path.join(TEMPDIR, "concat_reads", "{wildcards.sample}_concat.fq.gz".format(wildcards=wildcards))')
    (69, '')
    (70, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=rules/common.smk
context_key: ['if MERGER == "bbmerge"']
    (71, 'def get_reports(wildcards):')
    (72, '')
    (73, '    reports = []')
    (74, '    if MERGER == "bbmerge":')
    (75, '        reports.extend(expand(os.path.join(TEMPDIR, "bbmerge", "{sample}_ihist.txt"), sample = SAMPLE))')
    (76, '')
    (77, '    if REFERENCE != "":')
    (78, '        reports.extend(expand(os.path.join(RESULTDIR, "00-Log", "bowtie2", "bowtie2_map_{sample}.log"), sample = SAMPLE))')
    (79, '')
    (80, '    if FASTQC and MODE == "paired":')
    (81, '        reports.extend(expand(os.path.join(TEMPDIR, "qc", "fastqc_pe_pre", "{sample}_{mate}_fastqc.zip"), sample = SAMPLE, mate = ["1", "2"]))')
    (82, '        reports.extend(expand(os.path.join(TEMPDIR, "qc", "fastqc_pe_post", "{sample}_{mate}_fastqc.zip"), sample = SAMPLE, mate = ["1", "2"]))')
    (83, '    if FASTQC and MODE == "single":')
    (84, '        reports.extend(expand(os.path.join(TEMPDIR, "qc", "fastqc_se_pre", "{sample}_fastqc.zip"), sample = SAMPLE))')
    (85, '        reports.extend(expand(os.path.join(TEMPDIR, "qc", "fastqc_se_post", "{sample}_fastqc.zip"), sample = SAMPLE))')
    (86, '')
    (87, '    return reports')
    (88, '')
    (89, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=rules/common.smk
context_key: ['if MERGER == "pear"']
    (90, 'def get_concat_input(wildcards):')
    (91, '    if MERGER == "pear":')
    (92, '        return  [   os.path.join(RESULTDIR, "01-QualityControl", "merged", "{wildcards.sample}.assembled.fastq".format(wildcards=wildcards)),')
    (93, '                    os.path.join(RESULTDIR, "01-QualityControl", "merged", "{wildcards.sample}.unassembled.forward.fastq".format(wildcards=wildcards)),')
    (94, '                    os.path.join(RESULTDIR, "01-QualityControl", "merged", "{wildcards.sample}.unassembled.reverse.fastq".format(wildcards=wildcards)),')
    (95, '                    os.path.join(RESULTDIR, "01-QualityControl", "merged", "{wildcards.sample}.discarded.fastq".format(wildcards=wildcards)) ]')
    (96, '    elif MERGER == "bbmerge":')
    (97, '        return [    os.path.join(RESULTDIR, "01-QualityControl", "merged", "{wildcards.sample}_merged_fastq.gz".format(wildcards=wildcards)),')
    (98, '                    os.path.join(RESULTDIR, "01-QualityControl", "merged", "{wildcards.sample}_unmerged_fastq.gz".format(wildcards=wildcards))  ]')
    (99, '    elif MERGER == "none" and TRIM == "true":')
    (100, '        return [    os.path.join(RESULTDIR, "01-QualityControl", "trimmed_pe", "{sample}_1.fastq.gz"),')
    (101, '                    os.path.join(RESULTDIR, "01-QualityControl", "trimmed_pe", "{sample}_2.fastq.gz"),')
    (102, '                    os.path.join(TEMPDIR, "TRIMMOMATIC", "untrimmed_pe", "{sample}_1.unpaired.fastq.gz"),')
    (103, '                    os.path.join(TEMPDIR, "TRIMMOMATIC", "untrimmed_pe", "{sample}_2.unpaired.fastq.gz")  ]')
    (104, '    else:')
    (105, '        return [    os.path.join(READDIR, "{wildcards.sample}_1".format(wildcards=wildcards), EXT),')
    (106, '                    os.path.join(READDIR, "{wildcards.sample}_2".format(wildcards=wildcards), EXT)   ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zavolanlab/polyAsite_workflow, file=Snakefile
context_key: ['if samples.loc[wildcards.sample, "protocol"] == "QuantSeq_REV"']
    (26, 'def trim_5p_adapter_input(wildcards):')
    (27, '    if samples.loc[wildcards.sample, "protocol"] == "QuantSeq_REV":')
    (28, '        return os.path.join(config["samples_dir"],')
    (29, '                            wildcards.sample,')
    (30, '                            wildcards.sample + ".fa.gz")')
    (31, '    else:')
    (32, '        return os.path.join(config["samples_dir"],')
    (33, '                            wildcards.sample,')
    (34, '                            wildcards.sample + ".fa.gz")')
    (35, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zavolanlab/polyAsite_workflow, file=Snakefile
context_key: ['if samples.loc[wildcards.sample, "protocol"] == "A-seq2"']
    (36, 'def trim_adapter_input(wildcards):')
    (37, '    if samples.loc[wildcards.sample, "protocol"] == "A-seq2":')
    (38, '        return os.path.join(config["samples_dir"],')
    (39, '                            wildcards.sample,')
    (40, '                            wildcards.sample + ".5ptrimmed.A-seq2.fa.gz")')
    (41, '    elif samples.loc[wildcards.sample, "protocol"] == "3\\\'READS":')
    (42, '        return os.path.join(config["samples_dir"],')
    (43, '                            wildcards.sample,')
    (44, '                            wildcards.sample + ".5ptrimmed.3READS.fa.gz")')
    (45, '    elif samples.loc[wildcards.sample, "protocol"] == "PAS-Seq":')
    (46, '        return os.path.join(config["samples_dir"],')
    (47, '                            wildcards.sample,')
    (48, '                            wildcards.sample + ".leadingTs_trimmed.fa.gz")')
    (49, '    else:')
    (50, '        return os.path.join(config["samples_dir"],')
    (51, '                            wildcards.sample,')
    (52, '                            wildcards.sample + ".fa.gz")')
    (53, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zavolanlab/polyAsite_workflow, file=Snakefile
context_key: ['if samples.loc[wildcards.sample, "reverse_compl"]']
    (74, 'def get_valid_3p_3PSeq_file(wildcards):')
    (75, '    if samples.loc[wildcards.sample, "reverse_compl"]:')
    (76, '        return os.path.join(config["samples_dir"],')
    (77, '                            wildcards.sample,')
    (78, '                            wildcards.sample + ".trimmed.rev_cmpl.fa.gz")')
    (79, '    else:')
    (80, '        return os.path.join(config["samples_dir"],')
    (81, '                            wildcards.sample,')
    (82, '                            wildcards.sample + ".fa.gz")')
    (83, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zavolanlab/polyAsite_workflow, file=Snakefile
context_key: ['if samples.loc[wildcards.sample, "protocol"] == "3P-Seq"']
    (84, 'def get_input_polyAtail_removal(wildcards):')
    (85, '    if samples.loc[wildcards.sample, "protocol"] == "3P-Seq":')
    (86, '        return os.path.join(config["samples_dir"],')
    (87, '                               wildcards.sample,')
    (88, '                               wildcards.sample + ".valid_3p_configuration.fa.gz")')
    (89, '    elif samples.loc[wildcards.sample, "protocol"] == "PAPERCLIP":')
    (90, '        if pd.isna( samples.loc[wildcards.sample, "fiveAdapter"]):')
    (91, '            return os.path.join(config["samples_dir"],')
    (92, '                               wildcards.sample,')
    (93, '                               wildcards.sample + ".fa.gz")')
    (94, '        else:')
    (95, '            return os.path.join(config["samples_dir"],')
    (96, '                               wildcards.sample,')
    (97, '                               wildcards.sample + ".5ptrimmed.fa.gz")')
    (98, '    elif samples.loc[wildcards.sample, "protocol"] == "3\\\'-Seq (Mayr)":')
    (99, '        return os.path.join(config["samples_dir"],')
    (100, '                               wildcards.sample,')
    (101, '                               wildcards.sample + ".trimmed.fa.gz")')
    (102, '    else:')
    (103, '        return os.path.join(config["samples_dir"],')
    (104, '                                     wildcards.sample,')
    (105, '                                     wildcards.sample + ".trimmed.rev_cmpl.fa.gz")')
    (106, '')
    (107, '#--------------------------------------------------------------------------------')
    (108, '# Reverse complement will NOT be applied for Aseq, Mayr and 3P-seq;')
    (109, '# those go from trim_3p_adapter directly to get_valid_reads')
    (110, '# 3READS,3P-Seq,QuantSeq_REV,PAPERCLIP, Mayr: additionally, the poly(A) tail is trimmed')
    (111, '#--------------------------------------------------------------------------------')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zavolanlab/polyAsite_workflow, file=Snakefile
context_key: ['elif samples.loc[wildcards.sample, "protocol"] == "3\\\'-Seq (Mayr)"']
    (112, 'def get_reads_after_trimming(wildcards):')
    (113, '    if(samples.loc[wildcards.sample, "protocol"] == "A-seq" or')
    (114, '       samples.loc[wildcards.sample, "protocol"] == "3P-seq"):')
    (115, '        return os.path.join(config["samples_dir"],')
    (116, '                            wildcards.sample,')
    (117, '                            wildcards.sample + ".trimmed.fa.gz")')
    (118, '    elif samples.loc[wildcards.sample, "protocol"] == "3\\\'-Seq (Mayr)":')
    (119, '        return os.path.join(config["samples_dir"],')
    (120, '                            wildcards.sample,')
    (121, '                            wildcards.sample + ".trimmed_tail.fa.gz")')
    (122, '    elif( samples.loc[wildcards.sample, "protocol"] == "3\\\'READS" or')
    (123, '          samples.loc[wildcards.sample, "protocol"] == "3P-Seq" or')
    (124, '          samples.loc[wildcards.sample, "protocol"] == "PAPERCLIP" or')
    (125, '          samples.loc[wildcards.sample, "protocol"] == "QuantSeq_REV"):')
    (126, '        return os.path.join(config["samples_dir"],')
    (127, '                            wildcards.sample,')
    (128, '                            wildcards.sample + ".trimmed_tail.fa.gz")')
    (129, '    else:')
    (130, '        return os.path.join(config["samples_dir"],')
    (131, '                            wildcards.sample,')
    (132, '                            wildcards.sample + ".trimmed.rev_cmpl.fa.gz")')
    (133, '')
    (134, '#-------------------------------------------------------------------------------')
    (135, '# the valid reads change depending on whether reads are')
    (136, '# also length filtered')
    (137, '#-------------------------------------------------------------------------------')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zavolanlab/polyAsite_workflow, file=Snakefile
context_key: ['if samples.loc[wildcards.sample, "protocol"] == "3\\\'READS"']
    (152, 'def get_ds_patterns_for_ipAssignment(wildcards):')
    (153, '    if samples.loc[wildcards.sample, "protocol"] == "3\\\'READS":')
    (154, '        return " ".join(["--ds_pattern=%s"')
    (155, "                         % pat for pat in config[\\'IP.downstream_patterns.3READS\\'] ]) \\\\")
    (156, '                         if config[\\\'IP.downstream_patterns.3READS\\\'] is not None else ""')
    (157, '    else:')
    (158, '        return " ".join(["--ds_pattern=%s"')
    (159, "                         % pat for pat in config[\\'IP.downstream_patterns\\'] ]) \\\\")
    (160, '                         if config[\\\'IP.downstream_patterns\\\'] is not None else ""')
    (161, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if comparatorColumn != None']
    (20, "def getCohorts(metadata, columns=[\\'species_gambiae_coluzzii\\', \\'location\\'], comparatorColumn=None, minPopSize=15):")
    (21, '    ')
    (22, '    # subset metadata dataFrame and find combinations of variables with more than minPopSize individuals')
    (23, '    cohorts = metadata[columns]')
    (24, "    cohorts = cohorts.groupby(columns).size().reset_index().rename(columns={0:\\'size\\'})")
    (25, "    cohorts = cohorts[cohorts[\\'size\\'] > minPopSize][columns]")
    (26, '    ')
    (27, '    ')
    (28, '    if comparatorColumn != None:')
    (29, '        cols = [i for i in columns if i != comparatorColumn]')
    (30, '    else:')
    (31, '        cols = columns')
    (32, '')
    (33, '    idxs = []')
    (34, '    for _, row in cohorts.iterrows():   ')
    (35, '        # create the pandas metadata query for each cohort')
    (36, '        mycohortQuery = " & ".join([col + " == " + "\\\'" + row.astype(str)[col] + "\\\'" for col in cohorts.columns])')
    (37, '        # get indices of individuals in each cohort')
    (38, '        idxs.append(metadata.query(mycohortQuery).index.tolist())')
    (39, '    ')
    (40, "    cohorts[\\'indices\\'] = idxs")
    (41, "    cohorts[\\'cohortText\\'] = cohorts[cols].agg(\\' | \\'.join, axis=1)")
    (42, '    cohorts[\\\'cohortNoSpaceText\\\'] = cohorts[\\\'cohortText\\\'].str.replace("|", ".", regex=False).str.replace(" ", "",regex=False)')
    (43, '    #colours = get_colour_dict(cohorts[\\\'species_gambiae_coluzzii\\\'], palette="Set1")')
    (44, "    #cohorts[\\'colour\\'] = cohorts[\\'species_gambiae_coluzzii\\'].map(colours)")
    (45, '    if comparatorColumn != None: ')
    (46, "        cols = cols + [\\'cohortText\\', \\'cohortNoSpaceText\\']")
    (47, '        cohorts = cohorts.pivot(index=cols, columns=comparatorColumn)')
    (48, '        return(cohorts.reset_index())')
    (49, '')
    (50, '    return(cohorts.reset_index(drop=True))')
    (51, '')
    (52, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if cloud is False', "if config[\\'Zarr\\'][\\'activate\\'] == True", 'if all_contigs == True']
    (53, 'def getZarrArray(type_="Genotype", all_contigs=False, cloud=False):')
    (54, '')
    (55, '    if cloud is False:')
    (56, "        if config[\\'Zarr\\'][\\'activate\\'] == True:")
    (57, "            Array = config[\\'Zarr\\'][type_]")
    (58, '            if all_contigs == True:')
    (59, '                Array = Array.replace("{contig}", "{{contig}}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if cloud is False', "if config[\\'Zarr\\'][\\'activate\\'] == True", "elif config[\\'Zarr\\'][\\'activate\\'] == False", 'if type_ == "Genotype"']
    (53, 'def getZarrArray(type_="Genotype", all_contigs=False, cloud=False):')
    (54, '')
    (55, '    if cloud is False:')
    (56, "        if config[\\'Zarr\\'][\\'activate\\'] == True:")
    (57, "            Array = config[\\'Zarr\\'][type_]")
    (58, '            if all_contigs == True:')
    (59, '                Array = Array.replace("{contig}", "{{contig}}")')
    (60, "        elif config[\\'Zarr\\'][\\'activate\\'] == False:")
    (61, '            if type_ == "Genotype":')
    (62, '                Array = "resources/Zarr/{dataset}/{contig}/calldata/GT" ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if cloud is False', "if config[\\'Zarr\\'][\\'activate\\'] == True", "elif config[\\'Zarr\\'][\\'activate\\'] == False", "if type_ == \\'Haplotype\\'"]
    (53, 'def getZarrArray(type_="Genotype", all_contigs=False, cloud=False):')
    (54, '')
    (55, '    if cloud is False:')
    (56, "        if config[\\'Zarr\\'][\\'activate\\'] == True:")
    (57, "            Array = config[\\'Zarr\\'][type_]")
    (58, '            if all_contigs == True:')
    (59, '                Array = Array.replace("{contig}", "{{contig}}")')
    (60, "        elif config[\\'Zarr\\'][\\'activate\\'] == False:")
    (61, '            if type_ == "Genotype":')
    (62, '                Array = "resources/Zarr/{dataset}/{contig}/calldata/GT" ')
    (63, "            if type_ == \\'Haplotype\\':")
    (64, '                Array == "resources/Zarr/{dataset}/{contig}/calldata/GT" ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if cloud is False', "if config[\\'Zarr\\'][\\'activate\\'] == True", "elif config[\\'Zarr\\'][\\'activate\\'] == False", 'elif type_ == "Positions"']
    (53, 'def getZarrArray(type_="Genotype", all_contigs=False, cloud=False):')
    (54, '')
    (55, '    if cloud is False:')
    (56, "        if config[\\'Zarr\\'][\\'activate\\'] == True:")
    (57, "            Array = config[\\'Zarr\\'][type_]")
    (58, '            if all_contigs == True:')
    (59, '                Array = Array.replace("{contig}", "{{contig}}")')
    (60, "        elif config[\\'Zarr\\'][\\'activate\\'] == False:")
    (61, '            if type_ == "Genotype":')
    (62, '                Array = "resources/Zarr/{dataset}/{contig}/calldata/GT" ')
    (63, "            if type_ == \\'Haplotype\\':")
    (64, '                Array == "resources/Zarr/{dataset}/{contig}/calldata/GT" ')
    (65, '            elif type_ == "Positions":')
    (66, '                Array = "resources/Zarr/{dataset}/{contig}/variants/POS"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if cloud is False', "if config[\\'Zarr\\'][\\'activate\\'] == True", "elif config[\\'Zarr\\'][\\'activate\\'] == False", "elif type_ == \\'SiteFilters\\' and siteFilters is not None"]
    (53, 'def getZarrArray(type_="Genotype", all_contigs=False, cloud=False):')
    (54, '')
    (55, '    if cloud is False:')
    (56, "        if config[\\'Zarr\\'][\\'activate\\'] == True:")
    (57, "            Array = config[\\'Zarr\\'][type_]")
    (58, '            if all_contigs == True:')
    (59, '                Array = Array.replace("{contig}", "{{contig}}")')
    (60, "        elif config[\\'Zarr\\'][\\'activate\\'] == False:")
    (61, '            if type_ == "Genotype":')
    (62, '                Array = "resources/Zarr/{dataset}/{contig}/calldata/GT" ')
    (63, "            if type_ == \\'Haplotype\\':")
    (64, '                Array == "resources/Zarr/{dataset}/{contig}/calldata/GT" ')
    (65, '            elif type_ == "Positions":')
    (66, '                Array = "resources/Zarr/{dataset}/{contig}/variants/POS"')
    (67, "            elif type_ == \\'SiteFilters\\' and siteFilters is not None:")
    (68, '                Array = "resources/Zarr/{dataset}/{contig}/variants/siteFilter"')
    (69, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if cloud is False', "if config[\\'Zarr\\'][\\'activate\\'] == True", "elif config[\\'Zarr\\'][\\'activate\\'] == False", 'if all_contigs == True']
    (53, 'def getZarrArray(type_="Genotype", all_contigs=False, cloud=False):')
    (54, '')
    (55, '    if cloud is False:')
    (56, "        if config[\\'Zarr\\'][\\'activate\\'] == True:")
    (57, "            Array = config[\\'Zarr\\'][type_]")
    (58, '            if all_contigs == True:')
    (59, '                Array = Array.replace("{contig}", "{{contig}}")')
    (60, "        elif config[\\'Zarr\\'][\\'activate\\'] == False:")
    (61, '            if type_ == "Genotype":')
    (62, '                Array = "resources/Zarr/{dataset}/{contig}/calldata/GT" ')
    (63, "            if type_ == \\'Haplotype\\':")
    (64, '                Array == "resources/Zarr/{dataset}/{contig}/calldata/GT" ')
    (65, '            elif type_ == "Positions":')
    (66, '                Array = "resources/Zarr/{dataset}/{contig}/variants/POS"')
    (67, "            elif type_ == \\'SiteFilters\\' and siteFilters is not None:")
    (68, '                Array = "resources/Zarr/{dataset}/{contig}/variants/siteFilter"')
    (69, '')
    (70, '            if all_contigs == True:')
    (71, '                Array = Array.replace("{contig}", "{{contig}}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if cloud is False', "if config[\\'Zarr\\'][\\'activate\\'] == True", 'else']
    (53, 'def getZarrArray(type_="Genotype", all_contigs=False, cloud=False):')
    (54, '')
    (55, '    if cloud is False:')
    (56, "        if config[\\'Zarr\\'][\\'activate\\'] == True:")
    (57, "            Array = config[\\'Zarr\\'][type_]")
    (58, '            if all_contigs == True:')
    (59, '                Array = Array.replace("{contig}", "{{contig}}")')
    (60, "        elif config[\\'Zarr\\'][\\'activate\\'] == False:")
    (61, '            if type_ == "Genotype":')
    (62, '                Array = "resources/Zarr/{dataset}/{contig}/calldata/GT" ')
    (63, "            if type_ == \\'Haplotype\\':")
    (64, '                Array == "resources/Zarr/{dataset}/{contig}/calldata/GT" ')
    (65, '            elif type_ == "Positions":')
    (66, '                Array = "resources/Zarr/{dataset}/{contig}/variants/POS"')
    (67, "            elif type_ == \\'SiteFilters\\' and siteFilters is not None:")
    (68, '                Array = "resources/Zarr/{dataset}/{contig}/variants/siteFilter"')
    (69, '')
    (70, '            if all_contigs == True:')
    (71, '                Array = Array.replace("{contig}", "{{contig}}")')
    (72, '        else:')
    (73, '            Array = []')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if cloud is False', 'else']
    (53, 'def getZarrArray(type_="Genotype", all_contigs=False, cloud=False):')
    (54, '')
    (55, '    if cloud is False:')
    (56, "        if config[\\'Zarr\\'][\\'activate\\'] == True:")
    (57, "            Array = config[\\'Zarr\\'][type_]")
    (58, '            if all_contigs == True:')
    (59, '                Array = Array.replace("{contig}", "{{contig}}")')
    (60, "        elif config[\\'Zarr\\'][\\'activate\\'] == False:")
    (61, '            if type_ == "Genotype":')
    (62, '                Array = "resources/Zarr/{dataset}/{contig}/calldata/GT" ')
    (63, "            if type_ == \\'Haplotype\\':")
    (64, '                Array == "resources/Zarr/{dataset}/{contig}/calldata/GT" ')
    (65, '            elif type_ == "Positions":')
    (66, '                Array = "resources/Zarr/{dataset}/{contig}/variants/POS"')
    (67, "            elif type_ == \\'SiteFilters\\' and siteFilters is not None:")
    (68, '                Array = "resources/Zarr/{dataset}/{contig}/variants/siteFilter"')
    (69, '')
    (70, '            if all_contigs == True:')
    (71, '                Array = Array.replace("{contig}", "{{contig}}")')
    (72, '        else:')
    (73, '            Array = []')
    (74, '    else:')
    (75, '        Array = [] # if using VObs cloud just return an empty list')
    (76, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if cloud is False']
    (53, 'def getZarrArray(type_="Genotype", all_contigs=False, cloud=False):')
    (54, '')
    (55, '    if cloud is False:')
    (56, "        if config[\\'Zarr\\'][\\'activate\\'] == True:")
    (57, "            Array = config[\\'Zarr\\'][type_]")
    (58, '            if all_contigs == True:')
    (59, '                Array = Array.replace("{contig}", "{{contig}}")')
    (60, "        elif config[\\'Zarr\\'][\\'activate\\'] == False:")
    (61, '            if type_ == "Genotype":')
    (62, '                Array = "resources/Zarr/{dataset}/{contig}/calldata/GT" ')
    (63, "            if type_ == \\'Haplotype\\':")
    (64, '                Array == "resources/Zarr/{dataset}/{contig}/calldata/GT" ')
    (65, '            elif type_ == "Positions":')
    (66, '                Array = "resources/Zarr/{dataset}/{contig}/variants/POS"')
    (67, "            elif type_ == \\'SiteFilters\\' and siteFilters is not None:")
    (68, '                Array = "resources/Zarr/{dataset}/{contig}/variants/siteFilter"')
    (69, '')
    (70, '            if all_contigs == True:')
    (71, '                Array = Array.replace("{contig}", "{{contig}}")')
    (72, '        else:')
    (73, '            Array = []')
    (74, '    else:')
    (75, '        Array = [] # if using VObs cloud just return an empty list')
    (76, '')
    (77, '    return(Array)')
    (78, '')
    (79, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if allcontigs == False', "if config[\\'VCF\\'][\\'activate\\'] == True"]
    (80, "def getVCFs(gz=True, allelism = \\'biallelic\\', bothAllelisms=False, allcontigs=False, allcontigsseparately=False):")
    (81, '')
    (82, '    if allcontigs == False:')
    (83, "        if config[\\'VCF\\'][\\'activate\\'] == True:")
    (84, "            genotypes = config[\\'VCF\\'][allelism]")
    (85, '        elif gz == True:')
    (86, '            genotypes = expand("resources/vcfs/{dataset}_{{contig}}.{{allelism}}.vcf.gz", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/{dataset}_{{contig}}.{allelism}.vcf.gz", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (87, '        elif gz == False:')
    (88, '            genotypes = expand("resources/vcfs/{dataset}_{{contig}}.{{allelism}}.vcf", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/{dataset}_{{contig}}.{allelism}.vcf", dataset=config[\\\'dataset\\\'], allelism=allelism)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if allcontigs == False', 'elif allcontigs == True', "if config[\\'VCF\\'][\\'activate\\'] == True"]
    (80, "def getVCFs(gz=True, allelism = \\'biallelic\\', bothAllelisms=False, allcontigs=False, allcontigsseparately=False):")
    (81, '')
    (82, '    if allcontigs == False:')
    (83, "        if config[\\'VCF\\'][\\'activate\\'] == True:")
    (84, "            genotypes = config[\\'VCF\\'][allelism]")
    (85, '        elif gz == True:')
    (86, '            genotypes = expand("resources/vcfs/{dataset}_{{contig}}.{{allelism}}.vcf.gz", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/{dataset}_{{contig}}.{allelism}.vcf.gz", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (87, '        elif gz == False:')
    (88, '            genotypes = expand("resources/vcfs/{dataset}_{{contig}}.{{allelism}}.vcf", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/{dataset}_{{contig}}.{allelism}.vcf", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (89, '    elif allcontigs == True:')
    (90, "        if config[\\'VCF\\'][\\'activate\\'] == True:")
    (91, '            genotypes = expand("resources/vcfs/wholegenome/{dataset}.{{allelism}}.vcf.gz", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/wholegenome/{dataset}.{allelism}.vcf.gz", dataset=config[\\\'dataset\\\'], allelism=allelism)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if allcontigs == False', 'elif allcontigs == True', 'elif gz == True']
    (80, "def getVCFs(gz=True, allelism = \\'biallelic\\', bothAllelisms=False, allcontigs=False, allcontigsseparately=False):")
    (81, '')
    (82, '    if allcontigs == False:')
    (83, "        if config[\\'VCF\\'][\\'activate\\'] == True:")
    (84, "            genotypes = config[\\'VCF\\'][allelism]")
    (85, '        elif gz == True:')
    (86, '            genotypes = expand("resources/vcfs/{dataset}_{{contig}}.{{allelism}}.vcf.gz", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/{dataset}_{{contig}}.{allelism}.vcf.gz", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (87, '        elif gz == False:')
    (88, '            genotypes = expand("resources/vcfs/{dataset}_{{contig}}.{{allelism}}.vcf", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/{dataset}_{{contig}}.{allelism}.vcf", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (89, '    elif allcontigs == True:')
    (90, "        if config[\\'VCF\\'][\\'activate\\'] == True:")
    (91, '            genotypes = expand("resources/vcfs/wholegenome/{dataset}.{{allelism}}.vcf.gz", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/wholegenome/{dataset}.{allelism}.vcf.gz", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (92, '        elif gz == True:')
    (93, '            genotypes = expand("resources/vcfs/wholegenome/{dataset}.{{allelism}}.vcf.gz", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/wholegenome/{dataset}.{allelism}.vcf.gz", dataset=config[\\\'dataset\\\'], allelism=allelism)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if allcontigs == False', 'elif allcontigs == True', 'elif gz == False']
    (80, "def getVCFs(gz=True, allelism = \\'biallelic\\', bothAllelisms=False, allcontigs=False, allcontigsseparately=False):")
    (81, '')
    (82, '    if allcontigs == False:')
    (83, "        if config[\\'VCF\\'][\\'activate\\'] == True:")
    (84, "            genotypes = config[\\'VCF\\'][allelism]")
    (85, '        elif gz == True:')
    (86, '            genotypes = expand("resources/vcfs/{dataset}_{{contig}}.{{allelism}}.vcf.gz", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/{dataset}_{{contig}}.{allelism}.vcf.gz", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (87, '        elif gz == False:')
    (88, '            genotypes = expand("resources/vcfs/{dataset}_{{contig}}.{{allelism}}.vcf", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/{dataset}_{{contig}}.{allelism}.vcf", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (89, '    elif allcontigs == True:')
    (90, "        if config[\\'VCF\\'][\\'activate\\'] == True:")
    (91, '            genotypes = expand("resources/vcfs/wholegenome/{dataset}.{{allelism}}.vcf.gz", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/wholegenome/{dataset}.{allelism}.vcf.gz", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (92, '        elif gz == True:')
    (93, '            genotypes = expand("resources/vcfs/wholegenome/{dataset}.{{allelism}}.vcf.gz", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/wholegenome/{dataset}.{allelism}.vcf.gz", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (94, '        elif gz == False:')
    (95, '            genotypes = expand("resources/vcfs/wholegenome/{dataset}.{{allelism}}.vcf", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/wholegenome/{dataset}.{allelism}.vcf", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (96, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if allcontigs == False', 'if allcontigsseparately']
    (80, "def getVCFs(gz=True, allelism = \\'biallelic\\', bothAllelisms=False, allcontigs=False, allcontigsseparately=False):")
    (81, '')
    (82, '    if allcontigs == False:')
    (83, "        if config[\\'VCF\\'][\\'activate\\'] == True:")
    (84, "            genotypes = config[\\'VCF\\'][allelism]")
    (85, '        elif gz == True:')
    (86, '            genotypes = expand("resources/vcfs/{dataset}_{{contig}}.{{allelism}}.vcf.gz", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/{dataset}_{{contig}}.{allelism}.vcf.gz", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (87, '        elif gz == False:')
    (88, '            genotypes = expand("resources/vcfs/{dataset}_{{contig}}.{{allelism}}.vcf", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/{dataset}_{{contig}}.{allelism}.vcf", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (89, '    elif allcontigs == True:')
    (90, "        if config[\\'VCF\\'][\\'activate\\'] == True:")
    (91, '            genotypes = expand("resources/vcfs/wholegenome/{dataset}.{{allelism}}.vcf.gz", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/wholegenome/{dataset}.{allelism}.vcf.gz", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (92, '        elif gz == True:')
    (93, '            genotypes = expand("resources/vcfs/wholegenome/{dataset}.{{allelism}}.vcf.gz", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/wholegenome/{dataset}.{allelism}.vcf.gz", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (94, '        elif gz == False:')
    (95, '            genotypes = expand("resources/vcfs/wholegenome/{dataset}.{{allelism}}.vcf", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/wholegenome/{dataset}.{allelism}.vcf", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (96, '    ')
    (97, '    if allcontigsseparately:')
    (98, '        genotypes = expand(genotypes, contig=contigs)')
    (99, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/rules/common.smk
context_key: ['if allcontigs == False']
    (80, "def getVCFs(gz=True, allelism = \\'biallelic\\', bothAllelisms=False, allcontigs=False, allcontigsseparately=False):")
    (81, '')
    (82, '    if allcontigs == False:')
    (83, "        if config[\\'VCF\\'][\\'activate\\'] == True:")
    (84, "            genotypes = config[\\'VCF\\'][allelism]")
    (85, '        elif gz == True:')
    (86, '            genotypes = expand("resources/vcfs/{dataset}_{{contig}}.{{allelism}}.vcf.gz", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/{dataset}_{{contig}}.{allelism}.vcf.gz", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (87, '        elif gz == False:')
    (88, '            genotypes = expand("resources/vcfs/{dataset}_{{contig}}.{{allelism}}.vcf", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/{dataset}_{{contig}}.{allelism}.vcf", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (89, '    elif allcontigs == True:')
    (90, "        if config[\\'VCF\\'][\\'activate\\'] == True:")
    (91, '            genotypes = expand("resources/vcfs/wholegenome/{dataset}.{{allelism}}.vcf.gz", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/wholegenome/{dataset}.{allelism}.vcf.gz", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (92, '        elif gz == True:')
    (93, '            genotypes = expand("resources/vcfs/wholegenome/{dataset}.{{allelism}}.vcf.gz", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/wholegenome/{dataset}.{allelism}.vcf.gz", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (94, '        elif gz == False:')
    (95, '            genotypes = expand("resources/vcfs/wholegenome/{dataset}.{{allelism}}.vcf", dataset=config[\\\'dataset\\\']) if bothAllelisms == True else expand("resources/vcfs/wholegenome/{dataset}.{allelism}.vcf", dataset=config[\\\'dataset\\\'], allelism=allelism)')
    (96, '    ')
    (97, '    if allcontigsseparately:')
    (98, '        genotypes = expand(genotypes, contig=contigs)')
    (99, '')
    (100, '    return(genotypes)')
    (101, '')
    (102, '')
    (103, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ianvcaldas/smalldisco, file=workflow/Snakefile
context_key: ['if FASTQDIR.exists()']
    (8, 'def sample_names():')
    (9, '  if FASTQDIR.exists():')
    (10, '    fastqsamples = [f.stem for f in FASTQDIR.iterdir() if f.suffix == ".fastq"]')
    (11, '  else:')
    (12, '    fastqsamples = []')
    (13, '  if BAMDIR.exists():')
    (14, '    bamsamples = [f.stem for f in BAMDIR.iterdir() if f.suffix == ".bam"]')
    (15, '  else:')
    (16, '    bamsamples = []')
    (17, '  return list(set(fastqsamples + bamsamples))')
    (18, '')
    (19, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/Biscuit_Snakemake_Workflow, file=workflow/Snakefile
context_key: ['if step < 1000']
    (25, 'def create_tag(step):')
    (26, '    if step < 1000: # Just bp')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/Biscuit_Snakemake_Workflow, file=workflow/Snakefile
context_key: ['elif step < 1000000']
    (25, 'def create_tag(step):')
    (26, '    if step < 1000: # Just bp')
    (27, "        return f\\'{step}bp\\'")
    (28, '    elif step < 1000000: # kilobases')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBISweden/fungal-trans, file=Snakefile
context_key: ['if "ValidationError']
    (10, 'def parse_validation_error(e):')
    (11, '    """')
    (12, '    Catches errors thrown when validating the config file and attempts to')
    (13, '    print more meaningful messages.')
    (14, '')
    (15, '    :param e: Error')
    (16, '    :return:')
    (17, '    """')
    (18, '    instance = ""')
    (19, '    print("ERROR VALIDATING CONFIG FILE")')
    (20, '    for item in str(e).split("\\')
    (21, '"):')
    (22, '        item = item.replace(\\\'"\\\',\\\'\\\')')
    (23, '        if "ValidationError:" in item:')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBISweden/fungal-trans, file=Snakefile
context_key: ['if item[0']
    (10, 'def parse_validation_error(e):')
    (11, '    """')
    (12, '    Catches errors thrown when validating the config file and attempts to')
    (13, '    print more meaningful messages.')
    (14, '')
    (15, '    :param e: Error')
    (16, '    :return:')
    (17, '    """')
    (18, '    instance = ""')
    (19, '    print("ERROR VALIDATING CONFIG FILE")')
    (20, '    for item in str(e).split("\\')
    (21, '"):')
    (22, '        item = item.replace(\\\'"\\\',\\\'\\\')')
    (23, '        if "ValidationError:" in item:')
    (24, '            print(item)')
    (25, '        if item[0:11] == "On instance":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBISweden/fungal-trans, file=Snakefile
context_key: ['if not os.getenv("TMPDIR")']
    (10, 'def parse_validation_error(e):')
    (11, '    """')
    (12, '    Catches errors thrown when validating the config file and attempts to')
    (13, '    print more meaningful messages.')
    (14, '')
    (15, '    :param e: Error')
    (16, '    :return:')
    (17, '    """')
    (18, '    instance = ""')
    (19, '    print("ERROR VALIDATING CONFIG FILE")')
    (20, '    for item in str(e).split("\\')
    (21, '"):')
    (22, '        item = item.replace(\\\'"\\\',\\\'\\\')')
    (23, '        if "ValidationError:" in item:')
    (24, '            print(item)')
    (25, '        if item[0:11] == "On instance":')
    (26, '            instance = item.replace("On instance[\\\'", "INCORRECT CONFIG AT: ")')
    (27, '            instance = instance.replace("\\\']:","")')
    (28, '            print(instance)')
    (29, '    return')
    (30, '')
    (31, '# Set temporary dir')
    (32, 'if not os.getenv("TMPDIR"):')
    (33, '    os.environ["TMPDIR"] = "tmp"')
    (34, '    os.makedirs(os.environ["TMPDIR"],exist_ok=True)')
    (35, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBISweden/fungal-trans, file=Snakefile
context_key: ['if os.path.exists("config.yml")']
    (10, 'def parse_validation_error(e):')
    (11, '    """')
    (12, '    Catches errors thrown when validating the config file and attempts to')
    (13, '    print more meaningful messages.')
    (14, '')
    (15, '    :param e: Error')
    (16, '    :return:')
    (17, '    """')
    (18, '    instance = ""')
    (19, '    print("ERROR VALIDATING CONFIG FILE")')
    (20, '    for item in str(e).split("\\')
    (21, '"):')
    (22, '        item = item.replace(\\\'"\\\',\\\'\\\')')
    (23, '        if "ValidationError:" in item:')
    (24, '            print(item)')
    (25, '        if item[0:11] == "On instance":')
    (26, '            instance = item.replace("On instance[\\\'", "INCORRECT CONFIG AT: ")')
    (27, '            instance = instance.replace("\\\']:","")')
    (28, '            print(instance)')
    (29, '    return')
    (30, '')
    (31, '# Set temporary dir')
    (32, 'if not os.getenv("TMPDIR"):')
    (33, '    os.environ["TMPDIR"] = "tmp"')
    (34, '    os.makedirs(os.environ["TMPDIR"],exist_ok=True)')
    (35, '')
    (36, 'wildcard_constraints:')
    (37, '    sample_id = "[A-Za-z0-9_\\\\-\\\\.]+",')
    (38, '    assembler = "megahit|trinity|transabyss",')
    (39, '    filter_source = "unfiltered|filtered|taxmapper|bowtie2"')
    (40, '')
    (41, '# Set default config and validate against schema')
    (42, 'if os.path.exists("config.yml"):')
    (43, '    configfile: "config.yml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBISweden/fungal-trans, file=source/rules/filter.smk
context_key: ['if seqid in wanted']
    (494, 'def find_reads(infile, fhout, wanted):')
    (495, '    from Bio.SeqIO.QualityIO import FastqGeneralIterator')
    (496, '    with open(os.path.expandvars(infile)) as fhin:')
    (497, '        for title, seq, qual in FastqGeneralIterator(fhin):')
    (498, '            seqid = title.split(None, 1)[0]')
    (499, '            if seqid in wanted:')
    (500, '                fhout.write("@{}\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/ASCENT, file=workflow/rules/init.smk
context_key: ['if check_readaccess(filename)']
    (8, 'def get_file_size(filename):')
    (9, '    filename=filename.strip()')
    (10, '    if check_readaccess(filename):')
    (11, '        return os.stat(filename).st_size')
    (12, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/ASCENT, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(filename)']
    (22, 'def check_existence(filename):')
    (23, '    """Checks if file exists on filesystem')
    (24, '    :param filename <str>: Name of file to check')
    (25, '    """')
    (26, '    filename=filename.strip()')
    (27, '    if not os.path.exists(filename):')
    (28, '        sys.exit("File: {} does not exists!".format(filename))')
    (29, '    return True')
    (30, '')
    (31, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/ASCENT, file=workflow/rules/init.smk
context_key: ['if not os.access(filename,os.R_OK)']
    (32, 'def check_readaccess(filename):')
    (33, '    """Checks permissions to see if user can read a file')
    (34, '    :param filename <str>: Name of file to check')
    (35, '    """')
    (36, '    filename=filename.strip()')
    (37, '    check_existence(filename)')
    (38, '    if not os.access(filename,os.R_OK):')
    (39, '        sys.exit("File: {} exists, but user cannot read from file due to permissions!".format(filename))')
    (40, '    return True')
    (41, '')
    (42, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/ASCENT, file=workflow/rules/init.smk
context_key: ['if not os.access(filename,os.W_OK)']
    (43, 'def check_writeaccess(filename):')
    (44, '    """Checks permissions to see if user can write to a file')
    (45, '    :param filename <str>: Name of file to check')
    (46, '    """')
    (47, '    filename=filename.strip()')
    (48, '    check_existence(filename)')
    (49, '    if not os.access(filename,os.W_OK):')
    (50, '        sys.exit("File: {} exists, but user cannot write to file due to permissions!".format(filename))')
    (51, '    return True')
    (52, '')
    (53, '##### load config and sample sheets #####')
    (54, '')
    (55, '#validate(config, "config.schema.yaml")')
    (56, '')
    (57, '## set memory limit ')
    (58, '## used for sambamba sort, etc')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/ASCENT, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(join(WORKDIR,"fastqs"))']
    (43, 'def check_writeaccess(filename):')
    (44, '    """Checks permissions to see if user can write to a file')
    (45, '    :param filename <str>: Name of file to check')
    (46, '    """')
    (47, '    filename=filename.strip()')
    (48, '    check_existence(filename)')
    (49, '    if not os.access(filename,os.W_OK):')
    (50, '        sys.exit("File: {} exists, but user cannot write to file due to permissions!".format(filename))')
    (51, '    return True')
    (52, '')
    (53, '##### load config and sample sheets #####')
    (54, '')
    (55, '#validate(config, "config.schema.yaml")')
    (56, '')
    (57, '## set memory limit ')
    (58, '## used for sambamba sort, etc')
    (59, 'MEMORYG="100G"')
    (60, '')
    (61, '#resouce absolute path')
    (62, "WORKDIR=config[\\'workdir\\']")
    (63, 'CONFIGFILE=join(WORKDIR,"config.yaml")')
    (64, 'check_readaccess(CONFIGFILE)')
    (65, 'configfile: CONFIGFILE')
    (66, 'with open(CONFIGFILE) as f:')
    (67, '    CONFIG = yaml.safe_load(f)')
    (68, '')
    (69, "SCRIPTSDIR=config[\\'scriptsdir\\']")
    (70, "RESOURCESDIR=config[\\'resourcesdir\\']")
    (71, 'if not os.path.exists(join(WORKDIR,"fastqs")):')
    (72, '    os.mkdir(join(WORKDIR,"fastqs"))')
    (73, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/ASCENT, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(R1filenewname)']
    (43, 'def check_writeaccess(filename):')
    (44, '    """Checks permissions to see if user can write to a file')
    (45, '    :param filename <str>: Name of file to check')
    (46, '    """')
    (47, '    filename=filename.strip()')
    (48, '    check_existence(filename)')
    (49, '    if not os.access(filename,os.W_OK):')
    (50, '        sys.exit("File: {} exists, but user cannot write to file due to permissions!".format(filename))')
    (51, '    return True')
    (52, '')
    (53, '##### load config and sample sheets #####')
    (54, '')
    (55, '#validate(config, "config.schema.yaml")')
    (56, '')
    (57, '## set memory limit ')
    (58, '## used for sambamba sort, etc')
    (59, 'MEMORYG="100G"')
    (60, '')
    (61, '#resouce absolute path')
    (62, "WORKDIR=config[\\'workdir\\']")
    (63, 'CONFIGFILE=join(WORKDIR,"config.yaml")')
    (64, 'check_readaccess(CONFIGFILE)')
    (65, 'configfile: CONFIGFILE')
    (66, 'with open(CONFIGFILE) as f:')
    (67, '    CONFIG = yaml.safe_load(f)')
    (68, '')
    (69, "SCRIPTSDIR=config[\\'scriptsdir\\']")
    (70, "RESOURCESDIR=config[\\'resourcesdir\\']")
    (71, 'if not os.path.exists(join(WORKDIR,"fastqs")):')
    (72, '    os.mkdir(join(WORKDIR,"fastqs"))')
    (73, '')
    (74, 'for f in ["samples", "tools", "cluster"]:')
    (75, '    check_readaccess(config[f])')
    (76, '')
    (77, 'SAMPLESTSV = config["samples"]')
    (78, 'SAMPLESDF = pd.read_csv(SAMPLESTSV,sep="\\\\t",header=0,index_col="sampleName")')
    (79, 'SAMPLES = list(SAMPLESDF.index)')
    (80, 'SAMPLESDF["R1"]=join(RESOURCESDIR,"dummy")')
    (81, 'SAMPLESDF["R2"]=join(RESOURCESDIR,"dummy")')
    (82, 'SAMPLESDF["PEorSE"]="PE"')
    (83, '')
    (84, 'GROUPS=list(set(list(SAMPLESDF["group"])))')
    (85, 'GROUP2SAMPLES=dict()')
    (86, 'for g in GROUPS:')
    (87, '    GROUP2SAMPLES[g]=list(SAMPLESDF[SAMPLESDF.group==g].index)')
    (88, '')
    (89, 'for sample in SAMPLES:')
    (90, '    R1file=SAMPLESDF["path_to_R1_fastq"][sample]')
    (91, '    R2file=SAMPLESDF["path_to_R2_fastq"][sample]')
    (92, '    # print(sample,R1file,R2file)')
    (93, '    check_readaccess(R1file)')
    (94, '    R1filenewname=join(WORKDIR,"fastqs",sample+".R1.fastq.gz")')
    (95, '    if not os.path.exists(R1filenewname):')
    (96, '        os.symlink(R1file,R1filenewname)')
    (97, '        # os.link(R1file,R1filenewname)')
    (98, '    SAMPLESDF.loc[[sample],"R1"]=R1filenewname')
    (99, "    if str(R2file)!=\\'nan\\':")
    (100, '        check_readaccess(R2file)')
    (101, '        R2filenewname=join(WORKDIR,"fastqs",sample+".R2.fastq.gz")')
    (102, '        if not os.path.exists(R2filenewname):')
    (103, '            os.symlink(R2file,R2filenewname)')
    (104, '            # os.link(R2file,R2filenewname)')
    (105, '        SAMPLESDF.loc[[sample],"R2"]=R2filenewname')
    (106, '    else:')
    (107, '        SAMPLESDF.loc[[sample],"PEorSE"]="SE"')
    (108, '        sys.exit(" This pipeline only works for PAIRED end samples. If your samples are paired end, then please verify that they have been entered correctly in samples.tsv ")')
    (109, '# print(SAMPLESDF)')
    (110, '# sys.exit()')
    (111, '')
    (112, '#########################################################')
    (113, '# READ IN TOOLS REQUIRED BY PIPELINE')
    (114, '# THESE INCLUDE LIST OF BIOWULF MODULES (AND THEIR VERSIONS)')
    (115, '# MAY BE EMPTY IF ALL TOOLS ARE DOCKERIZED')
    (116, '#########################################################')
    (117, '## Load tools from YAML file')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=gibsramen/qadabra, file=qadabra/workflow/rules/common.smk
context_key: ['if "confounders" in keys', 'if not np.isnan(d["confounders"])']
    (7, 'def get_dataset_cfg(wildcards, keys):')
    (8, '    d = datasets.loc[wildcards.dataset, keys].to_dict()')
    (9, '    if "confounders" in keys:')
    (10, '        if not np.isnan(d["confounders"]):')
    (11, '            d["confounders"] = d["confounders"].split(";")')
    (12, '        else:')
    (13, '            d["confounders"] = []')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=gibsramen/qadabra, file=qadabra/workflow/rules/common.smk
context_key: ['if "confounders" in keys']
    (7, 'def get_dataset_cfg(wildcards, keys):')
    (8, '    d = datasets.loc[wildcards.dataset, keys].to_dict()')
    (9, '    if "confounders" in keys:')
    (10, '        if not np.isnan(d["confounders"]):')
    (11, '            d["confounders"] = d["confounders"].split(";")')
    (12, '        else:')
    (13, '            d["confounders"] = []')
    (14, '    return d')
    (15, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=gibsramen/qadabra, file=qadabra/workflow/rules/common.smk
context_key: ['if not np.isnan(d["confounders"])']
    (16, 'def get_songbird_formula(wildcards):')
    (17, '    d = datasets.loc[wildcards.dataset].to_dict()')
    (18, '')
    (19, '    covariate = d["factor_name"]')
    (20, '    reference = d["reference_level"]')
    (21, '    formula = f"C({covariate}, Treatment(\\\'{reference}\\\'))"')
    (22, '    if not np.isnan(d["confounders"]):')
    (23, '        confounders = d["confounders"].split(";")')
    (24, '        formula = f"{formula} + {\\\' + \\\'.join(confounders)}"')
    (25, '    return formula')
    (26, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=huddlej/example-nextstrain-workflow, file=rules/common.smk
context_key: ['if wildcards.region == "global"']
    (4, 'def _get_query_argument_by_wildcards(wildcards):')
    (5, '    if wildcards.region == "global":')
    (6, '        return ""')
    (7, '    else:')
    (8, '        region_name = _get_region_name_by_wildcards(wildcards)')
    (9, '        return f"--query \\\\"region == \\\'{region_name}\\\'\\\\""')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmenglund/FEGA-encryption-workflow, file=Snakefile
context_key: ['if os.path.isdir(full_path)']
    (22, 'def get_list_of_files(dir_path, base_path):')
    (23, '    """')
    (24, '    List paths to all files in the directory, including files in')
    (25, '    subdirectories (but not the subdirectories themselves).')
    (26, '')
    (27, '    Parameters')
    (28, '    ----------')
    (29, '    dir_path : str')
    (30, '        Path to directory for which all contained files should be listed.')
    (31, '    base_path : str')
    (32, '        Base path for constructing relative paths.')
    (33, '    """')
    (34, '    file_entries = os.listdir(dir_path)')
    (35, '    all_files = list()')
    (36, '    for entry in file_entries:')
    (37, '        full_path = os.path.join(dir_path, entry)')
    (38, '        if os.path.isdir(full_path):')
    (39, '            all_files = all_files + get_list_of_files(full_path, base_path)')
    (40, '        else:')
    (41, '            all_files.append(os.path.relpath(full_path, base_path))')
    (42, '    return all_files')
    (43, '')
    (44, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Mar111tiN/RNAseq, file=rules/common.smk
context_key: ["if not sample_sheet.startswith(\\'/\\')"]
    (10, 'def get_sample_dfs(sample_sheet=".", unit_sheet="."):')
    (11, '')
    (12, '    # check full path or append path to snakedir')
    (13, "    if not sample_sheet.startswith(\\'/\\'):")
    (14, '        sample_sheet = os.path.join(snakedir, sample_sheet)')
    (15, '    ')
    (16, '    samples = (')
    (17, '        pd.read_csv(sample_sheet, sep="\\\\t", dtype={"sample_name": str})')
    (18, '        .set_index("sample_name", drop=False)')
    (19, '        .sort_index()')
    (20, '    )')
    (21, '    # validate(samples, schema="../schemas/samples.schema.yaml")')
    (22, '')
    (23, "    if not unit_sheet.startswith(\\'/\\'):")
    (24, '        unit_sheet = os.path.join(snakedir, unit_sheet)')
    (25, '')
    (26, '    units = (')
    (27, '        pd.read_csv(unit_sheet, sep="\\\\t", dtype={"sample_name": str, "unit_name": str})')
    (28, '        .set_index(["sample_name", "unit_name"], drop=False)')
    (29, '        .sort_index()')
    (30, '        # fillna for easy checking of adapters in get_adapters')
    (31, '        .fillna("")')
    (32, '    )')
    (33, '')
    (34, '    # validate(units, schema="../schemas/units.schema.yaml")')
    (35, '    return (samples, units)')
    (36, '')
    (37, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Mar111tiN/RNAseq, file=rules/common.smk
context_key: ['if unit["fq1"].endswith("gz")']
    (85, 'def get_cutadapt_input(wildcards):')
    (86, '    unit = units.loc[wildcards.sample].loc[wildcards.unit]')
    (87, '')
    (88, '    if unit["fq1"].endswith("gz"):')
    (89, '        ending = ".gz"')
    (90, '    else:')
    (91, '        ending = ""')
    (92, '')
    (93, '    if pd.isna(unit["fq2"]):')
    (94, '        # single end local sample')
    (95, '        return "pipe/cutadapt/{S}/{U}.fq1.fastq{E}".format(')
    (96, '            S=unit.sample_name, U=unit.unit_name, E=ending')
    (97, '        )')
    (98, '    else:')
    (99, '        # paired end local sample')
    (100, '        return expand(')
    (101, '            "pipe/cutadapt/{S}/{U}.{{read}}.fastq{E}".format(')
    (102, '                S=unit.sample_name, U=unit.unit_name, E=ending')
    (103, '            ),')
    (104, '            read=["fq1", "fq2"],')
    (105, '        )')
    (106, '')
    (107, '')
    (108, '############## fastq ###############')
    (109, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Mar111tiN/RNAseq, file=rules/common.smk
context_key: ["if u[\\'fastq2\\']"]
    (110, 'def get_raw_fastq(w):')
    (111, '')
    (112, "    fastq_path = config[\\'input_dir\\']")
    (113, '    # no trimming, use raw reads')
    (114, '    u = units.loc[(w.sample, w.unit), :]')
    (115, '')
    (116, "    if u[\\'fastq2\\']:")
    (117, '    # PE case')
    (118, '        return {')
    (119, '            "fastq1": os.path.join(fastq_path, u.fastq1), ')
    (120, '            "fastq2": os.path.join(fastq_path, u.fastq2)')
    (121, '            }')
    (122, '    # SE case')
    (123, '    return {"fastq1": os.path.join(fastq_path, u.fastq1)}')
    (124, '')
    (125, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Mar111tiN/RNAseq, file=rules/common.smk
context_key: ["if unit[\\'fastq2\\']"]
    (126, 'def get_trim_fastq(w):')
    (127, '    # get the respective entry from the units_df (use tuple indexing for dual row index)')
    (128, '    unit = units.loc[(w.sample, w.unit), :]')
    (129, '')
    (130, "    if unit[\\'fastq2\\']:")
    (131, '    # PE case')
    (132, '        return {f"fastq{read}": f"results/trimmed/{w.sample}-{w.unit}_R{read}.fastq.gz" for read in [1, 2]}')
    (133, '    # SE case')
    (134, '    return {f"fastq1": f"results/trimmed/{w.sample}-{w.unit}.fastq.gz"}')
    (135, '')
    (136, '')
    (137, '############ STAR ###########################')
    (138, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Mar111tiN/RNAseq, file=rules/common.smk
context_key: ['if w.trim == "trim"']
    (177, 'def get_qc_fastq(w):')
    (178, "    \\'\\'\\'")
    (179, '    returns the trimmed/untrimmed mates/single fastq depending on trim wildcard and existence of second fastq in units_df')
    (180, "    \\'\\'\\'")
    (181, '')
    (182, '    # trimmed fastq as input')
    (183, '    if w.trim == "trim":')
    (184, '        # is SE or PE (read can be "" for SE')
    (185, '        return f"results/trimmed/{w.sample}-{w.unit}{w.read}.fastq.gz"')
    (186, '    # is raw fastq --> get fastq path from ')
    (187, "    fastq_path = config[\\'input_dir\\']")
    (188, '    # no trimming, use raw reads')
    (189, '    u = units.loc[(w.sample, w.unit), :]')
    (190, '    # is read2')
    (191, '    if w.read == "_R2":')
    (192, '        return os.path.join(fastq_path, u.fastq2)')
    (193, '    # raw fastq as input')
    (194, "    return os.path.join(fastq_path, u.fastq1)'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mike-molnar/nanopore-SV-analysis, file=Snakefile
context_key: ['if os.path.exists(file_name)', 'if re.search(pattern, line)']
    (21, 'def get_coverage(sample_name):')
    (22, '    file_name = str(sample_name) + "/analysis/nanoplot/" + str(sample_name) + "NanoStats.txt"')
    (23, '    if os.path.exists(file_name):')
    (24, '        f = open(file_name)')
    (25, '        pattern = "Total bases"')
    (26, '        for line in f:')
    (27, '            if re.search(pattern, line):')
    (28, '                total_bases = line.split(":")[1].strip().replace(\\\',\\\', \\\'\\\')')
    (29, '                return float(total_bases)/3100000000')
    (30, '')
    (31, '# Get the list of regions for the workflow')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/Snakefile
context_key: ['if runclear==True or runclear=="True" or runclear=="TRUE"']
    (16, 'def get_clear_target_files(runclear):')
    (17, '    targetfiles=[]')
    (18, '    if runclear==True or runclear=="True" or runclear=="TRUE":')
    (19, '        for s in SAMPLES:')
    (20, '            targetfiles.append(join(WORKDIR,"results",s,"CLEAR","quant.txt"))')
    (21, '            targetfiles.append(join(WORKDIR,"results",s,"CLEAR","quant.txt.annotated"))')
    (22, '    return targetfiles')
    (23, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/Snakefile
context_key: ['if rundcc==True or rundcc=="True" or rundcc=="TRUE"']
    (24, 'def get_dcc_target_files(rundcc):')
    (25, '    targetfiles=[]')
    (26, '    if rundcc==True or rundcc=="True" or rundcc=="TRUE":')
    (27, '        for s in SAMPLES:')
    (28, '            targetfiles.append(join(WORKDIR,"results",s,"DCC",s+".dcc.counts_table.tsv"))')
    (29, '    return targetfiles')
    (30, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/Snakefile
context_key: ['if runmapslice==True or runmapslice=="True" or runmapslice=="TRUE"']
    (31, 'def get_mapsplice_target_files(runmapslice):')
    (32, '    targetfiles=[]')
    (33, '    if runmapslice==True or runmapslice=="True" or runmapslice=="TRUE":')
    (34, '        for s in SAMPLES:')
    (35, '            targetfiles.append(join(WORKDIR,"results",s,"MapSplice","circular_RNAs.txt"))')
    (36, '            targetfiles.append(join(WORKDIR,"results",s,"MapSplice","alignments.bam"))')
    (37, '    return targetfiles')
    (38, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/Snakefile
context_key: ['if not os.path.exists(join(WORKDIR,"results"))']
    (39, 'def get_nclscan_target_files(runnclscan):')
    (40, '    targetfiles=[]')
    (41, '    if not os.path.exists(join(WORKDIR,"results")): os.mkdir(join(WORKDIR,"results"))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/Snakefile
context_key: ['if runnclscan==True or runnclscan=="True" or runnclscan=="TRUE"', 'if SAMPLESDF.loc[[s],"PEorSE"][0]=="SE"']
    (39, 'def get_nclscan_target_files(runnclscan):')
    (40, '    targetfiles=[]')
    (41, '    if not os.path.exists(join(WORKDIR,"results")): os.mkdir(join(WORKDIR,"results"))')
    (42, '    if runnclscan==True or runnclscan=="True" or runnclscan=="TRUE":')
    (43, '        for s in SAMPLES:')
    (44, '            if not os.path.exists(join(WORKDIR,"results",s)): os.mkdir(join(WORKDIR,"results",s))')
    (45, '            if not os.path.exists(join(WORKDIR,"results",s,"NCLscan")): os.mkdir(join(WORKDIR,"results",s,"NCLscan"))')
    (46, '            if SAMPLESDF.loc[[s],"PEorSE"][0]=="SE":')
    (47, '                Path(join(WORKDIR,"results",s,"NCLscan",s+".result")).touch() # nclscan cannot run for se ')
    (48, '                with open(join(WORKDIR,"results",s,"NCLscan",s+".nclscan.counts_table.tsv"),\\\'w\\\') as f:')
    (49, '                    f.write("chrom\\\\tend\\\\tstart\\\\tstrand\\\\tread_count\\\\tnclscan_annotation\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/init.smk
context_key: ['if check_readaccess(filename)']
    (2, 'def get_file_size(filename):')
    (3, '    filename=filename.strip()')
    (4, '    if check_readaccess(filename):')
    (5, '        return os.stat(filename).st_size')
    (6, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(filename)']
    (10, 'def check_existence(filename):')
    (11, '    """Checks if file exists on filesystem')
    (12, '    :param filename <str>: Name of file to check')
    (13, '    """')
    (14, '    filename=filename.strip()')
    (15, '    if not os.path.exists(filename):')
    (16, '        sys.exit("File: {} does not exists!".format(filename))')
    (17, '    return True')
    (18, '')
    (19, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/init.smk
context_key: ['if not os.access(filename,os.R_OK)']
    (20, 'def check_readaccess(filename):')
    (21, '    """Checks permissions to see if user can read a file')
    (22, '    :param filename <str>: Name of file to check')
    (23, '    """')
    (24, '    filename=filename.strip()')
    (25, '    check_existence(filename)')
    (26, '    if not os.access(filename,os.R_OK):')
    (27, '        sys.exit("File: {} exists, but user cannot read from file due to permissions!".format(filename))')
    (28, '    return True')
    (29, '')
    (30, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/init.smk
context_key: ['if not os.access(filename,os.W_OK)']
    (31, 'def check_writeaccess(filename):')
    (32, '    """Checks permissions to see if user can write to a file')
    (33, '    :param filename <str>: Name of file to check')
    (34, '    """')
    (35, '    filename=filename.strip()')
    (36, '    check_existence(filename)')
    (37, '    if not os.access(filename,os.W_OK):')
    (38, '        sys.exit("File: {} exists, but user cannot write to file due to permissions!".format(filename))')
    (39, '    return True')
    (40, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(ofile)']
    (41, 'def append_files_in_list(flist,ofile):')
    (42, '    if not os.path.exists(ofile):')
    (43, "        with open(ofile, \\'w\\') as outfile:")
    (44, '            for fname in flist:')
    (45, '                with open(fname) as infile:')
    (46, '                    outfile.write(infile.read())')
    (47, '    return True')
    (48, '')
    (49, '##### load config and sample sheets #####')
    (50, '')
    (51, '# check_readaccess("config/config.yaml")')
    (52, '# configfile: "config/config.yaml"')
    (53, '')
    (54, '## set memory limit ')
    (55, '## used for sambamba sort, etc')
    (56, '# MEMORYG="100G"')
    (57, '')
    (58, '#resouce absolute path')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(REF_DIR)']
    (41, 'def append_files_in_list(flist,ofile):')
    (42, '    if not os.path.exists(ofile):')
    (43, "        with open(ofile, \\'w\\') as outfile:")
    (44, '            for fname in flist:')
    (45, '                with open(fname) as infile:')
    (46, '                    outfile.write(infile.read())')
    (47, '    return True')
    (48, '')
    (49, '##### load config and sample sheets #####')
    (50, '')
    (51, '# check_readaccess("config/config.yaml")')
    (52, '# configfile: "config/config.yaml"')
    (53, '')
    (54, '## set memory limit ')
    (55, '## used for sambamba sort, etc')
    (56, '# MEMORYG="100G"')
    (57, '')
    (58, '#resouce absolute path')
    (59, "WORKDIR=config[\\'workdir\\']")
    (60, "SCRIPTS_DIR=config[\\'scriptsdir\\']")
    (61, "RESOURCES_DIR=config[\\'resourcesdir\\']")
    (62, "FASTAS_GTFS_DIR=config[\\'fastas_gts_dir\\']")
    (63, '')
    (64, 'REF_DIR=join(WORKDIR,"ref")')
    (65, 'if not os.path.exists(REF_DIR):')
    (66, '    os.mkdir(REF_DIR)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(STAR_INDEX_DIR)']
    (41, 'def append_files_in_list(flist,ofile):')
    (42, '    if not os.path.exists(ofile):')
    (43, "        with open(ofile, \\'w\\') as outfile:")
    (44, '            for fname in flist:')
    (45, '                with open(fname) as infile:')
    (46, '                    outfile.write(infile.read())')
    (47, '    return True')
    (48, '')
    (49, '##### load config and sample sheets #####')
    (50, '')
    (51, '# check_readaccess("config/config.yaml")')
    (52, '# configfile: "config/config.yaml"')
    (53, '')
    (54, '## set memory limit ')
    (55, '## used for sambamba sort, etc')
    (56, '# MEMORYG="100G"')
    (57, '')
    (58, '#resouce absolute path')
    (59, "WORKDIR=config[\\'workdir\\']")
    (60, "SCRIPTS_DIR=config[\\'scriptsdir\\']")
    (61, "RESOURCES_DIR=config[\\'resourcesdir\\']")
    (62, "FASTAS_GTFS_DIR=config[\\'fastas_gts_dir\\']")
    (63, '')
    (64, 'REF_DIR=join(WORKDIR,"ref")')
    (65, 'if not os.path.exists(REF_DIR):')
    (66, '    os.mkdir(REF_DIR)')
    (67, 'STAR_INDEX_DIR=join(REF_DIR,"STAR_no_GTF")')
    (68, 'if not os.path.exists(STAR_INDEX_DIR):')
    (69, '    os.mkdir(STAR_INDEX_DIR)')
    (70, '# strip trailing slashes if any')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/init.smk
context_key: ['if ADDITIVES != ""']
    (41, 'def append_files_in_list(flist,ofile):')
    (42, '    if not os.path.exists(ofile):')
    (43, "        with open(ofile, \\'w\\') as outfile:")
    (44, '            for fname in flist:')
    (45, '                with open(fname) as infile:')
    (46, '                    outfile.write(infile.read())')
    (47, '    return True')
    (48, '')
    (49, '##### load config and sample sheets #####')
    (50, '')
    (51, '# check_readaccess("config/config.yaml")')
    (52, '# configfile: "config/config.yaml"')
    (53, '')
    (54, '## set memory limit ')
    (55, '## used for sambamba sort, etc')
    (56, '# MEMORYG="100G"')
    (57, '')
    (58, '#resouce absolute path')
    (59, "WORKDIR=config[\\'workdir\\']")
    (60, "SCRIPTS_DIR=config[\\'scriptsdir\\']")
    (61, "RESOURCES_DIR=config[\\'resourcesdir\\']")
    (62, "FASTAS_GTFS_DIR=config[\\'fastas_gts_dir\\']")
    (63, '')
    (64, 'REF_DIR=join(WORKDIR,"ref")')
    (65, 'if not os.path.exists(REF_DIR):')
    (66, '    os.mkdir(REF_DIR)')
    (67, 'STAR_INDEX_DIR=join(REF_DIR,"STAR_no_GTF")')
    (68, 'if not os.path.exists(STAR_INDEX_DIR):')
    (69, '    os.mkdir(STAR_INDEX_DIR)')
    (70, '# strip trailing slashes if any')
    (71, 'for d in [WORKDIR,SCRIPTS_DIR,RESOURCES_DIR,FASTAS_GTFS_DIR,STAR_INDEX_DIR,REF_DIR]:')
    (72, "    d=d.strip(\\'r\\\\/\\')")
    (73, 'BWA_INDEX=join(REF_DIR,"ref")')
    (74, '')
    (75, "HOST=config[\\'host\\'] # hg38 or mm39")
    (76, "ADDITIVES=config[\\'additives\\'] # ERCC and/or BAC16Insert")
    (77, 'if ADDITIVES != "":')
    (78, '    HOST_ADDITIVES=HOST+","+ADDITIVES')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(join(WORKDIR,"fastqs"))']
    (41, 'def append_files_in_list(flist,ofile):')
    (42, '    if not os.path.exists(ofile):')
    (43, "        with open(ofile, \\'w\\') as outfile:")
    (44, '            for fname in flist:')
    (45, '                with open(fname) as infile:')
    (46, '                    outfile.write(infile.read())')
    (47, '    return True')
    (48, '')
    (49, '##### load config and sample sheets #####')
    (50, '')
    (51, '# check_readaccess("config/config.yaml")')
    (52, '# configfile: "config/config.yaml"')
    (53, '')
    (54, '## set memory limit ')
    (55, '## used for sambamba sort, etc')
    (56, '# MEMORYG="100G"')
    (57, '')
    (58, '#resouce absolute path')
    (59, "WORKDIR=config[\\'workdir\\']")
    (60, "SCRIPTS_DIR=config[\\'scriptsdir\\']")
    (61, "RESOURCES_DIR=config[\\'resourcesdir\\']")
    (62, "FASTAS_GTFS_DIR=config[\\'fastas_gts_dir\\']")
    (63, '')
    (64, 'REF_DIR=join(WORKDIR,"ref")')
    (65, 'if not os.path.exists(REF_DIR):')
    (66, '    os.mkdir(REF_DIR)')
    (67, 'STAR_INDEX_DIR=join(REF_DIR,"STAR_no_GTF")')
    (68, 'if not os.path.exists(STAR_INDEX_DIR):')
    (69, '    os.mkdir(STAR_INDEX_DIR)')
    (70, '# strip trailing slashes if any')
    (71, 'for d in [WORKDIR,SCRIPTS_DIR,RESOURCES_DIR,FASTAS_GTFS_DIR,STAR_INDEX_DIR,REF_DIR]:')
    (72, "    d=d.strip(\\'r\\\\/\\')")
    (73, 'BWA_INDEX=join(REF_DIR,"ref")')
    (74, '')
    (75, "HOST=config[\\'host\\'] # hg38 or mm39")
    (76, "ADDITIVES=config[\\'additives\\'] # ERCC and/or BAC16Insert")
    (77, 'if ADDITIVES != "":')
    (78, '    HOST_ADDITIVES=HOST+","+ADDITIVES')
    (79, 'else:')
    (80, '    HOST_ADDITIVES=HOST')
    (81, "VIRUSES=config[\\'viruses\\']")
    (82, 'REPEATS_GTF=join(FASTAS_GTFS_DIR,HOST+".repeats.gtf")')
    (83, '')
    (84, 'HOST_ADDITIVES_VIRUSES=HOST_ADDITIVES+","+VIRUSES')
    (85, 'HOST_ADDITIVES_VIRUSES=HOST_ADDITIVES_VIRUSES.split(",")')
    (86, 'FASTAS=[join(FASTAS_GTFS_DIR,f+".fa") for f in HOST_ADDITIVES_VIRUSES]')
    (87, 'REGIONS=[join(FASTAS_GTFS_DIR,f+".fa.regions") for f in HOST_ADDITIVES_VIRUSES]')
    (88, 'GTFS=[join(FASTAS_GTFS_DIR,f+".gtf") for f in HOST_ADDITIVES_VIRUSES]')
    (89, 'FASTAS_REGIONS_GTFS=FASTAS.copy()')
    (90, 'FASTAS_REGIONS_GTFS.extend(REGIONS)')
    (91, 'FASTAS_REGIONS_GTFS.extend(GTFS)')
    (92, '')
    (93, "ANNOTATION_LOOKUP=config[\\'annotation_lookups\\'][HOST]")
    (94, '')
    (95, 'if not os.path.exists(join(WORKDIR,"fastqs")):')
    (96, '    os.mkdir(join(WORKDIR,"fastqs"))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(join(WORKDIR,"results"))']
    (41, 'def append_files_in_list(flist,ofile):')
    (42, '    if not os.path.exists(ofile):')
    (43, "        with open(ofile, \\'w\\') as outfile:")
    (44, '            for fname in flist:')
    (45, '                with open(fname) as infile:')
    (46, '                    outfile.write(infile.read())')
    (47, '    return True')
    (48, '')
    (49, '##### load config and sample sheets #####')
    (50, '')
    (51, '# check_readaccess("config/config.yaml")')
    (52, '# configfile: "config/config.yaml"')
    (53, '')
    (54, '## set memory limit ')
    (55, '## used for sambamba sort, etc')
    (56, '# MEMORYG="100G"')
    (57, '')
    (58, '#resouce absolute path')
    (59, "WORKDIR=config[\\'workdir\\']")
    (60, "SCRIPTS_DIR=config[\\'scriptsdir\\']")
    (61, "RESOURCES_DIR=config[\\'resourcesdir\\']")
    (62, "FASTAS_GTFS_DIR=config[\\'fastas_gts_dir\\']")
    (63, '')
    (64, 'REF_DIR=join(WORKDIR,"ref")')
    (65, 'if not os.path.exists(REF_DIR):')
    (66, '    os.mkdir(REF_DIR)')
    (67, 'STAR_INDEX_DIR=join(REF_DIR,"STAR_no_GTF")')
    (68, 'if not os.path.exists(STAR_INDEX_DIR):')
    (69, '    os.mkdir(STAR_INDEX_DIR)')
    (70, '# strip trailing slashes if any')
    (71, 'for d in [WORKDIR,SCRIPTS_DIR,RESOURCES_DIR,FASTAS_GTFS_DIR,STAR_INDEX_DIR,REF_DIR]:')
    (72, "    d=d.strip(\\'r\\\\/\\')")
    (73, 'BWA_INDEX=join(REF_DIR,"ref")')
    (74, '')
    (75, "HOST=config[\\'host\\'] # hg38 or mm39")
    (76, "ADDITIVES=config[\\'additives\\'] # ERCC and/or BAC16Insert")
    (77, 'if ADDITIVES != "":')
    (78, '    HOST_ADDITIVES=HOST+","+ADDITIVES')
    (79, 'else:')
    (80, '    HOST_ADDITIVES=HOST')
    (81, "VIRUSES=config[\\'viruses\\']")
    (82, 'REPEATS_GTF=join(FASTAS_GTFS_DIR,HOST+".repeats.gtf")')
    (83, '')
    (84, 'HOST_ADDITIVES_VIRUSES=HOST_ADDITIVES+","+VIRUSES')
    (85, 'HOST_ADDITIVES_VIRUSES=HOST_ADDITIVES_VIRUSES.split(",")')
    (86, 'FASTAS=[join(FASTAS_GTFS_DIR,f+".fa") for f in HOST_ADDITIVES_VIRUSES]')
    (87, 'REGIONS=[join(FASTAS_GTFS_DIR,f+".fa.regions") for f in HOST_ADDITIVES_VIRUSES]')
    (88, 'GTFS=[join(FASTAS_GTFS_DIR,f+".gtf") for f in HOST_ADDITIVES_VIRUSES]')
    (89, 'FASTAS_REGIONS_GTFS=FASTAS.copy()')
    (90, 'FASTAS_REGIONS_GTFS.extend(REGIONS)')
    (91, 'FASTAS_REGIONS_GTFS.extend(GTFS)')
    (92, '')
    (93, "ANNOTATION_LOOKUP=config[\\'annotation_lookups\\'][HOST]")
    (94, '')
    (95, 'if not os.path.exists(join(WORKDIR,"fastqs")):')
    (96, '    os.mkdir(join(WORKDIR,"fastqs"))')
    (97, 'if not os.path.exists(join(WORKDIR,"results")):')
    (98, '    os.mkdir(join(WORKDIR,"results"))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(join(WORKDIR,"results","DCC"))']
    (41, 'def append_files_in_list(flist,ofile):')
    (42, '    if not os.path.exists(ofile):')
    (43, "        with open(ofile, \\'w\\') as outfile:")
    (44, '            for fname in flist:')
    (45, '                with open(fname) as infile:')
    (46, '                    outfile.write(infile.read())')
    (47, '    return True')
    (48, '')
    (49, '##### load config and sample sheets #####')
    (50, '')
    (51, '# check_readaccess("config/config.yaml")')
    (52, '# configfile: "config/config.yaml"')
    (53, '')
    (54, '## set memory limit ')
    (55, '## used for sambamba sort, etc')
    (56, '# MEMORYG="100G"')
    (57, '')
    (58, '#resouce absolute path')
    (59, "WORKDIR=config[\\'workdir\\']")
    (60, "SCRIPTS_DIR=config[\\'scriptsdir\\']")
    (61, "RESOURCES_DIR=config[\\'resourcesdir\\']")
    (62, "FASTAS_GTFS_DIR=config[\\'fastas_gts_dir\\']")
    (63, '')
    (64, 'REF_DIR=join(WORKDIR,"ref")')
    (65, 'if not os.path.exists(REF_DIR):')
    (66, '    os.mkdir(REF_DIR)')
    (67, 'STAR_INDEX_DIR=join(REF_DIR,"STAR_no_GTF")')
    (68, 'if not os.path.exists(STAR_INDEX_DIR):')
    (69, '    os.mkdir(STAR_INDEX_DIR)')
    (70, '# strip trailing slashes if any')
    (71, 'for d in [WORKDIR,SCRIPTS_DIR,RESOURCES_DIR,FASTAS_GTFS_DIR,STAR_INDEX_DIR,REF_DIR]:')
    (72, "    d=d.strip(\\'r\\\\/\\')")
    (73, 'BWA_INDEX=join(REF_DIR,"ref")')
    (74, '')
    (75, "HOST=config[\\'host\\'] # hg38 or mm39")
    (76, "ADDITIVES=config[\\'additives\\'] # ERCC and/or BAC16Insert")
    (77, 'if ADDITIVES != "":')
    (78, '    HOST_ADDITIVES=HOST+","+ADDITIVES')
    (79, 'else:')
    (80, '    HOST_ADDITIVES=HOST')
    (81, "VIRUSES=config[\\'viruses\\']")
    (82, 'REPEATS_GTF=join(FASTAS_GTFS_DIR,HOST+".repeats.gtf")')
    (83, '')
    (84, 'HOST_ADDITIVES_VIRUSES=HOST_ADDITIVES+","+VIRUSES')
    (85, 'HOST_ADDITIVES_VIRUSES=HOST_ADDITIVES_VIRUSES.split(",")')
    (86, 'FASTAS=[join(FASTAS_GTFS_DIR,f+".fa") for f in HOST_ADDITIVES_VIRUSES]')
    (87, 'REGIONS=[join(FASTAS_GTFS_DIR,f+".fa.regions") for f in HOST_ADDITIVES_VIRUSES]')
    (88, 'GTFS=[join(FASTAS_GTFS_DIR,f+".gtf") for f in HOST_ADDITIVES_VIRUSES]')
    (89, 'FASTAS_REGIONS_GTFS=FASTAS.copy()')
    (90, 'FASTAS_REGIONS_GTFS.extend(REGIONS)')
    (91, 'FASTAS_REGIONS_GTFS.extend(GTFS)')
    (92, '')
    (93, "ANNOTATION_LOOKUP=config[\\'annotation_lookups\\'][HOST]")
    (94, '')
    (95, 'if not os.path.exists(join(WORKDIR,"fastqs")):')
    (96, '    os.mkdir(join(WORKDIR,"fastqs"))')
    (97, 'if not os.path.exists(join(WORKDIR,"results")):')
    (98, '    os.mkdir(join(WORKDIR,"results"))')
    (99, 'if not os.path.exists(join(WORKDIR,"results","DCC")):')
    (100, '    os.mkdir(join(WORKDIR,"results","DCC"))')
    (101, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(R1filenewname)']
    (41, 'def append_files_in_list(flist,ofile):')
    (42, '    if not os.path.exists(ofile):')
    (43, "        with open(ofile, \\'w\\') as outfile:")
    (44, '            for fname in flist:')
    (45, '                with open(fname) as infile:')
    (46, '                    outfile.write(infile.read())')
    (47, '    return True')
    (48, '')
    (49, '##### load config and sample sheets #####')
    (50, '')
    (51, '# check_readaccess("config/config.yaml")')
    (52, '# configfile: "config/config.yaml"')
    (53, '')
    (54, '## set memory limit ')
    (55, '## used for sambamba sort, etc')
    (56, '# MEMORYG="100G"')
    (57, '')
    (58, '#resouce absolute path')
    (59, "WORKDIR=config[\\'workdir\\']")
    (60, "SCRIPTS_DIR=config[\\'scriptsdir\\']")
    (61, "RESOURCES_DIR=config[\\'resourcesdir\\']")
    (62, "FASTAS_GTFS_DIR=config[\\'fastas_gts_dir\\']")
    (63, '')
    (64, 'REF_DIR=join(WORKDIR,"ref")')
    (65, 'if not os.path.exists(REF_DIR):')
    (66, '    os.mkdir(REF_DIR)')
    (67, 'STAR_INDEX_DIR=join(REF_DIR,"STAR_no_GTF")')
    (68, 'if not os.path.exists(STAR_INDEX_DIR):')
    (69, '    os.mkdir(STAR_INDEX_DIR)')
    (70, '# strip trailing slashes if any')
    (71, 'for d in [WORKDIR,SCRIPTS_DIR,RESOURCES_DIR,FASTAS_GTFS_DIR,STAR_INDEX_DIR,REF_DIR]:')
    (72, "    d=d.strip(\\'r\\\\/\\')")
    (73, 'BWA_INDEX=join(REF_DIR,"ref")')
    (74, '')
    (75, "HOST=config[\\'host\\'] # hg38 or mm39")
    (76, "ADDITIVES=config[\\'additives\\'] # ERCC and/or BAC16Insert")
    (77, 'if ADDITIVES != "":')
    (78, '    HOST_ADDITIVES=HOST+","+ADDITIVES')
    (79, 'else:')
    (80, '    HOST_ADDITIVES=HOST')
    (81, "VIRUSES=config[\\'viruses\\']")
    (82, 'REPEATS_GTF=join(FASTAS_GTFS_DIR,HOST+".repeats.gtf")')
    (83, '')
    (84, 'HOST_ADDITIVES_VIRUSES=HOST_ADDITIVES+","+VIRUSES')
    (85, 'HOST_ADDITIVES_VIRUSES=HOST_ADDITIVES_VIRUSES.split(",")')
    (86, 'FASTAS=[join(FASTAS_GTFS_DIR,f+".fa") for f in HOST_ADDITIVES_VIRUSES]')
    (87, 'REGIONS=[join(FASTAS_GTFS_DIR,f+".fa.regions") for f in HOST_ADDITIVES_VIRUSES]')
    (88, 'GTFS=[join(FASTAS_GTFS_DIR,f+".gtf") for f in HOST_ADDITIVES_VIRUSES]')
    (89, 'FASTAS_REGIONS_GTFS=FASTAS.copy()')
    (90, 'FASTAS_REGIONS_GTFS.extend(REGIONS)')
    (91, 'FASTAS_REGIONS_GTFS.extend(GTFS)')
    (92, '')
    (93, "ANNOTATION_LOOKUP=config[\\'annotation_lookups\\'][HOST]")
    (94, '')
    (95, 'if not os.path.exists(join(WORKDIR,"fastqs")):')
    (96, '    os.mkdir(join(WORKDIR,"fastqs"))')
    (97, 'if not os.path.exists(join(WORKDIR,"results")):')
    (98, '    os.mkdir(join(WORKDIR,"results"))')
    (99, 'if not os.path.exists(join(WORKDIR,"results","DCC")):')
    (100, '    os.mkdir(join(WORKDIR,"results","DCC"))')
    (101, '')
    (102, 'REQUIRED_FILES=[config[f] for f in ["samples", "tools", "cluster"]]')
    (103, 'REQUIRED_FILES.append(ANNOTATION_LOOKUP)')
    (104, 'REQUIRED_FILES.extend(FASTAS)')
    (105, 'REQUIRED_FILES.extend(REGIONS)')
    (106, 'REQUIRED_FILES.extend(GTFS)')
    (107, 'for f in REQUIRED_FILES:')
    (108, '    check_readaccess(f)')
    (109, '')
    (110, 'REF_FA=join(REF_DIR,"ref.fa")')
    (111, 'REF_REGIONS=join(REF_DIR,"ref.fa.regions")')
    (112, 'REF_GTF=join(REF_DIR,"ref.gtf")')
    (113, 'append_files_in_list(FASTAS,REF_FA)')
    (114, 'append_files_in_list(REGIONS,REF_REGIONS)')
    (115, 'append_files_in_list(GTFS,REF_GTF)')
    (116, '')
    (117, 'SAMPLESDF = pd.read_csv(config["samples"],sep="\\\\t",header=0,index_col="sampleName")')
    (118, 'SAMPLES = list(SAMPLESDF.index)')
    (119, 'SAMPLESDF["R1"]=join(RESOURCES_DIR,"dummy")')
    (120, 'SAMPLESDF["R2"]=join(RESOURCES_DIR,"dummy")')
    (121, 'SAMPLESDF["PEorSE"]="PE"')
    (122, '')
    (123, 'for sample in SAMPLES:')
    (124, '    R1file=SAMPLESDF["path_to_R1_fastq"][sample]')
    (125, '    R2file=SAMPLESDF["path_to_R2_fastq"][sample]')
    (126, '    # print(sample,R1file,R2file)')
    (127, '    check_readaccess(R1file)')
    (128, '    R1filenewname=join(WORKDIR,"fastqs",sample+".R1.fastq.gz")')
    (129, '    if not os.path.exists(R1filenewname):')
    (130, '        os.symlink(R1file,R1filenewname)')
    (131, '        # os.link(R1file,R1filenewname)')
    (132, '    SAMPLESDF.loc[[sample],"R1"]=R1filenewname')
    (133, "    if str(R2file)!=\\'nan\\':")
    (134, '        check_readaccess(R2file)')
    (135, '        R2filenewname=join(WORKDIR,"fastqs",sample+".R2.fastq.gz")')
    (136, '        if not os.path.exists(R2filenewname):')
    (137, '            os.symlink(R2file,R2filenewname)')
    (138, '            # os.link(R2file,R2filenewname)')
    (139, '        SAMPLESDF.loc[[sample],"R2"]=R2filenewname')
    (140, '    else:')
    (141, '        SAMPLESDF.loc[[sample],"PEorSE"]="SE"')
    (142, '# print(SAMPLESDF)')
    (143, '# sys.exit()')
    (144, '')
    (145, '## Load tools from YAML file')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=clinical-genomics-uppsala/reference_grasnatter, file=workflow/rules/common.smk
context_key: ['if unit.provider == "https"']
    (43, 'def get_assembly_file(wildcards):')
    (44, '    unit = units.loc[')
    (45, '        (wildcards.species, wildcards.version, "assembly"), ["provider", "path"]')
    (46, '    ].dropna()')
    (47, '    if unit.provider == "https":')
    (48, '        return HTTP.remote("https://%s" % unit.path, keep_local=True)')
    (49, '    elif unit.provider == "gs":')
    (50, '        return GS.remote(unit.path)')
    (51, '    else:')
    (52, '        return path')
    (53, '')
    (54, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=clinical-genomics-uppsala/reference_grasnatter, file=workflow/rules/common.smk
context_key: ['if unit.provider == "https"']
    (69, 'def get_known_indels(wildcards):')
    (70, '    unit = units.loc[')
    (71, '        (wildcards.species, wildcards.version, wildcards.type), ["provider", "path"]')
    (72, '    ].dropna()')
    (73, '    if unit.provider == "https":')
    (74, '        return {')
    (75, '            "gz": HTTP.remote("https://%s" % unit.path, keep_local=True),')
    (76, '            "tbi": TTP.remote("https://%s.tbi" % unit.path, keep_local=True),')
    (77, '        }')
    (78, '    elif unit.provider == "gs":')
    (79, '        return {')
    (80, '            "gz": GS.remote(unit.path),')
    (81, '            "tbi": GS.remote("%s.tbi" % unit.path),')
    (82, '        }')
    (83, '    else:')
    (84, '        return path')
    (85, '')
    (86, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=clinical-genomics-uppsala/reference_grasnatter, file=workflow/rules/common.smk
context_key: ['if unit.provider == "https"']
    (87, 'def get_remote_file(wildcards):')
    (88, '    unit = units.loc[')
    (89, '        (wildcards.species, wildcards.version, wildcards.type), ["provider", "path"]')
    (90, '    ].dropna()')
    (91, '    if unit.provider == "https":')
    (92, '        return HTTP.remote("https://%s" % unit.path, keep_local=True)')
    (93, '    elif unit.provider == "gs":')
    (94, '        return GS.remote(unit.path)')
    (95, '    else:')
    (96, '        return path')
    (97, '')
    (98, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dbespiatykh/RDscan, file=workflow/rules/common.smk
context_key: ['if pd.notna(sample["R1"]) and pd.notna(sample["R2"])']
    (17, 'def get_fastq(wildcards):')
    (18, '    sample = samples.loc[wildcards.sample]')
    (19, '')
    (20, '    if pd.notna(sample["R1"]) and pd.notna(sample["R2"]):')
    (21, '        return [sample["R1"], sample["R2"]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/classifyReads.smk
context_key: ['if (seq_type == "pe")']
    (25, 'def kraken_inputs(wildcards):')
    (26, '    if (seq_type == "pe"):')
    (27, '        reads = expand(trimDir + "/{sample}_{strand}.trimmed.fastq.gz", strand=[strand1,strand2], sample=wildcards.sample)')
    (28, '    elif (seq_type == "se"):')
    (29, '        reads = trimDir + "/{sample}.trimmed.fastq.gz"')
    (30, '    else:')
    (31, '        sys.exit("Error: invalid sequencing type parameter. Must be \\\'se\\\' or \\\'pe\\\'")')
    (32, '    return reads')
    (33, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/classifyReads.smk
context_key: ['if (seq_type == "pe")']
    (34, 'def kaiju_inputs(wildcards):')
    (35, '    reads = trimDir + "/{sample}.pathogen.fastq.gz"')
    (36, '    if (seq_type == "pe"):')
    (37, '        reads = expand(trimDir + "/{sample}_{strand}.trimmed.fastq.gz", strand=[strand1,strand2], sample=wildcards.sample)')
    (38, '    elif (seq_type == "se"):')
    (39, '        reads = trimDir + "/{sample}.trimmed.fastq.gz"')
    (40, '    else:')
    (41, '        sys.exit("Error: invalid sequencing type parameter. Must be \\\'se\\\' or \\\'pe\\\'")')
    (42, '    return reads')
    (43, '')
    (44, "#since kraken takes a large memory, it can\\'t be used parallel")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ["if (mapReadType == \\'clean\\')", 'if (seq_type == "pe")']
    (25, 'def map_inputs(wildcards):')
    (26, "    if (mapReadType == \\'clean\\'):")
    (27, '      if (seq_type == "pe"):')
    (28, '          reads = expand(cleanDir + "/{sample}_{strand}.pathogen.fastq.gz", strand=[strand1,strand2], sample=wildcards.sample)')
    (29, '      elif (seq_type == "se"):')
    (30, '          reads = cleanDir + "/{sample}.pathogen.fastq.gz"')
    (31, '      else:')
    (32, '          sys.exit("Error: invalid sequencing type parameter. Must be \\\'se\\\' or \\\'pe\\\'")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ["if (mapReadType == \\'clean\\')", 'else', 'if (seq_type == "pe")']
    (25, 'def map_inputs(wildcards):')
    (26, "    if (mapReadType == \\'clean\\'):")
    (27, '      if (seq_type == "pe"):')
    (28, '          reads = expand(cleanDir + "/{sample}_{strand}.pathogen.fastq.gz", strand=[strand1,strand2], sample=wildcards.sample)')
    (29, '      elif (seq_type == "se"):')
    (30, '          reads = cleanDir + "/{sample}.pathogen.fastq.gz"')
    (31, '      else:')
    (32, '          sys.exit("Error: invalid sequencing type parameter. Must be \\\'se\\\' or \\\'pe\\\'")')
    (33, '    else:')
    (34, '      if (seq_type == "pe"):')
    (35, '          reads = expand(trimDir + "/{sample}_{strand}.trimmed.fastq.gz", strand=[strand1,strand2], sample=wildcards.sample)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ["if (mapReadType == \\'clean\\')", 'else', 'elif (seq_type == "se")']
    (25, 'def map_inputs(wildcards):')
    (26, "    if (mapReadType == \\'clean\\'):")
    (27, '      if (seq_type == "pe"):')
    (28, '          reads = expand(cleanDir + "/{sample}_{strand}.pathogen.fastq.gz", strand=[strand1,strand2], sample=wildcards.sample)')
    (29, '      elif (seq_type == "se"):')
    (30, '          reads = cleanDir + "/{sample}.pathogen.fastq.gz"')
    (31, '      else:')
    (32, '          sys.exit("Error: invalid sequencing type parameter. Must be \\\'se\\\' or \\\'pe\\\'")')
    (33, '    else:')
    (34, '      if (seq_type == "pe"):')
    (35, '          reads = expand(trimDir + "/{sample}_{strand}.trimmed.fastq.gz", strand=[strand1,strand2], sample=wildcards.sample)')
    (36, '      elif (seq_type == "se"):')
    (37, '          reads = trimDir + "/{sample}.trimmed.fastq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ["if (mapReadType == \\'clean\\')", 'else', 'else']
    (25, 'def map_inputs(wildcards):')
    (26, "    if (mapReadType == \\'clean\\'):")
    (27, '      if (seq_type == "pe"):')
    (28, '          reads = expand(cleanDir + "/{sample}_{strand}.pathogen.fastq.gz", strand=[strand1,strand2], sample=wildcards.sample)')
    (29, '      elif (seq_type == "se"):')
    (30, '          reads = cleanDir + "/{sample}.pathogen.fastq.gz"')
    (31, '      else:')
    (32, '          sys.exit("Error: invalid sequencing type parameter. Must be \\\'se\\\' or \\\'pe\\\'")')
    (33, '    else:')
    (34, '      if (seq_type == "pe"):')
    (35, '          reads = expand(trimDir + "/{sample}_{strand}.trimmed.fastq.gz", strand=[strand1,strand2], sample=wildcards.sample)')
    (36, '      elif (seq_type == "se"):')
    (37, '          reads = trimDir + "/{sample}.trimmed.fastq.gz"')
    (38, '      else:')
    (39, '          sys.exit("Error: invalid sequencing type parameter. Must be \\\'se\\\' or \\\'pe\\\'")')
    (40, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ["if (mapReadType == \\'clean\\')"]
    (25, 'def map_inputs(wildcards):')
    (26, "    if (mapReadType == \\'clean\\'):")
    (27, '      if (seq_type == "pe"):')
    (28, '          reads = expand(cleanDir + "/{sample}_{strand}.pathogen.fastq.gz", strand=[strand1,strand2], sample=wildcards.sample)')
    (29, '      elif (seq_type == "se"):')
    (30, '          reads = cleanDir + "/{sample}.pathogen.fastq.gz"')
    (31, '      else:')
    (32, '          sys.exit("Error: invalid sequencing type parameter. Must be \\\'se\\\' or \\\'pe\\\'")')
    (33, '    else:')
    (34, '      if (seq_type == "pe"):')
    (35, '          reads = expand(trimDir + "/{sample}_{strand}.trimmed.fastq.gz", strand=[strand1,strand2], sample=wildcards.sample)')
    (36, '      elif (seq_type == "se"):')
    (37, '          reads = trimDir + "/{sample}.trimmed.fastq.gz"')
    (38, '      else:')
    (39, '          sys.exit("Error: invalid sequencing type parameter. Must be \\\'se\\\' or \\\'pe\\\'")')
    (40, '')
    (41, '    return reads')
    (42, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/cleanReads.smk
context_key: ['if (seq_type == "pe")', 'if true001']
    (40, 'def raw_inputs(wildcards): ')
    (41, '    if (seq_type == "pe"):')
    (42, '      if true001:')
    (43, '        return expand(rawReadDir + "/{sample}_{strand}_001." + input_format, strand=[strand1,strand2], sample=wildcards.sample) ')
    (44, '      else:  ')
    (45, '        return expand(rawReadDir + "/{sample}_{strand}." + input_format,  strand=[strand1,strand2], sample=wildcards.sample) ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/cleanReads.smk
context_key: ['if (seq_type == "pe")', 'elif (seq_type == "se" )', 'if true001 and strand1']
    (40, 'def raw_inputs(wildcards): ')
    (41, '    if (seq_type == "pe"):')
    (42, '      if true001:')
    (43, '        return expand(rawReadDir + "/{sample}_{strand}_001." + input_format, strand=[strand1,strand2], sample=wildcards.sample) ')
    (44, '      else:  ')
    (45, '        return expand(rawReadDir + "/{sample}_{strand}." + input_format,  strand=[strand1,strand2], sample=wildcards.sample) ')
    (46, '    elif (seq_type == "se" ):')
    (47, '      if true001 and strand1:  ')
    (48, '        return rawReadDir + "/{sample}_" + strand1 + "_001." + input_format')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/cleanReads.smk
context_key: ['if (seq_type == "pe")', 'elif (seq_type == "se" )', 'elif strand1']
    (40, 'def raw_inputs(wildcards): ')
    (41, '    if (seq_type == "pe"):')
    (42, '      if true001:')
    (43, '        return expand(rawReadDir + "/{sample}_{strand}_001." + input_format, strand=[strand1,strand2], sample=wildcards.sample) ')
    (44, '      else:  ')
    (45, '        return expand(rawReadDir + "/{sample}_{strand}." + input_format,  strand=[strand1,strand2], sample=wildcards.sample) ')
    (46, '    elif (seq_type == "se" ):')
    (47, '      if true001 and strand1:  ')
    (48, '        return rawReadDir + "/{sample}_" + strand1 + "_001." + input_format')
    (49, '      elif strand1:')
    (50, '        return rawReadDir + "/{sample}_" + strand1 + "." + input_format')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/cleanReads.smk
context_key: ['if (seq_type == "pe")', 'elif (seq_type == "se" )', 'else']
    (40, 'def raw_inputs(wildcards): ')
    (41, '    if (seq_type == "pe"):')
    (42, '      if true001:')
    (43, '        return expand(rawReadDir + "/{sample}_{strand}_001." + input_format, strand=[strand1,strand2], sample=wildcards.sample) ')
    (44, '      else:  ')
    (45, '        return expand(rawReadDir + "/{sample}_{strand}." + input_format,  strand=[strand1,strand2], sample=wildcards.sample) ')
    (46, '    elif (seq_type == "se" ):')
    (47, '      if true001 and strand1:  ')
    (48, '        return rawReadDir + "/{sample}_" + strand1 + "_001." + input_format')
    (49, '      elif strand1:')
    (50, '        return rawReadDir + "/{sample}_" + strand1 + "." + input_format')
    (51, '      else:')
    (52, '        return rawReadDir + "/{sample}." + input_format')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/cleanReads.smk
context_key: ['if (seq_type == "pe")', 'else']
    (40, 'def raw_inputs(wildcards): ')
    (41, '    if (seq_type == "pe"):')
    (42, '      if true001:')
    (43, '        return expand(rawReadDir + "/{sample}_{strand}_001." + input_format, strand=[strand1,strand2], sample=wildcards.sample) ')
    (44, '      else:  ')
    (45, '        return expand(rawReadDir + "/{sample}_{strand}." + input_format,  strand=[strand1,strand2], sample=wildcards.sample) ')
    (46, '    elif (seq_type == "se" ):')
    (47, '      if true001 and strand1:  ')
    (48, '        return rawReadDir + "/{sample}_" + strand1 + "_001." + input_format')
    (49, '      elif strand1:')
    (50, '        return rawReadDir + "/{sample}_" + strand1 + "." + input_format')
    (51, '      else:')
    (52, '        return rawReadDir + "/{sample}." + input_format')
    (53, '    else:')
    (54, '        sys.exit("Error: invalid sequencing type parameter. Must be \\\'se\\\' or \\\'pe\\\'")')
    (55, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/cleanReads.smk
context_key: ['if (seq_type == "pe")', 'if true001']
    (56, 'def fastqc_raw_inputs(wildcards): ')
    (57, '    if (seq_type == "pe"):')
    (58, '      #print({wildcards.sample}, {wildcards.strand})')
    (59, '      if true001:')
    (60, '        return expand(rawReadDir + "/{sample}_{strand}_001." + input_format, strand=wildcards.strand, sample=wildcards.sample) ')
    (61, '      else:  ')
    (62, '        return expand(rawReadDir + "/{sample}_{strand}." + input_format,  strand=wildcards.strand, sample=wildcards.sample) ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/cleanReads.smk
context_key: ['if (seq_type == "pe")', 'elif (seq_type == "se" )', 'if true001 and strand1']
    (56, 'def fastqc_raw_inputs(wildcards): ')
    (57, '    if (seq_type == "pe"):')
    (58, '      #print({wildcards.sample}, {wildcards.strand})')
    (59, '      if true001:')
    (60, '        return expand(rawReadDir + "/{sample}_{strand}_001." + input_format, strand=wildcards.strand, sample=wildcards.sample) ')
    (61, '      else:  ')
    (62, '        return expand(rawReadDir + "/{sample}_{strand}." + input_format,  strand=wildcards.strand, sample=wildcards.sample) ')
    (63, '    elif (seq_type == "se" ):')
    (64, '      #print({wildcards.strand})')
    (65, '      if true001 and strand1:  ')
    (66, '        return rawReadDir + "/{sample}_" + strand1 + "_001." + input_format')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/cleanReads.smk
context_key: ['if (seq_type == "pe")', 'elif (seq_type == "se" )', 'elif strand1']
    (56, 'def fastqc_raw_inputs(wildcards): ')
    (57, '    if (seq_type == "pe"):')
    (58, '      #print({wildcards.sample}, {wildcards.strand})')
    (59, '      if true001:')
    (60, '        return expand(rawReadDir + "/{sample}_{strand}_001." + input_format, strand=wildcards.strand, sample=wildcards.sample) ')
    (61, '      else:  ')
    (62, '        return expand(rawReadDir + "/{sample}_{strand}." + input_format,  strand=wildcards.strand, sample=wildcards.sample) ')
    (63, '    elif (seq_type == "se" ):')
    (64, '      #print({wildcards.strand})')
    (65, '      if true001 and strand1:  ')
    (66, '        return rawReadDir + "/{sample}_" + strand1 + "_001." + input_format')
    (67, '      elif strand1:')
    (68, '        return rawReadDir + "/{sample}_" + strand1 + "." + input_format')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/cleanReads.smk
context_key: ['if (seq_type == "pe")', 'elif (seq_type == "se" )', 'else']
    (56, 'def fastqc_raw_inputs(wildcards): ')
    (57, '    if (seq_type == "pe"):')
    (58, '      #print({wildcards.sample}, {wildcards.strand})')
    (59, '      if true001:')
    (60, '        return expand(rawReadDir + "/{sample}_{strand}_001." + input_format, strand=wildcards.strand, sample=wildcards.sample) ')
    (61, '      else:  ')
    (62, '        return expand(rawReadDir + "/{sample}_{strand}." + input_format,  strand=wildcards.strand, sample=wildcards.sample) ')
    (63, '    elif (seq_type == "se" ):')
    (64, '      #print({wildcards.strand})')
    (65, '      if true001 and strand1:  ')
    (66, '        return rawReadDir + "/{sample}_" + strand1 + "_001." + input_format')
    (67, '      elif strand1:')
    (68, '        return rawReadDir + "/{sample}_" + strand1 + "." + input_format')
    (69, '      else:')
    (70, '        return rawReadDir + "/{sample}." + input_format')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/cleanReads.smk
context_key: ['if (seq_type == "pe")', 'else']
    (56, 'def fastqc_raw_inputs(wildcards): ')
    (57, '    if (seq_type == "pe"):')
    (58, '      #print({wildcards.sample}, {wildcards.strand})')
    (59, '      if true001:')
    (60, '        return expand(rawReadDir + "/{sample}_{strand}_001." + input_format, strand=wildcards.strand, sample=wildcards.sample) ')
    (61, '      else:  ')
    (62, '        return expand(rawReadDir + "/{sample}_{strand}." + input_format,  strand=wildcards.strand, sample=wildcards.sample) ')
    (63, '    elif (seq_type == "se" ):')
    (64, '      #print({wildcards.strand})')
    (65, '      if true001 and strand1:  ')
    (66, '        return rawReadDir + "/{sample}_" + strand1 + "_001." + input_format')
    (67, '      elif strand1:')
    (68, '        return rawReadDir + "/{sample}_" + strand1 + "." + input_format')
    (69, '      else:')
    (70, '        return rawReadDir + "/{sample}." + input_format')
    (71, '    else:')
    (72, '        sys.exit("Error: invalid sequencing type parameter. Must be \\\'se\\\' or \\\'pe\\\'")')
    (73, '')
    (74, '# Define raw multiQC input files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/cleanReads.smk
context_key: ['if (seq_type == "pe")']
    (75, 'def raw_multiqc_inputs(wildcards):')
    (76, '    seq_type = config["seq_type"]')
    (77, '    if (seq_type == "pe"):')
    (78, '        qcOut = expand(qcDir + "/raw_fastqc/{sample}_{strand}_fastqc.zip", strand=[strand1,strand2], sample=smps)')
    (79, '    elif (seq_type == "se"):')
    (80, '        qcOut = expand(rules.raw_fastqc_se.output, sample=smps) #ensure the rule can only be started after fastqc processes all samples')
    (81, '    else:')
    (82, '        sys.exit("Error: invalid sequencing type parameter. Must be \\\'se\\\' or \\\'pe\\\'")')
    (83, '    return qcOut')
    (84, '')
    (85, '# Define trimmed multiQC input files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/cleanReads.smk
context_key: ['if (seq_type == "pe")']
    (86, 'def trimmed_multiqc_inputs(wildcards):')
    (87, '    seq_type = config["seq_type"]')
    (88, '    if (seq_type == "pe"):')
    (89, '        qcOut = expand(qcDir + "/trimmed_fastqc/{sample}_{strand}.trimmed_fastqc.zip", strand=[strand1,strand2], sample=smps)')
    (90, '    elif (seq_type == "se"):')
    (91, '        qcOut = expand(rules.trim_fastqc_se.output, sample=smps) #ensure the rule can only be started after fastqc processes all samples')
    (92, '    else:')
    (93, '        sys.exit("Error: invalid sequencing type parameter. Must be \\\'se\\\' or \\\'pe\\\'")')
    (94, '    return qcOut')
    (95, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AnnaLorenc/ATACseq, file=workflow/rules/common.smk
context_key: ['if isinstance(fq2_present, pd.core.series.Series)']
    (33, 'def is_single_end(sample, unit):')
    (34, '    """Determine whether unit is single-end."""')
    (35, '    fq2_present = pd.isnull(units.loc[(sample, unit), "fq2"])')
    (36, '    if isinstance(fq2_present, pd.core.series.Series):')
    (37, '        # if this is the case, get_fastqs cannot work properly')
    (38, '        raise ValueError(')
    (39, '            f"Multiple fq2 entries found for sample-unit combination {sample}-{unit}.\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AnnaLorenc/ATACseq, file=workflow/rules/common.smk
context_key: ['if ( wildcards.read == "0" or wildcards.read == "1" )']
    (48, 'def get_individual_fastq(wildcards):')
    (49, '    """Get individual raw FASTQ files from unit sheet, based on a read (end) wildcard"""')
    (50, '    if ( wildcards.read == "0" or wildcards.read == "1" ):')
    (51, '        return units.loc[ (wildcards.sample, wildcards.unit), "fq1" ]')
    (52, '    elif wildcards.read == "2":')
    (53, '        return units.loc[ (wildcards.sample, wildcards.unit), "fq2" ]')
    (54, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AnnaLorenc/ATACseq, file=workflow/rules/common.smk
context_key: ['if is_single_end(sample, unit)']
    (55, 'def get_multiqc_input(wildcards):')
    (56, '    multiqc_input = []')
    (57, '    for (sample, unit) in units.index:')
    (58, '        reads = [ "1", "2" ]')
    (59, '        if is_single_end(sample, unit):')
    (60, '            reads = [ "0" ]')
    (61, '            multiqc_input.extend(expand (["logs/cutadapt/{sample}-{unit}.se.log"],')
    (62, '            sample = sample, unit = unit))')
    (63, '        else:')
    (64, '            multiqc_input.extend(expand (["logs/cutadapt/{sample}-{unit}.pe.log"],')
    (65, '            sample = sample, unit = unit))')
    (66, '')
    (67, '        multiqc_input.extend(')
    (68, '            expand (')
    (69, '                [')
    (70, '                    "results/qc/fastqc/{sample}.{unit}.{reads}_fastqc.zip",')
    (71, '                    "results/qc/fastqc/{sample}.{unit}.{reads}.html",')
    (72, '                    "results/mapped/{sample}-{unit}.mapped.flagstat",')
    (73, '                    "results/mapped/{sample}-{unit}.mapped.idxstats",')
    (74, '                    "results/mapped/{sample}-{unit}.mapped.stats.txt"')
    (75, '                ],')
    (76, '                sample = sample,')
    (77, '                unit = unit,')
    (78, '                reads = reads')
    (79, '            )')
    (80, '        )')
    (81, '    for sample in samples.index:')
    (82, '        multiqc_input.extend(')
    (83, '            expand (')
    (84, '                [')
    (85, '                    "results/picard_dedup/{sample}.metrics.txt",')
    (86, '                    "results/picard_dedup/{sample}.picard_dedup.flagstat",')
    (87, '                    "results/picard_dedup/{sample}.picard_dedup.idxstats",')
    (88, '                    "results/picard_dedup/{sample}.picard_dedup.stats.txt",')
    (89, '                    "results/filtered/{sample}.filtered.flagstat",')
    (90, '                    "results/filtered/{sample}.filtered.idxstats",')
    (91, '                    "results/filtered/{sample}.filtered.stats.txt",')
    (92, '                    "results/orphan_rm_sorted/{sample}.orphan_rm_sorted.idxstats",')
    (93, '                    "results/orphan_rm_sorted/{sample}.orphan_rm_sorted.flagstat",')
    (94, '                    "results/orphan_rm_sorted/{sample}.orphan_rm_sorted.stats.txt",')
    (95, '                    "results/qc/multiple_metrics/{sample}.alignment_summary_metrics",')
    (96, '                    "results/qc/multiple_metrics/{sample}.base_distribution_by_cycle_metrics",')
    (97, '                    "results/qc/multiple_metrics/{sample}.base_distribution_by_cycle.pdf",')
    (98, '                    "results/qc/multiple_metrics/{sample}.insert_size_metrics",')
    (99, '                    "results/qc/multiple_metrics/{sample}.insert_size_histogram.pdf",')
    (100, '                    "results/qc/multiple_metrics/{sample}.quality_by_cycle_metrics",')
    (101, '                    "results/qc/multiple_metrics/{sample}.quality_by_cycle.pdf",')
    (102, '                    "results/qc/multiple_metrics/{sample}.quality_distribution_metrics",')
    (103, '                    "results/qc/multiple_metrics/{sample}.quality_distribution.pdf",')
    (104, '                    "results/deeptools/plot_profile_data.tab",')
    (105, '                    "results/phantompeakqualtools/{sample}.phantompeak.spp.out",')
    (106, '                    "results/phantompeakqualtools/{sample}.spp_correlation_mqc.tsv",')
    (107, '                    "results/phantompeakqualtools/{sample}.spp_nsc_mqc.tsv",')
    (108, '                    "results/phantompeakqualtools/{sample}.spp_rsc_mqc.tsv"')
    (109, '                ],')
    (110, '                sample = sample')
    (111, '            )')
    (112, '        )')
    (113, '        if config["params"]["lc_extrap"]:')
    (114, '                multiqc_input.extend( expand(["results/preseq/{sample}.lc_extrap"], sample = sample))')
    (115, '    return multiqc_input')
    (116, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AnnaLorenc/ATACseq, file=workflow/rules/common.smk
context_key: ['if is_single_end(wildcards.sample, wildcards.unit)']
    (117, 'def get_fastqs(wildcards):')
    (118, '    """Get raw FASTQ files from unit sheet."""')
    (119, '    if is_single_end(wildcards.sample, wildcards.unit):')
    (120, '        return units.loc[ (wildcards.sample, wildcards.unit), "fq1" ]')
    (121, '    else:')
    (122, '        u = units.loc[ (wildcards.sample, wildcards.unit), ["fq1", "fq2"] ].dropna()')
    (123, '        return [ f"{u.fq1}", f"{u.fq2}" ]')
    (124, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AnnaLorenc/ATACseq, file=workflow/rules/common.smk
context_key: ['if is_single_end(wildcards.sample, wildcards.unit)']
    (125, 'def get_map_reads_input(wildcards):')
    (126, '    if is_single_end(wildcards.sample, wildcards.unit):')
    (127, '        return "results/trimmed/{sample}-{unit}.fastq.gz"')
    (128, '    return ["results/trimmed/{sample}-{unit}.1.fastq.gz", "results/trimmed/{sample}-{unit}.2.fastq.gz"]')
    (129, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AnnaLorenc/ATACseq, file=workflow/rules/common.smk
context_key: ['if is_single_end(sample, unit)']
    (137, 'def all_input(wildcards):')
    (138, '')
    (139, '    wanted_input = []')
    (140, '')
    (141, '    # QC with fastQC and multiQC')
    (142, '    wanted_input.extend(["results/qc/multiqc/multiqc.html"])')
    (143, '')
    (144, '    # trimming reads')
    (145, '    for (sample, unit) in units.index:')
    (146, '        if is_single_end(sample, unit):')
    (147, '            wanted_input.extend(expand(')
    (148, '                    [')
    (149, '                        "results/trimmed/{sample}-{unit}.fastq.gz",')
    (150, '                        "results/trimmed/{sample}-{unit}.se.qc.txt"')
    (151, '                    ],')
    (152, '                    sample = sample,')
    (153, '                    unit = unit')
    (154, '                )')
    (155, '            )')
    (156, '        else:')
    (157, '            wanted_input.extend(')
    (158, '                expand (')
    (159, '                    [')
    (160, '                        "results/trimmed/{sample}-{unit}.1.fastq.gz",')
    (161, '                        "results/trimmed/{sample}-{unit}.2.fastq.gz",')
    (162, '                        "results/trimmed/{sample}-{unit}.pe.qc.txt"')
    (163, '                    ],')
    (164, '                    sample = sample,')
    (165, '                    unit = unit')
    (166, '            )')
    (167, '        )')
    (168, '')
    (169, '    # mapping, merging and filtering bam-files')
    (170, '    for sample in samples.index:')
    (171, '        wanted_input.extend(')
    (172, '            expand (')
    (173, '                [')
    (174, '                    "results/IGV/merged_library.bigWig.igv.txt",')
    (175, '                    "results/deeptools/plot_profile.pdf",')
    (176, '                    "results/deeptools/heatmap.pdf",')
    (177, '                    "results/deeptools/heatmap_matrix.tab",')
    (178, '                    "results/phantompeakqualtools/{sample}.phantompeak.pdf"')
    (179, '                ],')
    (180, '                sample = sample')
    (181, '            )')
    (182, '        )')
    (183, '')
    (184, '    return wanted_input')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ding-lab/cptac_rna_expression, file=Snakefile
context_key: ['if replace is not None']
    (53, 'def find_sample_bam_path(wildcards, replace=REPLACE_BAM_PTH):')
    (54, '    bam_pth = str(SAMPLE_INFO[wildcards.sample].bam_pth)')
    (55, '    if replace is not None:')
    (56, '        src_part, dst_part = replace')
    (57, '        bam_pth = bam_pth.replace(src_part, dst_part)')
    (58, '    return {')
    (59, "        \\'bam\\': bam_pth,")
    (60, "        \\'bai\\': bam_pth + \\'.bai\\',")
    (61, '    }')
    (62, '')
    (63, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Roett/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if _sample_dict is None']
    (363, 'def get_sample_dict():')
    (364, '    global _sample_dict')
    (365, '    if _sample_dict is None:')
    (366, '        checkpoints.generate_sample_list.get()')
    (367, '        dat_samples = pd.read_csv(file_path_samples)')
    (368, '        _sample_dict = {pathlib.Path(x).stem: x for x in dat_samples[')
    (369, "            \\'mcd_folders\\']}")
    (370, '    return _sample_dict')
    (371, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=qbicsoftware-archive/rnaseq, file=Snakefile
context_key: ['if key not in parameters']
    (31, 'def etc(path):')
    (32, '    return os.path.join(ETC, path)')
    (33, '')
    (34, '')
    (35, 'try:')
    (36, '    with open(etc("params.json")) as f:')
    (37, '        parameters = json.load(f)')
    (38, 'except OSError as e:')
    (39, '    print("Could not read parameter file: " + str(e), file=sys.stderr)')
    (40, '    sys.exit(1)')
    (41, 'except ValueError as e:')
    (42, '    print("Invalid parameter file: " + str(e), file=sys.stderr)')
    (43, '    sys.exit(1)')
    (44, '')
    (45, 'default_params = {')
    (46, '    "stranded": \\\'no\\\',')
    (47, '    "overlap_mode": \\\'union\\\',')
    (48, '    "normalize_counts": "deseq2",')
    (49, '    "gff_attribute": \\\'gene_id\\\',')
    (50, '    "feature_type": \\\'exon\\\',')
    (51, '}')
    (52, 'default_params.update(parameters)')
    (53, 'parameters = default_params')
    (54, '')
    (55, "for key in [\\'gtf\\', \\'stranded\\', \\'overlap_mode\\', \\'indexed_genome\\',")
    (56, "            \\'gff_attribute\\', \\'feature_type\\', \\'normalize_counts\\']:")
    (57, '    if key not in parameters:')
    (58, '        print("Missing parameter %s in etc/params.json" % key, file=sys.stderr)')
    (59, '        exit(1)')
    (60, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=qbicsoftware-archive/rnaseq, file=Snakefile
context_key: ["if not os.path.exists(indexed_genome + \\'.fa\\')"]
    (31, 'def etc(path):')
    (32, '    return os.path.join(ETC, path)')
    (33, '')
    (34, '')
    (35, 'try:')
    (36, '    with open(etc("params.json")) as f:')
    (37, '        parameters = json.load(f)')
    (38, 'except OSError as e:')
    (39, '    print("Could not read parameter file: " + str(e), file=sys.stderr)')
    (40, '    sys.exit(1)')
    (41, 'except ValueError as e:')
    (42, '    print("Invalid parameter file: " + str(e), file=sys.stderr)')
    (43, '    sys.exit(1)')
    (44, '')
    (45, 'default_params = {')
    (46, '    "stranded": \\\'no\\\',')
    (47, '    "overlap_mode": \\\'union\\\',')
    (48, '    "normalize_counts": "deseq2",')
    (49, '    "gff_attribute": \\\'gene_id\\\',')
    (50, '    "feature_type": \\\'exon\\\',')
    (51, '}')
    (52, 'default_params.update(parameters)')
    (53, 'parameters = default_params')
    (54, '')
    (55, "for key in [\\'gtf\\', \\'stranded\\', \\'overlap_mode\\', \\'indexed_genome\\',")
    (56, "            \\'gff_attribute\\', \\'feature_type\\', \\'normalize_counts\\']:")
    (57, '    if key not in parameters:')
    (58, '        print("Missing parameter %s in etc/params.json" % key, file=sys.stderr)')
    (59, '        exit(1)')
    (60, '')
    (61, "parameters[\\'indexed_genome\\'] = ref(parameters[\\'indexed_genome\\'])")
    (62, "parameters[\\'gtf\\'] = ref(parameters[\\'gtf\\'])")
    (63, '')
    (64, 'indexed_genome = parameters["indexed_genome"]')
    (65, "if not os.path.exists(indexed_genome + \\'.fa\\'):")
    (66, '    raise ValueError("Could not find indexed genome file %s" % indexed_genome)')
    (67, '')
    (68, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=qbicsoftware-archive/rnaseq, file=Snakefile
context_key: ["if name.lower().endswith(\\'.sha256sum\\')"]
    (31, 'def etc(path):')
    (32, '    return os.path.join(ETC, path)')
    (33, '')
    (34, '')
    (35, 'try:')
    (36, '    with open(etc("params.json")) as f:')
    (37, '        parameters = json.load(f)')
    (38, 'except OSError as e:')
    (39, '    print("Could not read parameter file: " + str(e), file=sys.stderr)')
    (40, '    sys.exit(1)')
    (41, 'except ValueError as e:')
    (42, '    print("Invalid parameter file: " + str(e), file=sys.stderr)')
    (43, '    sys.exit(1)')
    (44, '')
    (45, 'default_params = {')
    (46, '    "stranded": \\\'no\\\',')
    (47, '    "overlap_mode": \\\'union\\\',')
    (48, '    "normalize_counts": "deseq2",')
    (49, '    "gff_attribute": \\\'gene_id\\\',')
    (50, '    "feature_type": \\\'exon\\\',')
    (51, '}')
    (52, 'default_params.update(parameters)')
    (53, 'parameters = default_params')
    (54, '')
    (55, "for key in [\\'gtf\\', \\'stranded\\', \\'overlap_mode\\', \\'indexed_genome\\',")
    (56, "            \\'gff_attribute\\', \\'feature_type\\', \\'normalize_counts\\']:")
    (57, '    if key not in parameters:')
    (58, '        print("Missing parameter %s in etc/params.json" % key, file=sys.stderr)')
    (59, '        exit(1)')
    (60, '')
    (61, "parameters[\\'indexed_genome\\'] = ref(parameters[\\'indexed_genome\\'])")
    (62, "parameters[\\'gtf\\'] = ref(parameters[\\'gtf\\'])")
    (63, '')
    (64, 'indexed_genome = parameters["indexed_genome"]')
    (65, "if not os.path.exists(indexed_genome + \\'.fa\\'):")
    (66, '    raise ValueError("Could not find indexed genome file %s" % indexed_genome)')
    (67, '')
    (68, '')
    (69, 'INPUT_FILES = []')
    (70, 'for name in os.listdir(DATA):')
    (71, "    if name.lower().endswith(\\'.sha256sum\\'):")
    (72, '        continue')
    (73, "    if name.lower().endswith(\\'.fastq\\'):")
    (74, "        if not name.endswith(\\'.fastq\\'):")
    (75, '            print("Extension fastq is case sensitive.", file=sys.stderr)')
    (76, '            exit(1)')
    (77, '        INPUT_FILES.append(os.path.basename(name)[:-6])')
    (78, "    elif name.lower().endswith(\\'.fastq.gz\\'):")
    (79, "        if not name.endswith(\\'.fastq.gz\\'):")
    (80, '            print("Extension fastq is case sensitive.", file=sys.stderr)')
    (81, '            exit(1)')
    (82, "        INPUT_FILES.append(os.path.basename(name)[:-len(\\'.fastq.gz\\')])")
    (83, '    else:')
    (84, '        print("Unknown data file: %s" % name, file=sys.stderr)')
    (85, '        exit(1)')
    (86, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=qbicsoftware-archive/rnaseq, file=Snakefile
context_key: ['if len(set(INPUT_FILES)) != len(INPUT_FILES)']
    (31, 'def etc(path):')
    (32, '    return os.path.join(ETC, path)')
    (33, '')
    (34, '')
    (35, 'try:')
    (36, '    with open(etc("params.json")) as f:')
    (37, '        parameters = json.load(f)')
    (38, 'except OSError as e:')
    (39, '    print("Could not read parameter file: " + str(e), file=sys.stderr)')
    (40, '    sys.exit(1)')
    (41, 'except ValueError as e:')
    (42, '    print("Invalid parameter file: " + str(e), file=sys.stderr)')
    (43, '    sys.exit(1)')
    (44, '')
    (45, 'default_params = {')
    (46, '    "stranded": \\\'no\\\',')
    (47, '    "overlap_mode": \\\'union\\\',')
    (48, '    "normalize_counts": "deseq2",')
    (49, '    "gff_attribute": \\\'gene_id\\\',')
    (50, '    "feature_type": \\\'exon\\\',')
    (51, '}')
    (52, 'default_params.update(parameters)')
    (53, 'parameters = default_params')
    (54, '')
    (55, "for key in [\\'gtf\\', \\'stranded\\', \\'overlap_mode\\', \\'indexed_genome\\',")
    (56, "            \\'gff_attribute\\', \\'feature_type\\', \\'normalize_counts\\']:")
    (57, '    if key not in parameters:')
    (58, '        print("Missing parameter %s in etc/params.json" % key, file=sys.stderr)')
    (59, '        exit(1)')
    (60, '')
    (61, "parameters[\\'indexed_genome\\'] = ref(parameters[\\'indexed_genome\\'])")
    (62, "parameters[\\'gtf\\'] = ref(parameters[\\'gtf\\'])")
    (63, '')
    (64, 'indexed_genome = parameters["indexed_genome"]')
    (65, "if not os.path.exists(indexed_genome + \\'.fa\\'):")
    (66, '    raise ValueError("Could not find indexed genome file %s" % indexed_genome)')
    (67, '')
    (68, '')
    (69, 'INPUT_FILES = []')
    (70, 'for name in os.listdir(DATA):')
    (71, "    if name.lower().endswith(\\'.sha256sum\\'):")
    (72, '        continue')
    (73, "    if name.lower().endswith(\\'.fastq\\'):")
    (74, "        if not name.endswith(\\'.fastq\\'):")
    (75, '            print("Extension fastq is case sensitive.", file=sys.stderr)')
    (76, '            exit(1)')
    (77, '        INPUT_FILES.append(os.path.basename(name)[:-6])')
    (78, "    elif name.lower().endswith(\\'.fastq.gz\\'):")
    (79, "        if not name.endswith(\\'.fastq.gz\\'):")
    (80, '            print("Extension fastq is case sensitive.", file=sys.stderr)')
    (81, '            exit(1)')
    (82, "        INPUT_FILES.append(os.path.basename(name)[:-len(\\'.fastq.gz\\')])")
    (83, '    else:')
    (84, '        print("Unknown data file: %s" % name, file=sys.stderr)')
    (85, '        exit(1)')
    (86, '')
    (87, 'if len(set(INPUT_FILES)) != len(INPUT_FILES):')
    (88, '    print("Some input file names are not unique")')
    (89, '    exit(1)')
    (90, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/qcrs4, file=workflow/rules/common.smk
context_key: ['if not os.path.isabs(filepath)']
    (25, 'def expand_filepath(filepath):')
    (26, '    filepath = os.path.expandvars(os.path.expanduser(filepath))')
    (27, '    if not os.path.isabs(filepath):')
    (28, '        raise FileNotFoundError(')
    (29, '            errno.ENOENT, os.strerror(errno.ENOENT)+" (path must be absolute)", filepath)')
    (30, '    return filepath')
    (31, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/qcrs4, file=workflow/rules/common.smk
context_key: ['if does not exists, create path and return it. If any errors, retur']
    (36, "def tmp_path(path=\\'\\'):")
    (37, '    """')
    (38, '    if does not exists, create path and return it. If any errors, return')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/qcrs4, file=workflow/rules/common.smk
context_key: ['if path', 'try']
    (36, "def tmp_path(path=\\'\\'):")
    (37, '    """')
    (38, '    if does not exists, create path and return it. If any errors, return')
    (39, '    default path')
    (40, '    :param path: path')
    (41, '    :return: path')
    (42, '    """')
    (43, '    default_path = os.getenv(\\\'TMPDIR\\\', config.get("processing").get("base_tmp"))')
    (44, '    if path:')
    (45, '        try:')
    (46, '            os.makedirs(path)')
    (47, '        except OSError as e:')
    (48, '            if e.errno != errno.EEXIST:')
    (49, '                return default_path')
    (50, '        return path')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/qcrs4, file=workflow/rules/common.smk
context_key: ['if path']
    (36, "def tmp_path(path=\\'\\'):")
    (37, '    """')
    (38, '    if does not exists, create path and return it. If any errors, return')
    (39, '    default path')
    (40, '    :param path: path')
    (41, '    :return: path')
    (42, '    """')
    (43, '    default_path = os.getenv(\\\'TMPDIR\\\', config.get("processing").get("base_tmp"))')
    (44, '    if path:')
    (45, '        try:')
    (46, '            os.makedirs(path)')
    (47, '        except OSError as e:')
    (48, '            if e.errno != errno.EEXIST:')
    (49, '                return default_path')
    (50, '        return path')
    (51, '    return default_path')
    (52, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/qcrs4, file=workflow/rules/common.smk
context_key: ['if n >= prefix[s]']
    (65, '    def bytes2human(n):')
    (66, '        # http://code.activestate.com/recipes/578019')
    (67, '        # >>> bytes2human(10000)')
    (68, "        # \\'9.8K\\'")
    (69, '        # >>> bytes2human(100001221)')
    (70, "        # \\'95.4M\\'")
    (71, "        symbols = (\\'K\\', \\'M\\', \\'G\\', \\'T\\', \\'P\\', \\'E\\', \\'Z\\', \\'Y\\')")
    (72, '        prefix = {}')
    (73, '        for i, s in enumerate(symbols):')
    (74, '            prefix[s] = 1 << (i + 1) * 10')
    (75, '        for s in reversed(symbols):')
    (76, '            if n >= prefix[s]:')
    (77, '                value = float(n) / prefix[s]')
    (78, "                return \\'%.0f%s\\' % (value, s)")
    (79, '        return "%sB" % n')
    (80, '')
    (81, '    def preserve(resource, percentage, stock):')
    (82, '        preserved = resource - max(resource * percentage // 100, stock)')
    (83, '        return preserved if preserved != 0 else stock')
    (84, '')
    (85, '    # def preserve(resource, percentage, stock):')
    (86, '    #     return resource - max(resource * percentage // 100, stock)')
    (87, '')
    (88, '    params_template = "\\\'-Xms{} -Xmx{} -XX:ParallelGCThreads={} " \\\\')
    (89, '                      "-Djava.io.tmpdir={}\\\'"')
    (90, '')
    (91, '    mem_min = 1024 ** 3 * 2  # 2GB')
    (92, '')
    (93, '    mem_size = preserve(total_physical_mem_size(), percentage_to_preserve,')
    (94, '                        stock_mem)')
    (95, '    cpu_nums = preserve(cpu_count(), percentage_to_preserve, stock_cpu)')
    (96, '    tmpdir = tmp_path(tmp_dir)')
    (97, '')
    (98, '    return params_template.format(bytes2human(mem_min).lower(),')
    (99, '                                  bytes2human(max(mem_size//cpu_nums*multiply_by,')
    (100, '                                                  mem_min)).lower(),')
    (101, '                                  min(cpu_nums, multiply_by),')
    (102, '                                  tmpdir)')
    (103, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/qcrs4, file=workflow/rules/common.smk
context_key: ['if not samples_master[wildcards.sample]["normal_bam"]']
    (134, 'def select_filtered(wildcards):')
    (135, '    with open(config["samples"],\\\'r\\\') as file:')
    (136, '        samples_master = yaml.load(file,Loader=yaml.FullLoader)')
    (137, '    if not samples_master[wildcards.sample]["normal_bam"]:')
    (138, '        return rules.filter_mutect_tumoronly.output.vcf')
    (139, '    else:')
    (140, '        return rules.filter_mutect.output.vcf')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hans-vg/snakemake_rnaseq_hisat2_workflow, file=rules/align.smk
context_key: ['if not is_single_end(**wildcards)']
    (14, 'def get_fq(wildcards):')
    (15, '    if not is_single_end(**wildcards):')
    (16, '        return expand("trimmed/{sample}-{unit}.{group}.fastq.gz", group=[1, 2], **wildcards)')
    (17, '    return "trimmed/{sample}-{unit}.fastq.gz".format(**wildcards)')
    (18, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=projectoriented/methylink, file=workflow/Snakefile
context_key: ['if not test.endswith(".bam")']
    (16, 'def get_methyl_bam(wildcards):')
    (17, '    test = manifest_df.at[wildcards.sample, "methyl_bam"]')
    (18, '    if not test.endswith(".bam"):')
    (19, '        with open(test, "r") as infile:')
    (20, '            return [line.rstrip() for line in infile]')
    (21, '    else:')
    (22, '        return [test]')
    (23, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cite-seq-alevin-fry-seurat, file=workflow/rules/common.smk
context_key: ['if "fq1" in sample and "fq2" in sample']
    (9, 'def get_reads(wildcards):')
    (10, '    sample = config["samples"][wildcards.sample]')
    (11, '    print(sample)')
    (12, '    if "fq1" in sample and "fq2" in sample:')
    (13, '        return {"fq1": sample["fq1"], "fq2": sample["fq2"]}')
    (14, '    else:')
    (15, '        accession = sample["sra"]')
    (16, '        return {')
    (17, '            "fq1": f"resources/sra/{accession}_1.fastq",')
    (18, '            "fq2": f"resources/sra/{accession}_2.fastq",')
    (19, '        }')
    (20, '')
    (21, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=myRNASeq/scRNASeqPipeline, file=rules/qc.smk
context_key: ['if constrain_celltypes']
    (53, 'def get_gene_vs_gene_fits(wildcards):')
    (54, '    constrain_celltypes = get_constrain_celltypes(wildcards)')
    (55, '    constrained_markers = markers')
    (56, '    if constrain_celltypes:')
    (57, '        constrained_markers = markers.loc[markers["name"].isin(constrain_celltypes)]')
    (58, '    return expand("analysis/cellassign.{parent}.rds", parent=constrained_markers["parent"].unique())')
    (59, '    ')
    (60, '')
    (61, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=genxnetwork/grape, file=workflows/simbig/Snakefile
context_key: ['if not(code in names)']
    (14, 'def generate_code():')
    (15, '    length = 6')
    (16, '    while True:')
    (17, "        code = \\'\\'.join(random.choices(string.ascii_uppercase, k=length))")
    (18, '        if not(code in names):')
    (19, '            break')
    (20, '    names.append(code)')
    (21, '    return code')
    (22, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=genxnetwork/grape, file=workflows/simbig/Snakefile
context_key: ['if ppl == 0']
    (23, 'def get_num_runs(def_file):')
    (24, '    with open("list.samples", "r") as f:')
    (25, '        lines = f.readlines()')
    (26, '        samples = [line.rstrip() for line in lines]')
    (27, '        total_founders = len(samples)')
    (28, '')
    (29, '    with open(f"{def_file}", "r") as f:')
    (30, '        lines = f.readlines()')
    (31, '        runs = int(lines[0].split(" ")[2])')
    (32, '        founders = 0')
    (33, '        for line in lines[1:-1]:')
    (34, '            ppl = int(line.split(" ")[2])')
    (35, '            if ppl == 0:')
    (36, '                ppl = 1')
    (37, '            founders += ppl')
    (38, '        founders += 1')
    (39, '        founders *= runs')
    (40, '        num_chips = int(total_founders / founders)')
    (41, '    print(f"Number of runs: {num_chips}")')
    (42, '    return num_chips, founders')
    (43, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=genxnetwork/grape, file=workflows/simbig/Snakefile
context_key: ['if not os.path.exists("background")']
    (44, 'def prepare_folders(num_runs, def_file):')
    (45, '    if not os.path.exists("background"):')
    (46, '        os.makedirs("background")')
    (47, '    for i in range(num_runs):')
    (48, '        if not os.path.exists(f"gen{i}"):')
    (49, '            os.makedirs(f"gen{i}")')
    (50, '        os.system(f"cp -R params gen{i}/params")')
    (51, '        with open(f"{def_file}", "r") as f:')
    (52, '            lines = f.readlines()')
    (53, '            first = lines[0].split(" ")')
    (54, '            name = generate_code()')
    (55, '            first[1] = name')
    (56, '            lines[0] = " ".join(first)')
    (57, '        with open(f"gen{i}/params/relatives_big.def", "w") as f:')
    (58, '            f.write("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=BDI-pathogens/ShiverCovid, file=snakemake/Snakefile
context_key: ["if dir_string.endswith(\\'/\\')"]
    (6, 'def remove_trailing_slash(dir_string):')
    (7, "    if dir_string.endswith(\\'/\\'):")
    (8, '        dir_string = dir_string[:-1]')
    (9, '    return dir_string')
    (10, '')
    (11, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ezherman/find-defence-systems, file=workflow/rules/common.smk
context_key: ['if program == "padloc"']
    (51, 'def create_subsystem_table(df, program):')
    (52, '')
    (53, '    if program == "padloc":')
    (54, "        df = df.groupby([\\'system.number\\',")
    (55, "                         \\'system\\'])         # group by system")
    (56, "        df = df.agg(\\';\\'.join)               # collapse rows within systems")
    (57, '        df = df.reset_index()               # ungroup')
    (58, '        df = df.rename(columns={"protein.name":"protein_names",')
    (59, '                                "target.name":"protein_IDs"}) # rename columns')
    (60, '        df = df[["system", "protein_names", "protein_IDs"]]   # choose columns')
    (61, '')
    (62, '    if program == "defense_finder":')
    (63, "        df = df.groupby([\\'sys_id\\'])         # group by system")
    (64, "        df = df.agg(\\';\\'.join)               # collapse rows within systems")
    (65, '        df = df.reset_index()               # ungroup')
    (66, '')
    (67, '        #rename columns')
    (68, '        df = df.rename(columns={"sys_id":"system",')
    (69, '                                "name_of_profiles_in_sys":"protein_names",')
    (70, '                                "protein_in_syst":"protein_IDs"})')
    (71, '        df = df[["system", "protein_names", "protein_IDs"]] # choose columns')
    (72, '')
    (73, '        #remove "UserReplicon_" from system names')
    (74, '        df["system"] = df["system"].str.replace(\\\'UserReplicon_\\\', \\\'\\\')')
    (75, '')
    (76, '    return df')
    (77, '')
    (78, '')
    (79, '## merge padloc and defense_finder dataframes')
    (80, '## if padloc is empty because no results were found')
    (81, '## process only defense_finder')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ezherman/find-defence-systems, file=workflow/rules/common.smk
context_key: ['if df_padloc is None']
    (82, 'def merge_subsystem_tables(df_defense_finder, df_padloc = None):')
    (83, '')
    (84, '    # if no df_padloc exists, final table is defense_finder only')
    (85, '    if df_padloc is None:')
    (86, '')
    (87, '        df = pd.concat([df_defense_finder],')
    (88, '                       keys = ["defense_finder"],')
    (89, '                       names = ("program", "row"))')
    (90, '')
    (91, '        df = df.reset_index(level = "row", drop = True)')
    (92, '')
    (93, '    # if df_padloc exists, merge padloc and defense_finder')
    (94, '    if isinstance(df_padloc, pd.DataFrame):')
    (95, '')
    (96, '        df = pd.concat([df_padloc, df_defense_finder],')
    (97, '                   keys = ["padloc", "defense_finder"],')
    (98, '                   names = ("program", "row"))          # merge tables')
    (99, '')
    (100, '        df = df.reset_index(level = "row", drop = True)     # drop row index')
    (101, '')
    (102, '')
    (103, '    return df')
    (104, '')
    (105, '')
    (106, '## merge system tables of all samples')
    (107, '## also reduce data from number of hits to presence-absence')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmeppley/np_read_clustering, file=rules/Snakefile.polish
context_key: ["if GROUP is set (config[\\'group\\']), only that group of clusters is processed"]
    (14, 'def get_polished_output_file_names(kind=None):')
    (15, '    """ return a list of output files for the polishing step for all subclusters')
    (16, "    if GROUP is set (config[\\'group\\']), only that group of clusters is processed ")
    (17, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmeppley/np_read_clustering, file=rules/Snakefile.minimap
context_key: ['if you hit a limit of open file handle']
    (91, 'def get_windows():')
    (92, '    reads_dir = checkpoints.apply_fasta_size_window.get().output.fasta')
    (93, '    windows, = glob_wildcards(reads_dir + "/reads.{window}.fasta")')
    (94, '    return windows')
    (95, '')
    (96, 'checkpoint apply_fasta_size_window:')
    (97, '    """ divide all reads into windows by size')
    (98, '')
    (99, '    for speed, this keeps an open handle to all output files. This may be ')
    (100, '        problamatic on some systems. Look into ulimt or file a bug report')
    (101, '        if you hit a limit of open file handles')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmeppley/np_read_clustering, file=rules/Snakefile.minimap
context_key: ['output', 'run', 'if l < MIN_SIZE']
    (91, 'def get_windows():')
    (92, '    reads_dir = checkpoints.apply_fasta_size_window.get().output.fasta')
    (93, '    windows, = glob_wildcards(reads_dir + "/reads.{window}.fasta")')
    (94, '    return windows')
    (95, '')
    (96, 'checkpoint apply_fasta_size_window:')
    (97, '    """ divide all reads into windows by size')
    (98, '')
    (99, '    for speed, this keeps an open handle to all output files. This may be ')
    (100, '        problamatic on some systems. Look into ulimt or file a bug report')
    (101, '        if you hit a limit of open file handles')
    (102, '            """')
    (103, '    input: ALL_FASTA')
    (104, '    output: ')
    (105, "        fasta=directory(f\\'{WORK_DIR}/windows/reads\\'),")
    (106, "        counts=f\\'{WORK_DIR}/windows/read_counts.txt\\',")
    (107, "        read_lens=f\\'{WORK_DIR}/all.reads.lengths.tsv\\'")
    (108, "    benchmark: f\\'{WORK_DIR}/all.reads.lengths.tsv.time\\'")
    (109, '    run:')
    (110, '        handles = {}')
    (111, '        counts = {}')
    (112, '        read_lens = {}')
    (113, '        os.makedirs(str(output.fasta), exist_ok=True)')
    (114, "        for i, read in enumerate(SeqIO.parse(str(input), \\'fasta\\')):")
    (115, '            l = len(read)')
    (116, '            if l < MIN_SIZE:')
    (117, '                continue')
    (118, '')
    (119, '            read_lens[read.id] = l')
    (120, '            ')
    (121, '            windows = []')
    (122, '            ')
    (123, '            window_num = int(l / (WINDOW_SPACING))')
    (124, '            window_start = window_num * WINDOW_SPACING')
    (125, '            window_end = window_start + WINDOW_SIZE')
    (126, '            ')
    (127, "            # shift down incase we\\'d be in the end of an earlier window")
    (128, '            while window_end > l + WINDOW_SPACING:')
    (129, '                window_num -= 1')
    (130, '                window_start = window_num * WINDOW_SPACING')
    (131, '                window_end = window_start + WINDOW_SIZE')
    (132, '')
    (133, '            # loop over all windows this read is in')
    (134, '            while window_start <= l:')
    (135, '                window_end = window_start + WINDOW_SIZE')
    (136, '                window = f"{window_start}_{window_end}"')
    (137, '')
    (138, '                counts[window] = counts.get(window, 0) + 1')
    (139, '')
    (140, '                windows.append(window)')
    (141, '')
    (142, '                try:')
    (143, '                    fasta_out = handles[window]')
    (144, '                except:')
    (145, '                    out_file = f"{output.fasta}/reads.{window}.fasta"')
    (146, "                    fasta_out = open(out_file, \\'wt\\')")
    (147, '                    handles[window] = fasta_out')
    (148, '')
    (149, "                fasta_out.write(read.format(\\'fasta\\'))")
    (150, '            ')
    (151, '                window_num += 1')
    (152, '                window_start = window_num * WINDOW_SPACING')
    (153, '            ')
    (154, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmeppley/np_read_clustering, file=rules/Snakefile.finish
context_key: ['if as_dict']
    (2, 'def get_refining_mcls(w, as_dict=False):')
    (3, '    """ Find all the stats files for subcluster choosing (one for each kept')
    (4, '    cluster)')
    (5, '')
    (6, '    Return a list if called from input, and dict for the python code')
    (7, '    """')
    (8, '    sc_mcls = \\\\')
    (9, "        {cluster: f\\'{WORK_DIR}/refine_lastal/group.{group}/cluster.{cluster}/cluster.{cluster}.self.m8.gt{MFRAC_CUTOFF}.I{MCL_I}.mcl\\'")
    (10, '         for group, cluster in get_pre_filtered_clusters()}')
    (11, '')
    (12, '    if as_dict:')
    (13, '        return sc_mcls')
    (14, '    return sc_mcls.values()')
    (15, '')
    (16, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmeppley/np_read_clustering, file=rules/Snakefile.refine
context_key: ['if GROUP >= 0']
    (60, 'def get_pre_filtered_clusters():')
    (61, '    reads_dir = checkpoints.choose_clusters.get().output.reads')
    (62, '')
    (63, '    # hack to process a single group')
    (64, '    if GROUP >= 0:')
    (65, '        logger.debug(f"run checkpoint check just on group: {GROUP}")')
    (66, '        clusters, = glob_wildcards(reads_dir + f"/group.{GROUP}/cluster.{{cluster}}.fasta")')
    (67, '        logger.debug(f"Found {len(clusters)} clusters in group {GROUP}")')
    (68, '        return [(GROUP,c) for c in clusters]')
    (69, '')
    (70, '    groups, clusters, = glob_wildcards(reads_dir + "/group.{group}/cluster.{cluster}.fasta")')
    (71, '    logger.debug(f"Found {len(clusters)} cluster in {len(set(groups))} groups")')
    (72, '    return [(g,c) for g,c in zip(groups, clusters)]')
    (73, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Zhanmengtao/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if _sample_dict is None']
    (363, 'def get_sample_dict():')
    (364, '    global _sample_dict')
    (365, '    if _sample_dict is None:')
    (366, '        checkpoints.generate_sample_list.get()')
    (367, '        dat_samples = pd.read_csv(file_path_samples)')
    (368, '        _sample_dict = {pathlib.Path(x).stem: x for x in dat_samples[')
    (369, "            \\'mcd_folders\\']}")
    (370, '    return _sample_dict')
    (371, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=IKIM-Essen/MSA-phylogeny, file=workflow/rules/common.smk
context_key: ['if wildcards.region == "genome"']
    (26, 'def get_fastas_by_region(wildcards):')
    (27, '    if wildcards.region == "genome":')
    (28, '        return get_genomes(sample=get_samples_for_tag(wildcards.tag))')
    (29, '    else:')
    (30, '        return expand(')
    (31, '            "results/{{tag}}/region-of-interest/{sample}~{{region}}.fasta",')
    (32, '            sample=get_samples_for_tag(wildcards.tag),')
    (33, '        )')
    (34, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=IKIM-Essen/MSA-phylogeny, file=workflow/rules/common.smk
context_key: ['if wildcards.state == "aligned"']
    (39, 'def get_msa_input(wildcards):')
    (40, '    if wildcards.state == "aligned":')
    (41, '        return "results/{tag}/msa-aligned/{region}~aligned.fasta"')
    (42, '    elif wildcards.state == "cleaned":')
    (43, '        return "results/{tag}/msa-cleaned/{region}~aligned_cleaned.fasta",\'')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=williamjeong2/snakemake_RNA-seq, file=Snakefile
context_key: ['if "fq2" not in samples.columns']
    (48, 'def sample_is_single_end(sample):')
    (49, '    """This function detect missing value in the column 2 of the units.tsv"""')
    (50, '    if "fq2" not in samples.columns:')
    (51, '        return True')
    (52, '    else:')
    (53, '        return pd.isnull(samples.loc[(sample), "fq2"])')
    (54, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=williamjeong2/snakemake_RNA-seq, file=Snakefile
context_key: ['if sample_is_single_end(wildcards.sample)']
    (55, 'def get_fastq(wildcards):')
    (56, '    """ This function checks if the sample has paired end or single end reads')
    (57, '    and returns 1 or 2 names of the fastq files """')
    (58, '    if sample_is_single_end(wildcards.sample):')
    (59, '        return samples.loc[(wildcards.sample), ["fq1"]].dropna()')
    (60, '    else:')
    (61, '        return samples.loc[(wildcards.sample), ["fq1", "fq2"]].dropna()')
    (62, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=williamjeong2/snakemake_RNA-seq, file=Snakefile
context_key: ['if sample_is_single_end(wildcards.sample)']
    (63, 'def get_trimmed(wildcards):')
    (64, '    """ This function checks if sample is paired end or single end')
    (65, '    and returns 1 or 2 names of the trimmed fastq files """')
    (66, '    if sample_is_single_end(wildcards.sample):')
    (67, '        return WORKING_DIR + "trimmed/" + wildcards.sample + "_R1_trimmed.fq.gz"')
    (68, '    else:')
    (69, '        return [WORKING_DIR + "trimmed/" + wildcards.sample + "_R1_trimmed.fq.gz", WORKING_DIR + "trimmed/" + wildcards.sample + "_R2_trimmed.fq.gz"]')
    (70, '')
    (71, '#################')
    (72, '# Desired outputs')
    (73, '#################')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=williamjeong2/snakemake_RNA-seq, file=Snakefile_toCount
context_key: ['if "fq2" not in samples.columns']
    (48, 'def sample_is_single_end(sample):')
    (49, '    """This function detect missing value in the column 2 of the units.tsv"""')
    (50, '    if "fq2" not in samples.columns:')
    (51, '        return True')
    (52, '    else:')
    (53, '        return pd.isnull(samples.loc[(sample), "fq2"])')
    (54, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=williamjeong2/snakemake_RNA-seq, file=Snakefile_toCount
context_key: ['if sample_is_single_end(wildcards.sample)']
    (55, 'def get_fastq(wildcards):')
    (56, '    """ This function checks if the sample has paired end or single end reads')
    (57, '    and returns 1 or 2 names of the fastq files """')
    (58, '    if sample_is_single_end(wildcards.sample):')
    (59, '        return samples.loc[(wildcards.sample), ["fq1"]].dropna()')
    (60, '    else:')
    (61, '        return samples.loc[(wildcards.sample), ["fq1", "fq2"]].dropna()')
    (62, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=williamjeong2/snakemake_RNA-seq, file=Snakefile_toCount
context_key: ['if sample_is_single_end(wildcards.sample)']
    (63, 'def get_trimmed(wildcards):')
    (64, '    """ This function checks if sample is paired end or single end')
    (65, '    and returns 1 or 2 names of the trimmed fastq files """')
    (66, '    if sample_is_single_end(wildcards.sample):')
    (67, '        return WORKING_DIR + "trimmed/" + wildcards.sample + "_R1_trimmed.fq.gz"')
    (68, '    else:')
    (69, '        return [WORKING_DIR + "trimmed/" + wildcards.sample + "_R1_trimmed.fq.gz", WORKING_DIR + "trimmed/" + wildcards.sample + "_R2_trimmed.fq.gz"]')
    (70, '')
    (71, '#################')
    (72, '# Desired outputs')
    (73, '#################')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=richardshuai/sv2a-rna-seq, file=workflow/rules/common.smk
context_key: ['if pd.isna(unit["fq1"])']
    (35, 'def get_cutadapt_input(wildcards):')
    (36, '    unit = units.loc[wildcards.sample].loc[wildcards.unit]')
    (37, '')
    (38, '    if pd.isna(unit["fq1"]):')
    (39, '        # SRA sample (always paired-end for now)')
    (40, '        accession = unit["sra"]')
    (41, '        return expand("sra/{accession}_{read}.fastq", accession=accession, read=[1, 2])')
    (42, '')
    (43, '    if unit["fq1"].endswith("gz"):')
    (44, '        ending = ".gz"')
    (45, '    else:')
    (46, '        ending = ""')
    (47, '')
    (48, '    if pd.isna(unit["fq2"]):')
    (49, '        # single end local sample')
    (50, '        return "pipe/cutadapt/{S}/{U}.fq1.fastq{E}".format(')
    (51, '            S=unit.sample_name, U=unit.unit_name, E=ending')
    (52, '        )')
    (53, '    else:')
    (54, '        # paired end local sample')
    (55, '        return expand(')
    (56, '            "pipe/cutadapt/{S}/{U}.{{read}}.fastq{E}".format(')
    (57, '                S=unit.sample_name, U=unit.unit_name, E=ending')
    (58, '            ),')
    (59, '            read=["fq1", "fq2"],')
    (60, '        )')
    (61, '')
    (62, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=richardshuai/sv2a-rna-seq, file=workflow/rules/common.smk
context_key: ['if not is_activated("mergeReads")', 'if config["trimming"]["activate"]']
    (86, 'def get_map_reads_input_R1(wildcards):')
    (87, '    if not is_activated("mergeReads"):')
    (88, '        if config["trimming"]["activate"]:')
    (89, '            return expand(')
    (90, '                "results/trimmed/{sample}_{unit}_R1.fastq.gz",')
    (91, '                unit=units.loc[wildcards.sample, "unit_name"],')
    (92, '                sample=wildcards.sample,')
    (93, '            )')
    (94, '        unit = units.loc[wildcards.sample]')
    (95, '        if all(pd.isna(unit["fq1"])):')
    (96, '            # SRA sample (always paired-end for now)')
    (97, '            accession = unit["sra"]')
    (98, '            return expand("sra/{accession}_R1.fastq", accession=accession)')
    (99, '        sample_units = units.loc[wildcards.sample]')
    (100, '        return sample_units["fq1"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=richardshuai/sv2a-rna-seq, file=workflow/rules/common.smk
context_key: ['if not is_activated("mergeReads")', 'if is_paired_end(wildcards.sample)']
    (86, 'def get_map_reads_input_R1(wildcards):')
    (87, '    if not is_activated("mergeReads"):')
    (88, '        if config["trimming"]["activate"]:')
    (89, '            return expand(')
    (90, '                "results/trimmed/{sample}_{unit}_R1.fastq.gz",')
    (91, '                unit=units.loc[wildcards.sample, "unit_name"],')
    (92, '                sample=wildcards.sample,')
    (93, '            )')
    (94, '        unit = units.loc[wildcards.sample]')
    (95, '        if all(pd.isna(unit["fq1"])):')
    (96, '            # SRA sample (always paired-end for now)')
    (97, '            accession = unit["sra"]')
    (98, '            return expand("sra/{accession}_R1.fastq", accession=accession)')
    (99, '        sample_units = units.loc[wildcards.sample]')
    (100, '        return sample_units["fq1"]')
    (101, '    if is_paired_end(wildcards.sample):')
    (102, '        return "results/merged/{sample}_R1.fastq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=richardshuai/sv2a-rna-seq, file=workflow/rules/common.smk
context_key: ['if not is_activated("mergeReads")']
    (86, 'def get_map_reads_input_R1(wildcards):')
    (87, '    if not is_activated("mergeReads"):')
    (88, '        if config["trimming"]["activate"]:')
    (89, '            return expand(')
    (90, '                "results/trimmed/{sample}_{unit}_R1.fastq.gz",')
    (91, '                unit=units.loc[wildcards.sample, "unit_name"],')
    (92, '                sample=wildcards.sample,')
    (93, '            )')
    (94, '        unit = units.loc[wildcards.sample]')
    (95, '        if all(pd.isna(unit["fq1"])):')
    (96, '            # SRA sample (always paired-end for now)')
    (97, '            accession = unit["sra"]')
    (98, '            return expand("sra/{accession}_R1.fastq", accession=accession)')
    (99, '        sample_units = units.loc[wildcards.sample]')
    (100, '        return sample_units["fq1"]')
    (101, '    if is_paired_end(wildcards.sample):')
    (102, '        return "results/merged/{sample}_R1.fastq.gz"')
    (103, '    return "results/merged/{sample}_single.fastq.gz"')
    (104, '')
    (105, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=richardshuai/sv2a-rna-seq, file=workflow/rules/common.smk
context_key: ['if is_paired_end(wildcards.sample)', 'if not is_activated("mergeReads")', 'if config["trimming"]["activate"]']
    (106, 'def get_map_reads_input_R2(wildcards):')
    (107, '    if is_paired_end(wildcards.sample):')
    (108, '        if not is_activated("mergeReads"):')
    (109, '            if config["trimming"]["activate"]:')
    (110, '                return expand(')
    (111, '                    "results/trimmed/{sample}_{unit}_R1.fastq.gz",')
    (112, '                    unit=units.loc[wildcards.sample, "unit_name"],')
    (113, '                    sample=wildcards.sample,')
    (114, '                )')
    (115, '            unit = units.loc[wildcards.sample]')
    (116, '            if all(pd.isna(unit["fq1"])):')
    (117, '                # SRA sample (always paired-end for now)')
    (118, '                accession = unit["sra"]')
    (119, '                return expand("sra/{accession}_R2.fastq", accession=accession)')
    (120, '            sample_units = units.loc[wildcards.sample]')
    (121, '            return sample_units["fq2"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=richardshuai/sv2a-rna-seq, file=workflow/rules/common.smk
context_key: ['if is_paired_end(wildcards.sample)', 'if not is_activated("mergeReads")']
    (106, 'def get_map_reads_input_R2(wildcards):')
    (107, '    if is_paired_end(wildcards.sample):')
    (108, '        if not is_activated("mergeReads"):')
    (109, '            if config["trimming"]["activate"]:')
    (110, '                return expand(')
    (111, '                    "results/trimmed/{sample}_{unit}_R1.fastq.gz",')
    (112, '                    unit=units.loc[wildcards.sample, "unit_name"],')
    (113, '                    sample=wildcards.sample,')
    (114, '                )')
    (115, '            unit = units.loc[wildcards.sample]')
    (116, '            if all(pd.isna(unit["fq1"])):')
    (117, '                # SRA sample (always paired-end for now)')
    (118, '                accession = unit["sra"]')
    (119, '                return expand("sra/{accession}_R2.fastq", accession=accession)')
    (120, '            sample_units = units.loc[wildcards.sample]')
    (121, '            return sample_units["fq2"]')
    (122, '        return ("results/merged/{sample}_R2.fastq.gz",)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=richardshuai/sv2a-rna-seq, file=workflow/rules/common.smk
context_key: ['if is_paired_end(wildcards.sample)']
    (106, 'def get_map_reads_input_R2(wildcards):')
    (107, '    if is_paired_end(wildcards.sample):')
    (108, '        if not is_activated("mergeReads"):')
    (109, '            if config["trimming"]["activate"]:')
    (110, '                return expand(')
    (111, '                    "results/trimmed/{sample}_{unit}_R1.fastq.gz",')
    (112, '                    unit=units.loc[wildcards.sample, "unit_name"],')
    (113, '                    sample=wildcards.sample,')
    (114, '                )')
    (115, '            unit = units.loc[wildcards.sample]')
    (116, '            if all(pd.isna(unit["fq1"])):')
    (117, '                # SRA sample (always paired-end for now)')
    (118, '                accession = unit["sra"]')
    (119, '                return expand("sra/{accession}_R2.fastq", accession=accession)')
    (120, '            sample_units = units.loc[wildcards.sample]')
    (121, '            return sample_units["fq2"]')
    (122, '        return ("results/merged/{sample}_R2.fastq.gz",)')
    (123, '    return ""')
    (124, '')
    (125, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=richardshuai/sv2a-rna-seq, file=workflow/rules/common.smk
context_key: ['if is_paired_end(unit.sample_name)']
    (126, 'def get_star_output(wildcards):')
    (127, '    res = []')
    (128, '    for unit in units.itertuples():')
    (129, '        if is_paired_end(unit.sample_name):')
    (130, '            lib = "pe"')
    (131, '        else:')
    (132, '            lib = "se"')
    (133, '        res.append(')
    (134, '            "results/star/{}/{}-{}/ReadsPerGene.out.tab".format(')
    (135, '                lib, unit.sample_name, unit.unit_name')
    (136, '            )')
    (137, '        )')
    (138, '    return res')
    (139, '')
    (140, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=richardshuai/sv2a-rna-seq, file=workflow/rules/common.smk
context_key: ['if "strandedness" in units.columns']
    (141, 'def get_strandedness(units):')
    (142, '    if "strandedness" in units.columns:')
    (143, '        return units["strandedness"].tolist()')
    (144, '    else:')
    (145, '        strand_list = ["none"]')
    (146, '        return strand_list * units.shape[0]')
    (147, '')
    (148, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=USGS-R/river-dl, file=workflow_examples/Snakefile_rgcn_pytorch.smk
context_key: ["if wildcards.metric_type == \\'overall\\'"]
    (165, 'def get_grp_arg(wildcards):')
    (166, "    if wildcards.metric_type == \\'overall\\':")
    (167, '        return None')
    (168, "    elif wildcards.metric_type == \\'month\\':")
    (169, "        return \\'month\\'")
    (170, "    elif wildcards.metric_type == \\'reach\\':")
    (171, "        return \\'seg_id_nat\\'")
    (172, "    elif wildcards.metric_type == \\'month_reach\\':")
    (173, "        return [\\'seg_id_nat\\', \\'month\\']")
    (174, '')
    (175, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hivlab/discover-virome, file=rules/common.smk
context_key: ['if len(fq_cols) == 2']
    (3, 'def get_fastq(wildcards):')
    (4, '    fq_cols = [col for col in df.columns if re.match("fq\\\\d$", col)]')
    (5, '    fqs = (')
    (6, '        df.reset_index(level="sample", drop=True)')
    (7, '        .loc[(wildcards.group, wildcards.run), fq_cols]')
    (8, '        .dropna()')
    (9, '    )')
    (10, '    assert len(fq_cols) in [1, 2], "Enter one or two FASTQ file paths"')
    (11, '    if len(fq_cols) == 2:')
    (12, '        return {"in1": fqs[0], "in2": fqs[1]}')
    (13, '    else:')
    (14, '        return {"input": fqs[0]}')
    (15, '')
    (16, '')
    (17, '# Helper function to import tables')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hivlab/discover-virome, file=rules/common.smk
context_key: ['if cols_to_integer']
    (26, 'def concatenate_tables(input, sep="\\\\s+", cols_to_integer=None):')
    (27, '    frames = [safely_read_csv(f, sep=sep) for f in input]')
    (28, '    frames_concatenated = pd.concat(frames, keys=input, sort=False)')
    (29, '    if cols_to_integer:')
    (30, '        frames_concatenated[cols_to_integer] = frames_concatenated[')
    (31, '            cols_to_integer')
    (32, '        ].apply(lambda x: pd.Series(x, dtype="Int64"))')
    (33, '    return frames_concatenated')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=allytrope/variant-analysis, file=workflow/rules/variant_recalibration.smk
context_key: ['if string == "training"']
    (8, 'def truth_or_training(string):')
    (9, '    if string == "training":')
    (10, '        return ">="')
    (11, '    elif string == "truth":')
    (12, '        return "<"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=allytrope/variant-analysis, file=workflow/rules/variant_recalibration.smk
context_key: ['if mode == "SNP"']
    (43, 'def an_flags(mode):')
    (44, '    """Return `-an` flags and arguments for variant recalibration."""')
    (45, '    if mode == "SNP":')
    (46, '        return "-an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -an DP"')
    (47, '    elif mode == "indel":')
    (48, '        return "-an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -an DP"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if (config["align_vs_reference"]["align"] == "T" and config["cutAdapters"] != "F")']
    (8, 'def selectInput():')
    (9, '    if (config["align_vs_reference"]["align"] == "T" and config["cutAdapters"] != "F"):')
    (10, '        return ["{PROJECT}/runs/{run}/{sample}_data/seqs_fw_rev_accepted.align.no_adapter.degapped.oneline.fna"]')
    (11, '    elif (config["align_vs_reference"]["align"] == "T" and config["cutAdapters"] == "F"):')
    (12, '        return ["{PROJECT}/runs/{run}/{sample}_data/seqs_fw_rev_accepted.align.degapped.oneline.fna"]')
    (13, '    elif (config["align_vs_reference"]["align"] != "T" and config["cutAdapters"] != "F" and config["ANALYSIS_TYPE"]=="OTU"):')
    (14, '        return ["{PROJECT}/runs/{run}/{sample}_data/seqs_fw_rev_accepted_no_adapters.fna"]')
    (15, '    else:')
    (16, '        return ["{PROJECT}/runs/{run}/{sample}_data/seqs_fw_rev_accepted.fna"]')
    (17, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_4.smk
context_key: ['if data_file.endswith(".fna.gz")']
    (115, 'def get_all_genomes_in_dataset_exp_4(wildcards):')
    (116, '    """ Returns a list of database in a certain dataset """')
    (117, '    input_files = []')
    (118, '    if(exp_type ==4):')
    (119, '        for data_file in os.listdir(f"input_type4/rest_of_set/dataset_{wildcards.num}/"):')
    (120, '            if data_file.endswith(".fna.gz"):')
    (121, '                file_name = data_file.split(".fna.gz")[0]')
    (122, '                input_files.append(f"genome_sets_type_4/rest_of_set/k_{wildcards.k}/dataset_{wildcards.num}/{file_name}.transformed.kmc_pre")')
    (123, '    return input_files')
    (124, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_5.smk
context_key: ['if exp_type == 5', 'if data_file.endswith(".fna")']
    (50, 'def get_all_non_pivot_genomes_exp_5():')
    (51, '    """ Returns list of all non-pivot genomes """')
    (52, '    input_files = []')
    (53, '    if exp_type == 5:')
    (54, '        for i in range(1, num_datasets + 1):')
    (55, '            for data_file in os.listdir(f"non_pivot_type_5/dataset_{i}/"):')
    (56, '                if data_file.endswith(".fna"):')
    (57, '                    file_name = data_file.split(".fna")[0]')
    (58, '                    input_files.append(f"non_pivot_type_5/dataset_{i}/{file_name}.fna")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_5.smk
context_key: ['if exp_type == 5']
    (50, 'def get_all_non_pivot_genomes_exp_5():')
    (51, '    """ Returns list of all non-pivot genomes """')
    (52, '    input_files = []')
    (53, '    if exp_type == 5:')
    (54, '        for i in range(1, num_datasets + 1):')
    (55, '            for data_file in os.listdir(f"non_pivot_type_5/dataset_{i}/"):')
    (56, '                if data_file.endswith(".fna"):')
    (57, '                    file_name = data_file.split(".fna")[0]')
    (58, '                    input_files.append(f"non_pivot_type_5/dataset_{i}/{file_name}.fna")')
    (59, '    return input_files')
    (60, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_5.smk
context_key: ['if data_file.endswith(".fna")']
    (61, 'def get_dataset_non_pivot_genomes_exp_5(wildcards):')
    (62, '    """ Returns list of non-pivot genomes for a given dataset """')
    (63, '    input_files = []')
    (64, '    if(exp_type == 5):')
    (65, '        for data_file in os.listdir(f"non_pivot_type_5/dataset_{wildcards.num}/"):')
    (66, '            if data_file.endswith(".fna"):')
    (67, '                file_name = data_file.split(".fna")[0]')
    (68, '                input_files.append(f"non_pivot_type_5/dataset_{wildcards}/{file_name}.fna")')
    (69, '    return input_files')
    (70, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_6.smk
context_key: ['if data_file.endswith(".fna.gz")']
    (71, 'def get_all_raw_genome_files_in_dataset_exp6(wildcards):')
    (72, '    """ Return path to all the raw genome files in dataset """')
    (73, '    input_files = []')
    (74, '    for data_file in os.listdir(f"exp6_input/rest_of_set/dataset_{wildcards.num}"):')
    (75, '        if data_file.endswith(".fna.gz"):')
    (76, '            input_files.append(f"exp6_input/rest_of_set/dataset_{wildcards.num}/{data_file}")')
    (77, '    return input_files')
    (78, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_6.smk
context_key: ['if data_file.endswith(".fna.gz")']
    (79, 'def get_all_genome_sets_in_dataset_exp6(wildcards):')
    (80, '    """ Returns a list of database in a certain dataset """')
    (81, '    input_files = []')
    (82, '    for data_file in os.listdir(f"exp6_input/rest_of_set/dataset_{wildcards.num}/"):')
    (83, '        if data_file.endswith(".fna.gz"):')
    (84, '            file_name = data_file.split(".fna.gz")[0]')
    (85, '            input_files.append(f"exp6_genome_sets/rest_of_set/k_{wildcards.k}/dataset_{wildcards.num}/{file_name}.transformed.kmc_pre")')
    (86, '    return input_files')
    (87, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_3.smk
context_key: ['if data_file.endswith(".fna.gz")']
    (98, 'def get_all_genomes_in_dataset_type3(wildcards):')
    (99, '    """ Returns a list of database in a certain dataset """')
    (100, '    input_files = []')
    (101, '    for data_file in os.listdir(f"input_type3/rest_of_set/dataset_{wildcards.num}/"):')
    (102, '        if data_file.endswith(".fna.gz"):')
    (103, '            file_name = data_file.split(".fna.gz")[0]')
    (104, '            input_files.append(f"genome_sets_type3/rest_of_set/k_{wildcards.k}/dataset_{wildcards.num}/{file_name}.transformed.kmc_pre")')
    (105, '    return input_files')
    (106, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_8.smk
context_key: ['if exp_type == 8', 'if data_file.endswith(".fna")']
    (49, 'def get_all_non_pivot_genomes_exp_8():')
    (50, '    """ Returns list of all non-pivot genomes """')
    (51, '    input_files = []')
    (52, '    if exp_type == 8:')
    (53, '        for i in range(1, num_datasets + 1):')
    (54, '            for data_file in os.listdir(f"non_pivot_type_8/dataset_{i}/"):')
    (55, '                if data_file.endswith(".fna"):')
    (56, '                    file_name = data_file.split(".fna")[0]')
    (57, '                    input_files.append(f"non_pivot_type_8/dataset_{i}/{file_name}.fna")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_8.smk
context_key: ['if exp_type == 8']
    (49, 'def get_all_non_pivot_genomes_exp_8():')
    (50, '    """ Returns list of all non-pivot genomes """')
    (51, '    input_files = []')
    (52, '    if exp_type == 8:')
    (53, '        for i in range(1, num_datasets + 1):')
    (54, '            for data_file in os.listdir(f"non_pivot_type_8/dataset_{i}/"):')
    (55, '                if data_file.endswith(".fna"):')
    (56, '                    file_name = data_file.split(".fna")[0]')
    (57, '                    input_files.append(f"non_pivot_type_8/dataset_{i}/{file_name}.fna")')
    (58, '    return input_files')
    (59, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_8.smk
context_key: ['if data_file.endswith(".fna")']
    (60, 'def get_dataset_non_pivot_genomes_exp_8(wildcards):')
    (61, '    """ Returns list of non-pivot genomes for a given dataset """')
    (62, '    input_files = []')
    (63, '    for data_file in os.listdir(f"non_pivot_type_8/dataset_{wildcards.num}/"):')
    (64, '        if data_file.endswith(".fna"):')
    (65, '            file_name = data_file.split(".fna")[0]')
    (66, '            input_files.append(f"non_pivot_type_8/dataset_{wildcards}/{file_name}.fna")')
    (67, '    return input_files')
    (68, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/prepare_data.smk
context_key: ['if data_file.endswith(".fna.gz")']
    (14, 'def get_all_genomes_in_dataset_exp0(wildcards):')
    (15, '    """ Returns a list of database in a certain dataset """')
    (16, '    input_files = []')
    (17, '    for data_file in os.listdir(f"{database_root}/dataset_{wildcards.num}/"):')
    (18, '        if data_file.endswith(".fna.gz"):')
    (19, '            input_files.append(f"{database_root}/dataset_{wildcards.num}/{data_file}")')
    (20, '    return input_files')
    (21, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_1.smk
context_key: ['if data_file.endswith(".fna.gz")']
    (103, 'def get_group_set(wildcards):')
    (104, '    """ Based on the group, this function will find all the input files """')
    (105, '    input_files = []')
    (106, '    for data_file in os.listdir(f"data/dataset_{wildcards.num}"):')
    (107, '        if data_file.endswith(".fna.gz"):')
    (108, '            file_name = data_file.split(".fna.gz")[0]')
    (109, '            input_files.append(f"step_2/k_{wildcards.k}/dataset_{wildcards.num}/{file_name}.transformed.kmc_pre")')
    (110, '    return input_files')
    (111, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_1.smk
context_key: ['if data_file.endswith(".fna.gz")']
    (118, 'def get_num_of_dataset_members(dataset_num):')
    (119, '    """ Returns the number of genomes in a particular dataset """')
    (120, '    num = 0')
    (121, '    for data_file in os.listdir(f"data/dataset_{dataset_num}"):')
    (122, '        if data_file.endswith(".fna.gz"):')
    (123, '            num += 1')
    (124, '    return num')
    (125, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_1.smk
context_key: ['if across_group_analysis']
    (126, 'def summarize_histogram_type1(hist_counts, num_dataset_members, across_group_analysis, k):')
    (127, '    """ ')
    (128, '        Takes in histogram of kmer occurrences, and returns the metrics for the ')
    (129, '        bar charts summarized below:')
    (130, '')
    (131, '            %_1_occ - percentage of unique kmers that only occur in one genome')
    (132, '            %_25_or_less - percentage of unique kmers that occur in multiple genomes, in 25% of the genomes or less')
    (133, '            %_25_to_75 - percentage of unique kmers that occur in multiple genomes, in 25% to 75% of the genomes')
    (134, '            %_75_or_more - percentage of unique kmers that occur in multiple genomes, in 75% or more of the genomes')
    (135, '            unique_stat - weighted sum of kmer occurrents = SUM([occ * %_unique_occ for occ in range(255)]) ')
    (136, '    """')
    (137, '    metrics = [0 for i in range(7)]')
    (138, '    total_unique_kmers = sum(hist_counts)')
    (139, '    ')
    (140, '    boundaries = [0.25, 0.75]')
    (141, '    boundary_indices = [max(int(percent * num_dataset_members), 1) for percent in boundaries]')
    (142, '')
    (143, '    # Special cases where indices are customized ...')
    (144, '    if across_group_analysis:')
    (145, '        boundary_indices = [5, 20]')
    (146, '    ')
    (147, '    metrics[0] = round(hist_counts[0]/total_unique_kmers, 3)')
    (148, '    metrics[1] = round(sum([hist_counts[i] for i in range(1, boundary_indices[0])])/total_unique_kmers, 3)')
    (149, '    metrics[2] = round(sum([hist_counts[i] for i in range(boundary_indices[0], boundary_indices[1])])/total_unique_kmers, 3)')
    (150, '    metrics[3] = round(sum([hist_counts[i] for i in range(boundary_indices[1], len(hist_counts))])/total_unique_kmers, 3)')
    (151, '')
    (152, '    rounding_error = abs(sum(metrics[0:4])-1) ')
    (153, '    assert rounding_error < 0.05, "Issue occurred with histogram summarization"')
    (154, '')
    (155, '    # Both unnormalized, and normalized uniqueness statistics')
    (156, '    metrics[4] = round(sum([((i+1) * (hist_counts[i]/total_unique_kmers)) for i in range(0, len(hist_counts))]), 4)')
    (157, '    metrics[5] = round(sum([(((i+1)/num_dataset_members) * (hist_counts[i]/total_unique_kmers)) for i in range(0, len(hist_counts))]), 4)')
    (158, '')
    (159, '    # Calculate the fraction used by the delta measure')
    (160, '    metrics[6] = round(total_unique_kmers/k, 4)')
    (161, '    return metrics')
    (162, '')
    (163, '####################################################')
    (164, '# Section 3: Rules needed for this experiment type')
    (165, '####################################################')
    (166, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_2.smk
context_key: ['if data_file.endswith(".fna.gz")']
    (141, 'def get_num_of_dataset_members_exp2(num):')
    (142, '    """ Return the number of genomes in the rest_of_set folder for a specific dataset """')
    (143, '    num_genomes = 0')
    (144, '    for data_file in os.listdir(f"input_type_2/rest_of_set/dataset_{num}/"):')
    (145, '        if data_file.endswith(".fna.gz"):')
    (146, '            num_genomes += 1')
    (147, '    return num_genomes')
    (148, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_2.smk
context_key: ['if data_file.endswith(".fna.gz")']
    (156, 'def get_all_genomes_in_dataset(wildcards):')
    (157, '    """ Returns a list of database in a certain dataset """')
    (158, '    input_files = []')
    (159, '    for data_file in os.listdir(f"input_type_2/rest_of_set/dataset_{wildcards.num}/"):')
    (160, '        if data_file.endswith(".fna.gz"):')
    (161, '            file_name = data_file.split(".fna.gz")[0]')
    (162, '            input_files.append(f"genome_sets_type_2/rest_of_set/k_{wildcards.k}/dataset_{wildcards.num}/{file_name}.transformed.kmc_pre")')
    (163, '    return input_files')
    (164, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_2.smk
context_key: ['if across_group_analysis']
    (189, 'def summarize_histogram_type2(sub_counts, inter_counts, num_genomes_in_dataset, across_group_analysis, k):')
    (190, '    """ ')
    (191, '        Takes in histogram of kmer occurrences, and returns the metrics for the ')
    (192, '        bar charts summarized below:')
    (193, '')
    (194, '            %_1_occ - percentage of unique kmers that only occur in one genome')
    (195, '            %_25_or_less - percentage of unique kmers that occur in multiple genomes, in 25% of the genomes or less')
    (196, '            %_25_to_75 - percentage of unique kmers that occur in multiple genomes, in 25% to 75% of the genomes')
    (197, '            %_75_or_more - percentage of unique kmers that occur in multiple genomes, in 75% or more of the genomes')
    (198, '            unique_stat - weighted sum of kmer occurrents = SUM([occ * %_unique_occ for occ in range(255)]) ')
    (199, '    """')
    (200, '    # Perform a couple of assertions to start ...')
    (201, '    assert inter_counts[0] == 0, "intersection counts should have 0 unique kmers"')
    (202, '    assert sum(sub_counts[1:]) == 0, "all of kmers in sub_counts should be unique"')
    (203, '')
    (204, '    # Start to calculate the metrics ...')
    (205, '    metrics = [0 for i in range(7)]')
    (206, '    total_unique_kmers = sum(sub_counts) + sum(inter_counts)')
    (207, '')
    (208, '    boundaries = [0.25, 0.75]')
    (209, '    boundary_indices = [max(int(percent * num_genomes_in_dataset), 1) for percent in boundaries]')
    (210, '')
    (211, '    # Special cases where indices are customized ...')
    (212, '    if across_group_analysis:')
    (213, '        boundary_indices = [3, 8]')
    (214, '')
    (215, '    metrics[0] = round(sub_counts[0]/total_unique_kmers, 3)')
    (216, '    metrics[1] = round(sum([inter_counts[i] for i in range(1, boundary_indices[0])])/total_unique_kmers, 3)')
    (217, '    metrics[2] = round(sum([inter_counts[i] for i in range(boundary_indices[0], boundary_indices[1])])/total_unique_kmers, 3)')
    (218, '    metrics[3] = round(sum([inter_counts[i] for i in range(boundary_indices[1], len(inter_counts))])/total_unique_kmers, 3)')
    (219, '')
    (220, '    rounding_error = abs(sum(metrics[0:4])-1) ')
    (221, '    assert rounding_error < 0.05, "Issue occurred with histogram summarization"')
    (222, '')
    (223, '    # Both unnormalized, and normalized uniqueness statistic')
    (224, '    metrics[4] = (1 * sub_counts[0]/total_unique_kmers)')
    (225, '    metrics[4] += sum([((i+1) * (inter_counts[i]/total_unique_kmers)) for i in range(1, len(inter_counts))])')
    (226, '    metrics[4] = round(metrics[4], 4)')
    (227, '')
    (228, '    metrics[5] = ((1/num_genomes_in_dataset) * sub_counts[0]/total_unique_kmers)')
    (229, '    metrics[5] += sum([(((i+1)/num_genomes_in_dataset) * (inter_counts[i]/total_unique_kmers)) for i in range(1, len(inter_counts))])')
    (230, '    metrics[5] = round(metrics[5], 4)')
    (231, '')
    (232, '    # Calculate the fraction used by the delta measure')
    (233, '    metrics[6] = round(total_unique_kmers/k, 4)')
    (234, '    return metrics')
    (235, '')
    (236, '')
    (237, '####################################################')
    (238, '# Section 3: Rules needed for this experiment type')
    (239, '####################################################')
    (240, '')
    (241, '#   Section 3.0: Copy over the data from the database folder')
    (242, '#   into the expected structure for the workflow. Build the')
    (243, '#   complex ops files used for KMC.')
    (244, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_7.smk
context_key: ['if data_file.endswith(".fna")']
    (56, 'def get_dataset_non_pivot_genomes_exp7(wildcards):')
    (57, '    """ Returns list of non-pivot genomes for a given dataset """')
    (58, '    input_files = []')
    (59, '    for data_file in os.listdir(f"exp7_non_pivot_data/dataset_{wildcards.num}/"):')
    (60, '        if data_file.endswith(".fna"):')
    (61, '            file_name = data_file.split(".fna")[0]')
    (62, '            input_files.append(f"exp7_non_pivot_data/dataset_{wildcards.num}/{file_name}.fna")')
    (63, '    return input_files')
    (64, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_7.smk
context_key: ['if data_file.endswith(".fna")']
    (65, 'def get_all_non_pivot_genomes_exp7(wildcards):')
    (66, '    """ Returns list of all non-pivot genomes """')
    (67, '    input_files = []')
    (68, '    for i in range(1, num_datasets + 1):')
    (69, '        for data_file in os.listdir(f"exp7_non_pivot_data/dataset_{i}/"):')
    (70, '            if data_file.endswith(".fna"):')
    (71, '                file_name = data_file.split(".fna")[0]')
    (72, '                input_files.append(f"exp7_non_pivot_data/dataset_{i}/{file_name}.fna")')
    (73, '    return input_files')
    (74, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snaktest, file=workflow/Snakefile
context_key: ['if _sample_dict is None']
    (363, 'def get_sample_dict():')
    (364, '    global _sample_dict')
    (365, '    if _sample_dict is None:')
    (366, '        checkpoints.generate_sample_list.get()')
    (367, '        dat_samples = pd.read_csv(file_path_samples)')
    (368, '        _sample_dict = {pathlib.Path(x).stem: x for x in dat_samples[')
    (369, "            \\'mcd_folders\\']}")
    (370, '    return _sample_dict')
    (371, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=moritzschaefer/srna-seq-star-deseq2, file=rules/align.smk
context_key: ['if not is_single_end(**wildcards)']
    (3, 'def get_trimmed(wildcards):')
    (4, '    if not is_single_end(**wildcards):')
    (5, '        # paired-end sample')
    (6, '        return expand("trimmed/{sample}-{unit}.{group}.fastq.gz",')
    (7, '                      group=[1, 2], **wildcards)')
    (8, '    # single end sample')
    (9, '    return "trimmed/{sample}-{unit}.fastq.gz".format(**wildcards)')
    (10, '')
    (11, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if is_activated("tmb")']
    (91, 'def get_tmb_targets():')
    (92, '    if is_activated("tmb"):')
    (93, '        return expand(')
    (94, '            "results/plots/tmb/{group}.{mode}.svg",')
    (95, '            group=samples[(samples.type == "tumor")]["sample"],')
    (96, '            mode=config["tmb"].get("mode", "curve"),')
    (97, '        )')
    (98, '    else:')
    (99, '        return []')
    (100, '')
    (101, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if pd.isna(unit["fq1"])']
    (111, 'def get_cutadapt_input(wildcards):')
    (112, '    unit = units.loc[wildcards.sample].loc[wildcards.unit].loc[wildcards.seqtype]')
    (113, '')
    (114, '    if pd.isna(unit["fq1"]):')
    (115, '        # SRA sample (always paired-end for now)')
    (116, '        accession = unit["sra"]')
    (117, '        return expand("sra/{accession}_{read}.fastq", accession=accession, read=[1, 2])')
    (118, '')
    (119, '    if unit["fq1"].endswith("gz"):')
    (120, '        ending = ".gz"')
    (121, '    else:')
    (122, '        ending = ""')
    (123, '')
    (124, '    if pd.isna(unit["fq2"]):')
    (125, '        # single end local sample')
    (126, '        return "pipe/cutadapt/{S}/{T}/{U}.fq1.fastq{E}".format(')
    (127, '            S=unit.sample, U=unit.unit, T=unit.sequencing_type, E=ending')
    (128, '        )')
    (129, '    else:')
    (130, '        # paired end local sample')
    (131, '        return expand(')
    (132, '            "pipe/cutadapt/{S}/{T}/{U}.{{read}}.fastq{E}".format(')
    (133, '                S=unit.sample, U=unit.unit, T=unit.sequencing_type, E=ending')
    (134, '            ),')
    (135, '            read=["fq1", "fq2"],')
    (136, '        )')
    (137, '')
    (138, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if is_paired_end(wildcards.sample, "DNA")']
    (185, 'def get_map_reads_input(wildcards):')
    (186, '    if is_paired_end(wildcards.sample, "DNA"):')
    (187, '        return [')
    (188, '            "results/merged/DNA/{sample}_R1.fastq.gz",')
    (189, '            "results/merged/DNA/{sample}_R2.fastq.gz",')
    (190, '        ]')
    (191, '    return "results/merged/DNA/{sample}_single.fastq.gz"')
    (192, '')
    (193, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if is_activated("remove_duplicates")']
    (201, 'def get_recalibrate_quality_input(wildcards, bai=False):')
    (202, '    ext = ".bai" if bai else ""')
    (203, '    if is_activated("remove_duplicates"):')
    (204, '        return "results/dedup/{}.sorted.bam{}".format(wildcards.sample, ext)')
    (205, '    else:')
    (206, '        return "results/mapped/{}.sorted.bam{}".format(wildcards.sample, ext)')
    (207, '')
    (208, '')
    (209, '## HLA Typing ##')
    (210, '')
    (211, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if is_activated("HLAtyping/optitype_prefiltering")', 'if is_paired_end(wildcards.sample, "DNA")']
    (212, 'def get_optitype_reads_input(wildcards):')
    (213, '    if is_activated("HLAtyping/optitype_prefiltering"):')
    (214, '        if is_paired_end(wildcards.sample, "DNA"):')
    (215, '            return expand(')
    (216, '                "results/razers3/fastq/{sample}_{fq}.fished.fastq",')
    (217, '                sample=wildcards.sample,')
    (218, '                fq=["R1", "R2"],')
    (219, '            )')
    (220, '        return "results/razers3/fastq/{sample}_single.fastq"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if is_activated("HLAtyping/optitype_prefiltering")', 'else']
    (212, 'def get_optitype_reads_input(wildcards):')
    (213, '    if is_activated("HLAtyping/optitype_prefiltering"):')
    (214, '        if is_paired_end(wildcards.sample, "DNA"):')
    (215, '            return expand(')
    (216, '                "results/razers3/fastq/{sample}_{fq}.fished.fastq",')
    (217, '                sample=wildcards.sample,')
    (218, '                fq=["R1", "R2"],')
    (219, '            )')
    (220, '        return "results/razers3/fastq/{sample}_single.fastq"')
    (221, '    else:')
    (222, '        return get_map_reads_input(wildcards)')
    (223, '')
    (224, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if wildcards.batch == "all"']
    (225, 'def get_oncoprint_batch(wildcards):')
    (226, '    if wildcards.batch == "all":')
    (227, '        groups = samples[samples["type"] == "tumor"]["sample"].unique()')
    (228, '    else:')
    (229, '        groups = samples.loc[')
    (230, '            samples[config["oncoprint"]["stratify"]["by-column"]] == wildcards.batch,')
    (231, '            "group",')
    (232, '        ].unique()')
    (233, '    return expand(')
    (234, '        "results/merged-calls/{group}.{{event}}.fdr-controlled.bcf", group=groups')
    (235, '    )')
    (236, '')
    (237, '')
    (238, '## variant calls ##')
    (239, '')
    (240, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if index']
    (268, 'def get_pair_variants(wildcards, index):')
    (269, '    if index:')
    (270, '        ext = ".csi"')
    (271, '    else:')
    (272, '        ext = ""')
    (273, '    variants = [')
    (274, '        "results/strelka/somatic/{}/results/variants/somatic.complete.tumor.bcf{}".format(')
    (275, '            wildcards.sample, ext')
    (276, '        )')
    (277, '    ]')
    (278, '    variants.append(')
    (279, '        "results/strelka/germline/{}/results/variants/variants.reheader.bcf{}".format(')
    (280, '            get_normal(wildcards), ext')
    (281, '        )')
    (282, '    )')
    (283, '    return variants')
    (284, '')
    (285, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if wildcards.format == "vcf"']
    (315, 'def get_tabix_params(wildcards):')
    (316, '    if wildcards.format == "vcf":')
    (317, '        return "-p vcf"')
    (318, '    if wildcards.format == "txt":')
    (319, '        return "-s 1 -b 2 -e 2"')
    (320, '    raise ValueError("Invalid format for tabix: {}".format(wildcards.format))')
    (321, '')
    (322, '')
    (323, '## RNA ##')
    (324, '')
    (325, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if is_paired_end(wildcards.sample, "RNA")']
    (326, 'def get_quant_reads_input(wildcards):')
    (327, '    if is_paired_end(wildcards.sample, "RNA"):')
    (328, '        return [')
    (329, '            "results/merged/RNA/{sample}_R1.fastq.gz",')
    (330, '            "results/merged/RNA/{sample}_R2.fastq.gz",')
    (331, '        ]')
    (332, '    return "results/merged/RNA/{sample}_single.fastq.gz"')
    (333, '')
    (334, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if len(input.fastq) == 1']
    (335, 'def kallisto_params(wildcards, input):')
    (336, '    extra = config["params"]["kallisto"]')
    (337, '    if len(input.fastq) == 1:')
    (338, '        extra += " --single"')
    (339, '        extra += (')
    (340, '            " --fragment-length {unit.fragment_len_mean} " "--sd {unit.fragment_len_sd}"')
    (341, '        ).format(unit=units.loc[(wildcards.sample, wildcards.unit)])')
    (342, '    else:')
    (343, '        extra += " --fusion"')
    (344, '    return extra')
    (345, '')
    (346, '')
    (347, '## helper functions ##')
    (348, '')
    (349, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if wildcards.group == "wt"']
    (389, 'def get_alleles_MHCI(wildcards):')
    (390, '    if wildcards.group == "wt":')
    (391, '        return "results/optitype/{S}/hla_alleles_{S}.tsv".format(')
    (392, '            S=get_normal(wildcards)')
    (393, '        )')
    (394, '    else:')
    (395, '        return "results/optitype/{S}/hla_alleles_{S}.tsv".format(S=wildcards.sample)')
    (396, '')
    (397, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if wildcards.group == "wt"']
    (398, 'def get_alleles_MHCII(wildcards):')
    (399, '    if wildcards.group == "wt":')
    (400, '        return "results/HLA-LA/hlaI_{S}.tsv".format(S=get_normal(wildcards))')
    (401, '    else:')
    (402, '        return "results/HLA-LA/hlaI_{S}.tsv".format(S=wildcards.sample)')
    (403, '')
    (404, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SaGeTOPK/sage-topk-experiments, file=Snakefile
context_key: ['if not os.path.isdir(path)']
    (6, 'def list_files(path):')
    (7, '    if not os.path.isdir(path):')
    (8, '        return glob.glob(path)')
    (9, '    files = list()')
    (10, '    for filename in os.listdir(path):')
    (11, '        if filename.endswith(".sparql"):')
    (12, '            files.append(f"{path}/{filename}")')
    (13, '    return files')
    (14, '')
    (15, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SaGeTOPK/sage-topk-experiments, file=Snakefile
context_key: ['if not config["experiments"][xp]["check"]']
    (43, 'def check_files(wcs):')
    (44, '    files = []')
    (45, '    output = "output" if "output" not in config else config["output"]')
    (46, '    for xp in config["experiments"]:')
    (47, '        if not config["experiments"][xp]["check"]:')
    (48, '            continue')
    (49, '        for workload in config["experiments"][xp]["workloads"]:')
    (50, '            for approach in config["experiments"][xp]["approaches"]:')
    (51, '                for filename, query in load_queries(f"workloads/{workload}"):')
    (52, '                    for limit in config["experiments"][xp]["limits"]:')
    (53, '                        for quota in config["experiments"][xp]["quotas"]:')
    (54, '                            files.append((')
    (55, '                                f"{output}/tmp/{xp}/{workload}/"')
    (56, '                                f"{approach}-{quota}ms/{limit}/"')
    (57, '                                f"check/{filename}.csv"))')
    (58, '    return files')
    (59, '')
    (60, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SaGeTOPK/sage-topk-experiments, file=Snakefile
context_key: ['if "autostart" in config and config["autostart"]']
    (61, 'def xp_files(wcs):')
    (62, '    name = config["name"]')
    (63, '    output = config["output"]')
    (64, '    for xp in config["experiments"]:')
    (65, '        if config["experiments"][xp]["check"]:')
    (66, '            return [f"{output}/{name}/run.csv", f"{output}/{name}/check.csv"]')
    (67, '    return [f"{output}/{name}/run.csv"]')
    (68, '')
    (69, '')
    (70, 'if "autostart" in config and config["autostart"]:')
    (71, '    onsuccess: shell("bash scripts/server.sh stop all")')
    (72, '    onerror: shell("bash scripts/server.sh stop all")')
    (73, '    onstart: shell("bash scripts/server.sh start all")')
    (74, '')
    (75, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lparsons/kas-seq-workflow, file=rules/common.smk
context_key: ['if is_single_end(wildcards.sample, wildcards.unit)']
    (4, 'def get_fastq(wildcards):')
    (5, '    u = units.loc[(wildcards.sample, wildcards.unit), ["fq1", "fq2"]].dropna()')
    (6, '    if is_single_end(wildcards.sample, wildcards.unit):')
    (7, '        return {"fq1": f"{u.fq1}"}')
    (8, '    else:')
    (9, '        return {"fq1": f"{u.fq1}", "fq2": f"{u.fq2}"}')
    (10, '')
    (11, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lparsons/kas-seq-workflow, file=rules/common.smk
context_key: ['if is_single_end(unit.sample, unit.unit)']
    (44, 'def get_fastqc(wildcards):')
    (45, '    fastqc_files = list()')
    (46, '    for unit in units.itertuples():')
    (47, '        if is_single_end(unit.sample, unit.unit):')
    (48, '            fastqc_files.append(')
    (49, '                "qc/fastqc/{sample}-{unit}_fastqc.zip".format(')
    (50, '                    sample=unit.sample, unit=unit.unit')
    (51, '                )')
    (52, '            )')
    (53, '        else:')
    (54, '            fastqc_files.extend(')
    (55, '                [')
    (56, '                    "qc/fastqc/{sample}-{unit}.1_fastqc.zip".format(')
    (57, '                        sample=unit.sample, unit=unit.unit')
    (58, '                    ),')
    (59, '                    "qc/fastqc/{sample}-{unit}.2_fastqc.zip".format(')
    (60, '                        sample=unit.sample, unit=unit.unit')
    (61, '                    ),')
    (62, '                ]')
    (63, '            )')
    (64, '    return fastqc_files')
    (65, '')
    (66, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=imgag/ont_tools, file=workflow/rules/helper_scripts.smk
context_key: ['if map_samples_barcode', "if config[\\'use_failed_reads\\']"]
    (2, 'def get_input_folders(wc):')
    (3, '    """')
    (4, '    Return the FASTQ data folder for a sample.')
    (5, '        Allows multiple runs per sample (restarted, repeated)')
    (6, '        Demultiplexed samples on a single flowcell with barcode information')
    (7, '        Includes folder with failed reads when specified in config')
    (8, '')
    (9, '        New option: Ifq folder contains a subfolder called "fastq_rebasecalled", ')
    (10, '        reads will be taken only from this folder. Option can be turned off in config option')
    (11, "        \\'fastq_prefer_rebasecalled\\' ")
    (12, '    """')
    (13, '    folders = map_samples_folder[wc.sample].copy()')
    (14, '    if config[\\\'verbose\\\']: print("Input Folders:" + str(folders), end = \\\'\\\')')
    (15, '    ')
    (16, '    if map_samples_barcode:')
    (17, "        [folders.append(\\'/fastq_pass/\\'.join(x)) for x in map_samples_barcode[wc.sample]]")
    (18, "        if config[\\'use_failed_reads\\']:")
    (19, "            folders.append([\\'/fastq_fail/\\'.join(x) for x in map_samples_barcode[wc.sample]])")
    (20, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=imgag/ont_tools, file=workflow/rules/helper_scripts.smk
context_key: ['if map_samples_barcode', "if config[\\'fastq_prefer_rebasecalled\\']", 'if f_rebasecalled']
    (2, 'def get_input_folders(wc):')
    (3, '    """')
    (4, '    Return the FASTQ data folder for a sample.')
    (5, '        Allows multiple runs per sample (restarted, repeated)')
    (6, '        Demultiplexed samples on a single flowcell with barcode information')
    (7, '        Includes folder with failed reads when specified in config')
    (8, '')
    (9, '        New option: Ifq folder contains a subfolder called "fastq_rebasecalled", ')
    (10, '        reads will be taken only from this folder. Option can be turned off in config option')
    (11, "        \\'fastq_prefer_rebasecalled\\' ")
    (12, '    """')
    (13, '    folders = map_samples_folder[wc.sample].copy()')
    (14, '    if config[\\\'verbose\\\']: print("Input Folders:" + str(folders), end = \\\'\\\')')
    (15, '    ')
    (16, '    if map_samples_barcode:')
    (17, "        [folders.append(\\'/fastq_pass/\\'.join(x)) for x in map_samples_barcode[wc.sample]]")
    (18, "        if config[\\'use_failed_reads\\']:")
    (19, "            folders.append([\\'/fastq_fail/\\'.join(x) for x in map_samples_barcode[wc.sample]])")
    (20, '    ')
    (21, '    folders_updated = list()')
    (22, '')
    (23, "    if config[\\'fastq_prefer_rebasecalled\\']:")
    (24, '        for f in folders:')
    (25, '            f_rebasecalled =  glob(f+"/**/fastq_rebasecalled", recursive = True)')
    (26, '            if f_rebasecalled:')
    (27, '                [folders_updated.append(f) for f in f_rebasecalled]')
    (28, '            else:')
    (29, '                folders_updated.append(f)')
    (30, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=imgag/ont_tools, file=workflow/rules/helper_scripts.smk
context_key: ['if map_samples_barcode', "if config[\\'verbose\\']"]
    (2, 'def get_input_folders(wc):')
    (3, '    """')
    (4, '    Return the FASTQ data folder for a sample.')
    (5, '        Allows multiple runs per sample (restarted, repeated)')
    (6, '        Demultiplexed samples on a single flowcell with barcode information')
    (7, '        Includes folder with failed reads when specified in config')
    (8, '')
    (9, '        New option: Ifq folder contains a subfolder called "fastq_rebasecalled", ')
    (10, '        reads will be taken only from this folder. Option can be turned off in config option')
    (11, "        \\'fastq_prefer_rebasecalled\\' ")
    (12, '    """')
    (13, '    folders = map_samples_folder[wc.sample].copy()')
    (14, '    if config[\\\'verbose\\\']: print("Input Folders:" + str(folders), end = \\\'\\\')')
    (15, '    ')
    (16, '    if map_samples_barcode:')
    (17, "        [folders.append(\\'/fastq_pass/\\'.join(x)) for x in map_samples_barcode[wc.sample]]")
    (18, "        if config[\\'use_failed_reads\\']:")
    (19, "            folders.append([\\'/fastq_fail/\\'.join(x) for x in map_samples_barcode[wc.sample]])")
    (20, '    ')
    (21, '    folders_updated = list()')
    (22, '')
    (23, "    if config[\\'fastq_prefer_rebasecalled\\']:")
    (24, '        for f in folders:')
    (25, '            f_rebasecalled =  glob(f+"/**/fastq_rebasecalled", recursive = True)')
    (26, '            if f_rebasecalled:')
    (27, '                [folders_updated.append(f) for f in f_rebasecalled]')
    (28, '            else:')
    (29, '                folders_updated.append(f)')
    (30, '    ')
    (31, '    if config[\\\'verbose\\\']: print(" | Updated to:" + str(folders_updated))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=imgag/ont_tools, file=workflow/rules/helper_scripts.smk
context_key: ['if map_samples_barcode']
    (2, 'def get_input_folders(wc):')
    (3, '    """')
    (4, '    Return the FASTQ data folder for a sample.')
    (5, '        Allows multiple runs per sample (restarted, repeated)')
    (6, '        Demultiplexed samples on a single flowcell with barcode information')
    (7, '        Includes folder with failed reads when specified in config')
    (8, '')
    (9, '        New option: Ifq folder contains a subfolder called "fastq_rebasecalled", ')
    (10, '        reads will be taken only from this folder. Option can be turned off in config option')
    (11, "        \\'fastq_prefer_rebasecalled\\' ")
    (12, '    """')
    (13, '    folders = map_samples_folder[wc.sample].copy()')
    (14, '    if config[\\\'verbose\\\']: print("Input Folders:" + str(folders), end = \\\'\\\')')
    (15, '    ')
    (16, '    if map_samples_barcode:')
    (17, "        [folders.append(\\'/fastq_pass/\\'.join(x)) for x in map_samples_barcode[wc.sample]]")
    (18, "        if config[\\'use_failed_reads\\']:")
    (19, "            folders.append([\\'/fastq_fail/\\'.join(x) for x in map_samples_barcode[wc.sample]])")
    (20, '    ')
    (21, '    folders_updated = list()')
    (22, '')
    (23, "    if config[\\'fastq_prefer_rebasecalled\\']:")
    (24, '        for f in folders:')
    (25, '            f_rebasecalled =  glob(f+"/**/fastq_rebasecalled", recursive = True)')
    (26, '            if f_rebasecalled:')
    (27, '                [folders_updated.append(f) for f in f_rebasecalled]')
    (28, '            else:')
    (29, '                folders_updated.append(f)')
    (30, '    ')
    (31, '    if config[\\\'verbose\\\']: print(" | Updated to:" + str(folders_updated))')
    (32, "    return{\\'folders\\': folders_updated}")
    (33, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=imgag/ont_tools, file=workflow/rules/helper_scripts.smk
context_key: ['if map_samples_barcode']
    (66, 'def get_summary_files(wc):')
    (67, '    """')
    (68, '    Get all summary files that belong to a single sample.')
    (69, '    Requests summaries split by barcodes if necessary')
    (70, '    """')
    (71, '    folders = map_samples_folder[wc.sample].copy()')
    (72, '    files = [s for t in [glob(x+"/**/sequencing_summary*", recursive = True) for x in folders] for s in t]')
    (73, '')
    (74, '    if map_samples_barcode:')
    (75, "        folders_barcode = [\\'Sample_\\' + wc.sample for x in map_samples_barcode[wc.sample]]")
    (76, '        files += [x+"/sequencing_summary_bc_"+ wc.sample+".txt" for x in folders_barcode]')
    (77, '')
    (78, "    if config[\\'fastq_prefer_rebasecalled\\']:")
    (79, '        folders_rebasecalled = [s for t in [glob(x+"/**/fastq_rebasecalled", recursive = True) for x in folders] for s in t]')
    (80, '        if folders_rebasecalled:')
    (81, '            folders = folders_rebasecalled')
    (82, '            files = [s for t in [glob(x+"/**/sequencing_summary*", recursive = True) for x in folders_rebasecalled] for s in t]')
    (83, '    ')
    (84, "    return{\\'summary_files\\': files}")
    (85, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=imgag/ont_tools, file=workflow/rules/helper_scripts.smk
context_key: ['if not md']
    (86, 'def get_db_report(wc):')
    (87, '    """')
    (88, '    Get Report .pdf and Report.md files')
    (89, '    Replace with empty dummy files if not available')
    (90, '    """')
    (91, '')
    (92, '    reports = [x for y in [glob(r + "/**/report_*") for r in map_runs_folder[wc.run]] for x in y]')
    (93, "    md = [x for x in reports if x.endswith(\\'.md\\')]")
    (94, '    reports = [x for x in reports if x not in md]')
    (95, '')
    (96, '    #print(md)')
    (97, '    #print(reports)')
    (98, '')
    (99, '    if not md:')
    (100, '        if config[\\\'verbose\\\']: print("Warning: No markdown report found for run(s) " + wc.run)')
    (101, '        md = str(os.path.join(workflow.basedir, "../resources/dummyfiles/report.md"))')
    (102, '    ')
    (103, "    return{\\'md\\': md, \\'reports\\': reports}")
    (104, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=imgag/ont_tools, file=workflow/rules/helper_scripts.smk
context_key: ['if not mux']
    (105, 'def get_db_mux(wc):')
    (106, '    """')
    (107, '    Get mux stats file')
    (108, '    Replace with empty dummy files if not available')
    (109, '    """')
    (110, '')
    (111, '    mux = [x for y in [glob(r + "/**/mux_scan_data*.csv") for r in map_runs_folder[wc.run]] for x in y]')
    (112, '    mux += [x for y in [glob(r + "/**/other_reports/pore_scan_data*.csv") for r in map_runs_folder[wc.run]] for x in y]')
    (113, '    ')
    (114, '    if not mux:')
    (115, '        if config[\\\'verbose\\\']: print("Warning: No mux stats (.csv) found for run(s) " + wc.run)')
    (116, '        mux = str(os.path.join(workflow.basedir, "../resources/dummyfiles/mux.csv"))')
    (117, '    ')
    (118, '    return(mux)')
    (119, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=imgag/ont_tools, file=workflow/rules/helper_scripts.smk
context_key: ['if not bc']
    (120, 'def get_db_barcode(wc):')
    (121, '    """')
    (122, '    Get barcode tsv file')
    (123, '    Replace with empty dummy files if not available')
    (124, '    """')
    (125, '')
    (126, '    bc = [x for y in [glob(r + "/**/barcode_alignment*.tsv") for r in map_runs_folder[wc.run]] for x in y]')
    (127, '')
    (128, '    if not bc:')
    (129, '        if config[\\\'verbose\\\']: print("Warning: No barcode file (.csv) found for run(s) " + wc.run)')
    (130, '        bc = str(os.path.join(workflow.basedir, "../resources/dummyfiles/barcodes.tsv"))')
    (131, '    ')
    (132, '    return(bc)')
    (133, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=imgag/ont_tools, file=workflow/rules/helper_scripts.smk
context_key: ['if ID_barcode_folders']
    (134, 'def aggregate_sample_pycoqc(wc):')
    (135, '    """')
    (136, '    Function that validates the checkpoint and checks for generated sample_pycoqcs')
    (137, '    that can be used in the multiqc report')
    (138, '    """')
    (139, '    barcode_qcs = []')
    (140, '    if ID_barcode_folders:')
    (141, '        barcode_qcs += [checkpoints.split_summary_perbarcode.get(folder=x).output[0] for x in ID_samples]')
    (142, '        return(barcode_qcs)')
    (143, '    else:')
    (144, '        return(barcode_qcs)')
    (145, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=imgag/ont_tools, file=workflow/rules/helper_scripts.smk
context_key: ['if os.path.isfile(f)']
    (167, 'def load_project_config(f):')
    (168, '    """Load additional project config files and overwrite default config"""')
    (169, '    if os.path.isfile(f):')
    (170, '        configfile: f')
    (171, '    else:')
    (172, '        print("Project config not available")')
    (173, '        pass')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tgstoecker/MuWU, file=workflow/rules/ref_utils.smk
context_key: ['if annotation.endswith("{}".format(ext))']
    (21, 'def is_valid_annotation_file(annotation):')
    (22, '    for ext in extensions:')
    (23, '        if annotation.endswith("{}".format(ext)):')
    (24, '            return True')
    (25, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tgstoecker/MuWU, file=workflow/rules/ref_utils.smk
context_key: ['if annotation.endswith("{}".format(ext))']
    (26, 'def get_file_ext(annotation):')
    (27, '    for ext in extensions:')
    (28, '        if annotation.endswith("{}".format(ext)):')
    (29, '            return ext')
    (30, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tgstoecker/MuWU, file=workflow/rules/ref_utils.smk
context_key: ['if file.endswith("{}".format(ext))']
    (31, 'def is_gbff(file):')
    (32, '    for ext in genbank:')
    (33, '        if file.endswith("{}".format(ext)):')
    (34, '            return True')
    (35, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tgstoecker/MuWU, file=workflow/rules/ref_utils.smk
context_key: ['if not is_gbff(annotation)', 'if is_url(fasta)', 'if is_gzipped(fasta)']
    (41, 'def handle_fasta(fasta, annotation):')
    (42, '    if not is_gbff(annotation):')
    (43, '        if is_url(fasta):')
    (44, '            print("Downloading fasta")')
    (45, '            if is_gzipped(fasta):')
    (46, '                fasta = requests.get(fasta)')
    (47, "                with open(\\'resources/genome.fa.gz\\', \\'wb\\') as outfile:")
    (48, '                    outfile.write(fasta.content)')
    (49, "#                urlretrieve(fasta,\\'resources/genome.fa.gz\\')")
    (50, '                print("Unpacking fasta")')
    (51, "                with gzip.open(\\'resources/genome.fa.gz\\', \\'rb\\') as f_in:")
    (52, "                    with open(\\'resources/genome.fa\\', \\'wb\\') as f_out:")
    (53, '                        shutil.copyfileobj(f_in, f_out)')
    (54, '            else:')
    (55, "#                urlretrieve(fasta,\\'resources/genome.fa\\')")
    (56, '                fasta = requests.get(fasta)')
    (57, "                with open(\\'resources/genome.fa\\', \\'wb\\') as outfile:")
    (58, '                    outfile.write(fasta.content) ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tgstoecker/MuWU, file=workflow/rules/ref_utils.smk
context_key: ['if not is_gbff(annotation)', 'if is_url(fasta)', 'elif not is_url(fasta)', 'if is_gzipped(fasta)']
    (41, 'def handle_fasta(fasta, annotation):')
    (42, '    if not is_gbff(annotation):')
    (43, '        if is_url(fasta):')
    (44, '            print("Downloading fasta")')
    (45, '            if is_gzipped(fasta):')
    (46, '                fasta = requests.get(fasta)')
    (47, "                with open(\\'resources/genome.fa.gz\\', \\'wb\\') as outfile:")
    (48, '                    outfile.write(fasta.content)')
    (49, "#                urlretrieve(fasta,\\'resources/genome.fa.gz\\')")
    (50, '                print("Unpacking fasta")')
    (51, "                with gzip.open(\\'resources/genome.fa.gz\\', \\'rb\\') as f_in:")
    (52, "                    with open(\\'resources/genome.fa\\', \\'wb\\') as f_out:")
    (53, '                        shutil.copyfileobj(f_in, f_out)')
    (54, '            else:')
    (55, "#                urlretrieve(fasta,\\'resources/genome.fa\\')")
    (56, '                fasta = requests.get(fasta)')
    (57, "                with open(\\'resources/genome.fa\\', \\'wb\\') as outfile:")
    (58, '                    outfile.write(fasta.content) ')
    (59, '        elif not is_url(fasta):')
    (60, '            print("Gunzipping or linking fasta file already on file system to resources/ dir")')
    (61, '            if is_gzipped(fasta):')
    (62, "                with gzip.open(fasta, \\'rb\\') as f_in:")
    (63, "                    with open(\\'resources/genome.fa\\', \\'wb\\') as f_out:")
    (64, '                        shutil.copyfileobj(f_in, f_out)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tgstoecker/MuWU, file=workflow/rules/ref_utils.smk
context_key: ['if not is_gbff(annotation)', 'if is_url(fasta)', 'elif not is_url(fasta)', 'else']
    (41, 'def handle_fasta(fasta, annotation):')
    (42, '    if not is_gbff(annotation):')
    (43, '        if is_url(fasta):')
    (44, '            print("Downloading fasta")')
    (45, '            if is_gzipped(fasta):')
    (46, '                fasta = requests.get(fasta)')
    (47, "                with open(\\'resources/genome.fa.gz\\', \\'wb\\') as outfile:")
    (48, '                    outfile.write(fasta.content)')
    (49, "#                urlretrieve(fasta,\\'resources/genome.fa.gz\\')")
    (50, '                print("Unpacking fasta")')
    (51, "                with gzip.open(\\'resources/genome.fa.gz\\', \\'rb\\') as f_in:")
    (52, "                    with open(\\'resources/genome.fa\\', \\'wb\\') as f_out:")
    (53, '                        shutil.copyfileobj(f_in, f_out)')
    (54, '            else:')
    (55, "#                urlretrieve(fasta,\\'resources/genome.fa\\')")
    (56, '                fasta = requests.get(fasta)')
    (57, "                with open(\\'resources/genome.fa\\', \\'wb\\') as outfile:")
    (58, '                    outfile.write(fasta.content) ')
    (59, '        elif not is_url(fasta):')
    (60, '            print("Gunzipping or linking fasta file already on file system to resources/ dir")')
    (61, '            if is_gzipped(fasta):')
    (62, "                with gzip.open(fasta, \\'rb\\') as f_in:")
    (63, "                    with open(\\'resources/genome.fa\\', \\'wb\\') as f_out:")
    (64, '                        shutil.copyfileobj(f_in, f_out)')
    (65, '            else:')
    (66, '                os.symlink(fasta, "resources/genome.fa")')
    (67, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tgstoecker/MuWU, file=workflow/rules/ref_utils.smk
context_key: ['if is_valid_annotation_file(annotation) and not is_gzipped(annotation)', 'if is_url(annotation)']
    (74, 'def handle_annotation(annotation):')
    (75, '#    urlcleanup()')
    (76, '    if is_valid_annotation_file(annotation) and not is_gzipped(annotation):')
    (77, '        if is_url(annotation):')
    (78, '            print("Downloading annotation")')
    (79, '            annotation = requests.get(annotation)')
    (80, "            with open(\\'resources/annotation\\', \\'wb\\') as outfile:")
    (81, '                outfile.write(annotation.content)')
    (82, "#            urlretrieve(annotation, \\'resources/annotation\\')")
    (83, '        elif not is_url(annotation):')
    (84, '            print("Linking annotation")')
    (85, "            os.symlink(annotation, \\'resources/annotation\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tgstoecker/MuWU, file=workflow/rules/ref_utils.smk
context_key: ['if is_valid_annotation_file(annotation) and not is_gzipped(annotation)', 'if is_valid_annotation_file(annotation) and is_gzipped(annotation)', 'if is_url(annotation)']
    (74, 'def handle_annotation(annotation):')
    (75, '#    urlcleanup()')
    (76, '    if is_valid_annotation_file(annotation) and not is_gzipped(annotation):')
    (77, '        if is_url(annotation):')
    (78, '            print("Downloading annotation")')
    (79, '            annotation = requests.get(annotation)')
    (80, "            with open(\\'resources/annotation\\', \\'wb\\') as outfile:")
    (81, '                outfile.write(annotation.content)')
    (82, "#            urlretrieve(annotation, \\'resources/annotation\\')")
    (83, '        elif not is_url(annotation):')
    (84, '            print("Linking annotation")')
    (85, "            os.symlink(annotation, \\'resources/annotation\\')")
    (86, '    if is_valid_annotation_file(annotation) and is_gzipped(annotation):')
    (87, '        if is_url(annotation):')
    (88, '            print("Downloading annotation")')
    (89, '            annotation = requests.get(annotation)')
    (90, "            with open(\\'resources/annotation.gz\\', \\'wb\\') as outfile:")
    (91, '                outfile.write(annotation.content)')
    (92, "#            urlretrieve(annotation, \\'resources/annotation.gz\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tgstoecker/MuWU, file=workflow/rules/ref_utils.smk
context_key: ['if is_valid_annotation_file(annotation) and not is_gzipped(annotation)', 'if is_valid_annotation_file(annotation) and is_gzipped(annotation)', 'elif not is_url(annotation)']
    (74, 'def handle_annotation(annotation):')
    (75, '#    urlcleanup()')
    (76, '    if is_valid_annotation_file(annotation) and not is_gzipped(annotation):')
    (77, '        if is_url(annotation):')
    (78, '            print("Downloading annotation")')
    (79, '            annotation = requests.get(annotation)')
    (80, "            with open(\\'resources/annotation\\', \\'wb\\') as outfile:")
    (81, '                outfile.write(annotation.content)')
    (82, "#            urlretrieve(annotation, \\'resources/annotation\\')")
    (83, '        elif not is_url(annotation):')
    (84, '            print("Linking annotation")')
    (85, "            os.symlink(annotation, \\'resources/annotation\\')")
    (86, '    if is_valid_annotation_file(annotation) and is_gzipped(annotation):')
    (87, '        if is_url(annotation):')
    (88, '            print("Downloading annotation")')
    (89, '            annotation = requests.get(annotation)')
    (90, "            with open(\\'resources/annotation.gz\\', \\'wb\\') as outfile:")
    (91, '                outfile.write(annotation.content)')
    (92, "#            urlretrieve(annotation, \\'resources/annotation.gz\\')")
    (93, '        elif not is_url(annotation):')
    (94, '            print("Linking annotation")')
    (95, '            os.symlink(annotation, "resources/annotation.gz")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tgstoecker/MuWU, file=workflow/rules/ref_utils.smk
context_key: ['if is_valid_annotation_file(annotation) and not is_gzipped(annotation)', 'if is_valid_annotation_file(annotation) and is_gzipped(annotation)']
    (74, 'def handle_annotation(annotation):')
    (75, '#    urlcleanup()')
    (76, '    if is_valid_annotation_file(annotation) and not is_gzipped(annotation):')
    (77, '        if is_url(annotation):')
    (78, '            print("Downloading annotation")')
    (79, '            annotation = requests.get(annotation)')
    (80, "            with open(\\'resources/annotation\\', \\'wb\\') as outfile:")
    (81, '                outfile.write(annotation.content)')
    (82, "#            urlretrieve(annotation, \\'resources/annotation\\')")
    (83, '        elif not is_url(annotation):')
    (84, '            print("Linking annotation")')
    (85, "            os.symlink(annotation, \\'resources/annotation\\')")
    (86, '    if is_valid_annotation_file(annotation) and is_gzipped(annotation):')
    (87, '        if is_url(annotation):')
    (88, '            print("Downloading annotation")')
    (89, '            annotation = requests.get(annotation)')
    (90, "            with open(\\'resources/annotation.gz\\', \\'wb\\') as outfile:")
    (91, '                outfile.write(annotation.content)')
    (92, "#            urlretrieve(annotation, \\'resources/annotation.gz\\')")
    (93, '        elif not is_url(annotation):')
    (94, '            print("Linking annotation")')
    (95, '            os.symlink(annotation, "resources/annotation.gz")')
    (96, '        print("Unpacking annotation")')
    (97, "        gunzip_annotation(\\'resources/annotation.gz\\')")
    (98, '        print("Done!")')
    (99, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tgstoecker/MuWU, file=workflow/rules/ref_utils.smk
context_key: ['if is_valid_annotation_file(annotation)', 'if is_gbff(annotation)', 'if is_url(assembly_report)']
    (100, 'def fasta_annotation_handling(fasta, annotation, assembly_report):')
    (101, '    # check if annotation is valid file - based on file extensions')
    (102, '    if is_valid_annotation_file(annotation):')
    (103, '        print("Annotation file type is supported - continuing:")')
    (104, '        # dealing with the fasta file')
    (105, '        # download/link fasta to resources/ dir')
    (106, '        # we only download/link/use fasta if annotation is NOT in GenBank format')
    (107, '        handle_fasta(fasta, annotation)')
    (108, '        # dealing with the annotation file')
    (109, '        handle_annotation(annotation)')
    (110, '        if is_gbff(annotation):')
    (111, '            print("Supplied annotation in GenBank format - also getting assembly report.")')
    (112, '            if is_url(assembly_report):')
    (113, "#                urlretrieve(assembly_report, \\'resources/assembly_report.txt\\')")
    (114, '                assembly_report = requests.get(assembly_report)')
    (115, "                with open(\\'resources/assembly_report.txt\\', \\'wb\\') as outfile:")
    (116, '                    outfile.write(assembly_report.content)')
    (117, '            else:')
    (118, '                os.symlink(assembly_report, "resources/assembly_report.txt")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tgstoecker/MuWU, file=workflow/rules/ref_utils.smk
context_key: ['if is_valid_annotation_file(annotation)', 'if is_gbff(annotation)']
    (100, 'def fasta_annotation_handling(fasta, annotation, assembly_report):')
    (101, '    # check if annotation is valid file - based on file extensions')
    (102, '    if is_valid_annotation_file(annotation):')
    (103, '        print("Annotation file type is supported - continuing:")')
    (104, '        # dealing with the fasta file')
    (105, '        # download/link fasta to resources/ dir')
    (106, '        # we only download/link/use fasta if annotation is NOT in GenBank format')
    (107, '        handle_fasta(fasta, annotation)')
    (108, '        # dealing with the annotation file')
    (109, '        handle_annotation(annotation)')
    (110, '        if is_gbff(annotation):')
    (111, '            print("Supplied annotation in GenBank format - also getting assembly report.")')
    (112, '            if is_url(assembly_report):')
    (113, "#                urlretrieve(assembly_report, \\'resources/assembly_report.txt\\')")
    (114, '                assembly_report = requests.get(assembly_report)')
    (115, "                with open(\\'resources/assembly_report.txt\\', \\'wb\\') as outfile:")
    (116, '                    outfile.write(assembly_report.content)')
    (117, '            else:')
    (118, '                os.symlink(assembly_report, "resources/assembly_report.txt")')
    (119, '        print("All done!")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tgstoecker/MuWU, file=workflow/rules/ref_utils.smk
context_key: ['if is_valid_annotation_file(annotation)', 'else']
    (100, 'def fasta_annotation_handling(fasta, annotation, assembly_report):')
    (101, '    # check if annotation is valid file - based on file extensions')
    (102, '    if is_valid_annotation_file(annotation):')
    (103, '        print("Annotation file type is supported - continuing:")')
    (104, '        # dealing with the fasta file')
    (105, '        # download/link fasta to resources/ dir')
    (106, '        # we only download/link/use fasta if annotation is NOT in GenBank format')
    (107, '        handle_fasta(fasta, annotation)')
    (108, '        # dealing with the annotation file')
    (109, '        handle_annotation(annotation)')
    (110, '        if is_gbff(annotation):')
    (111, '            print("Supplied annotation in GenBank format - also getting assembly report.")')
    (112, '            if is_url(assembly_report):')
    (113, "#                urlretrieve(assembly_report, \\'resources/assembly_report.txt\\')")
    (114, '                assembly_report = requests.get(assembly_report)')
    (115, "                with open(\\'resources/assembly_report.txt\\', \\'wb\\') as outfile:")
    (116, '                    outfile.write(assembly_report.content)')
    (117, '            else:')
    (118, '                os.symlink(assembly_report, "resources/assembly_report.txt")')
    (119, '        print("All done!")')
    (120, '    else:')
    (121, '        print("Unrecognized annotation file type!")')
    (122, '        print("Valid files are:")')
    (123, '        print(extensions)')
    (124, '')
    (125, '')
    (126, '#fasta_annotation_handling(fasta, annotation, report)')
    (127, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bcgsc/pori_graphkb_loader, file=Snakefile
context_key: ['if USE_FDA_UNII']
    (311, 'def get_drug_inputs(wildcards):')
    (312, '    inputs = [*rules.load_ncit.output]')
    (313, '    if USE_FDA_UNII:')
    (314, '        inputs.extend(rules.load_fda_srs.output)')
    (315, '    container: CONTAINER')
    (316, '    if USE_DRUGBANK:')
    (317, '        inputs.append(*rules.load_drugbank.output)')
    (318, '    return inputs')
    (319, '')
    (320, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva, file=workflow/rules/vqsr.smk
context_key: ['if wildcards.type == "snp"']
    (2, 'def _get_recal_params(wildcards):')
    (3, '    known_variants = resolve_multi_filepath(config["resources"]["known_variants"])')
    (4, '    if wildcards.type == "snp":')
    (5, '        return (')
    (6, '            "-mode SNP "')
    (7, '            "--max-gaussians 4 "')
    (8, '            "-an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR "')
    (9, '            "-resource:hapmap,known=false,training=true,truth=true,prior=15.0 {hapmap} "')
    (10, '            "-resource:omni,known=false,training=true,truth=true,prior=12.0 {omni} "')
    (11, '            "-resource:1000G,known=false,training=true,truth=false,prior=10.0 {g1k} "')
    (12, '            "-resource:dbsnp,known=true,training=false,truth=false,prior=2.0 {dbsnp}"')
    (13, '        ).format(**known_variants)')
    (14, '    else:')
    (15, '        return (')
    (16, '            "-mode INDEL " ')
    (17, '            "-an QD -an FS -an SOR -an MQRankSum -an ReadPosRankSum "')
    (18, '            "--max-gaussians 4 "')
    (19, '            "-resource:mills,known=false,training=true,truth=true,prior=12.0 {mills} "')
    (20, '            "-resource:dbsnp,known=true,training=false,truth=false,prior=2.0 {dbsnp}"')
    (21, '        ).format(**known_variants)')
    (22, '')
    (23, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva, file=workflow/rules/common.smk
context_key: ['if read_type == "se"', 'if conservative_cpu_count(reserve_cores=2, max_cores=99) > 2']
    (41, 'def threads_calculator(read_type="pe"):')
    (42, '    if read_type == "se":')
    (43, '        if conservative_cpu_count(reserve_cores=2, max_cores=99) > 2:')
    (44, '            return conservative_cpu_count(reserve_cores=2, max_cores=99) / 2')
    (45, '        else:')
    (46, '            return 1')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva, file=workflow/rules/common.smk
context_key: ['if read_type == "se"', 'else', 'if conservative_cpu_count(reserve_cores=2, max_cores=99) > 4']
    (41, 'def threads_calculator(read_type="pe"):')
    (42, '    if read_type == "se":')
    (43, '        if conservative_cpu_count(reserve_cores=2, max_cores=99) > 2:')
    (44, '            return conservative_cpu_count(reserve_cores=2, max_cores=99) / 2')
    (45, '        else:')
    (46, '            return 1')
    (47, '    else:')
    (48, '        if conservative_cpu_count(reserve_cores=2, max_cores=99) > 4:')
    (49, '            return conservative_cpu_count(reserve_cores=2, max_cores=99) / 4')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva, file=workflow/rules/common.smk
context_key: ['if read_type == "se"', 'else', 'else']
    (41, 'def threads_calculator(read_type="pe"):')
    (42, '    if read_type == "se":')
    (43, '        if conservative_cpu_count(reserve_cores=2, max_cores=99) > 2:')
    (44, '            return conservative_cpu_count(reserve_cores=2, max_cores=99) / 2')
    (45, '        else:')
    (46, '            return 1')
    (47, '    else:')
    (48, '        if conservative_cpu_count(reserve_cores=2, max_cores=99) > 4:')
    (49, '            return conservative_cpu_count(reserve_cores=2, max_cores=99) / 4')
    (50, '        else:')
    (51, '            return 1')
    (52, '')
    (53, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva, file=workflow/rules/common.smk
context_key: ['if not os.path.isabs(filepath)']
    (54, 'def expand_filepath(filepath):')
    (55, '    filepath = os.path.expandvars(os.path.expanduser(filepath))')
    (56, '    if not os.path.isabs(filepath):')
    (57, '        raise FileNotFoundError(')
    (58, '            errno.ENOENT,')
    (59, '            os.strerror(errno.ENOENT) + " (path must be absolute)",')
    (60, '            filepath,')
    (61, '        )')
    (62, '    return filepath')
    (63, '')
    (64, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva, file=workflow/rules/common.smk
context_key: ['if does not exists, create path and return it. If any errors, retur']
    (75, 'def tmp_path(path=""):')
    (76, '    """')
    (77, '    if does not exists, create path and return it. If any errors, return')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva, file=workflow/rules/common.smk
context_key: ['if path', 'try']
    (75, 'def tmp_path(path=""):')
    (76, '    """')
    (77, '    if does not exists, create path and return it. If any errors, return')
    (78, '    default path')
    (79, '    :param path: path')
    (80, '    :return: path')
    (81, '    """')
    (82, '    default_path = os.getenv("TMPDIR", config.get("paths").get("tmp_dir"))')
    (83, '    if path:')
    (84, '        try:')
    (85, '            os.makedirs(path)')
    (86, '        except OSError as e:')
    (87, '            if e.errno != errno.EEXIST:')
    (88, '                return default_path')
    (89, '        return path')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva, file=workflow/rules/common.smk
context_key: ['if path']
    (75, 'def tmp_path(path=""):')
    (76, '    """')
    (77, '    if does not exists, create path and return it. If any errors, return')
    (78, '    default path')
    (79, '    :param path: path')
    (80, '    :return: path')
    (81, '    """')
    (82, '    default_path = os.getenv("TMPDIR", config.get("paths").get("tmp_dir"))')
    (83, '    if path:')
    (84, '        try:')
    (85, '            os.makedirs(path)')
    (86, '        except OSError as e:')
    (87, '            if e.errno != errno.EEXIST:')
    (88, '                return default_path')
    (89, '        return path')
    (90, '    return default_path')
    (91, '')
    (92, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva, file=workflow/rules/common.smk
context_key: ['if n >= prefix[s]']
    (110, '    def bytes2human(n):')
    (111, '        # http://code.activestate.com/recipes/578019')
    (112, '        # >>> bytes2human(10000)')
    (113, "        # \\'9.8K\\'")
    (114, '        # >>> bytes2human(100001221)')
    (115, "        # \\'95.4M\\'")
    (116, '        symbols = ("K", "M", "G", "T", "P", "E", "Z", "Y")')
    (117, '        prefix = {}')
    (118, '        for i, s in enumerate(symbols):')
    (119, '            prefix[s] = 1 << (i + 1) * 10')
    (120, '        for s in reversed(symbols):')
    (121, '            if n >= prefix[s]:')
    (122, '                value = float(n) / prefix[s]')
    (123, '                return "%.0f%s" % (value, s)')
    (124, '        return "%sB" % n')
    (125, '')
    (126, '    def preserve(resource, percentage, stock):')
    (127, '        preserved = resource - max(resource * percentage // 100, stock)')
    (128, '        return preserved if preserved != 0 else stock')
    (129, '')
    (130, '    # def preserve(resource, percentage, stock):')
    (131, '    #     return resource - max(resource * percentage // 100, stock)')
    (132, '')
    (133, '    params_template = "\\\'-Xms{} -Xmx{} -XX:ParallelGCThreads={} " "-Djava.io.tmpdir={}\\\'"')
    (134, '')
    (135, '    mem_min = 1024**3 * 2  # 2GB')
    (136, '')
    (137, '    mem_size = preserve(total_physical_mem_size(), percentage_to_preserve, stock_mem)')
    (138, '    cpu_nums = preserve(cpu_count(), percentage_to_preserve, stock_cpu)')
    (139, '    tmpdir = tmp_path(tmp_dir)')
    (140, '')
    (141, '    return params_template.format(')
    (142, '        bytes2human(mem_min).lower(),')
    (143, '        bytes2human(max(mem_size // cpu_nums * multiply_by, mem_min)).lower(),')
    (144, '        min(cpu_nums, multiply_by),')
    (145, '        tmpdir,')
    (146, '    )')
    (147, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva, file=workflow/rules/common.smk
context_key: ['if arguments']
    (148, 'def multi_flag_dbi(flag, arguments):')
    (149, '    if arguments:')
    (150, '        return " ".join(flag + " " + arg for arg in arguments)')
    (151, "    return \\'\\'")
    (152, '')
    (153, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SilasK/16S-dada2, file=Snakefile
context_key: ["if \\'idtaxa_dbs\\' in config and config[\\'idtaxa_dbs\\'] is not None"]
    (30, 'def get_taxonomy_names():')
    (31, '')
    (32, "    if \\'idtaxa_dbs\\' in config and config[\\'idtaxa_dbs\\'] is not None:")
    (33, "        return config[\\'idtaxa_dbs\\'].keys()")
    (34, '    else:')
    (35, '        return []')
    (36, '')
    (37, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=rstats-tartu/geo-htseq, file=Snakefile
context_key: ['if line.strip() != "']
    (20, 'def get_parsed_suppfiles(wildcards):')
    (21, '    SUPPFILENAMES_FILE = checkpoints.query_suppfilenames.get().output[0]')
    (22, '    with open(SUPPFILENAMES_FILE, "r") as f:')
    (23, '        SUPPFILENAMES = [')
    (24, '            os.path.basename(line.rstrip())')
    (25, '            for line in f.readlines()')
    (26, '            if line.strip() != ""')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bposzewiecka/PhaseDancerSimulator, file=Snakefile
context_key: ["if chemistry in [\\'P4C2\\', \\'P5C3\\', \\'P6C4\\']"]
    (24, 'def get_technology(chemistry):')
    (25, "    if chemistry in [\\'P4C2\\', \\'P5C3\\', \\'P6C4\\']:")
    (26, '        return PACBIO')
    (27, "    if chemistry in [\\'R103\\', \\'R94\\', \\'R95\\']:")
    (28, '        return NANOPORE')
    (29, '    raise')
    (30, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bposzewiecka/PhaseDancerSimulator, file=Snakefile
context_key: ["if ttype == \\'leaves\\'"]
    (46, 'def get_genome_size(name, region_name, all_contigs_fn):')
    (47, '    ')
    (48, "    parameters = config[\\'simulations\\'][name]")
    (49, "    random_prefix_size = parameters.get(\\'random-prefix-size\\', 0)")
    (50, "    random_suffix_size = parameters.get(\\'random-suffix-size\\', 0)")
    (51, "    ttype = parameters.get(\\'type\\')")
    (52, "    topology = parameters.get(\\'topology\\')")
    (53, '')
    (54, "    if ttype == \\'leaves\\': ")
    (55, '        number_of_contigs = get_leaves_number(topology)')
    (56, '    else:')
    (57, '        number_of_contigs = get_number_of_reads(all_contigs_fn)')
    (58, '')
    (59, "    region = config[\\'regions\\'][region_name] ")
    (60, "    region_start = region[\\'start\\']")
    (61, "    region_end = region[\\'end\\']")
    (62, '    region_size = region_end - region_start')
    (63, '')
    (64, '    return number_of_contigs * ( random_prefix_size + random_suffix_size + region_size)')
    (65, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/pipeline-transcriptome-de, file=snakelib/utils.snake
context_key: ['if match']
    (2, 'def generate_help(sfile):')
    (3, '    """Parse out target and help message from file."""')
    (4, '    handler = open(sfile, "r")')
    (5, '    for line in handler:')
    (6, "        match = re.match(r\\'^rule\\\\s+([a-zA-Z_-]+):.*?## (.*)$$\\', line)")
    (7, '        if match:')
    (8, '            target, help = match.groups()')
    (9, '            print("%-20s %s" % (target, help))')
    (10, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/miRNA, file=rules/functions.py
context_key: ['else cpu_count() - reserve_cp']
    (6, 'def pipeline_cpu_count(reserve_cpu=2):')
    (7, '    cpu_nums = cpu_count() if reserve_cpu > cpu_count() \\\\')
    (8, '        else cpu_count() - reserve_cpu')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/miRNA, file=rules/functions.py
context_key: ['if does not exists, create path and return it. If any errors, retur']
    (22, "def tmp_path(path=\\'\\'):")
    (23, '    """')
    (24, '    if does not exists, create path and return it. If any errors, return')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/miRNA, file=rules/functions.py
context_key: ['if path', 'try']
    (22, "def tmp_path(path=\\'\\'):")
    (23, '    """')
    (24, '    if does not exists, create path and return it. If any errors, return')
    (25, '    default path')
    (26, '    :param path: path')
    (27, '    :return: path')
    (28, '    """')
    (29, "    default_path = os.getenv(\\'TMPDIR\\', \\'/tmp\\')")
    (30, '    if path:')
    (31, '        try:')
    (32, '            os.makedirs(path)')
    (33, '        except OSError as e:')
    (34, '            if e.errno != errno.EEXIST:')
    (35, '                return default_path')
    (36, '        return path')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/miRNA, file=rules/functions.py
context_key: ['if path']
    (22, "def tmp_path(path=\\'\\'):")
    (23, '    """')
    (24, '    if does not exists, create path and return it. If any errors, return')
    (25, '    default path')
    (26, '    :param path: path')
    (27, '    :return: path')
    (28, '    """')
    (29, "    default_path = os.getenv(\\'TMPDIR\\', \\'/tmp\\')")
    (30, '    if path:')
    (31, '        try:')
    (32, '            os.makedirs(path)')
    (33, '        except OSError as e:')
    (34, '            if e.errno != errno.EEXIST:')
    (35, '                return default_path')
    (36, '        return path')
    (37, '    return default_path')
    (38, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/miRNA, file=rules/functions.py
context_key: ['if n >= prefix[s]']
    (51, '    def bytes2human(n):')
    (52, '        # http://code.activestate.com/recipes/578019')
    (53, '        # >>> bytes2human(10000)')
    (54, "        # \\'9.8K\\'")
    (55, '        # >>> bytes2human(100001221)')
    (56, "        # \\'95.4M\\'")
    (57, "        symbols = (\\'K\\', \\'M\\', \\'G\\', \\'T\\', \\'P\\', \\'E\\', \\'Z\\', \\'Y\\')")
    (58, '        prefix = {}')
    (59, '        for i, s in enumerate(symbols):')
    (60, '            prefix[s] = 1 << (i + 1) * 10')
    (61, '        for s in reversed(symbols):')
    (62, '            if n >= prefix[s]:')
    (63, '                value = float(n) / prefix[s]')
    (64, "                return \\'%.0f%s\\' % (value, s)")
    (65, '        return "%sB" % n')
    (66, '')
    (67, '    def preserve(resource, percentage, stock):')
    (68, '        return resource - max(resource * percentage // 100, stock)')
    (69, '')
    (70, '    params_template = "\\\'-Xms{} -Xmx{} -XX:ParallelGCThreads={} " \\\\')
    (71, '                      "-Djava.io.tmpdir={}\\\'"')
    (72, '')
    (73, '    mem_min = 1024 ** 3 * 2  # 2GB')
    (74, '')
    (75, '    mem_size = preserve(total_physical_mem_size(), percentage_to_preserve,')
    (76, '                        stock_mem)')
    (77, '')
    (78, '    cpu_nums = preserve(cpu_count(), percentage_to_preserve, stock_cpu)')
    (79, '')
    (80, '    tmpdir = tmp_path(tmp_dir)')
    (81, '')
    (82, '    return params_template.format(bytes2human(mem_min).lower(),')
    (83, '                                  bytes2human(max(mem_size//cpu_nums*multiply_by,')
    (84, '                                                  mem_min)).lower(),')
    (85, '                                  min(cpu_nums, multiply_by),')
    (86, '                                  tmpdir)')
    (87, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/ImputationPipeline-snakemake, file=scripts/functions.py
context_key: ['if current_chr == "23"']
    (24, 'def getChrForPhasing(wildcards):')
    (25, '    current_chr=wildcards.chr')
    (26, '    if current_chr == "23" :')
    (27, '        converted_chr="X"')
    (28, '    elif current_chr == "24":')
    (29, '        converted_chr="X"')
    (30, '    elif current_chr == "25":')
    (31, '        converted_chr="X"')
    (32, '    else :')
    (33, '        converted_chr=current_chr')
    (34, '    return converted_chr')
    (35, '')
    (36, '')
    (37, '# extract snps to flip after strand check vs reference panel')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/ImputationPipeline-snakemake, file=scripts/functions.py
context_key: ['if re.match("Strand", line.strip().split("\\\\t")[0])']
    (38, 'def get_flippable(infile,outfile):')
    (39, '    import re ')
    (40, '    # fgrep -w "Strand" {input[0]} | awk \\\'length($9)==length($10) && $5!="D" && $5!="I"\\\' | awk \\\'{{if($5==$6 && ($5!=$9 && $5!=$10)) print $0;else if($5!=$6){if() print $0} }}\\\' |  | cut -f 4 | sort|uniq -u > {output.strand_rsid}')
    (41, '    # infile="ERBO_shapeit_refpanel.alignments.snp.strand"')
    (42, '    # outfile="ERBO_toflip"')
    (43, "    complement={\\'A\\':\\'T\\',\\'T\\':\\'A\\',\\'C\\':\\'G\\',\\'G\\':\\'C\\'}")
    (44, "    strand_file=open(\\'%s\\' %(infile),\\'r\\')")
    (45, '')
    (46, '    # get all duplicates ids, since we need to exclude them from the flipping, if we find them only once in the flippable list')
    (47, '    rs_ids=[]')
    (48, '    for line in strand_file:')
    (49, '        if re.match("Strand", line.strip().split("\\\\t")[0]):')
    (50, '            rs_ids.append(line.strip().split("\\\\t")[3])')
    (51, '    unique_rs=list(set(rs_ids))')
    (52, '    duplicates=list(set([rs_id for rs_id in unique_rs if rs_ids.count(rs_id)>1]))')
    (53, '    #now we have the duplicates we can remove from the flippable list ')
    (54, '    strand_file.seek(0)')
    (55, '    flippable=[]')
    (56, '    for line in strand_file:')
    (57, '        if re.match("Strand", line.strip().split("\\\\t")[0]):')
    (58, '            c_to_flip=line.strip().split("\\\\t")')
    (59, '            # check that we are working with snps and not indels')
    (60, '            if len(c_to_flip[8]) == len(c_to_flip[9]) and c_to_flip[4] != "D" and c_to_flip[4] != "I" :')
    (61, '                #handle monomorphic sites')
    (62, '                if c_to_flip[4] == c_to_flip[5] and (c_to_flip[4] != c_to_flip[8] and c_to_flip[4] != c_to_flip[9]):')
    (63, '                    flippable.append(c_to_flip[3])')
    (64, '                    # print(c_to_flip[3], file=open(outfile,"a"))')
    (65, '                elif (c_to_flip[4] != c_to_flip[5]):')
    (66, '                    #need to check if there are multiallelic sites')
    (67, '                    if (complement.get(c_to_flip[4]) == c_to_flip[8] or complement.get(c_to_flip[4]) == c_to_flip[9]) and (complement.get(c_to_flip[5]) == c_to_flip[8] or complement.get(c_to_flip[5]) == c_to_flip[9]):')
    (68, '                        flippable.append(c_to_flip[3])')
    (69, '                # else:')
    (70, '    # remove duplicates from flippable')
    (71, '    unique_flippable=[rs_id for rs_id in flippable if rs_id not in duplicates]')
    (72, '    open(outfile,"w").write("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/ImputationPipeline-snakemake, file=scripts/functions.py
context_key: ['if (re.match("Warning']
    (85, 'def get_not_assigned_snps(outfile,*infile):')
    (86, '    import re')
    (87, '    rs_ids=[]')
    (88, '    for inputfile in infile:')
    (89, "        c_input_file=open(\\'%s\\' %(inputfile),\\'r\\')")
    (90, '        for line in c_input_file:')
    (91, '            # if (re.match("Warning: Impossible A1 allele assignment",line.strip())):')
    (92, '            if (re.match("Warning: Impossible A1 allele assignment",line.strip()) or re.match("Warning: Impossible A2 allele assignment",line.strip())):')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/ImputationPipeline-snakemake, file=scripts/functions.py
context_key: ["if bim_line.strip().split(\\'\\\\t\\')[4]!=\\'0\\'"]
    (102, 'def update_mono_snps(allele_update,plink_bim,outfile):')
    (103, '    import re')
    (104, '    # read allele update file')
    (105, "    allele_update_file=open(\\'%s\\' %(allele_update))")
    (106, '    all_update={}')
    (107, '    for line in allele_update_file:')
    (108, "        variant=line.strip().split(\\'\\\\t\\')")
    (109, '        rsID=variant[0]')
    (110, "        alleles=variant[2].split(\\' \\')")
    (111, '        all_update[rsID]=alleles')
    (112, '')
    (113, '    # we also need to check for flipped alleles')
    (114, "    complement={\\'A\\':\\'T\\',\\'T\\':\\'A\\',\\'C\\':\\'G\\',\\'G\\':\\'C\\'}")
    (115, '    # read plink bim')
    (116, "    plink_bim_file=open(\\'%s\\' %(plink_bim),\\'r\\')")
    (117, '    # open the stream to the output file')
    (118, "    output_file=open(outfile,\\'w\\')")
    (119, '    new_bim=[]')
    (120, '    for bim_line in plink_bim_file:')
    (121, '        # read line')
    (122, '        # now check if we have a monomorphic site for wich we have to update the allele name ')
    (123, "        if bim_line.strip().split(\\'\\\\t\\')[4]!=\\'0\\':")
    (124, '            # print line as it is in the output file')
    (125, '            _ = output_file.write(bim_line)')
    (126, '        else:')
    (127, '            # do stuff to get the correct name')
    (128, "            c_line=bim_line.strip().split(\\'\\\\t\\')")
    (129, '            c_chr=c_line[0]')
    (130, '            c_rsID=c_line[1]')
    (131, '            c_cm=c_line[2]')
    (132, '            c_pos=c_line[3]')
    (133, '            c_a1=c_line[4]')
    (134, '            c_a2=c_line[5]')
    (135, '            # since we have a cleaned rsID, we need first to get the right one among update_alleles keys')
    (136, "            rs_key=[x for x in all_update.keys() if re.search(re.escape(c_rsID)+\\'$\\',x)]")
    (137, '            #this object could be empty (it happens for sure for chrX). In this case we need to print the line as we find it')
    (138, '            if rs_key:')
    (139, '                # now we have to get the alleles from the update allele dict')
    (140, '                # and check which one we have. Easy case is if there is no flipping')
    (141, '                if c_a2 in all_update[rs_key[0]]:')
    (142, '                    new_a1=list(set(all_update[rs_key[0]]) - set(list(c_a2)))[0]')
    (143, '                else:')
    (144, '                    # here it could be that we have flipped data, so we need to use the complement for both a2 lookup and a1 retrieve')
    (145, '                    new_a1=complement[list(set(all_update[rs_key[0]]) - set(list(complement[c_a2])))[0]]')
    (146, "                _ = output_file.write(\\'%s\\\\t%s\\\\t%s\\\\t%s\\\\t%s\\\\t%s\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/ImputationPipeline-snakemake, file=scripts/functions.py
context_key: ['if attempt > 1']
    (182, 'def get_mem_mb(def_mem, attempt):')
    (183, '    import math')
    (184, '    if attempt > 1:')
    (185, '        return floor(int(def_mem) + (int(def_mem) * attempt * 0.5))')
    (186, '    else :')
    (187, '        return int(def_mem)')
    (188, '')
    (189, '# function to create a temporary folder')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/ImputationPipeline-snakemake, file=scripts/functions.py
context_key: ['if isinstance(elem, list)']
    (200, 'def flattenNestedList(nestedList):')
    (201, "    \\'\\'\\' Converts a nested list to a flat list \\'\\'\\'")
    (202, '    flatList = []')
    (203, '    # Iterate over all the elements in given list')
    (204, '    for elem in nestedList:')
    (205, '        # Check if type of element is list')
    (206, '        if isinstance(elem, list):')
    (207, '            # Extend the flat list by adding contents of this element (list)')
    (208, '            flatList.extend(flattenNestedList(elem))')
    (209, '        else:')
    (210, '            # Append the elemengt to the list')
    (211, '            flatList.append(elem)    ')
    (212, '    return flatList')
    (213, '')
    (214, '# function to extract a list of duplicate sites to be removed using position')
    (215, '# it is possible that with the update step using the external reference, we will introduce some position duplicates')
    (216, '# we need a rule to take care of them, removing duplicates if they dont have an rsID')
    (217, '# first we want the list of duplicate rsID. We need to look for duplicate positions, then lookup the rsID in the bim file, than check if it has an rsID or a bs name from the array manufcturer.')
    (218, '# we will remove the bs named by using it in our list')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/ImputationPipeline-snakemake, file=scripts/functions.py
context_key: ['if len(rs_ids[k]) > 1', 'if not re.match("rs", v)']
    (219, 'def getDupeByPos(infile,outlist):')
    (220, '    import collections')
    (221, '    import re')
    (222, '    # get all duplicates positions, since we need to extract the relevant rsID, to create the exclusion list')
    (223, "    bim_file=open(\\'%s\\' %(infile),\\'r\\')")
    (224, '    rs_ids=collections.defaultdict(list)')
    (225, '    # define a list of ids to remove')
    (226, '    to_rem=[]')
    (227, '    for line in bim_file:')
    (228, '        pos=line.strip().split("\\\\t")[3]')
    (229, '        rsid=line.strip().split("\\\\t")[1]')
    (230, '        rs_ids[pos].append(rsid)')
    (231, '')
    (232, '    for k in rs_ids.keys():')
    (233, '        if len(rs_ids[k]) > 1:')
    (234, "            # we want to check if one of the ids starts with rs. If this is the case, we want to remove the other. If we have two rsId, we will remove both of them. If we don\\'t have any rsId, we will remove both of them")
    (235, '            pos_to_rem=[] #we need a variable to temporary store matching ids, so we can check how many of them we have')
    (236, '            for v in rs_ids[k]:')
    (237, '                if not re.match("rs", v) :')
    (238, '                    to_rem.append(v)')
    (239, '                elif re.match("rs", v) :')
    (240, '                    pos_to_rem.append(v)')
    (241, '            if len(pos_to_rem) > 1 : # we could also check if len(pos_to_rem) == len(rs_ids[k]), but if here we have 3 duplicates by position one without rs, and 2 starting with rs, we still have to remove all of them. So we just want one item in this list')
    (242, '                to_rem.append(pos_to_rem)')
    (243, '    # get unique values                ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/ImputationPipeline-snakemake, file=scripts/functions.py
context_key: ['if len(rs_ids[k]) > 1']
    (219, 'def getDupeByPos(infile,outlist):')
    (220, '    import collections')
    (221, '    import re')
    (222, '    # get all duplicates positions, since we need to extract the relevant rsID, to create the exclusion list')
    (223, "    bim_file=open(\\'%s\\' %(infile),\\'r\\')")
    (224, '    rs_ids=collections.defaultdict(list)')
    (225, '    # define a list of ids to remove')
    (226, '    to_rem=[]')
    (227, '    for line in bim_file:')
    (228, '        pos=line.strip().split("\\\\t")[3]')
    (229, '        rsid=line.strip().split("\\\\t")[1]')
    (230, '        rs_ids[pos].append(rsid)')
    (231, '')
    (232, '    for k in rs_ids.keys():')
    (233, '        if len(rs_ids[k]) > 1:')
    (234, "            # we want to check if one of the ids starts with rs. If this is the case, we want to remove the other. If we have two rsId, we will remove both of them. If we don\\'t have any rsId, we will remove both of them")
    (235, '            pos_to_rem=[] #we need a variable to temporary store matching ids, so we can check how many of them we have')
    (236, '            for v in rs_ids[k]:')
    (237, '                if not re.match("rs", v) :')
    (238, '                    to_rem.append(v)')
    (239, '                elif re.match("rs", v) :')
    (240, '                    pos_to_rem.append(v)')
    (241, '            if len(pos_to_rem) > 1 : # we could also check if len(pos_to_rem) == len(rs_ids[k]), but if here we have 3 duplicates by position one without rs, and 2 starting with rs, we still have to remove all of them. So we just want one item in this list')
    (242, '                to_rem.append(pos_to_rem)')
    (243, '    # get unique values                ')
    (244, '    unique_rs=list(set(flattenNestedList(to_rem)))')
    (245, '    open(outlist,"w").write("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/ImputationPipeline-snakemake, file=scripts/pdf_report.py
context_key: ['if isinstance(datum,str)']
    (10, '        def footer(self):')
    (11, '            # Position at 1.5 cm from bottom')
    (12, '            self.set_y(-15)')
    (13, '            # helvetica italic 8')
    (14, "            self.set_font(\\'Times\\', \\'I\\', 8)")
    (15, '            # Page number')
    (16, "            self.cell(0, 10, \\'Page \\' + str(self.page_no()) + \\'/{nb}\\', 0, 0, \\'C\\')")
    (17, '')
    (18, '')
    (19, '    #we need to build the template for the single chunk, than collect all chunks data together')
    (20, '    # define the current page elements')
    (21, "    # current_chr=\\'2\\' # this is something we need to pass as arguments of our function")
    (22, '    # stat_base_folder="/home/cocca/analyses/imputation/20210613/MOLISANI/07.stats/2/CHUNKS" #this is another argument needed to get the rest of the data')
    (23, '')
    (24, '    #open the PDF documents and start the page numbering')
    (25, '    pdf = PDF()')
    (26, '    # pdf.alias_nb_pages()')
    (27, '')
    (28, '    for chunk in list(range(1,int(chunk_n)+1)):')
    (29, '        current_chunk="{:02d}".format(chunk)')
    (30, '        #define files needed for the pdf generation')
    (31, "        current_info_af=stat_base_folder+\\'/\\'+current_chr+\\'_\\'+ current_chunk +\\'_impute_summary.png\\'")
    (32, "        current_manhattan=stat_base_folder+\\'/\\'+current_chr+\\'_\\'+ current_chunk +\\'_impute_manhattan.png\\'")
    (33, "        current_chunk_stats_by_maf=stat_base_folder+\\'/\\'+current_chr+\\'_\\'+ current_chunk +\\'_impute_summary_by_maf.csv\\'")
    (34, "        current_chunk_stats_by_maf_by_info=stat_base_folder+\\'/\\'+current_chr+\\'_\\'+ current_chunk +\\'_impute_summary_by_maf_by_info.csv\\'")
    (35, '        #create the new page for the current chunk')
    (36, '        pdf.add_page()')
    (37, "        pdf.set_font(\\'Times\\', \\'B\\', 16)")
    (38, '        #add plots')
    (39, '        page_header="Info score and allele frequency report for chr" + current_chr + " chunk " + current_chunk')
    (40, "        pdf.cell(0, 10, page_header, ln=1, align=\\'C\\')")
    (41, '        pdf.image(current_info_af,w=200)')
    (42, '        pdf.ln(1)')
    (43, '        pdf.image(current_manhattan,w=190)')
    (44, '        #now add table with report for each chunk')
    (45, '        #this is something generated by pandas, so should be easy to read in')
    (46, '        df_stats_by_maf=pd.read_csv(current_chunk_stats_by_maf,index_col=0)')
    (47, '        df_stats_by_maf_by_info=pd.read_csv(current_chunk_stats_by_maf_by_info,names=["INFO_CLASS","MAF_CLASS","count","mean","std","min","25%","50%","75%","max"], skiprows=3)')
    (48, '')
    (49, '        stats_by_maf = list(df_stats_by_maf.to_records(index=True))')
    (50, '        stats_by_maf_header=list(df_stats_by_maf.columns)')
    (51, "        stats_by_maf_header.insert(0,\\'\\')")
    (52, '        stats_by_maf.insert(0,tuple(stats_by_maf_header))')
    (53, '        stats_by_maf_by_info = list(df_stats_by_maf_by_info.to_records(index=False))')
    (54, '        stats_by_maf_by_info.insert(0,tuple(df_stats_by_maf_by_info.columns))')
    (55, '')
    (56, "        stats_by_maf_header=\\'Stats of INFO scores by MAF classes\\'")
    (57, "        stats_by_maf_by_info_header=\\'Stats of INFO scores by MAF classes and by INFO score classes\\'")
    (58, '        #add page with tables')
    (59, '        pdf.add_page()')
    (60, '        pdf.set_font("Times", size=10)')
    (61, "        pdf.cell(0, 10, stats_by_maf_header, ln=1, align=\\'C\\')")
    (62, '        line_height = pdf.font_size * 2.5')
    (63, '        col_width = pdf.epw / 10  # distribute content evenly')
    (64, '        for row in stats_by_maf:')
    (65, '            for datum in row:')
    (66, '                if isinstance(datum,str) :')
    (67, "                    pdf.multi_cell(col_width, line_height, datum, border=1, ln=3, max_line_height=pdf.font_size,align=\\'C\\')")
    (68, '                else:')
    (69, "                    pdf.multi_cell(col_width, line_height, str(round(datum,5)), border=1, ln=3, max_line_height=pdf.font_size,align=\\'C\\')")
    (70, '            pdf.ln(line_height)')
    (71, '')
    (72, '        pdf.ln(1)')
    (73, "        pdf.cell(0, 10, stats_by_maf_by_info_header, ln=1, align=\\'C\\')")
    (74, '        for row in stats_by_maf_by_info:')
    (75, '            for datum in row:')
    (76, '                if isinstance(datum,str) :')
    (77, "                    pdf.multi_cell(col_width, line_height, datum, border=1, ln=3, max_line_height=pdf.font_size,align=\\'C\\')")
    (78, '                else:')
    (79, "                    pdf.multi_cell(col_width, line_height, str(round(datum,5)), border=1, ln=3, max_line_height=pdf.font_size,align=\\'C\\')")
    (80, '            pdf.ln(line_height)')
    (81, '')
    (82, '    pdf.output(outfile)')
    (83, '')
    (84, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/ImputationPipeline-snakemake, file=scripts/pdf_report.py
context_key: ['if isinstance(datum,str)']
    (88, '        def footer(self):')
    (89, '            # Position at 1.5 cm from bottom')
    (90, '            self.set_y(-15)')
    (91, '            # helvetica italic 8')
    (92, "            self.set_font(\\'Times\\', \\'I\\', 8)")
    (93, '            # Page number')
    (94, "            self.cell(0, 10, \\'Page \\' + str(self.page_no()) + \\'/{nb}\\', 0, 0, \\'C\\')")
    (95, '')
    (96, '')
    (97, '    #we need to build the template for the single chunk, than collect all chunks data together')
    (98, '    # define the current page elements')
    (99, "    # current_chr=\\'2\\' # this is something we need to pass as arguments of our function")
    (100, '    # stat_base_folder="/home/cocca/analyses/imputation/20210613/MOLISANI/07.stats/2/CHUNKS" #this is another argument needed to get the rest of the data')
    (101, '')
    (102, '    #open the PDF documents and start the page numbering')
    (103, '    pdf = PDF()')
    (104, '')
    (105, "    current_info_af=stat_base_folder+\\'/\\'+current_chr+\\'_impute_summary.png\\'")
    (106, "    current_manhattan=stat_base_folder+\\'/\\'+current_chr+\\'_impute_manhattan.png\\'")
    (107, "    current_chunk_stats_by_maf=stat_base_folder+\\'/\\'+current_chr+\\'_impute_summary_by_maf.csv\\'")
    (108, "    current_chunk_stats_by_maf_by_info=stat_base_folder+\\'/\\'+current_chr+\\'_impute_summary_by_maf_by_info.csv\\'")
    (109, '    #create the new page for the current chunk')
    (110, '    pdf.add_page()')
    (111, "    pdf.set_font(\\'Times\\', \\'B\\', 16)")
    (112, '    #add plots')
    (113, '    page_header="Info score and allele frequency report for chr" + current_chr')
    (114, "    pdf.cell(0, 10, page_header, ln=1, align=\\'C\\')")
    (115, '    pdf.image(current_info_af,w=200)')
    (116, '    pdf.ln(1)')
    (117, '    pdf.image(current_manhattan,w=190)')
    (118, '    #now add table with report for each chunk')
    (119, '    #this is something generated by pandas, so should be easy to read in')
    (120, '    df_stats_by_maf=pd.read_csv(current_chunk_stats_by_maf,index_col=0)')
    (121, '    df_stats_by_maf_by_info=pd.read_csv(current_chunk_stats_by_maf_by_info,names=["INFO_CLASS","MAF_CLASS","count","mean","std","min","25%","50%","75%","max"], skiprows=3)')
    (122, '')
    (123, '    stats_by_maf = list(df_stats_by_maf.to_records(index=True))')
    (124, '    stats_by_maf_header=list(df_stats_by_maf.columns)')
    (125, "    stats_by_maf_header.insert(0,\\'\\')")
    (126, '    stats_by_maf.insert(0,tuple(stats_by_maf_header))')
    (127, '    stats_by_maf_by_info = list(df_stats_by_maf_by_info.to_records(index=False))')
    (128, '    stats_by_maf_by_info.insert(0,tuple(df_stats_by_maf_by_info.columns))')
    (129, '')
    (130, "    stats_by_maf_header=\\'Stats of INFO scores by MAF classes\\'")
    (131, "    stats_by_maf_by_info_header=\\'Stats of INFO scores by MAF classes and by INFO score classes\\'")
    (132, '    #add page with tables')
    (133, '    pdf.add_page()')
    (134, '    pdf.set_font("Times", size=10)')
    (135, "    pdf.cell(0, 10, stats_by_maf_header, ln=1, align=\\'C\\')")
    (136, '    line_height = pdf.font_size * 2.5')
    (137, '    col_width = pdf.epw / 10  # distribute content evenly')
    (138, '    for row in stats_by_maf:')
    (139, '        for datum in row:')
    (140, '            if isinstance(datum,str) :')
    (141, "                pdf.multi_cell(col_width, line_height, datum, border=1, ln=3, max_line_height=pdf.font_size,align=\\'C\\')")
    (142, '            else:')
    (143, "                pdf.multi_cell(col_width, line_height, str(round(datum,5)), border=1, ln=3, max_line_height=pdf.font_size,align=\\'C\\')")
    (144, '        pdf.ln(line_height)')
    (145, '')
    (146, '    pdf.ln(1)')
    (147, "    pdf.cell(0, 10, stats_by_maf_by_info_header, ln=1, align=\\'C\\')")
    (148, '    for row in stats_by_maf_by_info:')
    (149, '        for datum in row:')
    (150, '            if isinstance(datum,str) :')
    (151, "                pdf.multi_cell(col_width, line_height, datum, border=1, ln=3, max_line_height=pdf.font_size,align=\\'C\\')")
    (152, '            else:')
    (153, "                pdf.multi_cell(col_width, line_height, str(round(datum,5)), border=1, ln=3, max_line_height=pdf.font_size,align=\\'C\\')")
    (154, '        pdf.ln(line_height)')
    (155, '')
    (156, '    pdf.output(outfile)')
    (157, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/parabricks, file=.tests/compatibility/Snakefile
context_key: ['if result']
    (17, 'def extract_module_version_from_readme(modulename):')
    (18, '    search_string = modulename + ":(.+)\\')
    (19, '$"')
    (20, '    with open("../../README.md", "r") as reader:')
    (21, '        for line in reader:')
    (22, '            result = re.search(search_string, line)')
    (23, '            if result:')
    (24, '                return result[1]')
    (25, '')
    (26, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/common.smk
context_key: ['if is_human_genome()']
    (30, 'def get_all_inputs():')
    (31, '    input_list = []')
    (32, '')
    (33, '    input_list.extend(')
    (34, '        multiext(')
    (35, '            "resources/ensembl/genome",')
    (36, '            ".fa.gz",')
    (37, '            ".gtf.gz",')
    (38, '        )')
    (39, '    )')
    (40, '')
    (41, '    # Files that apply to any organism')
    (42, '    input_list.append("resources/ensembl/star_genome")')
    (43, '    input_list.append("resources/ensembl/genome_info.json")')
    (44, '    input_list.append("resources/ensembl/gffutils.db")')
    (45, '    input_list.append("resources/ensembl/transcript_annotation.bed")')
    (46, '')
    (47, '    # Files that only apply to human')
    (48, '    if is_human_genome():')
    (49, '        input_list.append("resources/ensembl/star_genome_mane")')
    (50, '        # input_list.append("resources/ensembl/clinically_associated_variants.vcf.gz")')
    (51, '        input_list.append("resources/ensembl/mane-gffutils.db")')
    (52, '        input_list.append("resources/ensembl/mane-cds_regions.tsv")')
    (53, '        input_list.append("resources/clinvar/clinvar.vcf.gz")')
    (54, '        input_list.append("resources/clinvar/clinvar.filtered.vcf.gz")')
    (55, '        input_list.append(f"resources/ensembl/MANE.{HUMAN_ACC}.v1.0.gtf.gz")')
    (56, '')
    (57, "    return input_list'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MaleloAS/Alice_MS, file=Snakefile
context_key: ['if r2 is not None']
    (3, 'def get_fastq(sample_sheet, run_id):')
    (4, '    """Query the sample sheet to return a list of fastq file(s) for the')
    (5, '    corresponding run_id')
    (6, '    """')
    (7, '    r1 = sample_sheet[sample_sheet.run_id == run_id].fastq_r1.iloc[0]')
    (8, '    fq = [r1]')
    (9, '    r2 = sample_sheet[sample_sheet.run_id == run_id].fastq_r2.iloc[0]')
    (10, '    if r2 is not None:')
    (11, '        fq.append(r2)')
    (12, '    return fq')
    (13, '')
    (14, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MaleloAS/Alice_MS, file=Snakefile
context_key: ["if pandas.isna(r2) or r2 is None or r2 == \\'\\'"]
    (23, 'def is_pe_library(sample_sheet, library_id):')
    (24, '    r2 = sample_sheet[sample_sheet.library_id == library_id].fastq_ftp_r2.iloc[0]')
    (25, "    if pandas.isna(r2) or r2 is None or r2 == \\'\\':")
    (26, '        return False')
    (27, '    else:')
    (28, '        return True')
    (29, '')
    (30, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MaleloAS/Alice_MS, file=Snakefile
context_key: ['if len(sample_sheet[sample_sheet.library_id == sample_sheet.run_id]) != 0']
    (23, 'def is_pe_library(sample_sheet, library_id):')
    (24, '    r2 = sample_sheet[sample_sheet.library_id == library_id].fastq_ftp_r2.iloc[0]')
    (25, "    if pandas.isna(r2) or r2 is None or r2 == \\'\\':")
    (26, '        return False')
    (27, '    else:')
    (28, '        return True')
    (29, '')
    (30, '')
    (31, "sample_sheet = pandas.read_csv(config[\\'sample_sheet\\'], sep= \\',\\', comment= \\'#\\')")
    (32, '')
    (33, 'if len(sample_sheet[sample_sheet.library_id == sample_sheet.run_id]) != 0:')
    (34, "    raise Exception(\\'Error in sample sheet: library_id and run_id cannot be the same\\')")
    (35, '')
    (36, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MaleloAS/Alice_MS, file=Snakefile
context_key: ['if not pandas.isna(row.fastq_ftp_r2)']
    (23, 'def is_pe_library(sample_sheet, library_id):')
    (24, '    r2 = sample_sheet[sample_sheet.library_id == library_id].fastq_ftp_r2.iloc[0]')
    (25, "    if pandas.isna(r2) or r2 is None or r2 == \\'\\':")
    (26, '        return False')
    (27, '    else:')
    (28, '        return True')
    (29, '')
    (30, '')
    (31, "sample_sheet = pandas.read_csv(config[\\'sample_sheet\\'], sep= \\',\\', comment= \\'#\\')")
    (32, '')
    (33, 'if len(sample_sheet[sample_sheet.library_id == sample_sheet.run_id]) != 0:')
    (34, "    raise Exception(\\'Error in sample sheet: library_id and run_id cannot be the same\\')")
    (35, '')
    (36, '')
    (37, "sample_sheet[\\'fastq_r1\\'] = None")
    (38, "sample_sheet[\\'fastq_r2\\'] = None")
    (39, '')
    (40, 'fastq_ftp = {}')
    (41, 'for i,row in sample_sheet.iterrows():')
    (42, "    fq = \\'fastq/\\' + os.path.basename(row.fastq_ftp_r1)")
    (43, "    sample_sheet.at[i, \\'fastq_r1\\'] = fq")
    (44, '    fastq_ftp[fq] = row.fastq_ftp_r1')
    (45, '')
    (46, '    if not pandas.isna(row.fastq_ftp_r2):')
    (47, "        fq = \\'fastq/\\' + os.path.basename(row.fastq_ftp_r2)")
    (48, "        sample_sheet.at[i, \\'fastq_r2\\'] = fq")
    (49, '        fastq_ftp[fq] = row.fastq_ftp_r2')
    (50, '')
    (51, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zerostwo/scrna-seq, file=workflow/Snakefile
context_key: ['if not os.path.exists("results/" + SAMPLES)']
    (23, 'def read_csv(file_name):')
    (24, '    f = open(file_name, "r")')
    (25, '    c = f.read()')
    (26, '    res = list()')
    (27, '    rows = c.split("\\')
    (28, '")')
    (29, '    for row in rows:')
    (30, '        res.append(row)')
    (31, '    res.pop()')
    (32, '    return(res)')
    (33, '')
    (34, "SAMPLES = os.path.basename(SEURAT_OBJ_PATH).split(\\'.\\')[0]")
    (35, '')
    (36, 'if not os.path.exists("results/" + SAMPLES):')
    (37, '    os.makedirs("results/" + SAMPLES)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zerostwo/scrna-seq, file=workflow/Snakefile
context_key: ['if not os.path.exists("results/" + SAMPLES + "/cell_types.csv")']
    (23, 'def read_csv(file_name):')
    (24, '    f = open(file_name, "r")')
    (25, '    c = f.read()')
    (26, '    res = list()')
    (27, '    rows = c.split("\\')
    (28, '")')
    (29, '    for row in rows:')
    (30, '        res.append(row)')
    (31, '    res.pop()')
    (32, '    return(res)')
    (33, '')
    (34, "SAMPLES = os.path.basename(SEURAT_OBJ_PATH).split(\\'.\\')[0]")
    (35, '')
    (36, 'if not os.path.exists("results/" + SAMPLES):')
    (37, '    os.makedirs("results/" + SAMPLES)')
    (38, 'if not os.path.exists("results/" + SAMPLES + "/cell_types.csv"):')
    (39, '    ## \\xe8\\x8e\\xb7\\xe5\\x8f\\x96\\xe7\\xbb\\x86\\xe8\\x83\\x9e\\xe7\\xb1\\xbb\\xe5\\x9e\\x8b')
    (40, '    str = "Rscript workflow/scripts/get_cell_types.R -i " + SEURAT_OBJ_PATH + " -o " + "./results/" + SAMPLES + "/cell_types.csv"')
    (41, '    os.system(str)')
    (42, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dpmerrell/TrialMDP-analyses, file=Snakefile
context_key: ['if pA == pB']
    (80, 'def compute_n_patients(pA, pB, alpha=SIM_ALPHA, beta=SIM_BETA):')
    (81, '    ')
    (82, '    if pA == pB:')
    (83, '        return NULL_HYP_N')
    (84, '')
    (85, '    delta = abs(pA - pB)')
    (86, '')
    (87, '    var_b = pB*(1.0-pB)')
    (88, '    var_a = pA*(1.0-pA)')
    (89, '')
    (90, '    k = 1.0 ')
    (91, '')
    (92, '    # one-sided:')
    (93, '    n_a = (norm.ppf(1.0-alpha) + norm.ppf(1.0-beta))**2.0 * (var_a + var_b/k) / (delta*delta)')
    (94, '    n_b = k * n_a')
    (95, '    n = n_a + n_b')
    (96, '    ')
    (97, '    # round *up* to an even number')
    (98, '    n_up = int(n) + 1')
    (99, '    if n_up % 2 == 1:')
    (100, '        n_up += 1')
    (101, '    print("pA: ", pA, "\\\\tpB: ", pB, "\\\\tN: ", n_up)')
    (102, '    return n_up')
    (103, '')
    (104, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dpmerrell/TrialMDP-analyses, file=Snakefile
context_key: ['if pat >= 92']
    (338, 'def compute_block_incr(wc):')
    (339, '    pat = int(wc["pat"])')
    (340, '    inc = 2')
    (341, '    if pat >= 92: # 100:')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dpmerrell/TrialMDP-analyses, file=Snakefile
context_key: ['elif pat > 128']
    (338, 'def compute_block_incr(wc):')
    (339, '    pat = int(wc["pat"])')
    (340, '    inc = 2')
    (341, '    if pat >= 92: # 100:')
    (342, '        inc = 4')
    (343, '    elif pat > 128:')
    (344, '        inc = 8')
    (345, '    return inc ')
    (346, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/wf-bulk-indexing, file=Snakefile
context_key: ['if bioentities_directories_to_stage']
    (37, 'def get_bioentities_directories_to_stage():')
    (38, '    """')
    (39, '    List all the directories from the main atlas bioentities that need to be staged')
    (40, '    to be able to run this in a per organism level.')
    (41, '')
    (42, '    The web application code running these processes descides on the species to')
    (43, '    run based on the files it find in the BIOENTITIES path given.')
    (44, '')
    (45, '    This is the structure of directories that we aim to match')
    (46, '    annotations_ensembl_104_51*  annotations_wbps_15      go_ens104_51*')
    (47, '    array_designs_104_51_15*     ensembl_104_51*          reactome_ens104_51*  wbps_15*')
    (48, '    """')
    (49, '    global bioentities_directories_to_stage')
    (50, '    global TYPES')
    (51, "    species = config[\\'species\\']")
    (52, '    print(f"Lenght of set {len(bioentities_directories_to_stage)}")')
    (53, '    if bioentities_directories_to_stage:')
    (54, '        return bioentities_directories_to_stage')
    (55, '    dirs=set()')
    (56, '    prefix=f"{config[\\\'bioentities_source\\\']}"')
    (57, '    for type in TYPES:')
    (58, '        dir = f"{prefix}/{type}"')
    (59, '        if os.path.isdir(dir):')
    (60, '            print(f"{dir} exists")')
    (61, '            dirs.add(dir)')
    (62, '')
    (63, '    bioentities_directories_to_stage = dirs')
    (64, '    return bioentities_directories_to_stage')
    (65, '')
    (66, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/wf-bulk-indexing, file=Snakefile
context_key: ['if staging_files']
    (71, 'def get_all_staging_files():')
    (72, '    global staging_files')
    (73, '    if staging_files:')
    (74, '        return staging_files')
    (75, "    species = config[\\'species\\']")
    (76, '    results = []')
    (77, '    source_dirs = get_bioentities_directories_to_stage()')
    (78, '    for sdir in source_dirs:')
    (79, '        dest = get_destination_dir(sdir)')
    (80, '        print(f"Looking at {sdir} with destination {dest}")')
    (81, '        if dest.endswith("go") or dest.endswith(\\\'interpro\\\'):')
    (82, '            files = [os.path.basename(f) for f in glob.glob(f"{sdir}/*.tsv")]')
    (83, '        else:')
    (84, '            files = [os.path.basename(f) for f in glob.glob(f"{sdir}/{species}.*.tsv")]')
    (85, '            # We have cases where the files are buried one directory below :-(')
    (86, '            files.extend([os.path.basename(f) for f in glob.glob(f"{sdir}/*/{species}.*.tsv")])')
    (87, '        results.extend([f"{dest}/{f}" for f in files])')
    (88, '')
    (89, '    staging_files.update(results)')
    (90, '    return staging_files')
    (91, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/wf-bulk-indexing, file=Snakefile
context_key: ['if "reactome" in input']
    (96, 'def get_jsonl_label(input):')
    (97, '    global wbps_annotations_re')
    (98, '    global ens_annotations_re')
    (99, '    global array_designs_re')
    (100, '')
    (101, '    if "reactome" in input:')
    (102, '        return "reactome"')
    (103, '    if "mirbase" in input:')
    (104, '        return "mature_mirna"')
    (105, '    if wbps_annotations_re.search(input):')
    (106, '        return "wbpsgene"')
    (107, '    if ens_annotations_re.search(input):')
    (108, '        return "ensgene"')
    (109, '    m_array = array_designs_re.search(input)')
    (110, '    if m_array:')
    (111, '        return m_array.group(1)')
    (112, '')
    (113, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/wf-bulk-indexing, file=Snakefile
context_key: ['if json_label']
    (114, 'def get_jsonl_paths():')
    (115, '    inputs_for_jsonl = get_all_staging_files()')
    (116, '    jsonls = set()')
    (117, '    for input in inputs_for_jsonl:')
    (118, '        json_label = get_jsonl_label(input)')
    (119, '        if json_label:')
    (120, '            jsonls.add(f"{config[\\\'output_dir\\\']}/{config[\\\'species\\\']}.{json_label}.jsonl")')
    (121, '    print(f"Number of JSONLs expected: {len(jsonls)}")')
    (122, '    return jsonls')
    (123, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/wf-bulk-indexing, file=Snakefile
context_key: ["if \\'exp_update_sync_dest\\' in config"]
    (159, 'def get_exp_design_sync_destination():')
    (160, '    # The destination for a sync as expected by rsync: user@host:<path>')
    (161, "    if \\'exp_update_sync_dest\\' in config:")
    (162, "        return config[\\'exp_update_sync_dest\\']")
    (163, '')
    (164, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva.wgs, file=rules/vqsr.smk
context_key: ['if wildcards.type == "snp"']
    (16, 'def _get_recal_params(wildcards):')
    (17, '    known_variants = resolve_multi_filepath(*references_abs_path(), config["known_variants"])')
    (18, '    if wildcards.type == "snp":')
    (19, '        return (')
    (20, '            "--mode SNP "')
    (21, '            "-an DP -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR "')
    (22, '            "--resource hapmap,known=false,training=true,truth=true,prior=15.0:{hapmap} "')
    (23, '            "--resource omni,known=false,training=true,truth=true,prior=12.0:{omni} "')
    (24, '            "--resource 1000G,known=false,training=true,truth=false,prior=10.0:{g1k} "')
    (25, '            "--resource dbsnp,known=true,training=false,truth=false,prior=2.0:{dbsnp} "')
    (26, '        ).format(**known_variants)')
    (27, '    else:')
    (28, '        return (')
    (29, '            "-mode INDEL "')
    (30, '            "-an DP -an QD -an FS -an SOR -an MQRankSum -an ReadPosRankSum "')
    (31, '            "--max-gaussians 4 "')
    (32, '            "--resource dbsnp,known=true,training=false,truth=false,prior=2.0:{dbsnp} "')
    (33, '            "--resource mills,known=false,training=true,truth=true,prior=12.0:{mills} "')
    (34, '        ).format(**known_variants)')
    (35, '')
    (36, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva.wgs, file=rules/functions.py
context_key: ['if not os.path.isabs(filepath)']
    (32, 'def expand_filepath(filepath):')
    (33, '    filepath = os.path.expandvars(os.path.expanduser(filepath))')
    (34, '    if not os.path.isabs(filepath):')
    (35, '        raise FileNotFoundError(')
    (36, '            errno.ENOENT, os.strerror(errno.ENOENT)+" (path must be absolute)", filepath)')
    (37, '    return filepath')
    (38, '')
    (39, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva.wgs, file=rules/functions.py
context_key: ['if does not exists, create path and return it. If any errors, retur']
    (59, "def tmp_path(path=\\'\\'):")
    (60, '    """')
    (61, '    if does not exists, create path and return it. If any errors, return')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva.wgs, file=rules/functions.py
context_key: ['if path', 'try']
    (59, "def tmp_path(path=\\'\\'):")
    (60, '    """')
    (61, '    if does not exists, create path and return it. If any errors, return')
    (62, '    default path')
    (63, '    :param path: path')
    (64, '    :return: path')
    (65, '    """')
    (66, "    default_path = os.getenv(\\'TMPDIR\\', \\'/tmp\\')")
    (67, '    if path:')
    (68, '        try:')
    (69, '            os.makedirs(path)')
    (70, '        except OSError as e:')
    (71, '            if e.errno != errno.EEXIST:')
    (72, '                return default_path')
    (73, '        return path')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva.wgs, file=rules/functions.py
context_key: ['if path']
    (59, "def tmp_path(path=\\'\\'):")
    (60, '    """')
    (61, '    if does not exists, create path and return it. If any errors, return')
    (62, '    default path')
    (63, '    :param path: path')
    (64, '    :return: path')
    (65, '    """')
    (66, "    default_path = os.getenv(\\'TMPDIR\\', \\'/tmp\\')")
    (67, '    if path:')
    (68, '        try:')
    (69, '            os.makedirs(path)')
    (70, '        except OSError as e:')
    (71, '            if e.errno != errno.EEXIST:')
    (72, '                return default_path')
    (73, '        return path')
    (74, '    return default_path')
    (75, '')
    (76, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva.wgs, file=rules/functions.py
context_key: ['if n >= prefix[s]']
    (89, '    def bytes2human(n):')
    (90, '        # http://code.activestate.com/recipes/578019')
    (91, '        # >>> bytes2human(10000)')
    (92, "        # \\'9.8K\\'")
    (93, '        # >>> bytes2human(100001221)')
    (94, "        # \\'95.4M\\'")
    (95, "        symbols = (\\'K\\', \\'M\\', \\'G\\', \\'T\\', \\'P\\', \\'E\\', \\'Z\\', \\'Y\\')")
    (96, '        prefix = {}')
    (97, '        for i, s in enumerate(symbols):')
    (98, '            prefix[s] = 1 << (i + 1) * 10')
    (99, '        for s in reversed(symbols):')
    (100, '            if n >= prefix[s]:')
    (101, '                value = float(n) / prefix[s]')
    (102, "                return \\'%.0f%s\\' % (value, s)")
    (103, '        return "%sB" % n')
    (104, '')
    (105, '    def preserve(resource, percentage, stock):')
    (106, '        preserved = resource - max(resource * percentage // 100, stock)')
    (107, '        return preserved if preserved != 0 else stock')
    (108, '')
    (109, '    # def preserve(resource, percentage, stock):')
    (110, '    #     return resource - max(resource * percentage // 100, stock)')
    (111, '')
    (112, '    params_template = "\\\'-Xms{} -Xmx{} -XX:ParallelGCThreads={} " \\\\')
    (113, '                      "-Djava.io.tmpdir={}\\\'"')
    (114, '')
    (115, '    mem_min = 1024 ** 3 * 2  # 2GB')
    (116, '')
    (117, '    mem_size = preserve(total_physical_mem_size(), percentage_to_preserve,')
    (118, '                        stock_mem)')
    (119, '    cpu_nums = preserve(cpu_count(), percentage_to_preserve, stock_cpu)')
    (120, '    tmpdir = tmp_path(tmp_dir)')
    (121, '')
    (122, '    return params_template.format(bytes2human(mem_min).lower(),')
    (123, '                                  bytes2human(max(mem_size//cpu_nums*multiply_by,')
    (124, '                                                  mem_min)).lower(),')
    (125, '                                  min(cpu_nums, multiply_by),')
    (126, '                                  tmpdir)')
    (127, '')
    (128, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva.wgs, file=rules/functions.py
context_key: ['if arguments']
    (129, 'def _multi_flag(flag, arguments):')
    (130, '    if arguments:')
    (131, '        return " ".join(flag + " " + arg for arg in arguments)')
    (132, "    return \\'\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/diva.wgs, file=rules/functions.py
context_key: ['if arguments']
    (133, 'def _multi_flag_dbi(flag, arguments):')
    (134, '    if arguments:')
    (135, '        return " ".join(flag + " " + arg for arg in arguments)')
    (136, "    return \\'\\'")
    (137, '')
    (138, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexmsalmeida/magscreen, file=Snakefile
context_key: ["if line.strip() == \\'stop\\'"]
    (33, 'def checkDrep(wildcards):')
    (34, '    checkpoint_file = checkpoints.cluster_unknown.get().output[1]')
    (35, '    checkpoint_dir = checkpoints.cluster_unknown.get().output[0]')
    (36, '    with open(checkpoint_file) as f:')
    (37, '        line = f.readline()')
    (38, "        if line.strip() == \\'stop\\':")
    (39, '            return OUTPUT_DIR+"/drep/done.txt"')
    (40, '        else:')
    (41, '            filter_out = expand(os.path.join(checkpoint_dir, "{id}.checked"), id=glob_wildcards(os.path.join(checkpoint_dir, "{id}.fa")).id)')
    (42, '            return filter_out')
    (43, '')
    (44, '# rule that specifies the final expected output files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=osvaldoreisss/miseq_bac_assembly_annot_workflow, file=workflow/rules/common.smk
context_key: ['if len(fastqs) == 2']
    (18, 'def get_fastq(wildcards):')
    (19, '    """Get fastq files of given sample."""')
    (20, '    fastqs = samples.loc[(wildcards.sample), ["fq1", "fq2"]].dropna()')
    (21, '    print(wildcards.sample, fastqs)')
    (22, '    if len(fastqs) == 2:')
    (23, '        return fastqs.fq1, fastqs.fq2')
    (24, "    return fastqs.fq1'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jhamman/chains, file=metsim.snakefile
context_key: ["if \\'rcp\\' in scen"]
    (8, 'def metsim_state(wcs):')
    (9, '    scen = wcs.scen')
    (10, "    if \\'rcp\\' in scen:")
    (11, "        scen = \\'hist\\'")
    (12, "    # if wcs.dsm == \\'bcsd\\' and scen == \\'hist\\':")
    (13, "    #     scen = \\'rcp45\\'")
    (14, '')
    (15, '    state = DOWNSCALING_DATA.format(dsm=wcs.dsm, gcm=wcs.gcm, scen=scen)')
    (16, '')
    (17, '    return state')
    (18, '')
    (19, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jhamman/chains, file=metsim.snakefile
context_key: ["if isinstance(obj, str) or not hasattr(obj, \\'__iter__\\')"]
    (20, 'def maybe_make_cfg_list(obj):')
    (21, "    if isinstance(obj, str) or not hasattr(obj, \\'__iter__\\'):")
    (22, '        return obj')
    (23, '    elif len(obj) == 1:')
    (24, '        return obj[0]')
    (25, "    return \\'%s\\' % \\', \\'.join(obj)")
    (26, '')
    (27, '')
    (28, '# rule run_metsim_obs:')
    (29, '#     input:')
    (30, '#         readme = README,')
    (31, '#         config = DISAGG_CONFIG,')
    (32, '#         forcing = DOWNSCALING_DATA,')
    (33, '#         state = metsim_state')
    (34, '#     output: OBS_OUTPUT')
    (35, '#     log: NOW.strftime(DISAGG_LOG)')
    (36, '#     threads: 18')
    (37, '#     shell: "ms -s distributed -n {threads} {input.config} > {log} 2>&1"')
    (38, ' ')
    (39, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jhamman/chains, file=prms.snakefile
context_key: ["if wcs.scen in [\\'hist\\', \\'obs_hist\\']"]
    (8, 'def prms_init_state(wcs):')
    (9, "    # print(\\'init state %s\\' % wcs)")
    (10, "    if wcs.scen in [\\'hist\\', \\'obs_hist\\']:")
    (11, '        state = NULL_STATE')
    (12, '    else:')
    (13, '        kwargs = dict(wcs)')
    (14, "        kwargs[\\'scen\\'] = \\'hist\\'")
    (15, "        # print(\\'init state 2 --> %s\\' % wcs)")
    (16, '        state = PRMS_STATE.format(**kwargs)')
    (17, '    return state')
    (18, '')
    (19, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jhamman/chains, file=downscaling.snakefile
context_key: ['if v == key']
    (15, 'def inverse_lookup(d, key):')
    (16, '    for k, v in d.items():')
    (17, '        if v == key:')
    (18, '            return k')
    (19, '    raise KeyError(key)')
    (20, '')
    (21, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jhamman/chains, file=downscaling.snakefile
context_key: ["if \\'hist\\' in wcs.scen"]
    (22, 'def get_downscaling_data(wcs):')
    (23, "    if \\'hist\\' in wcs.scen:")
    (24, '        # get one year before start of simulation for metsim')
    (25, '        start_offset = -1')
    (26, '    else:')
    (27, '        start_offset = 0')
    (28, "    years = get_year_range(config[\\'SCEN_YEARS\\'][wcs.scen],")
    (29, '                           start_offset=start_offset)')
    (30, '')
    (31, "    if wcs.dsm in config[\\'OBS_FORCING\\']:")
    (32, '        # historical / obs data')
    (33, "        template = config[\\'OBS_FORCING\\'][wcs.dsm][\\'data\\']")
    (34, '        scen = wcs.scen')
    (35, '    else:')
    (36, '        # downscaling_data')
    (37, "        gcm = inverse_lookup(config[\\'DOWNSCALING\\'][wcs.dsm][\\'gcms\\'], wcs.gcm)")
    (38, "        template = config[\\'DOWNSCALING\\'][wcs.dsm][\\'data\\']")
    (39, '')
    (40, "        if wcs.dsm == \\'bcsd\\' and wcs.scen == \\'hist\\':")
    (41, "            scen = \\'rcp45\\'")
    (42, "        elif wcs.dsm == \\'loca\\' and wcs.scen == \\'hist\\':")
    (43, "            scen = \\'historical\\'")
    (44, '        else:')
    (45, '            scen = wcs.scen')
    (46, '')
    (47, '    # get a pattern without wildcards (could still have glob patterns)')
    (48, '    patterns = set([template.format(gcm=gcm, scen=scen, dsm=wcs.dsm, year=year)')
    (49, '                    for year in years])')
    (50, '')
    (51, '    # get actual filepaths')
    (52, '    files = []')
    (53, '    for pattern in patterns:')
    (54, '        files.extend(glob.glob(pattern))')
    (55, '    files.sort()')
    (56, '')
    (57, '    # make sure all these files exist')
    (58, '    missing = []')
    (59, '    for file in files:')
    (60, '        if not os.path.isfile(file):')
    (61, '            missing.append(file)')
    (62, '    if missing:')
    (63, '        raise RuntimeError(')
    (64, "            \\'Failed to find any files for template %s. \\'")
    (65, "            \\'Missing: %s\\' % (pattern, \\'\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jhamman/chains, file=downscaling.snakefile
context_key: ['if var in ds']
    (70, 'def preproc(ds):')
    (71, '    for var in vars_to_drop:')
    (72, '        if var in ds:')
    (73, '            ds = ds.drop(var)')
    (74, '    for key, val in pre_rename.items():')
    (75, '        if key in ds:')
    (76, '            ds = ds.rename({key: val})')
    (77, '')
    (78, "    lons = ds[\\'lon\\'].values")
    (79, "    ds[\\'lon\\'].values[lons > 180] -= 360.")
    (80, '    return ds.astype(np.float32)')
    (81, '')
    (82, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jhamman/chains, file=downscaling.snakefile
context_key: ['if like']
    (83, 'def process_downscaling_dataset(input_files, output_file, kind, times,')
    (84, '                                like=None, rename=None, chunks=None):')
    (85, '')
    (86, "    variables = [\\'prec\\', \\'wind\\', \\'t_max\\', \\'t_min\\']")
    (87, '    if like:')
    (88, "        print(\\'like %s\\' % like)")
    (89, "        like = xr.open_dataset(like, engine=\\'netcdf4\\').load()")
    (90, '')
    (91, '    ds = xr.open_mfdataset(sorted(input_files),')
    (92, "                           combine=\\'by_coords\\',")
    (93, "                           concat_dim=\\'time\\',")
    (94, '                           preprocess=preproc,')
    (95, "                           engine=\\'netcdf4\\').load()")
    (96, '')
    (97, "    print(\\'renaming\\', flush=True)")
    (98, '    if rename:')
    (99, '        for key, val in rename.items():')
    (100, '            if val in ds:')
    (101, '                ds = ds.rename({val: key})')
    (102, '')
    (103, '    ds = ds.sel(time=times)')
    (104, '')
    (105, '    # drop bound variables')
    (106, '    drops = []')
    (107, "    for v in [\\'lon_bnds\\', \\'lat_bnds\\', \\'time_bnds\\']:")
    (108, '        if v in ds or v in ds.coords:')
    (109, '            drops.append(v)')
    (110, '    ds = ds.drop(drops)')
    (111, "    print(\\'reindexing like %s\\' % like, flush=True)")
    (112, '')
    (113, '    if like:')
    (114, "        ds = ds.reindex_like(like, method=\\'nearest\\', tolerance=0.001)")
    (115, '')
    (116, "    if \\'wind\\' not in ds:")
    (117, "        ds[\\'wind\\'] = xr.full_like(ds[\\'prec\\'], DEFAULT_WIND)")
    (118, "        ds[\\'wind\\'].attrs[\\'units\\'] = \\'m s-1\\'")
    (119, "        ds[\\'wind\\'].attrs[\\'long_name\\'] = \\'wind speed\\'")
    (120, '')
    (121, "    if \\'loca\\' in kind.lower():")
    (122, '        # "normalize" units')
    (123, "        ds[\\'prec\\'] = ds[\\'prec\\'] * SEC_PER_DAY")
    (124, "        ds[\\'prec\\'].attrs[\\'units\\'] = \\'mm d-1\\'")
    (125, "        ds[\\'prec\\'].attrs[\\'long_name\\'] = \\'precipitation\\'")
    (126, '')
    (127, "        for v in [\\'t_max\\', \\'t_min\\']:")
    (128, '            ds[v] = ds[v] - KELVIN')
    (129, "            ds[v].attrs[\\'units\\'] = \\'C\\'")
    (130, "    if \\'t_max\\' not in ds:")
    (131, "        print(\\'calculating t_max\\')")
    (132, "        ds[\\'t_max\\'] = ds[\\'t_mean\\'] + 0.5 * ds[\\'t_range\\']")
    (133, "    if \\'t_min\\' not in ds:")
    (134, "        print(\\'calculating t_min\\')")
    (135, "        ds[\\'t_min\\'] = ds[\\'t_mean\\'] - 0.5 * ds[\\'t_range\\']")
    (136, '')
    (137, '    # quality control checks')
    (138, "    ds[\\'t_max\\'] = ds[\\'t_max\\'].where(ds[\\'t_max\\'] > ds[\\'t_min\\'],")
    (139, "                                    ds[\\'t_min\\'] + TINY_TEMP)")
    (140, "    ds[\\'prec\\'] = ds[\\'prec\\'].where(ds[\\'prec\\'] > 0, 0.)")
    (141, "    ds[\\'wind\\'] = ds[\\'wind\\'].where(ds[\\'wind\\'] > 0, 0.)")
    (142, '')
    (143, "    print(\\'masking\\', flush=True)")
    (144, "    # if like and \\'mask\\' in like:")
    (145, "    #     ds = ds.where(like[\\'mask\\'])")
    (146, '')
    (147, '    ds = ds[variables].astype(np.float32)')
    (148, '')
    (149, "    print(\\'loading %0.1fGB\\' % (ds.nbytes / 1e9), flush=True)")
    (150, '    print(ds, flush=True)')
    (151, "    ds = ds.transpose(\\'time\\', \\'lat\\', \\'lon\\')  # .load()")
    (152, '')
    (153, '    # TODO: save the original attributes and put them back')
    (154, '')
    (155, '    # make sure time index in dataset cover the full period')
    (156, "    times = ds.indexes[\\'time\\']")
    (157, '    start_year, stop_year = times.year[0], times.year[-1]')
    (158, "    dates = pd.date_range(start=f\\'{start_year}-01-01\\',")
    (159, "                          end=f\\'{stop_year}-12-31\\', freq=\\'D\\')")
    (160, "    if ds.dims[\\'time\\'] != len(dates):")
    (161, "        print(\\'filling missing days\\', flush=True)")
    (162, '        # fill in missing days at the end or begining of the record')
    (163, "        # removed: reindex(time=dates, method=\\'ffill\\', copy=False)")
    (164, "        if isinstance(ds.indexes[\\'time\\'], xr.CFTimeIndex):")
    (165, "            ds[\\'time\\'] = ds.indexes[\\'time\\'].to_datetimeindex()")
    (166, "        ds = ds.reindex(time=dates, method=\\'nearest\\', copy=False)")
    (167, '')
    (168, '    encoding = {}')
    (169, '    if chunks is not None:')
    (170, "        chunksizes = (len(ds.time), chunks[\\'lat\\'], chunks[\\'lon\\'])")
    (171, '        for var in variables:')
    (172, '            encoding[var] = dict(chunksizes=chunksizes)')
    (173, '')
    (174, "    print(\\'writing %s:\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jhamman/chains, file=Snakefile
context_key: ["if \\'vic\\' in wcs.model.lower()"]
    (25, 'def hydro_forcings(wcs):')
    (26, "    force_timestep = config[\\'HYDROLOGY\\'][wcs.model][\\'force_timestep\\']")
    (27, '')
    (28, "    if \\'vic\\' in wcs.model.lower():")
    (29, "        years = get_year_range(config[\\'SCEN_YEARS\\'][wcs.scen])")
    (30, '        return [VIC_FORCING.format(year=year, gcm=wcs.gcm, scen=wcs.scen,')
    (31, '                                   dsm=wcs.dsm,')
    (32, '                                   disagg_method=wcs.disagg_method,')
    (33, '                                   disagg_ts=force_timestep)')
    (34, '                for year in years]')
    (35, "    elif \\'prms\\' in wcs.model.lower():")
    (36, '        return PRMS_FORCINGS.format(disagg_ts=force_timestep, **wcs)')
    (37, '    else:')
    (38, '        raise NotImplementedError')
    (39, '')
    (40, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jhamman/chains, file=Snakefile
context_key: ["if d[\\'gcm\\'] in config[\\'DOWNSCALING\\'][d[\\'dsm\\']][\\'gcms\\'].values()", "if \\'model_id\\' in d", "if d[\\'model_id\\'] in d[\\'model\\']"]
    (185, '    def filtered_combinator(*args, **kwargs):')
    (186, '        for wc_comb in combinator(*args, **kwargs):')
    (187, '            # Use frozenset instead of tuple')
    (188, '            # in order to accomodate')
    (189, '            # unpredictable wildcard order')
    (190, '            d = dict(wc_comb)')
    (191, '')
    (192, '            # is this combo of gcm/dsm active')
    (193, "            if d[\\'gcm\\'] in config[\\'DOWNSCALING\\'][d[\\'dsm\\']][\\'gcms\\'].values():")
    (194, '                # if running hydro models')
    (195, "                if \\'model_id\\' in d:")
    (196, '                    # if this hydro model matches the model id.')
    (197, '                    # e.g. model_id=prms_default, model=prms')
    (198, "                    if d[\\'model_id\\'] in d[\\'model\\']:")
    (199, '                        yield frozenset(wc_comb)')
    (200, '                    else:')
    (201, '                        pass')
    (202, '                else:')
    (203, '                    yield frozenset(wc_comb)')
    (204, '            else:')
    (205, '                pass')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jhamman/chains, file=Snakefile
context_key: ["if d[\\'gcm\\'] in config[\\'DOWNSCALING\\'][d[\\'dsm\\']][\\'gcms\\'].values()"]
    (185, '    def filtered_combinator(*args, **kwargs):')
    (186, '        for wc_comb in combinator(*args, **kwargs):')
    (187, '            # Use frozenset instead of tuple')
    (188, '            # in order to accomodate')
    (189, '            # unpredictable wildcard order')
    (190, '            d = dict(wc_comb)')
    (191, '')
    (192, '            # is this combo of gcm/dsm active')
    (193, "            if d[\\'gcm\\'] in config[\\'DOWNSCALING\\'][d[\\'dsm\\']][\\'gcms\\'].values():")
    (194, '                # if running hydro models')
    (195, "                if \\'model_id\\' in d:")
    (196, '                    # if this hydro model matches the model id.')
    (197, '                    # e.g. model_id=prms_default, model=prms')
    (198, "                    if d[\\'model_id\\'] in d[\\'model\\']:")
    (199, '                        yield frozenset(wc_comb)')
    (200, '                    else:')
    (201, '                        pass')
    (202, '                else:')
    (203, '                    yield frozenset(wc_comb)')
    (204, '            else:')
    (205, '                pass')
    (206, '    return filtered_combinator')
    (207, '')
    (208, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jhamman/chains, file=vic.snakefile
context_key: ["if wcs.scen in [\\'hist\\', \\'obs_hist\\']"]
    (7, 'def vic_init_state(wcs):')
    (8, "    if wcs.scen in [\\'hist\\', \\'obs_hist\\']:")
    (9, '        state = NULL_STATE')
    (10, '    else:')
    (11, '        kwargs = dict(wcs)')
    (12, "        kwargs[\\'scen\\'] = \\'hist\\'")
    (13, "        kwargs[\\'date\\'] = \\'{:4d}-12-31-82800\\'.format(")
    (14, "            config[\\'SCEN_YEARS\\'][wcs.scen][\\'start\\'])")
    (15, '        state = VIC_STATE.format(**kwargs)')
    (16, '    return state')
    (17, '')
    (18, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bihealth/varfish-data-igsr, file=workflow/Snakefile
context_key: ['if not header']
    (54, 'def load_sheet():')
    (55, '    """Load the sample sheet from the PED files')
    (56, '    ')
    (57, '    Restrict to those in the VCF file for chr1.')
    (58, '')
    (59, '    Generate the samples-in-vcf.txt file with')
    (60, '')
    (61, '    ::')
    (62, '    ')
    (63, '        bcftools view -h igsr-data/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz \\\\')
    (64, "        | grep CHROM | cut -f 10- | tr \\'\\\\t\\' \\'\\")
    (65, "\\' \\\\")
    (66, '        > igsr-metadata/samples-in-vcf.txt')
    (67, '    """')
    (68, '    in_vcf = []')
    (69, '    with open("igsr-metadata/samples-in-vcf.txt", "rt") as inputf:')
    (70, '        for row in inputf:')
    (71, '            in_vcf.append(row.strip())')
    (72, '    in_vcf = set(in_vcf)')
    (73, '')
    (74, '    header = None')
    (75, '    with open(')
    (76, '        "igsr-metadata/integrated_call_samples_v3.20200731.ALL.ped", "rt"')
    (77, '    ) as inputf:')
    (78, '        reader = csv.reader(inputf, delimiter="\\\\t")')
    (79, '        for row in reader:')
    (80, '            if not header:')
    (81, '                header = row')
    (82, '            else:')
    (83, '                vals = dict(zip(header, row))')
    (84, '                if vals[COL_INDIVIDUAL] in in_vcf:')
    (85, '                    yield vals')
    (86, '')
    (87, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bihealth/varfish-data-igsr, file=workflow/Snakefile
context_key: ['if row[COL_RELATIONSHIP] in (RELATIONSHIP_UNRELATED, RELATIONSHIP_CHILD']
    (54, 'def load_sheet():')
    (55, '    """Load the sample sheet from the PED files')
    (56, '    ')
    (57, '    Restrict to those in the VCF file for chr1.')
    (58, '')
    (59, '    Generate the samples-in-vcf.txt file with')
    (60, '')
    (61, '    ::')
    (62, '    ')
    (63, '        bcftools view -h igsr-data/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz \\\\')
    (64, "        | grep CHROM | cut -f 10- | tr \\'\\\\t\\' \\'\\")
    (65, "\\' \\\\")
    (66, '        > igsr-metadata/samples-in-vcf.txt')
    (67, '    """')
    (68, '    in_vcf = []')
    (69, '    with open("igsr-metadata/samples-in-vcf.txt", "rt") as inputf:')
    (70, '        for row in inputf:')
    (71, '            in_vcf.append(row.strip())')
    (72, '    in_vcf = set(in_vcf)')
    (73, '')
    (74, '    header = None')
    (75, '    with open(')
    (76, '        "igsr-metadata/integrated_call_samples_v3.20200731.ALL.ped", "rt"')
    (77, '    ) as inputf:')
    (78, '        reader = csv.reader(inputf, delimiter="\\\\t")')
    (79, '        for row in reader:')
    (80, '            if not header:')
    (81, '                header = row')
    (82, '            else:')
    (83, '                vals = dict(zip(header, row))')
    (84, '                if vals[COL_INDIVIDUAL] in in_vcf:')
    (85, '                    yield vals')
    (86, '')
    (87, '')
    (88, 'RELATIONSHIP_CHILD = "child"')
    (89, 'RELATIONSHIP_UNRELATED = "unrel"')
    (90, '')
    (91, 'COL_RELATIONSHIP = "Relationship"')
    (92, 'COL_INDIVIDUAL = "Individual ID"')
    (93, 'COL_GENDER = "Gender"')
    (94, '')
    (95, 'SEX_MALE = "1"')
    (96, 'SEX_FEMALE = "2"')
    (97, '')
    (98, '')
    (99, '#: All samples')
    (100, 'SHEET_FULL = list(load_sheet())')
    (101, '#: Only children or unrelated samples')
    (102, 'SHEET_INDEX = {')
    (103, '    row[COL_INDIVIDUAL]: row')
    (104, '    for row in SHEET_FULL')
    (105, '    if row[COL_RELATIONSHIP] in (RELATIONSHIP_UNRELATED, RELATIONSHIP_CHILD)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MGXlab/phap, file=workflow/rules/common.smk
context_key: ["if line.startswith(\\'#\\')"]
    (3, 'def parse_samplesheet(samples_tsv):')
    (4, '    samples_dic = {}')
    (5, "    with open(samples_tsv, \\'r\\') as fin:")
    (6, '        header_line = fin.readline()')
    (7, "        header_fields = [f.strip() for f in header_line.split(\\'\\\\t\\')]")
    (8, '        assert header_fields == [\\\'sample\\\', \\\'fasta\\\'], "Malformatted samplesheet"')
    (9, '        for line in fin:')
    (10, "            if line.startswith(\\'#\\'):")
    (11, '                pass')
    (12, '            else:')
    (13, "                fields = [f.strip() for f in line.split(\\'\\\\t\\')]")
    (14, '                samples_dic[fields[0]] = fields[1]')
    (15, '    return samples_dic')
    (16, '')
    (17, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tniranjan1/RRBS_PIPELINE, file=workflow/Snakefile
context_key: ['if return_value == "files"']
    (60, 'def distinguish_lrs_rrbs(middle_folder, return_value):')
    (61, "    samples2use = lrs_sample_names if middle_folder == \\'lrs_methyl\\' else rrbs_sample_names")
    (62, '    expansion = []')
    (63, '    for u in samples2use: expansion.append(f"{results_dir}/{middle_folder}/samples/" + u + ".agePrediction.txt")')
    (64, '    if return_value == "files":')
    (65, '        return expansion')
    (66, '    else:')
    (67, '        return samples2use')
    (68, '')
    (69, '# Install DMRFinder from github if not done already')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tniranjan1/RRBS_PIPELINE, file=workflow/Snakefile
context_key: ['if not os.path.isdir(DMRFinder_repo_path)']
    (60, 'def distinguish_lrs_rrbs(middle_folder, return_value):')
    (61, "    samples2use = lrs_sample_names if middle_folder == \\'lrs_methyl\\' else rrbs_sample_names")
    (62, '    expansion = []')
    (63, '    for u in samples2use: expansion.append(f"{results_dir}/{middle_folder}/samples/" + u + ".agePrediction.txt")')
    (64, '    if return_value == "files":')
    (65, '        return expansion')
    (66, '    else:')
    (67, '        return samples2use')
    (68, '')
    (69, '# Install DMRFinder from github if not done already')
    (70, "DMRFinder_repo_path = abspath(\\'resources/DMRFinder\\')")
    (71, 'if not os.path.isdir(DMRFinder_repo_path):')
    (72, "    git.Repo.clone_from(config[\\'DMRfinder\\'][\\'DMRFinder_git_URL\\'], DMRFinder_repo_path)")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tniranjan1/RRBS_PIPELINE, file=workflow/Snakefile
context_key: ['if DMRFinder_repo.bare']
    (60, 'def distinguish_lrs_rrbs(middle_folder, return_value):')
    (61, "    samples2use = lrs_sample_names if middle_folder == \\'lrs_methyl\\' else rrbs_sample_names")
    (62, '    expansion = []')
    (63, '    for u in samples2use: expansion.append(f"{results_dir}/{middle_folder}/samples/" + u + ".agePrediction.txt")')
    (64, '    if return_value == "files":')
    (65, '        return expansion')
    (66, '    else:')
    (67, '        return samples2use')
    (68, '')
    (69, '# Install DMRFinder from github if not done already')
    (70, "DMRFinder_repo_path = abspath(\\'resources/DMRFinder\\')")
    (71, 'if not os.path.isdir(DMRFinder_repo_path):')
    (72, "    git.Repo.clone_from(config[\\'DMRfinder\\'][\\'DMRFinder_git_URL\\'], DMRFinder_repo_path)")
    (73, 'DMRFinder_repo = git.Repo(DMRFinder_repo_path)')
    (74, '# check that the repository loaded correctly')
    (75, 'if DMRFinder_repo.bare:')
    (76, "    DMRF_error = \\'Could not load repository at {}.\\'.format(DMRFinder_repo_path)")
    (77, "    DMRF_error = DMRF_error + \\' Recommend checking DMRFinder git url in ../config/config.yaml. Workflow aborted.\\'")
    (78, '    print(DMRF_error)')
    (79, '')
    (80, '# Which Cytosine contexts to Analyze')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mardzix/bcd_nano_CUTnTag, file=workflow/Snakefile_single_modality.smk
context_key: ['if modality == m']
    (5, 'def get_fragments_per_modality(modality, barcodes_dict):')
    (6, '    result = []')
    (7, '    for s in barcodes_dict:')
    (8, '        for m in barcodes_dict[s]:')
    (9, '            if modality == m:')
    (10, "               result.append(\\'results/multimodal_data/{sample}/{modality}_{barcode}/fragments/fragments.tsv.gz\\'.format(sample = s, modality = m, barcode=barcodes_dict[s][m]))")
    (11, '    return result')
    (12, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mardzix/bcd_nano_CUTnTag, file=workflow/Snakefile_single_modality.smk
context_key: ['if modality == m']
    (13, 'def get_seurat_per_modality(modality, barcodes_dict,feature):')
    (14, '    result = []')
    (15, '    for s in barcodes_dict:')
    (16, '        for m in barcodes_dict[s]:')
    (17, '            if modality == m:')
    (18, "               result.append(\\'results/multimodal_data/{sample}/{modality}_{barcode}/seurat/{feature}/Seurat_object.Rds\\'.format(sample = s, modality = m, barcode=barcodes_dict[s][m], feature = feature))")
    (19, '    return result')
    (20, '')
    (21, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mardzix/bcd_nano_CUTnTag, file=workflow/Snakefile_single_modality.smk
context_key: ['if modality in barcodes_dict[sample].keys()']
    (134, 'def get_bamfiles_per_modality(modality, barcodes_dict):')
    (135, '    bam_files = []')
    (136, '    for sample in barcodes_dict.keys():')
    (137, '        if modality in barcodes_dict[sample].keys():')
    (138, "            bam = \\'results/multimodal_data/{sample}/{modality}_{barcode}/bam/possorted_bam_sampleID.bam\\'.format(sample=sample,modality=modality,barcode=")
    (139, '            barcodes_dict[sample][modality])')
    (140, '            bam_files.append(bam)')
    (141, '    return bam_files')
    (142, '')
    (143, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mardzix/bcd_nano_CUTnTag, file=workflow/Snakefile_preprocess.smk
context_key: ['if modality == "ATAC"']
    (22, 'def get_peaks_file_from_modality(sample,modality,barcode):')
    (23, '    if modality == "ATAC":')
    (24, '        # Cellranger default peak calling')
    (25, '        # peaks = "results/multimodal_data/{sample}/cellranger/{sample}_{modality}_{barcode}/outs/peaks.bed".format(sample=sample, modality=modality, barcode=barcode)')
    (26, "        peaks = \\'results/multimodal_data/{sample}/{modality}_{barcode}/peaks/macs_broad/{modality}_peaks.broadPeak\\'.format(sample=sample,modality=modality,barcode=barcode)")
    (27, '    elif modality == "H3K27ac":')
    (28, '        # Broad peaks')
    (29, "        peaks = \\'results/multimodal_data/{sample}/{modality}_{barcode}/peaks/macs_broad/{modality}_peaks.broadPeak\\'.format(sample=sample, modality=modality, barcode=barcode)")
    (30, '    elif modality == "H3K27me3":')
    (31, '        # Broad peaks')
    (32, "        peaks = \\'results/multimodal_data/{sample}/{modality}_{barcode}/peaks/macs_broad/{modality}_peaks.broadPeak\\'.format(sample=sample, modality=modality, barcode=barcode)")
    (33, '    else:')
    (34, "        peaks = \\'results/multimodal_data/{sample}/{modality}_{barcode}/peaks/macs_broad/{modality}_peaks.broadPeak\\'.format(sample=sample, modality=modality, barcode=barcode)")
    (35, '    return peaks')
    (36, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mardzix/bcd_nano_CUTnTag, file=workflow/Snakefile_preprocess.smk
context_key: ['if modality == m']
    (37, 'def get_fragments_per_modality(modality, barcodes_dict):')
    (38, '    result = []')
    (39, '    for s in barcodes_dict:')
    (40, '        for m in barcodes_dict[s]:')
    (41, '            if modality == m:')
    (42, "               result.append(\\'results/multimodal_data/{sample}/{modality}_{barcode}/fragments/fragments.tsv.gz\\'.format(sample = s, modality = m, barcode=barcodes_dict[s][m]))")
    (43, '    return result')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mardzix/bcd_nano_CUTnTag, file=workflow/Snakefile_preprocess.smk
context_key: ['if modality == m']
    (44, 'def get_seurat_per_modality(modality, barcodes_dict,feature):')
    (45, '    result = []')
    (46, '    for s in barcodes_dict:')
    (47, '        for m in barcodes_dict[s]:')
    (48, '            if modality == m:')
    (49, "               result.append(\\'results/multimodal_data/{sample}/{modality}_{barcode}/seurat/{feature}/Seurat_object.Rds\\'.format(sample = s, modality = m, barcode=barcodes_dict[s][m], feature = feature))")
    (50, '    return result')
    (51, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhaddow/pmindia-crawler, file=Snakefile
context_key: ['if fields[0] and fields[1]']
    (19, 'def get_pairs(tsv_file):')
    (20, '  pairs = []')
    (21, '  with open(tsv_file) as tfh:')
    (22, '    for line in tfh:')
    (23, '      fields = line[:-1].split("\\\\t")')
    (24, '      if fields[0] and fields[1]:')
    (25, '        pairs.append((fields[0], fields[1]))')
    (26, '  return pairs')
    (27, '')
    (28, '')
    (29, '# BEGIN entry-point rules')
    (30, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dilworthlab/CnT_pipeline_snakemake, file=Snakefile
context_key: ['try', "if fnmatch.fnmatch(_file_, \\'*md5*.txt\\')"]
    (73, 'def findmd5(ROOT_DIR):')
    (74, '    try:')
    (75, '        for _file_ in os.listdir(READS_DIR):')
    (76, "            if fnmatch.fnmatch(_file_, \\'*md5*.txt\\'):")
    (77, '                PATH2MD5 = os.path.join(READS_DIR, _file_)')
    (78, "                logger.info(f\\'md5sum file: {PATH2MD5}\\')")
    (79, '                return PATH2MD5')
    (80, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bakeronit/snakemake-gatk4-non-model, file=Snakefile
context_key: ['if trim']
    (28, 'def get_fastq2(wildcards):')
    (29, '    fastqs = samples.loc[(wildcards.prefix), ["fq1", "fq2"]].dropna()')
    (30, '    if trim:')
    (31, '        return {"r1": os.path.join("data/trimmed",wildcards.prefix + ".1P.fastq.gz"), "r2": os.path.join("data/trimmed",wildcards.prefix + ".2P.fastq.gz")}')
    (32, '    else:')
    (33, '        return {"r1": fastqs.fq1, "r2": fastqs.fq2}')
    (34, '')
    (35, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tanaes/snakemake_assemble, file=bin/snakefiles/report
context_key: ['if pos_filter_regex is not None']
    (16, 'def load_multiqc_data(fp, label_col=None, label_val=None, column_prefix=None,')
    (17, '                      pos_filter_regex=None, neg_filter_regex=None, name_mod_from=None, name_mod_to=None):')
    (18, '    """')
    (19, '    loads a multiqc data file and modifies sample names. ')
    (20, '    ')
    (21, '    fp : str')
    (22, '        filepath to multiqc data file')
    (23, '    label_col : str')
    (24, '        name of column to append as a label')
    (25, '    label_val : str')
    (26, '        value for column to append as a label')
    (27, '    column_prefix : str')
    (28, '        thing to add to column names')
    (29, '    pos_filter_regex : regex str')
    (30, '        regex to filter sample names (positive)')
    (31, '    neg_filter_regex : regex str')
    (32, '        regex to filter sample names (negative)')
    (33, '    name_mod_from : list of str')
    (34, '        list of strings to match in filenames for replacement. Can be multiple strings.')
    (35, '    name_mod_to : list of str')
    (36, '        corresponding list of strings for matches to be replaced by')
    (37, '    """')
    (38, '    ')
    (39, "    mqc_df = pd.read_csv(fp, header=0, sep=\\'\\\\t\\')")
    (40, '    ')
    (41, '    if pos_filter_regex is not None:')
    (42, "        mqc_df = mqc_df.loc[(mqc_df[\\'Sample\\'].str.contains(pos_filter_regex)),]")
    (43, '    ')
    (44, '    if neg_filter_regex is not None:')
    (45, "        mqc_df = mqc_df.loc[~(mqc_df[\\'Sample\\'].str.contains(neg_filter_regex)),]            ")
    (46, '            ')
    (47, '    if name_mod_from is not None and name_mod_to is not None:')
    (48, "        mqc_df[\\'Sample\\'] = mqc_df[\\'Sample\\'].replace(name_mod_from, name_mod_to, regex=True)")
    (49, '        ')
    (50, '    if column_prefix is not None:')
    (51, "        mqc_df.columns = [column_prefix + x if x != \\'Sample\\' else x for x in mqc_df.columns]")
    (52, '    ')
    (53, '    if label_col is not None:')
    (54, '        mqc_df[label_col] = label_val')
    (55, '    ')
    (56, '    return(mqc_df)')
    (57, '')
    (58, '')
    (59, '# function to read a benchmark file')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tanaes/snakemake_assemble, file=bin/snakefiles/report
context_key: ['if m']
    (60, "def read_benchmark(fp, pattern = \\'^.*benchmarks/(.+?)/(.+?)\\\\.sample=\\\\[(.+?)\\\\](\\\\.abund_sample=\\\\[(.+?)\\\\])?\\\\.txt$\\'):")
    (61, '    # check to make sure .txt filepath matches expected format')
    (62, '    m = re.search(pattern, fp)')
    (63, '')
    (64, '    if m:')
    (65, "        benchmarks = pd.read_csv(fp, sep=\\'\\\\t\\', header=0)")
    (66, '        ')
    (67, "        benchmarks[\\'benchmark_file\\'] = fp")
    (68, "        benchmarks[\\'rule\\'] = m.group(2)")
    (69, "        benchmarks[\\'module\\'] = m.group(1)")
    (70, "        benchmarks[\\'sample\\'] = m.group(3)")
    (71, "        benchmarks[\\'abund_sample\\'] = m.group(5)")
    (72, '')
    (73, '        ')
    (74, '        return(benchmarks)')
    (75, '    else:')
    (76, '        return(None)')
    (77, '')
    (78, '')
    (79, '# function to load all benchmarks into a data frame')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tanaes/snakemake_assemble, file=bin/snakefiles/report
context_key: ['if pos_filter_regex is not None']
    (16, 'def load_multiqc_data(fp, label_col=None, label_val=None, column_prefix=None,')
    (17, '                      pos_filter_regex=None, neg_filter_regex=None, name_mod_from=None, name_mod_to=None):')
    (18, '    """')
    (19, '    loads a multiqc data file and modifies sample names. ')
    (20, '    ')
    (21, '    fp : str')
    (22, '        filepath to multiqc data file')
    (23, '    label_col : str')
    (24, '        name of column to append as a label')
    (25, '    label_val : str')
    (26, '        value for column to append as a label')
    (27, '    column_prefix : str')
    (28, '        thing to add to column names')
    (29, '    pos_filter_regex : regex str')
    (30, '        regex to filter sample names (positive)')
    (31, '    neg_filter_regex : regex str')
    (32, '        regex to filter sample names (negative)')
    (33, '    name_mod_from : list of str')
    (34, '        list of strings to match in filenames for replacement. Can be multiple strings.')
    (35, '    name_mod_to : list of str')
    (36, '        corresponding list of strings for matches to be replaced by')
    (37, '    """')
    (38, '    ')
    (39, "    mqc_df = pd.read_csv(fp, header=0, sep=\\'\\\\t\\')")
    (40, '    ')
    (41, '    if pos_filter_regex is not None:')
    (42, "        mqc_df = mqc_df.loc[(mqc_df[\\'Sample\\'].str.contains(pos_filter_regex)),]")
    (43, '    ')
    (44, '    if neg_filter_regex is not None:')
    (45, "        mqc_df = mqc_df.loc[~(mqc_df[\\'Sample\\'].str.contains(neg_filter_regex)),]            ")
    (46, '            ')
    (47, '    if name_mod_from is not None and name_mod_to is not None:')
    (48, "        mqc_df[\\'Sample\\'] = mqc_df[\\'Sample\\'].replace(name_mod_from, name_mod_to, regex=True)")
    (49, '        ')
    (50, '    if column_prefix is not None:')
    (51, "        mqc_df.columns = [column_prefix + x if x != \\'Sample\\' else x for x in mqc_df.columns]")
    (52, '    ')
    (53, '    if label_col is not None:')
    (54, '        mqc_df[label_col] = label_val')
    (55, '    ')
    (56, '    return(mqc_df)')
    (57, '')
    (58, '')
    (59, '# function to read a benchmark file')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tanaes/snakemake_assemble, file=bin/snakefiles/report
context_key: ['if m']
    (60, "def read_benchmark(fp, pattern = \\'^.*benchmarks/(.+?)/(.+?)\\\\.sample=\\\\[(.+?)\\\\](\\\\.abund_sample=\\\\[(.+?)\\\\])?\\\\.txt$\\'):")
    (61, '    # check to make sure .txt filepath matches expected format')
    (62, '    m = re.search(pattern, fp)')
    (63, '')
    (64, '    if m:')
    (65, "        benchmarks = pd.read_csv(fp, sep=\\'\\\\t\\', header=0)")
    (66, '        ')
    (67, "        benchmarks[\\'benchmark_file\\'] = fp")
    (68, "        benchmarks[\\'rule\\'] = m.group(2)")
    (69, "        benchmarks[\\'module\\'] = m.group(1)")
    (70, "        benchmarks[\\'sample\\'] = m.group(3)")
    (71, "        benchmarks[\\'abund_sample\\'] = m.group(5)")
    (72, '')
    (73, '        ')
    (74, '        return(benchmarks)')
    (75, '    else:')
    (76, '        return(None)')
    (77, '')
    (78, '')
    (79, '# function to load all benchmarks into a data frame')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if replace_only_if_true and string or not replace_only_if_true']
    (473, 'def apply_format(string, replace_format, replace_token="{}", replace_only_if_true=True):')
    (474, '    if replace_only_if_true and string or not replace_only_if_true:')
    (475, '        return replace_format.replace(replace_token, string)')
    (476, '')
    (477, '    return string')
    (478, '')
    (479, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if replace_only_if_true and string or not replace_only_if_true']
    (401, 'def apply_format(string, replace_format, replace_token="{}", replace_only_if_true=True):')
    (402, '    if replace_only_if_true and string or not replace_only_if_true:')
    (403, '        return replace_format.replace(replace_token, string)')
    (404, '')
    (405, '    return string')
    (406, '')
    (407, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/Peaks_workflow, file=Snakefile
context_key: ['if wildcards.read == "R1"']
    (87, 'def get_orig_fastq(wildcards):')
    (88, '    if wildcards.read == "R1":')
    (89, '            fastq = expand("raw_data/{fq}", fq = units[units["sample"] == wildcards.sample]["fq1"].values)')
    (90, '    elif wildcards.read == "R2":')
    (91, '            fastq = expand("raw_data/{fq}", fq = units[units["sample"] == wildcards.sample]["fq2"].values)')
    (92, '    return fastq ')
    (93, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/Peaks_workflow, file=Snakefile
context_key: ['if (wildcards.norm_type == "csaw_bkgd")']
    (525, 'def get_bigwig_norm_factors_file(wildcards):')
    (526, "    curr_enriched = samples[samples[\\'sample\\']==wildcards.sample][\\'enriched_factor\\'].values[0]")
    (527, '    if (wildcards.norm_type == "csaw_bkgd"):')
    (528, '        return "analysis/bigwig_norm_factors/{enriched_factor}_csaw_bkgd.tsv".format(enriched_factor=curr_enriched)')
    (529, '    if (wildcards.norm_type == "csaw_hiAbund"):')
    (530, '        return "analysis/bigwig_norm_factors/{enriched_factor}_csaw_hiAbund.tsv".format(enriched_factor=curr_enriched)')
    (531, '    if (wildcards.norm_type == "ecoli"):')
    (532, '        return "analysis/bigwig_norm_factors/cutnrun_ecoli_scale_factors.tsv"')
    (533, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/Peaks_workflow, file=Snakefile
context_key: ['if (config[\\\'atacseq\\\'] and wildcards.macs2_type == "macs2_nfr")']
    (861, 'def get_macs2_bams(wildcards):')
    (862, "    # pass deduped bams to macs2. Note that BAMs in \\'filt_bams_nfr/\\' directory are deduped.")
    (863, '    if (config[\\\'atacseq\\\'] and wildcards.macs2_type == "macs2_nfr"):')
    (864, '        macs2_bams = { \\\'trt\\\': "analysis/filt_bams_nfr/{sample}_filt_alns_nfr.bam".format(sample=wildcards.sample) }')
    (865, '    elif (wildcards.macs2_type == "macs2"):')
    (866, '        macs2_bams = { \\\'trt\\\': "analysis/dedup_bams/{sample}_filt_alns.dedup.bam".format(sample=wildcards.sample) }')
    (867, '    ')
    (868, "    control = samples[samples[\\'sample\\'] == wildcards.sample][\\'control\\'].values[0]")
    (869, '    ')
    (870, '    if (not pd.isnull(control)):')
    (871, '        macs2_bams[\\\'control\\\'] = expand("analysis/dedup_bams/{sample}_filt_alns.dedup.bam", sample = control.split(\\\',\\\'))')
    (872, '    return macs2_bams')
    (873, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/Peaks_workflow, file=Snakefile
context_key: ['if wildcards.peak_type == "macs2_narrow"']
    (1111, 'def get_peaks_for_merging (wildcards):')
    (1112, '    if wildcards.peak_type == "macs2_narrow":')
    (1113, '        return(expand("analysis/macs2/{sample}_macs2_narrow_peaks.narrowPeak", sample=samples_no_controls[\\\'sample\\\'].values))')
    (1114, '    if wildcards.peak_type == "macs2_broad":')
    (1115, '        return(expand("analysis/macs2/{sample}_macs2_broad_peaks.broadPeak", sample=samples_no_controls[\\\'sample\\\'].values))')
    (1116, '    if wildcards.peak_type == "macs2_nfr_narrow":')
    (1117, '        return(expand("analysis/macs2_nfr/{sample}_macs2_narrow_peaks.narrowPeak", sample=samples_no_controls[\\\'sample\\\'].values))')
    (1118, '    if wildcards.peak_type == "macs2_nfr_broad":')
    (1119, '        return(expand("analysis/macs2_nfr/{sample}_macs2_broad_peaks.broadPeak", sample=samples_no_controls[\\\'sample\\\'].values))')
    (1120, '    if wildcards.peak_type == "hmmratac_nfr":')
    (1121, '        return(expand("analysis/hmmratac/{sample}_peaks.filteredPeaks.openOnly.bed", sample=samples_no_controls[\\\'sample\\\'].values))')
    (1122, '    if wildcards.peak_type == "macs2_ENCODE_atac_narrow":')
    (1123, '        return(expand("analysis/macs2_ENCODE_atac/{sample}_macs2_narrow_peaks.narrowPeak", sample=samples_no_controls[\\\'sample\\\'].values))')
    (1124, '    if wildcards.peak_type == "macs2_ENCODE_atac_broad":')
    (1125, '        return(expand("analysis/macs2_ENCODE_atac/{sample}_macs2_broad_peaks.broadPeak", sample=samples_no_controls[\\\'sample\\\'].values))')
    (1126, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/Peaks_workflow, file=Snakefile
context_key: ["if (config[\\'atacseq\\'])", "if (config[\\'run_hmmratac\\'])"]
    (1255, 'def get_peaks_for_venn (wildcards):')
    (1256, '    peaks = {')
    (1257, '            "broad": expand("analysis/macs2/rm_blacklist/{sample}_macs2_broad_peaks.rm_blacklist.broadPeak", sample=samples_no_controls[\\\'sample\\\'].values),')
    (1258, '            "narrow": expand("analysis/macs2/rm_blacklist/{sample}_macs2_narrow_peaks.rm_blacklist.narrowPeak", sample=samples_no_controls[\\\'sample\\\'].values)    ')
    (1259, '    }')
    (1260, "    if (config[\\'atacseq\\']):")
    (1261, '        peaks[\\\'broad_nfr\\\'] = expand("analysis/macs2_nfr/rm_blacklist/{sample}_macs2_broad_peaks.rm_blacklist.broadPeak", sample=samples_no_controls[\\\'sample\\\'].values)')
    (1262, '        peaks[\\\'narrow_nfr\\\'] = expand("analysis/macs2_nfr/rm_blacklist/{sample}_macs2_narrow_peaks.rm_blacklist.narrowPeak", sample=samples_no_controls[\\\'sample\\\'].values)')
    (1263, '        peaks[\\\'broad_ENCODE_atac\\\'] = expand("analysis/macs2_ENCODE_atac/rm_blacklist/{sample}_macs2_broad_peaks.rm_blacklist.broadPeak", sample=samples_no_controls[\\\'sample\\\'].values)')
    (1264, '        peaks[\\\'narrow_ENCODE_atac\\\'] = expand("analysis/macs2_ENCODE_atac/rm_blacklist/{sample}_macs2_narrow_peaks.rm_blacklist.narrowPeak", sample=samples_no_controls[\\\'sample\\\'].values)')
    (1265, "        if (config[\\'run_hmmratac\\']):")
    (1266, '            peaks[\\\'hmmratac_nfr\\\'] = expand("analysis/hmmratac/{sample}_peaks.filteredPeaks.openOnly.bed", sample=samples_no_controls[\\\'sample\\\'].values)')
    (1267, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/Peaks_workflow, file=Snakefile
context_key: ["if (config[\\'atacseq\\'])"]
    (1255, 'def get_peaks_for_venn (wildcards):')
    (1256, '    peaks = {')
    (1257, '            "broad": expand("analysis/macs2/rm_blacklist/{sample}_macs2_broad_peaks.rm_blacklist.broadPeak", sample=samples_no_controls[\\\'sample\\\'].values),')
    (1258, '            "narrow": expand("analysis/macs2/rm_blacklist/{sample}_macs2_narrow_peaks.rm_blacklist.narrowPeak", sample=samples_no_controls[\\\'sample\\\'].values)    ')
    (1259, '    }')
    (1260, "    if (config[\\'atacseq\\']):")
    (1261, '        peaks[\\\'broad_nfr\\\'] = expand("analysis/macs2_nfr/rm_blacklist/{sample}_macs2_broad_peaks.rm_blacklist.broadPeak", sample=samples_no_controls[\\\'sample\\\'].values)')
    (1262, '        peaks[\\\'narrow_nfr\\\'] = expand("analysis/macs2_nfr/rm_blacklist/{sample}_macs2_narrow_peaks.rm_blacklist.narrowPeak", sample=samples_no_controls[\\\'sample\\\'].values)')
    (1263, '        peaks[\\\'broad_ENCODE_atac\\\'] = expand("analysis/macs2_ENCODE_atac/rm_blacklist/{sample}_macs2_broad_peaks.rm_blacklist.broadPeak", sample=samples_no_controls[\\\'sample\\\'].values)')
    (1264, '        peaks[\\\'narrow_ENCODE_atac\\\'] = expand("analysis/macs2_ENCODE_atac/rm_blacklist/{sample}_macs2_narrow_peaks.rm_blacklist.narrowPeak", sample=samples_no_controls[\\\'sample\\\'].values)')
    (1265, "        if (config[\\'run_hmmratac\\']):")
    (1266, '            peaks[\\\'hmmratac_nfr\\\'] = expand("analysis/hmmratac/{sample}_peaks.filteredPeaks.openOnly.bed", sample=samples_no_controls[\\\'sample\\\'].values)')
    (1267, '')
    (1268, '    return peaks')
    (1269, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Biochemistry1-FFM/uORF-Tools, file=Snakefile
context_key: ['onstart', 'if not os.path.exists("logs")']
    (13, 'def replicate_check(samples):')
    (14, "    if(samples[\\'replicate\\'].nunique() < 2):")
    (15, '        print("Warning: Please make sure your experiment contains replicates!")')
    (16, '')
    (17, '')
    (18, "with pd.option_context(\\'display.max_rows\\', None, \\'display.max_columns\\', None):")
    (19, '    print(samples)')
    (20, '')
    (21, 'validate(samples, schema="schemas/samples.schema.yaml")')
    (22, '')
    (23, 'onstart:')
    (24, '    if not os.path.exists("logs"):')
    (25, '        os.makedirs("logs")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/prep_bbc_shared, file=Snakefile
context_key: ['if len(species_ids_list) > 1']
    (661, 'def get_species_and_spikein_seqs(wildcards):')
    (662, '    # get the comma-separated species ids and prefixes')
    (663, "    species_ids = hybrid_genomes[hybrid_genomes.id == wildcards.hybrid_id][\\'species_ids\\'].values[0]")
    (664, "    species_prefs = hybrid_genomes[hybrid_genomes.id == wildcards.hybrid_id][\\'species_prefs\\'].values[0]")
    (665, '')
    (666, '    # split into a list')
    (667, '    species_ids_list = species_ids.split(",")')
    (668, '')
    (669, '    if len(species_ids_list) > 1:')
    (670, '        species_prefs_list = species_prefs.split(",")')
    (671, '')
    (672, '        # get the original files for the first species')
    (673, '        first_species_paths = ["data/{id}/sequence/{id}.fa".format(id=species_ids_list[0])]')
    (674, '')
    (675, '        # get the species-prefixed files for the 2nd and up to the last species')
    (676, '        other_species_paths = expand("data/{id}/add_species_prefs_for_hybrid/{id}.{pref}_prefixed.fa", zip, id=species_ids_list[1:], pref=species_prefs_list[1:])')
    (677, '')
    (678, '        species_paths = first_species_paths + other_species_paths')
    (679, '    else:')
    (680, '        # else get the original species files')
    (681, '        species_paths = ["data/{id}/sequence/{id}.fa".format(id=species_ids)]')
    (682, '')
    (683, '    # get spikein ids')
    (684, "    spikein_ids = hybrid_genomes[hybrid_genomes.id == wildcards.hybrid_id][\\'spikein_ids\\'].values[0]")
    (685, '')
    (686, '    if not pd.isnull(spikein_ids):')
    (687, '        # split into a list')
    (688, '        spikein_ids_list = spikein_ids.split(",")')
    (689, '        # get the corresponding filenames')
    (690, "        spikein_filenames = [spikeins[spikeins.spikein_id == spikein_id][\\'spikein_fasta\\'].values[0] for spikein_id in spikein_ids_list]")
    (691, '        # setup paths')
    (692, '        spikein_paths = expand("bin/spikeins/sequence/{spikein_file}", spikein_file = spikein_filenames)')
    (693, '')
    (694, '        input_paths = species_paths + spikein_paths')
    (695, '    else:')
    (696, '        input_paths = species_paths')
    (697, '')
    (698, '    return input_paths')
    (699, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/prep_bbc_shared, file=Snakefile
context_key: ['if len(species_ids_list) > 1']
    (723, 'def get_species_and_spikein_gtfs(wildcards):')
    (724, '    # get the comma-separated species ids and prefixes')
    (725, "    species_ids = hybrid_genomes[hybrid_genomes.id == wildcards.hybrid_id][\\'species_ids\\'].values[0]")
    (726, "    species_prefs = hybrid_genomes[hybrid_genomes.id == wildcards.hybrid_id][\\'species_prefs\\'].values[0]")
    (727, '')
    (728, '    # split into a list')
    (729, '    species_ids_list = species_ids.split(",")')
    (730, '')
    (731, '    if len(species_ids_list) > 1:')
    (732, '        species_prefs_list = species_prefs.split(",")')
    (733, '')
    (734, '        # get the original files for the first species')
    (735, '        first_species_paths = ["data/{id}/annotation/{id}.gtf".format(id=species_ids_list[0])]')
    (736, '')
    (737, '        # get the species-prefixed files for the 2nd and up to the last species')
    (738, '        other_species_paths = expand("data/{id}/add_species_prefs_for_hybrid/{id}.{pref}_prefixed.gtf", zip, id=species_ids_list[1:], pref=species_prefs_list[1:])')
    (739, '')
    (740, '        species_paths = first_species_paths + other_species_paths')
    (741, '')
    (742, '    else:')
    (743, '        # else get the original species files')
    (744, '        species_paths = ["data/{id}/annotation/{id}.gtf".format(id=species_ids)]')
    (745, '')
    (746, '    # get spikein ids')
    (747, "    spikein_ids = hybrid_genomes[hybrid_genomes.id == wildcards.hybrid_id][\\'spikein_ids\\'].values[0]")
    (748, '')
    (749, '    if not pd.isnull(spikein_ids):')
    (750, '        # split into a list')
    (751, '        spikein_ids_list = spikein_ids.split(",")')
    (752, '        # get the corresponding filenames')
    (753, "        spikein_filenames = [spikeins[spikeins.spikein_id == spikein_id][\\'spikein_gtf\\'].values[0] for spikein_id in spikein_ids_list]")
    (754, '        # setup paths')
    (755, '        spikein_paths = ["bin/spikeins/annotation/{filename}".format(filename=x) for x in spikein_filenames if str(x) != \\\'nan\\\']')
    (756, '')
    (757, '        input_paths = species_paths + spikein_paths')
    (758, '    else:')
    (759, '        input_paths = species_paths')
    (760, '')
    (761, '    return input_paths')
    (762, '')
    (763, "# need to keep cat_hybrid_seq and cat_hybrid_gtf separate because some spikeins don\\'t have a GTF file.")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sebschmi/snakemake-turso, file=Snakefile
context_key: ['try', 'if species == "ecoli"']
    (128, 'def get_reads_url(wildcards):')
    (129, '    # Snakemake just swallows whatever errors happen in functions.')
    (130, '    # The only way around that is to handle and print the error ourselves.')
    (131, '    try:')
    (132, '        species = wildcards.species')
    (133, '')
    (134, '        if species == "ecoli":')
    (135, '            return "https://sra-downloadb.be-md.ncbi.nlm.nih.gov/sos3/sra-pub-run-20/SRR12793243/SRR12793243.1"')
    (136, '        elif species == "scerevisiae":')
    (137, '            return "https://sra-download.ncbi.nlm.nih.gov/traces/sra78/SRR/014398/SRR14744387"')
    (138, '        else:')
    (139, '            raise Exception("Unknown species: {}".format(species))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pangenome/phage-evo-paper, file=workflow/Snakefile
context_key: ["if re.match(\\'^[\\\\w\\\\d]{7}$\\', git_hash)"]
    (17, "def get_current_HEAD_hash(default=\\'unnamed\\'):")
    (18, '    shell_CL = "git show-ref --head --heads --abbrev | grep HEAD | awk \\\'{print $1}\\\'"')
    (19, '    with os.popen(shell_CL) as f:')
    (20, '        git_hash = f.read().strip()')
    (21, "        if re.match(\\'^[\\\\w\\\\d]{7}$\\', git_hash):")
    (22, '            return git_hash')
    (23, '        return default')
    (24, '# FUNCTIONS:1 ends here')
    (25, '')
    (26, '# [[file:../main.org::*GLOBAL VARIABLES][GLOBAL VARIABLES:1]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=steveped/GRAVI, file=workflow/Snakefile
context_key: ['if not all_exist']
    (17, 'def get_here_file():')
    (18, '\\twd = os.getcwd()')
    (19, '\\trproj = os.path.basename(wd) + ".Rproj"')
    (20, '\\trproj_path = os.path.join(wd, rproj)')
    (21, '\\there_file = os.path.join(wd, ".here")')
    (22, '\\tif os.path.isfile(rproj_path):')
    (23, '\\t\\t## Check contents of file for Version in line 1')
    (24, '\\t\\twith open(rproj_path) as f:')
    (25, '\\t\\t\\tln = f.readline().rstrip()')
    (26, "\\t\\tif \\'Version:\\' in ln:")
    (27, '\\t\\t\\there_file = rproj_path')
    (28, '\\treturn(here_file)')
    (29, '')
    (30, '####################################')
    (31, '## Check all external files exist ##')
    (32, '####################################')
    (33, 'all_exist=True')
    (34, "# if config[\\'external\\'][\\'rnaseq\\'] != \\'\\':")
    (35, "# \\tif not os.path.isfile(config[\\'external\\'][\\'rnaseq\\']):")
    (36, '# \\t\\tall_exist=False')
    (37, '# \\t\\tprint(config[\\\'external\\\'][\\\'rnaseq\\\'] + " does not exist")')
    (38, '')
    (39, '')
    (40, 'if not all_exist:')
    (41, '\\tsys.exit(1)')
    (42, '')
    (43, 'def get_ucsc_genome(x):')
    (44, "\\tmap = pd.Series([\\'hg19\\', \\'hg38\\'], index = [\\'GRCh37\\', \\'GRCh38\\'])")
    (45, '\\tif not x in map.keys():')
    (46, '\\t\\tprint("The only currently supported genome builds are:")')
    (47, "\\t\\tprint(*map.keys(), sep = \\' & \\')")
    (48, '\\t\\tsys.exit(1)')
    (49, '\\telse:')
    (50, '\\t\\treturn(map[x])')
    (51, '')
    (52, 'def check_git(x = "."):')
    (53, '\\tis_installed = subprocess.run(')
    (54, "    \\t[\\'which\\', \\'git\\'], universal_newlines=True, check=True,")
    (55, '    \\tstdout=subprocess.PIPE')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=steveped/GRAVI, file=workflow/Snakefile
context_key: ['if type(fig_type) is list', 'if pairs', 'rule all', 'input']
    (85, 'def make_pairwise(x):')
    (86, '\\tret_val = []')
    (87, '\\tall_cont = make_contrasts(x)')
    (88, '\\tall_cont.sort()')
    (89, '\\tall_pairs = list(')
    (90, '\\t\\titertools.combinations(all_cont, 2)')
    (91, '\\t)')
    (92, '\\tfor p in all_pairs:')
    (93, '\\t\\tret_val.extend([p[0] + "_" + p[1]])')
    (94, '\\treturn(ret_val)')
    (95, '')
    (96, '')
    (97, '')
    (98, '###############################################################')
    (99, '## Check whether Git is installed & the directory has a repo ##')
    (100, '###############################################################')
    (101, 'git_add = check_git(".")')
    (102, 'git_tries = 100')
    (103, '# git_add = False')
    (104, '')
    (105, 'max_threads = workflow.cores')
    (106, '')
    (107, '####################')
    (108, '## Define Samples ##')
    (109, '####################')
    (110, "df = pd.read_table(config[\\'samples\\'][\\'file\\'])")
    (111, '')
    (112, '## Now set all values as required')
    (113, "samples = list(set(df[\\'sample\\']))")
    (114, "targets = list(set(df[\\'target\\']))")
    (115, "treats = list(set(df[\\'treat\\']))")
    (116, "pairs=make_pairwise(config[\\'comparisons\\'][\\'contrasts\\'])")
    (117, '')
    (118, '###############')
    (119, '## Key Paths ##')
    (120, '###############')
    (121, 'here_file = get_here_file()')
    (122, "bam_path = config[\\'paths\\'][\\'bam\\']")
    (123, 'rmd_path = "analysis"')
    (124, 'annotation_path = os.path.join("output", "annotations")')
    (125, 'diff_path = os.path.join("output", "differential_binding")')
    (126, 'macs2_path = os.path.join("output", "macs2")')
    (127, 'log_path = os.path.join("workflow", "logs")')
    (128, '')
    (129, '')
    (130, '#################')
    (131, '## Figure Type ##')
    (132, '#################')
    (133, 'rmdconfigfile = "config/rmarkdown.yml"')
    (134, 'rmd_config = yaml.safe_load(open(rmdconfigfile))')
    (135, "fig_type = rmd_config[\\'knitr_opts\\'][\\'dev\\']")
    (136, 'if type(fig_type) is list:')
    (137, '\\tfig_type = fig_type[0]')
    (138, '')
    (139, '')
    (140, '###############################')
    (141, '## External Annotation Files ##')
    (142, '###############################')
    (143, '## These are required input for multiple steps')
    (144, "ucsc_build = get_ucsc_genome(config[\\'genome\\'][\\'build\\'])")
    (145, 'gtf = os.path.join(')
    (146, '\\tannotation_path,')
    (147, '\\t"gencode.v" + config[\\\'genome\\\'][\\\'gencode\\\'] + "lift" +')
    (148, "\\tconfig[\\'genome\\'][\\'build\\'][-2:] +")
    (149, '\\t".annotation.gtf.gz"')
    (150, ')')
    (151, 'blacklist = os.path.join(annotation_path, "blacklist.bed.gz")')
    (152, 'chrom_sizes = os.path.join(annotation_path, "chrom.sizes")')
    (153, '')
    (154, '#####################')
    (155, '## Prepare Outputs ##')
    (156, '#####################')
    (157, 'ALL_OUTPUTS = []')
    (158, '')
    (159, '#####################################')
    (160, '## Annotations Defined in worfklow ##')
    (161, '#####################################')
    (162, '')
    (163, '## These are dependent on H3K27Ac being a target. If so, promoters, enhancers')
    (164, '## and super-enhancers will be created. If not, only the tss regions will be')
    (165, '## created')
    (166, 'ALL_RDS = expand(')
    (167, '\\tos.path.join(annotation_path, "{file}.rds"),')
    (168, "\\tfile = [\\'all_gr\\', \\'colours\\', \\'gene_regions\\', \\'seqinfo\\', \\'trans_models\\',\\'tss\\']")
    (169, ')')
    (170, 'ALL_OUTPUTS.extend(ALL_RDS)')
    (171, '')
    (172, '#######################')
    (173, '## Rmarkdown Outputs ##')
    (174, '#######################')
    (175, 'HTML_OUT = expand(')
    (176, '\\tos.path.join("docs", "{file}.html"),')
    (177, "\\tfile = [\\'annotation_description\\']")
    (178, ')')
    (179, '')
    (180, '## Macs2 Summaries')
    (181, 'HTML_OUT.extend(')
    (182, '\\texpand(')
    (183, '\\t\\tos.path.join("docs", "{target}_macs2_summary.html"),')
    (184, '\\t\\ttarget = targets')
    (185, '\\t)')
    (186, ')')
    (187, '')
    (188, '## Differential Binding')
    (189, 'HTML_OUT.extend(')
    (190, '\\texpand(')
    (191, '\\t\\tos.path.join("docs", "{cont}_differential_binding.html"),')
    (192, "\\t\\tcont = make_contrasts(config[\\'comparisons\\'][\\'contrasts\\'])")
    (193, '\\t)')
    (194, ')')
    (195, '')
    (196, '## Pairwise Comparisons: Only if required')
    (197, 'if pairs:')
    (198, '\\tHTML_OUT.extend(')
    (199, '\\t\\texpand(')
    (200, '\\t\\t\\tos.path.join("docs", "{comp}_pairwise_comparison.html"),')
    (201, '\\t\\t\\tcomp = pairs')
    (202, '\\t\\t)')
    (203, '\\t)')
    (204, '')
    (205, '')
    (206, 'ALL_OUTPUTS.extend(HTML_OUT)')
    (207, '## Keep the final index separate for easiers passing to other rules')
    (208, 'ALL_OUTPUTS.extend([os.path.join("docs", "index.html")])')
    (209, '')
    (210, '## Peaks generated from the Rmd files')
    (211, 'CONS_PEAKS = expand(')
    (212, '\\tos.path.join(macs2_path, "{target}", "{file}"),')
    (213, '\\ttarget = targets,')
    (214, "\\tfile = [\\'consensus_peaks.bed\\', \\'oracle_peaks.rds\\']")
    (215, ')')
    (216, 'ALL_OUTPUTS.extend(CONS_PEAKS)')
    (217, '')
    (218, '')
    (219, '###########################')
    (220, '## Peak Files from macs2 ##')
    (221, '###########################')
    (222, "indiv_pre = df[[\\'target\\', \\'sample\\']].apply(")
    (223, "\\tlambda row: \\'/\\'.join(row.values.astype(str)), axis=1")
    (224, ')')
    (225, 'INDIV_PEAKS = expand(')
    (226, '\\tos.path.join(macs2_path, "{path}_peaks.narrowPeak"),')
    (227, '\\tpath = indiv_pre')
    (228, ')')
    (229, 'ALL_OUTPUTS.extend(INDIV_PEAKS)')
    (230, 'merged_pre = set(')
    (231, "\\tdf[[\\'target\\', \\'treat\\']].apply(")
    (232, "\\t\\tlambda row: \\'/\\'.join(row.values.astype(str)), axis=1")
    (233, '\\t)')
    (234, ')')
    (235, 'MERGED_PEAKS = expand(')
    (236, '\\tos.path.join(macs2_path, "{pre}_merged_peaks.narrowPeak"),')
    (237, '\\tpre = merged_pre')
    (238, ')')
    (239, 'ALL_OUTPUTS.extend(MERGED_PEAKS)')
    (240, '')
    (241, '')
    (242, '##################')
    (243, '## BigWig Files ##')
    (244, '##################')
    (245, 'INDIV_BW = expand(')
    (246, '\\tos.path.join(macs2_path, "{path}_treat_pileup.{suffix}"),')
    (247, '\\tpath = indiv_pre,')
    (248, "\\tsuffix = [\\'bw\\']#, \\'summary\\']")
    (249, ')')
    (250, 'ALL_OUTPUTS.extend(INDIV_BW)')
    (251, 'MERGED_BW = expand(')
    (252, '\\tos.path.join(macs2_path, "{path}_merged_treat_pileup.{suffix}"),')
    (253, '\\tpath = merged_pre,')
    (254, "\\tsuffix = [\\'bw\\', \\'summary\\']")
    (255, ')')
    (256, 'ALL_OUTPUTS.extend(MERGED_BW)')
    (257, '')
    (258, 'rule all:')
    (259, '    input:')
    (260, '        ALL_OUTPUTS')
    (261, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=genomewalker/aMGSIM-smk, file=Snakefile
context_key: ['if not all']
    (5, 'def get_md5sum(x):')
    (6, '    import hashlib')
    (7, '')
    (8, '    return hashlib.md5(x.encode("utf-8")).hexdigest()[:10]')
    (9, '')
    (10, '')
    (11, '"""')
    (12, 'Author: A. Fernandez-Guerra')
    (13, 'Affiliation: Lundbeck Foundation GeoGenetics Centre')
    (14, 'Aim: Create synthetic data to test metaDMG')
    (15, 'Run: snakemake   -s Snakefile')
    (16, '"""')
    (17, '#')
    (18, '##### set minimum snakemake version #####')
    (19, 'min_version("5.20.1")')
    (20, '')
    (21, '# configfile: "config/config.yaml"')
    (22, '# report: "report/workflow.rst"')
    (23, '')
    (24, '# This should be placed in the Snakefile.')
    (25, '')
    (26, '"""')
    (27, 'Working directory')
    (28, '"""')
    (29, '')
    (30, '')
    (31, 'workdir: config["wdir"]')
    (32, '')
    (33, '')
    (34, '# message("The current working directory is " + WDIR)')
    (35, '')
    (36, '"""')
    (37, ' The list of samples to be processed')
    (38, '"""')
    (39, '')
    (40, 'sample_table_read = pd.read_table(')
    (41, '    config["sample_file_read"], sep="\\\\t", lineterminator="\\')
    (42, '"')
    (43, ')')
    (44, '')
    (45, "# let\\'s check that the basic columns are present")
    (46, 'if not all(')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=genomewalker/aMGSIM-smk, file=Snakefile
context_key: ['if not "libprep" in sample_table_read.columns']
    (5, 'def get_md5sum(x):')
    (6, '    import hashlib')
    (7, '')
    (8, '    return hashlib.md5(x.encode("utf-8")).hexdigest()[:10]')
    (9, '')
    (10, '')
    (11, '"""')
    (12, 'Author: A. Fernandez-Guerra')
    (13, 'Affiliation: Lundbeck Foundation GeoGenetics Centre')
    (14, 'Aim: Create synthetic data to test metaDMG')
    (15, 'Run: snakemake   -s Snakefile')
    (16, '"""')
    (17, '#')
    (18, '##### set minimum snakemake version #####')
    (19, 'min_version("5.20.1")')
    (20, '')
    (21, '# configfile: "config/config.yaml"')
    (22, '# report: "report/workflow.rst"')
    (23, '')
    (24, '# This should be placed in the Snakefile.')
    (25, '')
    (26, '"""')
    (27, 'Working directory')
    (28, '"""')
    (29, '')
    (30, '')
    (31, 'workdir: config["wdir"]')
    (32, '')
    (33, '')
    (34, '# message("The current working directory is " + WDIR)')
    (35, '')
    (36, '"""')
    (37, ' The list of samples to be processed')
    (38, '"""')
    (39, '')
    (40, 'sample_table_read = pd.read_table(')
    (41, '    config["sample_file_read"], sep="\\\\t", lineterminator="\\')
    (42, '"')
    (43, ')')
    (44, '')
    (45, "# let\\'s check that the basic columns are present")
    (46, 'if not all(')
    (47, '    item in sample_table_read.columns')
    (48, '    for item in [')
    (49, '        "label",')
    (50, '        "libprep",')
    (51, '        "read_length_freqs_file",')
    (52, '        "mapping_stats_filtered_file",')
    (53, '        "metadmg_results",')
    (54, '        "metadmg_misincorporations",')
    (55, '    ]')
    (56, '):')
    (57, '    raise ValueError("The sample table must contain the columns \\\'label\\\' and \\\'file\\\'")')
    (58, '    exit(1)')
    (59, '')
    (60, 'if not "libprep" in sample_table_read.columns:')
    (61, '    sample_table_read["libprep"] = "double"')
    (62, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=genomewalker/aMGSIM-smk, file=Snakefile
context_key: ['if not "short_label" in sample_table_read.columns']
    (5, 'def get_md5sum(x):')
    (6, '    import hashlib')
    (7, '')
    (8, '    return hashlib.md5(x.encode("utf-8")).hexdigest()[:10]')
    (9, '')
    (10, '')
    (11, '"""')
    (12, 'Author: A. Fernandez-Guerra')
    (13, 'Affiliation: Lundbeck Foundation GeoGenetics Centre')
    (14, 'Aim: Create synthetic data to test metaDMG')
    (15, 'Run: snakemake   -s Snakefile')
    (16, '"""')
    (17, '#')
    (18, '##### set minimum snakemake version #####')
    (19, 'min_version("5.20.1")')
    (20, '')
    (21, '# configfile: "config/config.yaml"')
    (22, '# report: "report/workflow.rst"')
    (23, '')
    (24, '# This should be placed in the Snakefile.')
    (25, '')
    (26, '"""')
    (27, 'Working directory')
    (28, '"""')
    (29, '')
    (30, '')
    (31, 'workdir: config["wdir"]')
    (32, '')
    (33, '')
    (34, '# message("The current working directory is " + WDIR)')
    (35, '')
    (36, '"""')
    (37, ' The list of samples to be processed')
    (38, '"""')
    (39, '')
    (40, 'sample_table_read = pd.read_table(')
    (41, '    config["sample_file_read"], sep="\\\\t", lineterminator="\\')
    (42, '"')
    (43, ')')
    (44, '')
    (45, "# let\\'s check that the basic columns are present")
    (46, 'if not all(')
    (47, '    item in sample_table_read.columns')
    (48, '    for item in [')
    (49, '        "label",')
    (50, '        "libprep",')
    (51, '        "read_length_freqs_file",')
    (52, '        "mapping_stats_filtered_file",')
    (53, '        "metadmg_results",')
    (54, '        "metadmg_misincorporations",')
    (55, '    ]')
    (56, '):')
    (57, '    raise ValueError("The sample table must contain the columns \\\'label\\\' and \\\'file\\\'")')
    (58, '    exit(1)')
    (59, '')
    (60, 'if not "libprep" in sample_table_read.columns:')
    (61, '    sample_table_read["libprep"] = "double"')
    (62, '')
    (63, 'sample_table_read = sample_table_read.drop_duplicates(')
    (64, '    subset="label", keep="first", inplace=False')
    (65, ')')
    (66, 'sample_table_read = sample_table_read.dropna()')
    (67, '')
    (68, 'if not "short_label" in sample_table_read.columns:')
    (69, '    sample_table_read["short_label"] = sample_table_read.apply(')
    (70, '        lambda row: get_md5sum(row.label), axis=1')
    (71, '    )')
    (72, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cgob/codonDT_snakemake, file=Snakefile
context_key: ["if df[df[\\'SAMPLES\\'] == sample_rna].size == 0"]
    (44, 'def get_rna_from_ribo(wildcards):')
    (45, "    sample_name = str(wildcards.sample).split(\\'_RIBO\\')[0]")
    (46, '    sample_rna = sample_name + "_RNA"')
    (47, '')
    (48, "    if df[df[\\'SAMPLES\\'] == sample_rna].size == 0:")
    (49, '        return "Data/Counting/" + str(wildcards.sample) + "_ncount.RData"')
    (50, '    else:')
    (51, '        return "Data/Fit/" + sample_rna + "_fit_" + str(wildcards.pair) + ".RData"')
    (52, '')
    (53, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no")']
    (50, 'def get_cohort_snp_filtering_command(resource):')
    (51, '    """Return a string, a portion of the gatk VariantRecalibrator command (used in the gatk_VariantRecalibrator_snp rule)')
    (52, '    (for cohort analyses) which dynamically includes each of the recalibration resources defined by the user in the')
    (53, '    configuration file. For each recalibration resource (element in the list), we construct the command by adding')
    (54, '    --resource:<recalibration resource file>')
    (55, '    """')
    (56, '    ')
    (57, '    command = ""')
    (58, '')
    (59, '    if (config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no"):')
    (60, "        for resource,param in zip(config[\\'COHORT\\'][\\'SNPS\\'][\\'RESOURCES\\'], config[\\'COHORT\\'][\\'SNPS\\'][\\'PARAMS\\']):")
    (61, '            command += "--resource:" + param + " " + resource + " "')
    (62, '    if (config[\\\'GPU_ACCELERATED\\\'] == "Yes" or config[\\\'GPU_ACCELERATED\\\'] == "yes"):')
    (63, "        for resource,param in zip(config[\\'COHORT\\'][\\'SNPS\\'][\\'RESOURCES\\'], config[\\'COHORT\\'][\\'SNPS\\'][\\'PARAMS\\']):")
    (64, '            command += "--resource " + param + ":" + resource + " "')
    (65, '')
    (66, '    return command')
    (67, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no")']
    (68, 'def get_cohort_indel_filtering_command(resource):')
    (69, '    """Return a string, a portion of the gatk VariantRecalibrator command (used in the gatk_VariantRecalibrator_indel rule)')
    (70, '    (for cohort analyses) which dynamically includes each of the recalibration resources defined by the user in the')
    (71, '    configuration file. For each recalibration resource (element in the list), we construct the command by adding')
    (72, '    --resource:<recalibration resource file>')
    (73, '    """')
    (74, '')
    (75, '    command = ""')
    (76, '')
    (77, '    if (config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no"):')
    (78, "        for resource,param in zip(config[\\'COHORT\\'][\\'INDELS\\'][\\'RESOURCES\\'], config[\\'COHORT\\'][\\'INDELS\\'][\\'PARAMS\\']):")
    (79, '            command += "--resource:" + param + " " + resource + " "')
    (80, '    if (config[\\\'GPU_ACCELERATED\\\'] == "Yes" or config[\\\'GPU_ACCELERATED\\\'] == "yes"):')
    (81, "        for resource,param in zip(config[\\'COHORT\\'][\\'INDELS\\'][\\'RESOURCES\\'], config[\\'COHORT\\'][\\'INDELS\\'][\\'PARAMS\\']):")
    (82, '            command += "--resource " + param + ":" + resource + " "')
    (83, '')
    (84, '    return command')
    (85, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\') and (config[\\\'PREPARE_FOR_SCOUT\\\'] == "No" or config[\\\'PREPARE_FOR_SCOUT\\\'] == \\\'no\\\')', 'rule all', 'input']
    (86, 'def get_single_filtering_command(resource):')
    (87, '    """Return a string, a portion of the gatk FilterVariantTranches command (used in the gatk_FilterVariantTranches rule)')
    (88, '    (for single sample analyses) which dynamically includes each of the recalibration resources defined by the user in the')
    (89, '    configuration file. For each recalibration resource (element in the list), we construct the command by adding')
    (90, '    --resource <recalibration resource file>')
    (91, '    """')
    (92, '')
    (93, '    command = ""')
    (94, '    ')
    (95, "    for resource in config[\\'SINGLE\\'][\\'RESOURCES\\']:")
    (96, '        command += "--resource " + resource + " "')
    (97, '')
    (98, '    return command')
    (99, '')
    (100, '##### Set up report #####')
    (101, '')
    (102, 'report: "report/workflow.rst"')
    (103, '')
    (104, '')
    (105, '##### Target rules #####')
    (106, '')
    (107, 'if (config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\') and (config[\\\'PREPARE_FOR_SCOUT\\\'] == "No" or config[\\\'PREPARE_FOR_SCOUT\\\'] == \\\'no\\\'):')
    (108, '    rule all:')
    (109, '           input:')
    (110, '               expand("../results/filtered/{sample}_filtered.vcf", sample = SAMPLES),')
    (111, '               expand("../results/annotated/{sample}_filtered_annotated.vcf", sample = SAMPLES)')
    (112, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nioo-knaw/PhyloFunDB, file=Snakefile
context_key: ['if os.path.exists("interm/{gene}.aligned.good.filter.pick.good.names")']
    (255, 'def get_namesfile(gene=config["gene"]):')
    (256, '    if os.path.exists("interm/{gene}.aligned.good.filter.pick.good.names"):')
    (257, '       return "interm/{gene}.aligned.good.filter.pick.good.names"')
    (258, '    else:')
    (259, '       return "interm/{gene}.aligned.good.filter.pick.names"')
    (260, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if kind == "bed"']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['else [']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
    (183, '    pileup_resolution_limits = config["pileups"]["resolution_limits"]')
    (184, '    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]')
    (185, '    pileup_resolutions = list(')
    (186, '        filter(')
    (187, '            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],')
    (188, '            resolutions,')
    (189, '        )')
    (190, '    )')
    (191, '    mindists = [')
    (192, '        int(pileups_mindist * 2**i)')
    (193, '        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))')
    (194, '    ]')
    (195, '    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]')
    (196, '')
    (197, 'if config["insulation"]["do"]:')
    (198, '    insul_res_win = []')
    (199, '    for resolution in config["insulation"]["resolutions"]:')
    (200, '        for win in config["insulation"]["resolutions"][resolution]:')
    (201, '            insul_res_win.append(f"{resolution}_{win}")')
    (202, '')
    (203, 'if config["call_TADs"]["do"]:')
    (204, '    tad_res_win = []')
    (205, '    for resolution in config["call_TADs"]["resolutions"]:')
    (206, '        for win in config["call_TADs"]["resolutions"][resolution]:')
    (207, '            tad_res_win.append(f"{resolution}_{win}")')
    (208, '')
    (209, "# chroms = cooler.Cooler(f\\'{coolers_folder}/{coolfiles[0]}::resolutions/{resolutions[0]}\\').chroms()[:]")
    (210, '')
    (211, '# bedpe_mindists = [int(bedpe_pileups_mindist*2**i) for i in np.arange(0, np.log2(bedpe_pileups_maxdist/bedpe_pileups_mindist))]')
    (212, "# bedpe_separations = [f\\'{mindist}-{mindist*2}\\' for mindist in bedpe_mindists]")
    (213, '')
    (214, 'expecteds = expand(')
    (215, '    f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",')
    (216, '    sample=samples,')
    (217, '    resolution=resolutions,')
    (218, ')')
    (219, '')
    (220, 'diff_boundaries = (')
    (221, '    expand(')
    (222, '        f"{boundary_folder}/Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (223, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}.bed",')
    (224, '        insul_res_win=insul_res_win,')
    (225, '    )')
    (226, '    if config["compare_boundaries"]["do"]')
    (227, '    else []')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['else [']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
    (183, '    pileup_resolution_limits = config["pileups"]["resolution_limits"]')
    (184, '    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]')
    (185, '    pileup_resolutions = list(')
    (186, '        filter(')
    (187, '            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],')
    (188, '            resolutions,')
    (189, '        )')
    (190, '    )')
    (191, '    mindists = [')
    (192, '        int(pileups_mindist * 2**i)')
    (193, '        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))')
    (194, '    ]')
    (195, '    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]')
    (196, '')
    (197, 'if config["insulation"]["do"]:')
    (198, '    insul_res_win = []')
    (199, '    for resolution in config["insulation"]["resolutions"]:')
    (200, '        for win in config["insulation"]["resolutions"][resolution]:')
    (201, '            insul_res_win.append(f"{resolution}_{win}")')
    (202, '')
    (203, 'if config["call_TADs"]["do"]:')
    (204, '    tad_res_win = []')
    (205, '    for resolution in config["call_TADs"]["resolutions"]:')
    (206, '        for win in config["call_TADs"]["resolutions"][resolution]:')
    (207, '            tad_res_win.append(f"{resolution}_{win}")')
    (208, '')
    (209, "# chroms = cooler.Cooler(f\\'{coolers_folder}/{coolfiles[0]}::resolutions/{resolutions[0]}\\').chroms()[:]")
    (210, '')
    (211, '# bedpe_mindists = [int(bedpe_pileups_mindist*2**i) for i in np.arange(0, np.log2(bedpe_pileups_maxdist/bedpe_pileups_mindist))]')
    (212, "# bedpe_separations = [f\\'{mindist}-{mindist*2}\\' for mindist in bedpe_mindists]")
    (213, '')
    (214, 'expecteds = expand(')
    (215, '    f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",')
    (216, '    sample=samples,')
    (217, '    resolution=resolutions,')
    (218, ')')
    (219, '')
    (220, 'diff_boundaries = (')
    (221, '    expand(')
    (222, '        f"{boundary_folder}/Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (223, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}.bed",')
    (224, '        insul_res_win=insul_res_win,')
    (225, '    )')
    (226, '    if config["compare_boundaries"]["do"]')
    (227, '    else []')
    (228, ')')
    (229, 'diff_boundaries_pileups = (')
    (230, '    expand(')
    (231, '        f"{pileups_folder}/{boundary_folder_name}/{{sample}}-{{resolution}}_over_Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (232, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}_{{norm}}_local.clpy",')
    (233, '        sample=samples,')
    (234, '        resolution=pileup_resolutions,')
    (235, '        insul_res_win=insul_res_win,')
    (236, '        norm=pileup_norms,')
    (237, '    )')
    (238, '    if config["compare_boundaries"]["do"] and config["pileups"]["do"]')
    (239, '    else []')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['else [']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
    (183, '    pileup_resolution_limits = config["pileups"]["resolution_limits"]')
    (184, '    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]')
    (185, '    pileup_resolutions = list(')
    (186, '        filter(')
    (187, '            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],')
    (188, '            resolutions,')
    (189, '        )')
    (190, '    )')
    (191, '    mindists = [')
    (192, '        int(pileups_mindist * 2**i)')
    (193, '        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))')
    (194, '    ]')
    (195, '    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]')
    (196, '')
    (197, 'if config["insulation"]["do"]:')
    (198, '    insul_res_win = []')
    (199, '    for resolution in config["insulation"]["resolutions"]:')
    (200, '        for win in config["insulation"]["resolutions"][resolution]:')
    (201, '            insul_res_win.append(f"{resolution}_{win}")')
    (202, '')
    (203, 'if config["call_TADs"]["do"]:')
    (204, '    tad_res_win = []')
    (205, '    for resolution in config["call_TADs"]["resolutions"]:')
    (206, '        for win in config["call_TADs"]["resolutions"][resolution]:')
    (207, '            tad_res_win.append(f"{resolution}_{win}")')
    (208, '')
    (209, "# chroms = cooler.Cooler(f\\'{coolers_folder}/{coolfiles[0]}::resolutions/{resolutions[0]}\\').chroms()[:]")
    (210, '')
    (211, '# bedpe_mindists = [int(bedpe_pileups_mindist*2**i) for i in np.arange(0, np.log2(bedpe_pileups_maxdist/bedpe_pileups_mindist))]')
    (212, "# bedpe_separations = [f\\'{mindist}-{mindist*2}\\' for mindist in bedpe_mindists]")
    (213, '')
    (214, 'expecteds = expand(')
    (215, '    f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",')
    (216, '    sample=samples,')
    (217, '    resolution=resolutions,')
    (218, ')')
    (219, '')
    (220, 'diff_boundaries = (')
    (221, '    expand(')
    (222, '        f"{boundary_folder}/Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (223, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}.bed",')
    (224, '        insul_res_win=insul_res_win,')
    (225, '    )')
    (226, '    if config["compare_boundaries"]["do"]')
    (227, '    else []')
    (228, ')')
    (229, 'diff_boundaries_pileups = (')
    (230, '    expand(')
    (231, '        f"{pileups_folder}/{boundary_folder_name}/{{sample}}-{{resolution}}_over_Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (232, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}_{{norm}}_local.clpy",')
    (233, '        sample=samples,')
    (234, '        resolution=pileup_resolutions,')
    (235, '        insul_res_win=insul_res_win,')
    (236, '        norm=pileup_norms,')
    (237, '    )')
    (238, '    if config["compare_boundaries"]["do"] and config["pileups"]["do"]')
    (239, '    else []')
    (240, ')')
    (241, '')
    (242, 'tads = (')
    (243, '    expand(')
    (244, '        f"{tad_folder}/TADs_{{sampleTADs}}_{{tad_res_win}}.bed",')
    (245, '        sampleTADs=config["call_TADs"]["samples"],')
    (246, '        tad_res_win=tad_res_win,')
    (247, '    )')
    (248, '    if config["call_TADs"]["do"]')
    (249, '    else []')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['else [']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
    (183, '    pileup_resolution_limits = config["pileups"]["resolution_limits"]')
    (184, '    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]')
    (185, '    pileup_resolutions = list(')
    (186, '        filter(')
    (187, '            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],')
    (188, '            resolutions,')
    (189, '        )')
    (190, '    )')
    (191, '    mindists = [')
    (192, '        int(pileups_mindist * 2**i)')
    (193, '        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))')
    (194, '    ]')
    (195, '    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]')
    (196, '')
    (197, 'if config["insulation"]["do"]:')
    (198, '    insul_res_win = []')
    (199, '    for resolution in config["insulation"]["resolutions"]:')
    (200, '        for win in config["insulation"]["resolutions"][resolution]:')
    (201, '            insul_res_win.append(f"{resolution}_{win}")')
    (202, '')
    (203, 'if config["call_TADs"]["do"]:')
    (204, '    tad_res_win = []')
    (205, '    for resolution in config["call_TADs"]["resolutions"]:')
    (206, '        for win in config["call_TADs"]["resolutions"][resolution]:')
    (207, '            tad_res_win.append(f"{resolution}_{win}")')
    (208, '')
    (209, "# chroms = cooler.Cooler(f\\'{coolers_folder}/{coolfiles[0]}::resolutions/{resolutions[0]}\\').chroms()[:]")
    (210, '')
    (211, '# bedpe_mindists = [int(bedpe_pileups_mindist*2**i) for i in np.arange(0, np.log2(bedpe_pileups_maxdist/bedpe_pileups_mindist))]')
    (212, "# bedpe_separations = [f\\'{mindist}-{mindist*2}\\' for mindist in bedpe_mindists]")
    (213, '')
    (214, 'expecteds = expand(')
    (215, '    f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",')
    (216, '    sample=samples,')
    (217, '    resolution=resolutions,')
    (218, ')')
    (219, '')
    (220, 'diff_boundaries = (')
    (221, '    expand(')
    (222, '        f"{boundary_folder}/Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (223, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}.bed",')
    (224, '        insul_res_win=insul_res_win,')
    (225, '    )')
    (226, '    if config["compare_boundaries"]["do"]')
    (227, '    else []')
    (228, ')')
    (229, 'diff_boundaries_pileups = (')
    (230, '    expand(')
    (231, '        f"{pileups_folder}/{boundary_folder_name}/{{sample}}-{{resolution}}_over_Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (232, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}_{{norm}}_local.clpy",')
    (233, '        sample=samples,')
    (234, '        resolution=pileup_resolutions,')
    (235, '        insul_res_win=insul_res_win,')
    (236, '        norm=pileup_norms,')
    (237, '    )')
    (238, '    if config["compare_boundaries"]["do"] and config["pileups"]["do"]')
    (239, '    else []')
    (240, ')')
    (241, '')
    (242, 'tads = (')
    (243, '    expand(')
    (244, '        f"{tad_folder}/TADs_{{sampleTADs}}_{{tad_res_win}}.bed",')
    (245, '        sampleTADs=config["call_TADs"]["samples"],')
    (246, '        tad_res_win=tad_res_win,')
    (247, '    )')
    (248, '    if config["call_TADs"]["do"]')
    (249, '    else []')
    (250, ')')
    (251, '')
    (252, 'tads_pileups = (')
    (253, '    expand(')
    (254, '        f"{pileups_folder}/{tad_folder_name}/{{sample}}-{{resolution}}_over_TADs_{{sampleTADs}}_{{tad_res_win}}_{{norm}}_local_rescaled.clpy",')
    (255, '        sample=samples,')
    (256, '        resolution=pileup_resolutions,')
    (257, '        sampleTADs=config["call_TADs"]["samples"],')
    (258, '        tad_res_win=tad_res_win,')
    (259, '        norm=pileup_norms,')
    (260, '    )')
    (261, '    if config["call_TADs"]["do"] and config["pileups"]["do"]')
    (262, '    else []')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if dot_methods']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
    (183, '    pileup_resolution_limits = config["pileups"]["resolution_limits"]')
    (184, '    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]')
    (185, '    pileup_resolutions = list(')
    (186, '        filter(')
    (187, '            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],')
    (188, '            resolutions,')
    (189, '        )')
    (190, '    )')
    (191, '    mindists = [')
    (192, '        int(pileups_mindist * 2**i)')
    (193, '        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))')
    (194, '    ]')
    (195, '    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]')
    (196, '')
    (197, 'if config["insulation"]["do"]:')
    (198, '    insul_res_win = []')
    (199, '    for resolution in config["insulation"]["resolutions"]:')
    (200, '        for win in config["insulation"]["resolutions"][resolution]:')
    (201, '            insul_res_win.append(f"{resolution}_{win}")')
    (202, '')
    (203, 'if config["call_TADs"]["do"]:')
    (204, '    tad_res_win = []')
    (205, '    for resolution in config["call_TADs"]["resolutions"]:')
    (206, '        for win in config["call_TADs"]["resolutions"][resolution]:')
    (207, '            tad_res_win.append(f"{resolution}_{win}")')
    (208, '')
    (209, "# chroms = cooler.Cooler(f\\'{coolers_folder}/{coolfiles[0]}::resolutions/{resolutions[0]}\\').chroms()[:]")
    (210, '')
    (211, '# bedpe_mindists = [int(bedpe_pileups_mindist*2**i) for i in np.arange(0, np.log2(bedpe_pileups_maxdist/bedpe_pileups_mindist))]')
    (212, "# bedpe_separations = [f\\'{mindist}-{mindist*2}\\' for mindist in bedpe_mindists]")
    (213, '')
    (214, 'expecteds = expand(')
    (215, '    f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",')
    (216, '    sample=samples,')
    (217, '    resolution=resolutions,')
    (218, ')')
    (219, '')
    (220, 'diff_boundaries = (')
    (221, '    expand(')
    (222, '        f"{boundary_folder}/Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (223, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}.bed",')
    (224, '        insul_res_win=insul_res_win,')
    (225, '    )')
    (226, '    if config["compare_boundaries"]["do"]')
    (227, '    else []')
    (228, ')')
    (229, 'diff_boundaries_pileups = (')
    (230, '    expand(')
    (231, '        f"{pileups_folder}/{boundary_folder_name}/{{sample}}-{{resolution}}_over_Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (232, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}_{{norm}}_local.clpy",')
    (233, '        sample=samples,')
    (234, '        resolution=pileup_resolutions,')
    (235, '        insul_res_win=insul_res_win,')
    (236, '        norm=pileup_norms,')
    (237, '    )')
    (238, '    if config["compare_boundaries"]["do"] and config["pileups"]["do"]')
    (239, '    else []')
    (240, ')')
    (241, '')
    (242, 'tads = (')
    (243, '    expand(')
    (244, '        f"{tad_folder}/TADs_{{sampleTADs}}_{{tad_res_win}}.bed",')
    (245, '        sampleTADs=config["call_TADs"]["samples"],')
    (246, '        tad_res_win=tad_res_win,')
    (247, '    )')
    (248, '    if config["call_TADs"]["do"]')
    (249, '    else []')
    (250, ')')
    (251, '')
    (252, 'tads_pileups = (')
    (253, '    expand(')
    (254, '        f"{pileups_folder}/{tad_folder_name}/{{sample}}-{{resolution}}_over_TADs_{{sampleTADs}}_{{tad_res_win}}_{{norm}}_local_rescaled.clpy",')
    (255, '        sample=samples,')
    (256, '        resolution=pileup_resolutions,')
    (257, '        sampleTADs=config["call_TADs"]["samples"],')
    (258, '        tad_res_win=tad_res_win,')
    (259, '        norm=pileup_norms,')
    (260, '    )')
    (261, '    if config["call_TADs"]["do"] and config["pileups"]["do"]')
    (262, '    else []')
    (263, ')')
    (264, 'dot_methods = [')
    (265, '    m for m in config["call_dots"]["methods"] if config["call_dots"]["methods"][m]["do"]')
    (266, ']')
    (267, 'if dot_methods:')
    (268, '    loops = expand(')
    (269, '        f"{loop_folder}/merged_resolutions/Loops_{{method}}_{{sampleLoops}}.bedpe",')
    (270, '        method=dot_methods,')
    (271, '        sampleLoops=config["call_dots"]["samples"],')
    (272, '    )')
    (273, '')
    (274, '    loops_pileups = (')
    (275, '        expand(')
    (276, '            f"{pileups_folder}/{loop_folder_name}/{{sample}}-{{resolution}}_over_Loops_{{method}}_{{sampleLoops}}_{{norm}}_{{mode}}.clpy",')
    (277, '            sample=samples,')
    (278, '            resolution=pileup_resolutions,')
    (279, '            method=dot_methods,')
    (280, '            sampleLoops=config["call_dots"]["samples"],')
    (281, '            norm=pileup_norms,')
    (282, '            mode=["distal", "by_distance"],')
    (283, '        )')
    (284, '        if config["pileups"]["do"]')
    (285, '        else []')
    (286, '    )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['else [']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
    (183, '    pileup_resolution_limits = config["pileups"]["resolution_limits"]')
    (184, '    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]')
    (185, '    pileup_resolutions = list(')
    (186, '        filter(')
    (187, '            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],')
    (188, '            resolutions,')
    (189, '        )')
    (190, '    )')
    (191, '    mindists = [')
    (192, '        int(pileups_mindist * 2**i)')
    (193, '        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))')
    (194, '    ]')
    (195, '    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]')
    (196, '')
    (197, 'if config["insulation"]["do"]:')
    (198, '    insul_res_win = []')
    (199, '    for resolution in config["insulation"]["resolutions"]:')
    (200, '        for win in config["insulation"]["resolutions"][resolution]:')
    (201, '            insul_res_win.append(f"{resolution}_{win}")')
    (202, '')
    (203, 'if config["call_TADs"]["do"]:')
    (204, '    tad_res_win = []')
    (205, '    for resolution in config["call_TADs"]["resolutions"]:')
    (206, '        for win in config["call_TADs"]["resolutions"][resolution]:')
    (207, '            tad_res_win.append(f"{resolution}_{win}")')
    (208, '')
    (209, "# chroms = cooler.Cooler(f\\'{coolers_folder}/{coolfiles[0]}::resolutions/{resolutions[0]}\\').chroms()[:]")
    (210, '')
    (211, '# bedpe_mindists = [int(bedpe_pileups_mindist*2**i) for i in np.arange(0, np.log2(bedpe_pileups_maxdist/bedpe_pileups_mindist))]')
    (212, "# bedpe_separations = [f\\'{mindist}-{mindist*2}\\' for mindist in bedpe_mindists]")
    (213, '')
    (214, 'expecteds = expand(')
    (215, '    f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",')
    (216, '    sample=samples,')
    (217, '    resolution=resolutions,')
    (218, ')')
    (219, '')
    (220, 'diff_boundaries = (')
    (221, '    expand(')
    (222, '        f"{boundary_folder}/Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (223, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}.bed",')
    (224, '        insul_res_win=insul_res_win,')
    (225, '    )')
    (226, '    if config["compare_boundaries"]["do"]')
    (227, '    else []')
    (228, ')')
    (229, 'diff_boundaries_pileups = (')
    (230, '    expand(')
    (231, '        f"{pileups_folder}/{boundary_folder_name}/{{sample}}-{{resolution}}_over_Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (232, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}_{{norm}}_local.clpy",')
    (233, '        sample=samples,')
    (234, '        resolution=pileup_resolutions,')
    (235, '        insul_res_win=insul_res_win,')
    (236, '        norm=pileup_norms,')
    (237, '    )')
    (238, '    if config["compare_boundaries"]["do"] and config["pileups"]["do"]')
    (239, '    else []')
    (240, ')')
    (241, '')
    (242, 'tads = (')
    (243, '    expand(')
    (244, '        f"{tad_folder}/TADs_{{sampleTADs}}_{{tad_res_win}}.bed",')
    (245, '        sampleTADs=config["call_TADs"]["samples"],')
    (246, '        tad_res_win=tad_res_win,')
    (247, '    )')
    (248, '    if config["call_TADs"]["do"]')
    (249, '    else []')
    (250, ')')
    (251, '')
    (252, 'tads_pileups = (')
    (253, '    expand(')
    (254, '        f"{pileups_folder}/{tad_folder_name}/{{sample}}-{{resolution}}_over_TADs_{{sampleTADs}}_{{tad_res_win}}_{{norm}}_local_rescaled.clpy",')
    (255, '        sample=samples,')
    (256, '        resolution=pileup_resolutions,')
    (257, '        sampleTADs=config["call_TADs"]["samples"],')
    (258, '        tad_res_win=tad_res_win,')
    (259, '        norm=pileup_norms,')
    (260, '    )')
    (261, '    if config["call_TADs"]["do"] and config["pileups"]["do"]')
    (262, '    else []')
    (263, ')')
    (264, 'dot_methods = [')
    (265, '    m for m in config["call_dots"]["methods"] if config["call_dots"]["methods"][m]["do"]')
    (266, ']')
    (267, 'if dot_methods:')
    (268, '    loops = expand(')
    (269, '        f"{loop_folder}/merged_resolutions/Loops_{{method}}_{{sampleLoops}}.bedpe",')
    (270, '        method=dot_methods,')
    (271, '        sampleLoops=config["call_dots"]["samples"],')
    (272, '    )')
    (273, '')
    (274, '    loops_pileups = (')
    (275, '        expand(')
    (276, '            f"{pileups_folder}/{loop_folder_name}/{{sample}}-{{resolution}}_over_Loops_{{method}}_{{sampleLoops}}_{{norm}}_{{mode}}.clpy",')
    (277, '            sample=samples,')
    (278, '            resolution=pileup_resolutions,')
    (279, '            method=dot_methods,')
    (280, '            sampleLoops=config["call_dots"]["samples"],')
    (281, '            norm=pileup_norms,')
    (282, '            mode=["distal", "by_distance"],')
    (283, '        )')
    (284, '        if config["pileups"]["do"]')
    (285, '        else []')
    (286, '    )')
    (287, 'else:')
    (288, '    loops = []')
    (289, '    loops_pileups = []')
    (290, '')
    (291, 'for file in loops:')
    (292, '    name = path.splitext(path.basename(file))[0]')
    (293, '    bedfiles_dict[name] = file')
    (294, '    bedtype_dict[name] = "bedpe"')
    (295, 'for file in tads + diff_boundaries:')
    (296, '    name = path.splitext(path.basename(file))[0]')
    (297, '    bedfiles_dict[name] = file')
    (298, '    bedtype_dict[name] = "bed"')
    (299, '')
    (300, 'beds_pileups = []')
    (301, 'if config["pileups"]["do"]:')
    (302, '    for bedname, row in bed_df.iterrows():')
    (303, '        modes = []')
    (304, '        for mode in pileup_params.keys():')
    (305, '            if row[mode]:')
    (306, '                modes += [mode]')
    (307, '')
    (308, '        for sample in samples:')
    (309, '            if sample not in samples_annotations.index:')
    (310, '                continue')
    (311, '            if (')
    (312, '                bedname in samples_annotations.columns')
    (313, '                and not samples_annotations.loc[sample, bedname]')
    (314, '            ):')
    (315, '                continue')
    (316, '            beds_pileups += expand(')
    (317, '                f"{pileups_folder}/{beds_folder_name}/{sample}-{{resolution}}_over_{bedname}_{{norm}}_{{mode}}.clpy",')
    (318, '                resolution=pileup_resolutions,')
    (319, '                norm=pileup_norms,')
    (320, '                mode=modes,')
    (321, '            )')
    (322, '')
    (323, 'saddles = (')
    (324, '    expand(')
    (325, '        f"{saddles_folder}/{{sample}}_{{resolution}}_{{bins}}{{dist}}.{{ending}}",')
    (326, '        sample=samples,')
    (327, '        resolution=eigenvector_resolutions,')
    (328, '        bins=config["saddle"]["bins"],')
    (329, '        dist=saddle_separations + [""],')
    (330, '        ending=["saddledump.npz", "digitized.tsv"],')
    (331, '    )')
    (332, '    if config["saddle"]["do"]')
    (333, '    else []')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if dist_wildcard == ""']
    (337, 'def split_dist(dist_wildcard, mindist_arg="--mindist", maxdist_arg="--maxdist"):')
    (338, '    if dist_wildcard == "":')
    (339, '        return ""')
    (340, '    else:')
    (341, '        assert dist_wildcard.startswith("_dist_")')
    (342, '        dists = dist_wildcard.split("_")[-1]')
    (343, '        mindist, maxdist = dists.split("-")')
    (344, '        return f"{mindist_arg} {mindist} {maxdist_arg} {maxdist}"')
    (345, '')
    (346, '')
    (347, '###### Define rules #######')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if ires == jres']
    (399, 'def dedup_dots(dots, hiccups_filter=False):')
    (400, '    newdots = []')
    (401, '    ress = list(sorted(set(dots["res"])))')
    (402, '    for chrom in sorted(set(dots["chrom1"])):')
    (403, '        chromdots = (')
    (404, '            dots[dots["chrom1"] == chrom]')
    (405, '            .sort_values(["start1", "start2"])')
    (406, '            .reset_index(drop=True)')
    (407, '        )')
    (408, '        for res in ress:')
    (409, '            chromdots["Supported_%s" % res] = chromdots["res"] == res')
    (410, '        tree = spatial.cKDTree(chromdots[["start1", "start2"]])')
    (411, '        drop = []')
    (412, '        for i, j in tree.query_pairs(r=20000):')
    (413, '            ires = chromdots.at[i, "res"]')
    (414, '            jres = chromdots.at[j, "res"]')
    (415, '            chromdots.at[j, "Supported_%s" % ires] = True')
    (416, '            chromdots.at[i, "Supported_%s" % jres] = True')
    (417, '            if ires == jres:')
    (418, '                continue')
    (419, '            elif ires > jres:')
    (420, "                # if ress[-1] in (ires, jres) or abs(chromdots.at[j, \\'start1\\']-chromdots.at[i, \\'start1\\'])<=20000:")
    (421, '                drop.append(i)')
    (422, '            else:')
    (423, '                drop.append(j)')
    (424, '        newdots.append(chromdots.drop(drop))')
    (425, '    deduped = pd.concat(newdots).sort_values(["chrom1", "start1", "start2"])')
    (426, '    if hiccups_filter:')
    (427, '        l = len(deduped)')
    (428, '        deduped = deduped[')
    (429, '            ~(')
    (430, '                (deduped["start2"] - deduped["start1"] > 100000)')
    (431, '                & (~np.any(deduped[["Supported_%s" % res for res in ress[1:]]], axis=1))')
    (432, '            )')
    (433, '        ]')
    (434, '        print(')
    (435, '            l - len(deduped),')
    (436, '            "loops filtered out as unreliable %s resolution calls" % ress[0],')
    (437, '        )')
    (438, '    return deduped')
    (439, '')
    (440, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if re.findall("filename=(.+)", r.headers["content-disposition"])']
    (759, 'def download_file(file, local_filename):')
    (760, '    import requests')
    (761, '    import tqdm')
    (762, '    import re')
    (763, '')
    (764, '    with requests.get(file, stream=True) as r:')
    (765, '        ext_gz = (')
    (766, '            ".gz"')
    (767, '            if re.findall("filename=(.+)", r.headers["content-disposition"])[')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['else "']
    (759, 'def download_file(file, local_filename):')
    (760, '    import requests')
    (761, '    import tqdm')
    (762, '    import re')
    (763, '')
    (764, '    with requests.get(file, stream=True) as r:')
    (765, '        ext_gz = (')
    (766, '            ".gz"')
    (767, '            if re.findall("filename=(.+)", r.headers["content-disposition"])[')
    (768, '                0')
    (769, '            ].endswith(".gz")')
    (770, '            else ""')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if file == output']
    (780, 'def get_file(file, output):')
    (781, '    """')
    (782, "    If input is URL, it download via python\\'s requests. Uncompress if needed.")
    (783, '    """')
    (784, '    if file == output:')
    (785, '        exit()')
    (786, '')
    (787, '    from urllib.parse import urlparse')
    (788, '')
    (789, '    parsed_path = urlparse(file)')
    (790, '    if parsed_path.scheme == "http" or parsed_path.scheme == "https":')
    (791, '        output_file = download_file(file, output)')
    (792, '    else:')
    (793, '        raise Exception(')
    (794, '            f"Unable to download from: {file}\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if output_file.endswith(".gz")']
    (780, 'def get_file(file, output):')
    (781, '    """')
    (782, "    If input is URL, it download via python\\'s requests. Uncompress if needed.")
    (783, '    """')
    (784, '    if file == output:')
    (785, '        exit()')
    (786, '')
    (787, '    from urllib.parse import urlparse')
    (788, '')
    (789, '    parsed_path = urlparse(file)')
    (790, '    if parsed_path.scheme == "http" or parsed_path.scheme == "https":')
    (791, '        output_file = download_file(file, output)')
    (792, '    else:')
    (793, '        raise Exception(')
    (794, '            f"Unable to download from: {file}\\')
    (795, 'Scheme {parsed_url.scheme} is not supported"')
    (796, '        )')
    (797, '    if output_file.endswith(".gz"):')
    (798, '        shell(f"gzip -d {output_file}")')
    (799, '')
    (800, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=arabidopsisca/snakemake-rna-seq-kallisto-sleuth, file=workflow/rules/diffexp.smk
context_key: ['if wildcards.model == "all"']
    (21, 'def get_model(wildcards):')
    (22, '    if wildcards.model == "all":')
    (23, '        return {"full": None}')
    (24, '    return config["diffexp"]["models"][wildcards.model]')
    (25, '')
    (26, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/Genrich_compare, file=Snakefile
context_key: ['if not config["paired_end"]']
    (70, 'def get_input_files(wildcards):')
    (71, '    sample_name = "_".join(wildcards.sample.split("_")[0:-1])')
    (72, '    mark = wildcards.sample.split("_")[-1]')
    (73, '    if not config["paired_end"]:')
    (74, "        return FILES[sample_name][mark][\\'R1\\']")
    (75, '    ## start with bam files')
    (76, '    if not config["from_fastq"]:')
    (77, '        return FILES[sample_name][mark]')
    (78, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RBL-NCI/4SUseq, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(filename)']
    (8, 'def check_existence(filename):')
    (9, '    """Checks if file exists on filesystem')
    (10, '    :param filename <str>: Name of file to check')
    (11, '    """')
    (12, '    filename=filename.strip()')
    (13, '    if not os.path.exists(filename):')
    (14, '        sys.exit("File: {} does not exists!".format(filename))')
    (15, '    return True')
    (16, '')
    (17, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RBL-NCI/4SUseq, file=workflow/rules/init.smk
context_key: ['if not os.access(filename,os.R_OK)']
    (18, 'def check_readaccess(filename):')
    (19, '    """Checks permissions to see if user can read a file')
    (20, '    :param filename <str>: Name of file to check')
    (21, '    """')
    (22, '    filename=filename.strip()')
    (23, '    check_existence(filename)')
    (24, '    if not os.access(filename,os.R_OK):')
    (25, '        sys.exit("File: {} exists, but user cannot read from file due to permissions!".format(filename))')
    (26, '    return True')
    (27, '')
    (28, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RBL-NCI/4SUseq, file=workflow/rules/init.smk
context_key: ['if not os.access(filename,os.W_OK)']
    (29, 'def check_writeaccess(filename):')
    (30, '    """Checks permissions to see if user can write to a file')
    (31, '    :param filename <str>: Name of file to check')
    (32, '    """')
    (33, '    filename=filename.strip()')
    (34, '    check_existence(filename)')
    (35, '    if not os.access(filename,os.W_OK):')
    (36, '        sys.exit("File: {} exists, but user cannot write to file due to permissions!".format(filename))')
    (37, '    return True')
    (38, '')
    (39, '# this container defines the underlying OS for each job when using the workflow')
    (40, '# with --use-conda --use-singularity')
    (41, '# singularity: "docker://continuumio/miniconda3"')
    (42, '')
    (43, '##### load config and sample sheets #####')
    (44, '')
    (45, '# configfile: "config/config.yaml"')
    (46, '# validate(config, schema="../schemas/config.schema.yaml")')
    (47, '')
    (48, '# set memory')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RBL-NCI/4SUseq, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(RAWFASTQDIR)', 'if not os.path.exists(TRIMDIR)', 'if not os.path.exists(FASTUNIQDIR)', 'try']
    (29, 'def check_writeaccess(filename):')
    (30, '    """Checks permissions to see if user can write to a file')
    (31, '    :param filename <str>: Name of file to check')
    (32, '    """')
    (33, '    filename=filename.strip()')
    (34, '    check_existence(filename)')
    (35, '    if not os.access(filename,os.W_OK):')
    (36, '        sys.exit("File: {} exists, but user cannot write to file due to permissions!".format(filename))')
    (37, '    return True')
    (38, '')
    (39, '# this container defines the underlying OS for each job when using the workflow')
    (40, '# with --use-conda --use-singularity')
    (41, '# singularity: "docker://continuumio/miniconda3"')
    (42, '')
    (43, '##### load config and sample sheets #####')
    (44, '')
    (45, '# configfile: "config/config.yaml"')
    (46, '# validate(config, schema="../schemas/config.schema.yaml")')
    (47, '')
    (48, '# set memory')
    (49, 'MEMORY="200"')
    (50, '')
    (51, 'WORKDIR = config["workdir"]')
    (52, 'RESOURCESDIR = config["resourcesdir"]')
    (53, 'SCRIPTSDIR = config["scriptsdir"]')
    (54, '# define subfolders')
    (55, 'TRIMDIR = join(WORKDIR,"trim")')
    (56, 'FASTUNIQDIR = join(WORKDIR,"fastuniq")')
    (57, 'RAWFASTQDIR = join(WORKDIR,"raw_fastq")')
    (58, 'if not os.path.exists(RAWFASTQDIR):')
    (59, '\\tos.mkdir(RAWFASTQDIR)')
    (60, '')
    (61, 'SAMPLESDF = pd.read_csv(config["samples"],sep="\\\\t",header=0,index_col="sampleName")')
    (62, '')
    (63, 'SAMPLES = list(SAMPLESDF.index)')
    (64, '#now path to r1 fastq file for sampleA will be sampledf["path_to_R1_fastq"]["sampleA"]')
    (65, '# validate(SAMPLESDF, schema="../schemas/samples.schema.yaml")')
    (66, 'for sample in SAMPLES:')
    (67, '\\tf1=os.path.basename(SAMPLESDF["path_to_R1_fastq"][sample])')
    (68, '\\tf2=os.path.basename(SAMPLESDF["path_to_R2_fastq"][sample])')
    (69, '\\tif not os.path.exists(join(RAWFASTQDIR,f1)):')
    (70, '\\t\\tos.symlink(SAMPLESDF["path_to_R1_fastq"][sample],join(RAWFASTQDIR,f1))')
    (71, '\\tif not os.path.exists(join(RAWFASTQDIR,f2)):')
    (72, '\\t\\tos.symlink(SAMPLESDF["path_to_R2_fastq"][sample],join(RAWFASTQDIR,f2))')
    (73, '# print(SAMPLESDF)')
    (74, '')
    (75, '## Load tools from YAML file')
    (76, 'with open(config["tools"]) as f:')
    (77, '\\tTOOLS = yaml.safe_load(f)')
    (78, '# pprint.pprint(tools)')
    (79, '')
    (80, '')
    (81, 'if not os.path.exists(TRIMDIR):')
    (82, '\\tos.mkdir(TRIMDIR)')
    (83, 'if not os.path.exists(FASTUNIQDIR):')
    (84, '\\tos.mkdir(FASTUNIQDIR)')
    (85, '')
    (86, 'GENOME=config["genome"]')
    (87, 'GTF=config["gtf"][GENOME]')
    (88, 'check_readaccess(GTF)')
    (89, 'print("# ANNOTATION FILE :",GTF)')
    (90, 'REFFA=config["reffa"][GENOME]')
    (91, 'check_readaccess(REFFA)')
    (92, 'print("# REFERENCE FASTA :",REFFA)')
    (93, 'RSEMINDEXDIR=join(WORKDIR,"rsem_index")')
    (94, '')
    (95, '#########################################################')
    (96, '# READ CLUSTER PER-RULE REQUIREMENTS')
    (97, '#########################################################')
    (98, '')
    (99, '## Load cluster.json')
    (100, 'try:')
    (101, '    CLUSTERJSON = config["clusterjson"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khalillab/coop-TF-rnaseq, file=Snakefile
context_key: ['if "all" in groups']
    (38, 'def get_samples(status, groups):')
    (39, '    if "all" in groups:')
    (40, '        return(list(status_norm_sample_dict[status].keys()))')
    (41, '    else:')
    (42, '        return([k for k,v in status_norm_sample_dict[status].items() if v["group"] in groups])')
    (43, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khalillab/coop-TF-rnaseq, file=Snakefile
context_key: ['if strand in ["sense", "both"]']
    (44, 'def cluster_samples(status, cluster_groups, cluster_strands):')
    (45, '    ll = []')
    (46, '    for group, strand in zip(cluster_groups, cluster_strands):')
    (47, '        sublist = [k for k,v in status_norm_sample_dict[status].items() if v["group"] in cluster_groups]')
    (48, '        if strand in ["sense", "both"]:')
    (49, '            ll.append([f"{sample}-sense" for sample in sublist])')
    (50, '        if strand in ["antisense", "both"]:')
    (51, '            ll.append([f"{sample}-antisense" for sample in sublist])')
    (52, '    return(list(itertools.chain(*ll)))')
    (53, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tdayris/cel-cnv-eacon, file=rules/common.smk
context_key: ['if is_cyto_bool is True']
    (37, 'def EaCoN_in(wildcards) -> Dict[str, str]:')
    (38, '    """')
    (39, '    This function returns the correct couple of samples')
    (40, '    """')
    (41, '    if is_cyto_bool is True:')
    (42, '        return {')
    (43, '            "ATChannelCel": f"raw_data/{wildcards.sample}_A.CEL",')
    (44, '            "GCChannelCel": f"raw_data/{wildcards.sample}_C.CEL"')
    (45, '        }')
    (46, '    return {')
    (47, '        "CEL": f"raw_data/{wildcards.sample}.CEL"')
    (48, '    }')
    (49, '')
    (50, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tdayris/cel-cnv-eacon, file=rules/common.smk
context_key: ['if is_cyto_bool is True']
    (71, 'def paircheck_qc() -> List[str]:')
    (72, '    """')
    (73, '    Return several file extensions related to Cytoscan/Oncoscan')
    (74, '    """')
    (75, '    if is_cyto_bool is True:')
    (76, '        return ["qc.txt", "log", "paircheck.txt"]')
    (77, '    return ["qc.txt", "log"]')
    (78, '')
    (79, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tdayris/cel-cnv-eacon, file=rules/common.smk
context_key: ['if is_cyto_bool is True']
    (80, 'def plot_qc() -> List[str]:')
    (81, '    """')
    (82, '    Return several file extensions related to Cytoscan/Oncoscan')
    (83, '    """')
    (84, '    if is_cyto_bool is True:')
    (85, '        return [')
    (86, '            "pairs.txt",')
    (87, '            "{}_{}_rawplot.png".format(')
    (88, "                config[\\'params\\'][\\'arraytype\\'],")
    (89, "                config[\\'params\\'][\\'genome\\']")
    (90, '            )')
    (91, '        ]')
    (92, '    return ["CELfile.txt"]')
    (93, '')
    (94, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=deevdevil88/snakemake_alevin_10x, file=workflow/rules/common.smk
context_key: ['if len(fastq1) == 0']
    (18, 'def get_gex_fastq(wildcards):')
    (19, "    \\'\\'\\'")
    (20, '    Identify pairs of FASTQ files from the sample sheet.')
    (21, '    ')
    (22, '    wildcards')
    (23, '    - sample: name of the sample to process.')
    (24, "    \\'\\'\\'")
    (25, '    fastq1_pattern = config["pattern"]["read1"]')
    (26, '    fastq1_glob = f"data/{wildcards.sample}_fastqs/*{fastq1_pattern}*"')
    (27, '    fastq1 = glob.glob(fastq1_glob)')
    (28, '    ')
    (29, '    if len(fastq1) == 0:')
    (30, '        raise OSError(f"No file matched pattern: {fastq1_glob}")')
    (31, '    ')
    (32, '    fastq2 = [file.replace(config["pattern"]["read1"], config["pattern"]["read2"]) for file in fastq1]')
    (33, '    for file in fastq2:')
    (34, '        if not os.path.exists(file):')
    (35, '            raise OSError(f"Paired file not found: {file}")')
    (36, '    ')
    (37, "    return {\\'fastq1\\' : fastq1, \\'fastq2\\' : fastq2 }")
    (38, '')
    (39, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=deevdevil88/snakemake_alevin_10x, file=workflow/rules/common.smk
context_key: ['if expect_cells > 0']
    (40, 'def get_cells_option(wildcards):')
    (41, "    \\'\\'\\'")
    (42, '    Build an string of command line options from the sample sheet.')
    (43, '    ')
    (44, '    wildcards')
    (45, '    - sample: name of the sample to process.')
    (46, '    ')
    (47, "    Note that users should supply only one of \\'expect_cells\\' or \\'force_cells\\'.")
    (48, '    The other one should be set to 0, to be ignored.')
    (49, "    \\'\\'\\'")
    (50, '    option_str = ""')
    (51, '    ')
    (52, "    expect_cells = samples[\\'expect_cells\\'][wildcards.sample]")
    (53, "    force_cells = samples[\\'force_cells\\'][wildcards.sample]")
    (54, '')
    (55, '    if expect_cells > 0:')
    (56, '        option_str += f" --expectCells {expect_cells}"')
    (57, '    ')
    (58, '    if force_cells > 0:')
    (59, '        option_str += f" --forceCells {force_cells}"')
    (60, '    ')
    (61, "    return option_str'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/sequencing.smk
context_key: ['if metadata_file != ""']
    (129, 'def get_metadata_opt(wildcards):')
    (130, '    metadata_file = get_metadata_file(wildcards)')
    (131, '    if metadata_file != "":')
    (132, '        return "-m %s" % get_metadata_file(wildcards)')
    (133, '    else:')
    (134, '        return ""')
    (135, '')
    (136, '# plot coverage along the genome for each sample')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ['if "samples" in config']
    (19, 'def get_sample_names():')
    (20, '')
    (21, '    # if defined in the config, use that')
    (22, '    # otherwise we try to auto-detect based on the bam names')
    (23, '    if "samples" in config:')
    (24, '        return config["samples"]')
    (25, '')
    (26, '    # get all bams in the data directory based on the pattern')
    (27, '    # the bam files follow')
    (28, '    pattern = get_bam_pattern()')
    (29, '')
    (30, '    # form a glob we can use to get the bam files for each sample')
    (31, '    gs = pattern.format(data_root=config["data_root"], sample="*")')
    (32, '')
    (33, '    bams = glob.glob(gs)')
    (34, '    samples = list()')
    (35, '    for b in bams:')
    (36, '        f = os.path.basename(b)')
    (37, '        fields = f.split(".")')
    (38, '        samples.append(fields[0])')
    (39, '')
    (40, '    # remove any possible duplicates')
    (41, '    return list(set(samples))')
    (42, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ['if "negative_control_samples" in config']
    (43, 'def get_negative_control_samples():')
    (44, '    if "negative_control_samples" in config:')
    (45, '        return config["negative_control_samples"]')
    (46, '    else:')
    (47, '        return []')
    (48, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ['if os.path.exists(bam)']
    (49, 'def get_valid_negative_control_samples():')
    (50, '    valid_samples = list()')
    (51, '    bp = get_bam_pattern()')
    (52, '    for sample in get_negative_control_samples():')
    (53, '        bam = bp.format(data_root=config["data_root"], sample=sample)')
    (54, '        if os.path.exists(bam):')
    (55, '            valid_samples.append(sample)')
    (56, '    return valid_samples')
    (57, '')
    (58, '# negative controls may not have bams so we need to filter')
    (59, "# out those that don\\'t have alignments")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ['if os.path.exists(bam)']
    (60, 'def get_negative_control_bed(wildcards):')
    (61, '    bp = get_bam_pattern()')
    (62, '    out = list()')
    (63, '    for s in get_valid_negative_control_samples():')
    (64, '        bam = bp.format(data_root=config["data_root"], sample=s)')
    (65, '        if os.path.exists(bam):')
    (66, '            out.append("qc_sequencing/{sample}.amplicon_base_coverage.bed".format(sample=s))')
    (67, '    return out')
    (68, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ['if "primer_trimmed_bam_pattern" in config']
    (82, 'def get_primer_trimmed_bam_for_sample(wildcards):')
    (83, '')
    (84, '    # If the pattern has been provided in the config, use it')
    (85, '    if "primer_trimmed_bam_pattern" in config:')
    (86, '        return config["primer_trimmed_bam_pattern"].format(data_root=config["data_root"], sample=wildcards.sample)')
    (87, '')
    (88, '    # Otherwise, rely on what we expect the platform to produce')
    (89, '    if "platform" not in config:')
    (90, '        sys.stderr.write("Error: platform not defined in config")')
    (91, '        sys.exit(1)')
    (92, '')
    (93, "    if config[\\'platform\\'] == \\'oxford-nanopore\\':")
    (94, '        return "%s/{sample}.primertrimmed.rg.sorted.bam" % config[\\\'data_root\\\']')
    (95, "    elif config[\\'platform\\'] == \\'illumina\\':")
    (96, '        return "%s/{sample}.mapped.primertrimmed.sorted.bam" % config[\\\'data_root\\\']')
    (97, '    else:')
    (98, '        sys.stderr.write("Error: unrecognized platform")')
    (99, '        sys.exit(1)')
    (100, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ['if completeness != ""']
    (101, 'def get_completeness_threshold_opt(wildcards):')
    (102, '    completeness = get_completeness_threshold(wildcards)')
    (103, '    if completeness != "":')
    (104, '        return "--completeness %s" % (completeness)')
    (105, '    else:')
    (106, '        return ""')
    (107, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ['if "tree_include_consensus" in config']
    (108, 'def get_tree_consensus_sequences(wildcards):')
    (109, '    pattern = get_consensus_pattern()')
    (110, '    consensus_sequences = [pattern.format(data_root=config["data_root"], sample=s) for s in get_sample_names()]')
    (111, '')
    (112, '    if "tree_include_consensus" in config:')
    (113, '        consensus_sequences.append(config["tree_include_consensus"])')
    (114, '')
    (115, '    return consensus_sequences')
    (116, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ['if get_metadata_file(wildcards) != ""']
    (129, 'def get_qc_sequencing_plots(wildcards):')
    (130, '    prefix = get_run_name()')
    (131, '    out = [ "plots/%s_amplicon_covered_fraction.pdf" % (prefix),')
    (132, '            "plots/%s_depth_by_position.pdf" % (prefix) ]')
    (133, '')
    (134, '    # add plots that need Ct values')
    (135, '    if get_metadata_file(wildcards) != "":')
    (136, '        out.append("plots/%s_amplicon_depth_by_ct.pdf" % (prefix))')
    (137, '    return out')
    (138, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ['if get_snp_tree_flag()']
    (139, 'def get_qc_analysis_plots(wildcards):')
    (140, '    prefix = get_run_name()')
    (141, '    out = [ "plots/%s_amplicon_coverage_heatmap.pdf" % (prefix) ]')
    (142, '')
    (143, '    if get_snp_tree_flag():')
    (144, '        out.append("plots/%s_tree_snps.pdf" % (prefix))')
    (145, '')
    (146, '    return out')
    (147, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ['if metadata != ""']
    (152, 'def get_qc_summary_metadata_opt(wildcards):')
    (153, '    metadata = get_metadata_file(wildcards)')
    (154, '    if metadata != "":')
    (155, '        return "--meta %s" % (metadata)')
    (156, '    else:')
    (157, '        return ""')
    (158, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ['if "platform" in config']
    (170, 'def get_platform_opt(wildcards):')
    (171, '    if "platform" in config:')
    (172, '        return "--platform %s" % (config[\\\'platform\\\'])')
    (173, '    else:')
    (174, '        return ""')
    (175, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ['if get_snp_tree_flag()']
    (215, 'def get_report_tex_input(wildcards):')
    (216, '    out = get_qc_reports(wildcards)')
    (217, '')
    (218, '    if get_snp_tree_flag():')
    (219, '        out.append("plots/%s_tree_snps.pdf" % (wildcards.prefix))')
    (220, '')
    (221, '    if len(get_valid_negative_control_samples()) > 0:')
    (222, '        out.append("plots/%s_depth_by_position_negative_control.pdf" % (wildcards.prefix))')
    (223, '    return out')
    (224, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ['if "pangolin_version" in config']
    (237, 'def get_pangolin_version_opt(wildcards):')
    (238, '    if "pangolin_version" in config:')
    (239, '        return "--pangolin_ver %s" % (config[\\\'pangolin_version\\\'])')
    (240, '    else:')
    (241, '        return ""')
    (242, '')
    (243, '    ')
    (244, '')
    (245, '')
    (246, '')
    (247, '# generate the amplicon-level bed file from the input primer bed')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/annotation.smk
context_key: ['if is_vcf']
    (11, 'def get_vcf_file(wildcards):')
    (12, "    data_root = config[\\'data_root\\']")
    (13, '    variants_pattern = get_variants_pattern()')
    (14, '    is_vcf = variants_pattern.endswith(".vcf") or variants_pattern.endswith(".vcf.gz")')
    (15, '    ')
    (16, '    if is_vcf:')
    (17, "        pattern = variants_pattern.format(data_root=config[\\'data_root\\'], sample=wildcards.sample)")
    (18, '    else:')
    (19, '        # assume illumina ivar pipeline, this will trigger conversion')
    (20, "        assert(config[\\'platform\\'] == \\'illumina\\')")
    (21, '        pattern = "qc_annotation/{sample}.pass.vcf.gz"')
    (22, '    return pattern')
    (23, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/annotation.smk
context_key: ["if snpeff_dir.name.startswith(\\'snpeff\\')"]
    (48, 'def get_snpeff_dirs():')
    (49, '    snpeff_dirs = list()')
    (50, "    for snpeff_dir in os.scandir(\\'/\\'.join([os.environ[\\'CONDA_PREFIX\\'], \\'share\\'])):")
    (51, "        if snpeff_dir.name.startswith(\\'snpeff\\'):")
    (52, '            snpeff_dirs.append(snpeff_dir.name)')
    (53, '    return snpeff_dirs')
    (54, '')
    (55, '#')
    (56, '# Rules for annotating variants with functional consequence')
    (57, '#')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=scottdbrown/minion-plasmid-consensus, file=Snakefile
context_key: ['if len(lines) == setcount']
    (158, '            def process(lines=None):\\r')
    (159, "                ks = [\\'name\\', \\'sequence\\', \\'optional\\', \\'quality\\']\\r")
    (160, '                return {k: v for k, v in zip(ks, lines)}\\r')
    (161, '            \\r')
    (162, '            out = open(output.fastq, "w")\\r')
    (163, '            setcount = 4\\r')
    (164, '            num_seq_passed = 0\\r')
    (165, '            fastq_file = os.listdir(os.path.join(input.fastq_dir,"pass"))[0]\\r')
    (166, '            with open(os.path.join(input.fastq_dir,"pass",fastq_file), \\\'r\\\') as fh:\\r')
    (167, '                lines = []\\r')
    (168, '                for line in fh:\\r')
    (169, '                    lines.append(line.rstrip())\\r')
    (170, '                    if len(lines) == setcount:\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=scottdbrown/minion-plasmid-consensus, file=Snakefile
context_key: ['if len(record["sequence"]) >= params.min_length and len(record["sequence"]) <= params.max_length']
    (158, '            def process(lines=None):\\r')
    (159, "                ks = [\\'name\\', \\'sequence\\', \\'optional\\', \\'quality\\']\\r")
    (160, '                return {k: v for k, v in zip(ks, lines)}\\r')
    (161, '            \\r')
    (162, '            out = open(output.fastq, "w")\\r')
    (163, '            setcount = 4\\r')
    (164, '            num_seq_passed = 0\\r')
    (165, '            fastq_file = os.listdir(os.path.join(input.fastq_dir,"pass"))[0]\\r')
    (166, '            with open(os.path.join(input.fastq_dir,"pass",fastq_file), \\\'r\\\') as fh:\\r')
    (167, '                lines = []\\r')
    (168, '                for line in fh:\\r')
    (169, '                    lines.append(line.rstrip())\\r')
    (170, '                    if len(lines) == setcount:\\r')
    (171, '                        record = process(lines)\\r')
    (172, '\\r')
    (173, '                        if len(record["sequence"]) >= params.min_length and len(record["sequence"]) <= params.max_length:\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jaleezyy/covid-19-signal, file=Snakefile
context_key: ["if r == \\'1\\'"]
    (75, 'def get_input_fastq_files(sample_name, r):')
    (76, "    sample_fastqs = samples[samples[\\'sample\\'] == sample_name]")
    (77, "    if r == \\'1\\':")
    (78, "        relpath = sample_fastqs[\\'r1_path\\'].values[0]")
    (79, "    elif r == \\'2\\':")
    (80, "        relpath = sample_fastqs[\\'r2_path\\'].values[0]")
    (81, '')
    (82, '    return os.path.abspath(os.path.join(exec_dir, relpath))')
    (83, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jaleezyy/covid-19-signal, file=Snakefile
context_key: ["if r == \\'1\\'"]
    (84, 'def get_pooled_fastq_files(sample_name, r):')
    (85, "    sample_fastqs = samples[samples[\\'sample\\'] == sample_name]")
    (86, "    if r == \\'1\\':")
    (87, "        relpath = sample_fastqs[\\'r1_path\\'].values")
    (88, "    elif r == \\'2\\':")
    (89, "        relpath = sample_fastqs[\\'r2_path\\'].values")
    (90, '')
    (91, '    return [ os.path.abspath(os.path.join(exec_dir, r)) for r in relpath ]')
    (92, '')
    (93, '# determine raw FASTQ handling')
    (94, '# if duplicate sample names in table, run legacy concat_and_sort')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jaleezyy/covid-19-signal, file=Snakefile
context_key: ["if samples[\\'sample\\'].duplicated().any()"]
    (84, 'def get_pooled_fastq_files(sample_name, r):')
    (85, "    sample_fastqs = samples[samples[\\'sample\\'] == sample_name]")
    (86, "    if r == \\'1\\':")
    (87, "        relpath = sample_fastqs[\\'r1_path\\'].values")
    (88, "    elif r == \\'2\\':")
    (89, "        relpath = sample_fastqs[\\'r2_path\\'].values")
    (90, '')
    (91, '    return [ os.path.abspath(os.path.join(exec_dir, r)) for r in relpath ]')
    (92, '')
    (93, '# determine raw FASTQ handling')
    (94, '# if duplicate sample names in table, run legacy concat_and_sort')
    (95, "if samples[\\'sample\\'].duplicated().any():")
    (96, '    print("Duplicate sample names in sample table. Assuming multi-lane samples exist")')
    (97, '    ruleorder: concat_and_sort > link_raw_data')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yiolino/snakemake-deepvariant, file=workflow/rules/common.smk
context_key: ['if len(fastqs) == 2']
    (35, 'def get_fastq(wildcards):')
    (36, '    """')
    (37, '    Get fastq files of given sample-unit.')
    (38, '    # fastq file\\xe3\\x81\\xae\\xe3\\x83\\x91\\xe3\\x82\\xb9\\xe3\\x82\\x92returen ')
    (39, '    """')
    (40, '    fastqs = units.loc[(wildcards.sample), ["fq1", "fq2"]].dropna()   # dropna()\\xe3\\x81\\xa7NaN\\xe3\\x82\\x92\\xe5\\x90\\xab\\xe3\\x82\\x80\\xe8\\xa1\\x8c\\xe3\\x82\\x92\\xe5\\x89\\x8a\\xe9\\x99\\xa4')
    (41, '    if len(fastqs) == 2:')
    (42, '        return {"r1": fastqs.fq1, "r2": fastqs.fq2}')
    (43, '    return {"r1": fastqs.fq1}')
    (44, '')
    (45, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yiolino/snakemake-deepvariant, file=workflow/rules/common.smk
context_key: ['if len(fastqs) == 2']
    (46, 'def get_fastq_for_fastp(wildcards):')
    (47, '    """')
    (48, '    Get fastq files of given sample-unit.')
    (49, '    # fastq file\\xe3\\x81\\xae\\xe3\\x83\\x91\\xe3\\x82\\xb9\\xe3\\x82\\x92returen ')
    (50, '    """')
    (51, '    fastqs = units.loc[(wildcards.sample), ["fq1", "fq2"]].dropna()   # dropna()\\xe3\\x81\\xa7NaN\\xe3\\x82\\x92\\xe5\\x90\\xab\\xe3\\x82\\x80\\xe8\\xa1\\x8c\\xe3\\x82\\x92\\xe5\\x89\\x8a\\xe9\\x99\\xa4')
    (52, '    if len(fastqs) == 2:')
    (53, '        return {"sample": [fastqs.fq1, fastqs.fq2]}')
    (54, '')
    (55, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yiolino/snakemake-deepvariant, file=workflow/rules/common.smk
context_key: ['if not is_single_end(**wildcards)']
    (68, 'def get_trimmed_reads(wildcards):')
    (69, '    """Get trimmed reads of given sample-unit."""')
    (70, '    if not is_single_end(**wildcards):')
    (71, '        # paired-end sample')
    (72, '        return expand("data/output/trimmed/{sample}.{group}.fq.gz",')
    (73, '                      group=[1, 2], **wildcards)')
    (74, '    # single end sample')
    (75, '    return "data/output/trimmed/{sample}.fq.gz".format(**wildcards)')
    (76, '')
    (77, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=BeeCSI-Microbiome/taxonomic_profiling_pipeline, file=rules/common.smk
context_key: ['if flag']
    (4, 'def retain(flag, path):')
    (5, '    """Returns given path if flag is true, else returns temp(path)')
    (6, '       which causes the output to be deleted after it is no longer needed"""')
    (7, '    if flag: return path')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=LCrossman/ribosomal_snakemake, file=Snakefile
context_key: ["if \\'no\\' in config[\\'download_genbank\\'][\\'options\\']"]
    (38, 'def glob_files():')
    (39, "      suffixes = [\\'*.gbff\\', \\'*gbf\\', \\'*.gbk.gbk\\', \\'*.gbk\\', \\'*.gb\\', \\'*.genbank\\']")
    (40, '      files_glob = []')
    (41, '      for suf in suffixes:')
    (42, '          files_glob.append([Path(fa).name for fa in glob.glob(suf)])')
    (43, "      if \\'no\\' in config[\\'download_genbank\\'][\\'options\\']:")
    (44, '          shell("touch \\\'logs/gunzip_complete.txt\\\'")')
    (45, '      else:')
    (46, '          pass')
    (47, '      return files_glob')
    (48, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=LCrossman/ribosomal_snakemake, file=workflow/Snakefile
context_key: ["if \\'no\\' in config[\\'download_genbank\\'][\\'options\\']"]
    (38, 'def glob_files():')
    (39, "      suffixes = [\\'*.gbff\\', \\'*gbf\\', \\'*.gbk.gbk\\', \\'*.gbk\\', \\'*.gb\\', \\'*.genbank\\']")
    (40, '      files_glob = []')
    (41, '      for suf in suffixes:')
    (42, '          files_glob.append([Path(fa).name for fa in glob.glob(suf)])')
    (43, "      if \\'no\\' in config[\\'download_genbank\\'][\\'options\\']:")
    (44, '          shell("touch \\\'logs/gunzip_complete.txt\\\'")')
    (45, '      else:')
    (46, '          pass')
    (47, '      return files_glob')
    (48, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/fusions, file=workflow/rules/common.smk
context_key: ['if "N" in get_unit_types(units, sample']
    (44, 'def compile_output_list(wildcards):')
    (45, '    files = {')
    (46, '        "fusions/gene_fuse_report": ["_gene_fuse_fusions_report.txt"],')
    (47, '    }')
    (48, '    output_files = [')
    (49, '        "%s/%s_N%s" % (prefix, sample, suffix)')
    (50, '        for prefix in files.keys()')
    (51, '        for sample in get_samples(samples)')
    (52, '        if "N" in get_unit_types(units, sample)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/fusions, file=workflow/rules/common.smk
context_key: ['if "R" in get_unit_types(units, sample']
    (44, 'def compile_output_list(wildcards):')
    (45, '    files = {')
    (46, '        "fusions/gene_fuse_report": ["_gene_fuse_fusions_report.txt"],')
    (47, '    }')
    (48, '    output_files = [')
    (49, '        "%s/%s_N%s" % (prefix, sample, suffix)')
    (50, '        for prefix in files.keys()')
    (51, '        for sample in get_samples(samples)')
    (52, '        if "N" in get_unit_types(units, sample)')
    (53, '        for suffix in files[prefix]')
    (54, '    ]')
    (55, '    files = {')
    (56, '        "fusions/arriba_draw_fusion": [".pdf"],')
    (57, '    }')
    (58, '    output_files.extend(')
    (59, '        [')
    (60, '            "%s/%s_R%s" % (prefix, sample, suffix)')
    (61, '            for prefix in files.keys()')
    (62, '            for sample in get_samples(samples)')
    (63, '            if "R" in get_unit_types(units, sample)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/fusions, file=workflow/rules/common.smk
context_key: ['if "R" in get_unit_types(units, sample']
    (44, 'def compile_output_list(wildcards):')
    (45, '    files = {')
    (46, '        "fusions/gene_fuse_report": ["_gene_fuse_fusions_report.txt"],')
    (47, '    }')
    (48, '    output_files = [')
    (49, '        "%s/%s_N%s" % (prefix, sample, suffix)')
    (50, '        for prefix in files.keys()')
    (51, '        for sample in get_samples(samples)')
    (52, '        if "N" in get_unit_types(units, sample)')
    (53, '        for suffix in files[prefix]')
    (54, '    ]')
    (55, '    files = {')
    (56, '        "fusions/arriba_draw_fusion": [".pdf"],')
    (57, '    }')
    (58, '    output_files.extend(')
    (59, '        [')
    (60, '            "%s/%s_R%s" % (prefix, sample, suffix)')
    (61, '            for prefix in files.keys()')
    (62, '            for sample in get_samples(samples)')
    (63, '            if "R" in get_unit_types(units, sample)')
    (64, '            for suffix in files[prefix]')
    (65, '        ]')
    (66, '    )')
    (67, '    files = {')
    (68, '        "fusions/star_fusion": ["star-fusion.fusion_predictions.tsv"],')
    (69, '        "fusions/fusioncatcher": ["final-list_candidate-fusion-genes.hg19.txt"],')
    (70, '    }')
    (71, '    output_files.extend(')
    (72, '        [')
    (73, '            "%s/%s_R/%s" % (prefix, sample, suffix)')
    (74, '            for prefix in files.keys()')
    (75, '            for sample in get_samples(samples)')
    (76, '            if "R" in get_unit_types(units, sample)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexpenson/rna-seq-kallisto-sleuth, file=workflow/rules/diffexp.smk
context_key: ['if wildcards.model == "all"']
    (21, 'def get_model(wildcards):')
    (22, '    if wildcards.model == "all":')
    (23, '        return {"full": None}')
    (24, '    return config["diffexp"]["models"][wildcards.model]')
    (25, '')
    (26, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexpenson/rna-seq-kallisto-sleuth, file=workflow/rules/quant.smk
context_key: ['if len(input.fq) == 1']
    (13, 'def kallisto_params(wildcards, input):')
    (14, '    extra = config["params"]["kallisto"]')
    (15, '    if len(input.fq) == 1:')
    (16, '        extra += " --single"')
    (17, '        extra += (" --fragment-length {unit.fragment_len_mean} "')
    (18, '                  "--sd {unit.fragment_len_sd}").format(')
    (19, '                    unit=units.loc[')
    (20, '                        (wildcards.sample, wildcards.unit)])')
    (21, '    else:')
    (22, '        extra += " --fusion"')
    (23, '    return extra')
    (24, '')
    (25, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=rules/process.rules
context_key: ['if (config["UMItags"])']
    (45, 'def all_umitag_inputs(wildcards):')
    (46, '  if (config["UMItags"]):')
    (47, '    return expand(')
    (48, '      RUN_DIR + "/process_data/indices/{sample}.{bin}.umitags.fasta.gz", ')
    (49, '      sample=SAMPLES, bin=BINS)')
    (50, '  else:')
    (51, '    return []')
    (52, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=rules/process.rules
context_key: ['if (config["recoverMultihits"])']
    (53, 'def all_multi_inputs(wildcards):')
    (54, '  if (config["recoverMultihits"]):')
    (55, '    return expand(')
    (56, '      RUN_DIR + "/process_data/multihits/{sample}.multihits.rds", ')
    (57, '      sample=SAMPLES)')
    (58, '  else:')
    (59, '    return []')
    (60, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=rules/process.rules
context_key: ['if (config["tables"])']
    (109, 'def report_supp(wildcards):')
    (110, '  supp_str = str()')
    (111, '  if (config["tables"]): ')
    (112, '      supp_str = supp_str + "-b "')
    (113, '  if (config["figures"]): ')
    (114, '      supp_str = supp_str + "-f "')
    (115, '  if (config["reportData"]):')
    (116, '      supp_str = supp_str + "-d "')
    (117, '  if (config["infoGraphic"]):')
    (118, '      supp_str = supp_str + "-g "')
    (119, '  return supp_str')
    (120, '        ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=maehler/gwas-workflow, file=Snakefile
context_key: ["if 'plink_prefix' not in config or not config['plink_prefix']", "if variant_fname.endswith('.gz')"]
    (17, 'def get_plink_prefix(trait=False):')
    (18, "    if 'plink_prefix' not in config or not config['plink_prefix']:")
    (19, '        # Make a PLINK prefix based on the VCF file')
    (20, '        variant_fname = get_variants()')
    (21, "        if variant_fname.endswith('.gz'):")
    (22, '            variant_fname = os.path.splitext(variant_fname)[0]')
    (23, '        prefix = os.path.splitext(variant_fname)[0]')
    (24, '        if not trait:')
    (25, "            return '{prefix}_plink'.format(prefix=prefix)")
    (26, "        return 'results/{{trait}}/plink/{prefix}_plink'.format(")
    (27, '            prefix=os.path.basename(prefix))')
    (28, '    # A PLINK prefix has been supplied')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=maehler/gwas-workflow, file=Snakefile
context_key: ["if 'plink_prefix' not in config or not config['plink_prefix']", 'if not trait']
    (17, 'def get_plink_prefix(trait=False):')
    (18, "    if 'plink_prefix' not in config or not config['plink_prefix']:")
    (19, '        # Make a PLINK prefix based on the VCF file')
    (20, '        variant_fname = get_variants()')
    (21, "        if variant_fname.endswith('.gz'):")
    (22, '            variant_fname = os.path.splitext(variant_fname)[0]')
    (23, '        prefix = os.path.splitext(variant_fname)[0]')
    (24, '        if not trait:')
    (25, "            return '{prefix}_plink'.format(prefix=prefix)")
    (26, "        return 'results/{{trait}}/plink/{prefix}_plink'.format(")
    (27, '            prefix=os.path.basename(prefix))')
    (28, '    # A PLINK prefix has been supplied')
    (29, '    if not trait:')
    (30, "        return config['plink_prefix']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=maehler/gwas-workflow, file=Snakefile
context_key: ["if 'plink_prefix' not in config or not config['plink_prefix']"]
    (17, 'def get_plink_prefix(trait=False):')
    (18, "    if 'plink_prefix' not in config or not config['plink_prefix']:")
    (19, '        # Make a PLINK prefix based on the VCF file')
    (20, '        variant_fname = get_variants()')
    (21, "        if variant_fname.endswith('.gz'):")
    (22, '            variant_fname = os.path.splitext(variant_fname)[0]')
    (23, '        prefix = os.path.splitext(variant_fname)[0]')
    (24, '        if not trait:')
    (25, "            return '{prefix}_plink'.format(prefix=prefix)")
    (26, "        return 'results/{{trait}}/plink/{prefix}_plink'.format(")
    (27, '            prefix=os.path.basename(prefix))')
    (28, '    # A PLINK prefix has been supplied')
    (29, '    if not trait:')
    (30, "        return config['plink_prefix']")
    (31, "    return 'results/{{trait}}/plink/{prefix}'.format(")
    (32, "        prefix=os.path.basename(config['plink_prefix']))")
    (33, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=maehler/gwas-workflow, file=Snakefile
context_key: ['if not trait']
    (34, "def get_plink_files(trait=False, extensions=['fam', 'bed', 'bim']):")
    (35, '    if not trait:')
    (36, "        return expand('{prefix}.{ext}',")
    (37, '            prefix=get_plink_prefix(trait), ext=extensions)')
    (38, "    return expand('results/{{trait}}/plink/{prefix}.{ext}',")
    (39, '        prefix=os.path.basename(get_plink_prefix(trait)), ext=extensions)')
    (40, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nixonlab/HERV_scGTEX, file=workflow/rules/stellarscope_13.smk
context_key: ['if wc.smode == "U"']
    (6, 'def get_strand_cmd(wc):')
    (7, '    # input functions are passed a single parameter, wildcards')
    (8, '    # wildcards is a namespace so refer to the variable with a "."')
    (9, '    if wc.smode == "U": # handle the unstranded case')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=callumparr/ont_tutorial_transcriptome, file=Snakefile
context_key: ['if (p.search(xlink))']
    (16, 'def checkExternalLinks(xlink):')
    (17, '  yfile = xlink')
    (18, '  p = re.compile("(^http:|^ftp:|^https:)")')
    (19, '  if (p.search(xlink)):')
    (20, '    yfile = os.path.basename(xlink)')
    (21, '    ylink = re.sub("^[^:]+://", "", xlink)')
    (22, '    downloadSource[yfile]=ylink')
    (23, '  return yfile')
    (24, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=callumparr/ont_tutorial_transcriptome, file=Snakefile
context_key: ['if (yfile != xfile)']
    (29, 'def handleExternalZip(xfile):')
    (30, '  yfile = re.sub("\\\\.gz$","",xfile)')
    (31, '  if (yfile != xfile):')
    (32, '    unzipDict[yfile]=xfile')
    (33, '  return yfile')
    (34, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mike-molnar/nanopore-workflow, file=Snakefile
context_key: ['if os.path.exists(file_name)', 'if re.search(pattern, line)']
    (32, 'def get_coverage(sample_name):')
    (33, '    file_name = str(sample_name) + "/analysis/nanoplot/" + str(sample_name) + "NanoStats.txt"')
    (34, '    if os.path.exists(file_name):')
    (35, '        f = open(file_name)')
    (36, '        pattern = "Total bases"')
    (37, '        for line in f:')
    (38, '            if re.search(pattern, line):')
    (39, '                total_bases = line.split(":")[1].strip().replace(\\\',\\\', \\\'\\\')')
    (40, '                return float(total_bases)/3100000000')
    (41, '')
    (42, '# Get the list of regions for the workflow')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hivlab/sarscov2-variation, file=workflow/Snakefile
context_key: ['if len(fq_cols) == 2']
    (43, 'def get_fastq(wildcards):')
    (44, '    df = pep.sample_table.set_index(["sample_name", "run"])')
    (45, '    fq_cols = [col for col in df.columns if "fq" in col]')
    (46, '    fqs = df.loc[(wildcards.sample, wildcards.run), fq_cols].dropna()')
    (47, '    assert len(fq_cols) in [1, 2], "Enter one or two FASTQ file paths"')
    (48, '    if len(fq_cols) == 2:')
    (49, '        return {"in1": fqs[0], "in2": fqs[1]}')
    (50, '    else:')
    (51, '        return {"input": fqs[0]}')
    (52, '')
    (53, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/gbs-genetic-fitness, file=Snakefile
context_key: ['if RUN_SCAFFOLDING == "Y"']
    (86, 'def run_scaffolding(RUN_SCAFFOLDING):')
    (87, '    if RUN_SCAFFOLDING == "Y":')
    (88, '        return("{prefix}_oneline.k32.w100.ntLink-arks.longstitch-scaffolds.fa")')
    (89, '    elif RUN_SCAFFOLDING == "N":')
    (90, '        return(ASSEMBLY) ')
    (91, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/gbs-genetic-fitness, file=Snakefile
context_key: ['if wildcards.asm == "ref"']
    (111, 'def assembly_stats_input(wildcards):')
    (112, '    if wildcards.asm == "ref":')
    (113, '        return(ASSEMBLY)')
    (114, '    if wildcards.asm == "new":')
    (115, '        return("refgenome/{prefix}.fa")')
    (116, '')
    (117, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/Struo, file=bin/Snakefile
context_key: ['if not os.path.isdir(out_dir)']
    (4, 'def write_config(out_file):')
    (5, '    out_dir = os.path.split(out_file)[0]')
    (6, '    if not os.path.isdir(out_dir):')
    (7, '        os.makedirs(out_dir)')
    (8, '    config_tmp = {k:(v.to_string(max_rows=1, max_cols=10) if isinstance(v, pd.DataFrame) else v) \\\\')
    (9, '                  for k,v in config.items()}')
    (10, "    with open(out_file, \\'w\\') as outF:")
    (11, '        json.dump(config_tmp, outF, indent=4)')
    (12, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/Struo, file=bin/Snakefile
context_key: ['if os.path.isfile(file_path) and os.stat(file_path).st_size > 0']
    (13, 'def file_atch(file_path, file_type):')
    (14, '    """ Attachments for email')
    (15, '    """')
    (16, '    if os.path.isfile(file_path) and os.stat(file_path).st_size > 0:')
    (17, "        attach = \\'-a {}\\'.format(file_path)   ")
    (18, '        file_path = os.path.split(file_path)[1]')
    (19, "        msg = \\'See attached {} file: {}\\'.format(file_type, file_path)")
    (20, '    else:')
    (21, "        attach = \\'\\'")
    (22, '        file_path = os.path.split(file_path)[1]')
    (23, "        msg = \\'WARNING: could not attach {}: {}\\'.format(file_type, file_path)")
    (24, '    return attach,msg')
    (25, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/Struo, file=Snakefile
context_key: ["if not config['databases']['kraken2'].startswith('Skip')", "if config['keep_intermediate'] == True"]
    (67, 'def all_which_input(wildcards):')
    (68, '    input_files = []')
    (69, '    # kraken2')
    (70, "    if not config['databases']['kraken2'].startswith('Skip'):")
    (71, "        if config['keep_intermediate'] == True:")
    (72, "            x = expand(kraken2_dir + 'added/{sample}.done',")
    (73, "                       sample = config['samples_unique'])")
    (74, '            input_files += x\\t    ')
    (75, "        input_files.append(os.path.join(kraken2_dir, 'hash.k2d'))")
    (76, "        input_files.append(os.path.join(kraken2_dir, 'opts.k2d'))")
    (77, "        input_files.append(os.path.join(kraken2_dir, 'taxo.k2d'))")
    (78, "        input_files.append(os.path.join(kraken2_dir, 'seqid2taxid.map'))")
    (79, '')
    (80, '    # bracken')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/Struo, file=Snakefile
context_key: ["if not config['databases']['kraken2'].startswith('Skip')", "if (not config['databases']['kraken2'].startswith('Skip') an"]
    (67, 'def all_which_input(wildcards):')
    (68, '    input_files = []')
    (69, '    # kraken2')
    (70, "    if not config['databases']['kraken2'].startswith('Skip'):")
    (71, "        if config['keep_intermediate'] == True:")
    (72, "            x = expand(kraken2_dir + 'added/{sample}.done',")
    (73, "                       sample = config['samples_unique'])")
    (74, '            input_files += x\\t    ')
    (75, "        input_files.append(os.path.join(kraken2_dir, 'hash.k2d'))")
    (76, "        input_files.append(os.path.join(kraken2_dir, 'opts.k2d'))")
    (77, "        input_files.append(os.path.join(kraken2_dir, 'taxo.k2d'))")
    (78, "        input_files.append(os.path.join(kraken2_dir, 'seqid2taxid.map'))")
    (79, '')
    (80, '    # bracken')
    (81, "    if (not config['databases']['kraken2'].startswith('Skip') and")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/Struo, file=Snakefile
context_key: ["if not config['databases']['kraken2'].startswith('Skip')"]
    (67, 'def all_which_input(wildcards):')
    (68, '    input_files = []')
    (69, '    # kraken2')
    (70, "    if not config['databases']['kraken2'].startswith('Skip'):")
    (71, "        if config['keep_intermediate'] == True:")
    (72, "            x = expand(kraken2_dir + 'added/{sample}.done',")
    (73, "                       sample = config['samples_unique'])")
    (74, '            input_files += x\\t    ')
    (75, "        input_files.append(os.path.join(kraken2_dir, 'hash.k2d'))")
    (76, "        input_files.append(os.path.join(kraken2_dir, 'opts.k2d'))")
    (77, "        input_files.append(os.path.join(kraken2_dir, 'taxo.k2d'))")
    (78, "        input_files.append(os.path.join(kraken2_dir, 'seqid2taxid.map'))")
    (79, '')
    (80, '    # bracken')
    (81, "    if (not config['databases']['kraken2'].startswith('Skip') and")
    (82, "        not config['databases']['bracken'].startswith('Skip')):")
    (83, "    \\tx = expand(os.path.join(kraken2_dir, 'database{read_len}mers.kraken'),")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/Struo, file=Snakefile
context_key: ["if not config['databases']['humann2_bowtie2'].startswith('Skip') and \\"]
    (67, 'def all_which_input(wildcards):')
    (68, '    input_files = []')
    (69, '    # kraken2')
    (70, "    if not config['databases']['kraken2'].startswith('Skip'):")
    (71, "        if config['keep_intermediate'] == True:")
    (72, "            x = expand(kraken2_dir + 'added/{sample}.done',")
    (73, "                       sample = config['samples_unique'])")
    (74, '            input_files += x\\t    ')
    (75, "        input_files.append(os.path.join(kraken2_dir, 'hash.k2d'))")
    (76, "        input_files.append(os.path.join(kraken2_dir, 'opts.k2d'))")
    (77, "        input_files.append(os.path.join(kraken2_dir, 'taxo.k2d'))")
    (78, "        input_files.append(os.path.join(kraken2_dir, 'seqid2taxid.map'))")
    (79, '')
    (80, '    # bracken')
    (81, "    if (not config['databases']['kraken2'].startswith('Skip') and")
    (82, "        not config['databases']['bracken'].startswith('Skip')):")
    (83, "    \\tx = expand(os.path.join(kraken2_dir, 'database{read_len}mers.kraken'),")
    (84, "\\t           read_len = config['params']['bracken_build_read_lens'])")
    (85, '        input_files += x')
    (86, '')
    (87, '    # humann2')
    (88, "    if not config['databases']['humann2_bowtie2'].startswith('Skip') and \\\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['if not os.path.exists(os.path.dirname(output[0]))']
    (171, '        def run_varifier(assembly,')
    (172, '                        covered,')
    (173, '                        dropped_amplicons,')
    (174, '                        primer_df,')
    (175, '                        reference,')
    (176, '                        output_dir,')
    (177, '                        container_dir):')
    (178, '            """Run varifier make_truth_vcf on the masked assemblies"""')
    (179, '            covered_start = covered["start"]')
    (180, '            covered_end = covered["end"]')
    (181, '            varifier_command = "singularity run " + container_dir + "/varifier/varifier.img make_truth_vcf --global_align "')
    (182, '            varifier_command += "--global_align_min_coord " + covered_start + " --global_align_max_coord " + covered_end')
    (183, '            varifier_command += " " + assembly + " " + reference + " " + output_dir')
    (184, '            shell(varifier_command)')
    (185, '            # append dropped amplicon information to the truth vcf')
    (186, '            #with open(os.path.join(output_dir, "04.truth.vcf"), "r") as truth_vcf_in:')
    (187, '            with open(os.path.join(output_dir, "04.truth.vcf"), "r") as truth_vcf_in:')
    (188, '                truth_vcf = truth_vcf_in.read()')
    (189, '            to_add = []')
    (190, '            variant_count = int(truth_vcf.splitlines()[-1].split("\\\\t")[2])')
    (191, '            for dropped in dropped_amplicons:')
    (192, '                start_pos = primer_df.loc[primer_df[\\\'name\\\'] == dropped[0]].reset_index(drop=True)["ref_start"][0]')
    (193, '                end_pos = primer_df.loc[primer_df[\\\'name\\\'] == dropped[1]].reset_index(drop=True)["ref_end"][0]')
    (194, '                variant_count += 1')
    (195, '                variant_line = "MN908947.3\\\\t" + str(start_pos) + "\\\\t" + str(variant_count)')
    (196, '                variant_line += "\\\\tG\\\\tN\\\\t.\\\\tDROPPED_AMP\\\\tAMP_START=" + str(int(start_pos)-1) + ";AMP_END=" + str(int(end_pos)-1) + "\\\\tGT\\\\t1/1\\')
    (197, '"')
    (198, '                truth_vcf += variant_line')
    (199, '            truth_vcf = truth_vcf.split("#CHROM\\\\tPOS\\\\tID\\\\tREF\\\\tALT\\\\tQUAL\\\\tFILTER\\\\tINFO\\\\tFORMAT\\\\tsample")')
    (200, '            truth_vcf_variants = truth_vcf[1].splitlines()[1:]')
    (201, '            first_line = [truth_vcf[1].splitlines()[1]]')
    (202, '            truth_vcf_variants.sort(key=lambda x: int(x.split("\\\\t")[1]))')
    (203, '            truth_vcf[1] = "\\')
    (204, '".join(truth_vcf_variants)')
    (205, '            truth_vcf = "#CHROM\\\\tPOS\\\\tID\\\\tREF\\\\tALT\\\\tQUAL\\\\tFILTER\\\\tINFO\\\\tFORMAT\\\\tsample\\')
    (206, '".join(truth_vcf) + "\\')
    (207, '"')
    (208, '            with open(os.path.join(output_dir, "04.truth_dropped.vcf"), "w") as truth_vcf_out:')
    (209, '                truth_vcf_out.write("\\')
    (210, '".join(truth_vcf.splitlines()[1:]))')
    (211, '')
    (212, '        # check the truth vcf input sample matches the output')
    (213, '        assert os.path.basename(input[0].replace(".fasta", "")) == os.path.basename(output[0])')
    (214, '        # make directory')
    (215, '        if not os.path.exists(os.path.dirname(output[0])):')
    (216, '            os.mkdir(os.path.dirname(output[0]))')
    (217, '        # import the amplicon statistics file to extract what parts of the assembly are covered by amplicons')
    (218, "        with open(os.path.join(input[2], \\'amplicon_statistics.pickle\\'), \\'rb\\') as statIn:")
    (219, '            amplicon_stats = pickle.load(statIn)')
    (220, '        regions_covered = {}')
    (221, '        amplicons_dropped = {}')
    (222, '        sample = os.path.basename(input[0]).replace(".fasta", "")')
    (223, '        amplicons = list(amplicon_stats[sample].keys())')
    (224, '        regions_covered[sample] = {"start": str(amplicon_stats[sample][amplicons[0]]["amplicon_start"]),')
    (225, '                                "end": str(amplicon_stats[sample][amplicons[len(amplicons)-1]]["amplicon_end"])}')
    (226, '        # record amplicons dropped for each sample so dropped amplicons can be marked in the truth vcf')
    (227, '        amplicons_dropped[sample] = []')
    (228, '        for a in amplicons:')
    (229, '            if amplicon_stats[sample][a]["has_error"] and \\\\')
    (230, '                    any(amplicon_stats[sample][a]["errors"][0] == mode for mode in ["primer_SNP", "random_dropout", "primer_dimer"]):')
    (231, '                amplicons_dropped[sample].append((a.split("---")[0], a.split("---")[1]))')
    (232, '        # import the primer scheme df to get amplicon positions relative to ref')
    (233, '        primer_df, pool1_primers, pool2_primers = find_primer_scheme(params.primer_scheme,')
    (234, '                                                                    "primer_schemes")')
    (235, '        # parallelise make_truth_vcf')
    (236, '        run_varifier(input[0],')
    (237, '                    regions_covered[sample],')
    (238, '                    amplicons_dropped[sample],')
    (239, '                    primer_df,')
    (240, '                    input[1],')
    (241, '                    output[0],')
    (242, '                    params.container_dir)')
    (243, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['log', 'run', 'if not os.path.exists(output_dir)']
    (259, '        def simulate_ART_reads(genome,')
    (260, '                                output_dir,')
    (261, '                                sample_coverages,')
    (262, '                                read_length,')
    (263, '                                container_dir):')
    (264, '            """Function to run ART on amplicon sequences per simulated genomic sequence"""')
    (265, '            sample_name = os.path.basename(genome)')
    (266, '            if not os.path.exists(output_dir):')
    (267, '                os.mkdir(output_dir)')
    (268, '            for amplicon in sample_coverages[sample_name]:')
    (269, '                try:')
    (270, '                    coverage = sample_coverages[sample_name][amplicon]')
    (271, '                except:')
    (272, '                    continue')
    (273, "                amplicon_file = os.path.join(genome, amplicon + \\'.fasta\\')")
    (274, '                read_file = os.path.join(output_dir, amplicon)')
    (275, "                shell_command = \\'singularity run \\' + container_dir +\\'/images/ART.img --quiet -amp -p -sam -na -i \\' + amplicon_file + \\\\")
    (276, "                        \\' -l \\' + read_length + \\' -f \\' + str(coverage) + \\' -o \\' + read_file")
    (277, '                shell(shell_command)')
    (278, '')
    (279, '        # check the input sample matches the output sample')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['log', 'run', 'if not os.path.exists(o)']
    (259, '        def simulate_ART_reads(genome,')
    (260, '                                output_dir,')
    (261, '                                sample_coverages,')
    (262, '                                read_length,')
    (263, '                                container_dir):')
    (264, '            """Function to run ART on amplicon sequences per simulated genomic sequence"""')
    (265, '            sample_name = os.path.basename(genome)')
    (266, '            if not os.path.exists(output_dir):')
    (267, '                os.mkdir(output_dir)')
    (268, '            for amplicon in sample_coverages[sample_name]:')
    (269, '                try:')
    (270, '                    coverage = sample_coverages[sample_name][amplicon]')
    (271, '                except:')
    (272, '                    continue')
    (273, "                amplicon_file = os.path.join(genome, amplicon + \\'.fasta\\')")
    (274, '                read_file = os.path.join(output_dir, amplicon)')
    (275, "                shell_command = \\'singularity run \\' + container_dir +\\'/images/ART.img --quiet -amp -p -sam -na -i \\' + amplicon_file + \\\\")
    (276, "                        \\' -l \\' + read_length + \\' -f \\' + str(coverage) + \\' -o \\' + read_file")
    (277, '                shell(shell_command)')
    (278, '')
    (279, '        # check the input sample matches the output sample')
    (280, '        assert os.path.basename(input[0]) == os.path.basename(output[0])')
    (281, '        # make output dirs')
    (282, '        output_dirs = [os.path.dirname(output[0]), output[0]]')
    (283, '        for o in output_dirs:')
    (284, '            if not os.path.exists(o):')
    (285, '                os.mkdir(o)')
    (286, '        # get rid of undeleted temp files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['log', 'run', 'if not os.path.exists(output_dir)']
    (313, '        def simulate_badreads(genome,')
    (314, '                            output_dir,')
    (315, '                            sample_coverages,')
    (316, '                            container_dir):')
    (317, '            sample_name = os.path.basename(genome)')
    (318, '            if not os.path.exists(output_dir):')
    (319, '                os.mkdir(output_dir)')
    (320, '            for amplicon in sample_coverages[sample_name]:')
    (321, '                try:')
    (322, '                    coverage = sample_coverages[sample_name][amplicon]')
    (323, '                except:')
    (324, '                    continue')
    (325, "                amplicon_file = os.path.join(genome, amplicon + \\'.fasta\\')")
    (326, "                read_file = os.path.join(output_dir, amplicon) + \\'.fastq.gz\\'")
    (327, '                # skip badread if the coverage is 0')
    (328, '                if str(coverage) == "0":')
    (329, '                    continue')
    (330, "                shell_command = \\'singularity run \\' + container_dir + \\'/images/Badread.img simulate --identity 94,98.5,3 --reference \\'")
    (331, "                shell_command += amplicon_file + \\' --quantity \\' + str(coverage) + \\'x |         gzip > \\' + read_file")
    (332, '                shell(shell_command)')
    (333, '            return')
    (334, '')
    (335, '        # check the input sample matches the output sample')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['log', 'run', 'if not os.path.exists(o)']
    (313, '        def simulate_badreads(genome,')
    (314, '                            output_dir,')
    (315, '                            sample_coverages,')
    (316, '                            container_dir):')
    (317, '            sample_name = os.path.basename(genome)')
    (318, '            if not os.path.exists(output_dir):')
    (319, '                os.mkdir(output_dir)')
    (320, '            for amplicon in sample_coverages[sample_name]:')
    (321, '                try:')
    (322, '                    coverage = sample_coverages[sample_name][amplicon]')
    (323, '                except:')
    (324, '                    continue')
    (325, "                amplicon_file = os.path.join(genome, amplicon + \\'.fasta\\')")
    (326, "                read_file = os.path.join(output_dir, amplicon) + \\'.fastq.gz\\'")
    (327, '                # skip badread if the coverage is 0')
    (328, '                if str(coverage) == "0":')
    (329, '                    continue')
    (330, "                shell_command = \\'singularity run \\' + container_dir + \\'/images/Badread.img simulate --identity 94,98.5,3 --reference \\'")
    (331, "                shell_command += amplicon_file + \\' --quantity \\' + str(coverage) + \\'x |         gzip > \\' + read_file")
    (332, '                shell(shell_command)')
    (333, '            return')
    (334, '')
    (335, '        # check the input sample matches the output sample')
    (336, '        assert os.path.basename(input[0]) == os.path.basename(output[0])')
    (337, '        # make output dirs')
    (338, '        output_dirs = [os.path.dirname(output[0]), output[0]]')
    (339, '        for o in output_dirs:')
    (340, '            if not os.path.exists(o):')
    (341, '                os.mkdir(o)')
    (342, '        # get rid of undeleted temp files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['log', 'run', 'if not os.path.exists(os.path.dirname(output[0]))']
    (425, '        def illumina_viridian_workflow(reference_genome,')
    (426, '                                        fw_read,')
    (427, '                                        rv_read,')
    (428, '                                        output,')
    (429, '                                        viridian_container):')
    (430, '            """Function to run viridian on ART read sets"""')
    (431, '            viridian_command = "singularity run " + viridian_container + " run_one_sample \\\\')
    (432, '                    --tech illumina \\\\')
    (433, '                    --ref_fasta " + reference_genome + " \\\\')
    (434, '                    --reads1 " + fw_read + " \\\\')
    (435, '                    --reads2 " + rv_read + " \\\\')
    (436, '                    --outdir " + output + "/"')
    (437, '            shell(viridian_command)')
    (438, '')
    (439, '        # check the input sample matches the output sample')
    (440, '        assert os.path.basename(input[0]).replace("_1.fastq", "") == os.path.basename(output[0]) and \\\\')
    (441, '            os.path.basename(input[1]).replace("_2.fastq", "") == os.path.basename(output[0])')
    (442, '        # make the output directory')
    (443, '        if not os.path.exists(os.path.dirname(output[0])):')
    (444, '            os.mkdir(os.path.dirname(output[0]))')
    (445, '        # run viridian_workflow illumina pipeline')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['log', 'run', 'if not os.path.exists(os.path.dirname(output[0]))']
    (466, '        def nanopore_viridian_workflow(reference_genome,')
    (467, '                                        sample,')
    (468, '                                        output,')
    (469, '                                        viridian_container):')
    (470, '            """Function to run viridian on nanopore read sets"""')
    (471, '            viridian_command = "singularity run " + viridian_container + " run_one_sample \\\\')
    (472, '                    --tech ont \\\\')
    (473, '                    --ref_fasta " + reference_genome + " \\\\')
    (474, '                    --reads " + sample + " \\\\')
    (475, '                    --outdir " + output + "/"')
    (476, '            shell(viridian_command)')
    (477, '        # check the input sample matches the output sample')
    (478, '        assert os.path.basename(input[0]).replace(".fastq", "") == os.path.basename(output[0])')
    (479, '        # make the output directory')
    (480, '        if not os.path.exists(os.path.dirname(output[0])):')
    (481, '            os.mkdir(os.path.dirname(output[0]))')
    (482, '        # run viridian_workflow illumina pipeline')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['log', 'run', 'if not os.path.exists(forward_rd + ".gz")']
    (507, '        def illumina_artic_assemble(forward_rd,')
    (508, '                                    reverse_rd,')
    (509, '                                    sif_file,')
    (510, '                                    main_nf,')
    (511, '                                    scheme_url,')
    (512, '                                    output_dir,')
    (513, '                                    nextflow_path,')
    (514, '                                    primer_scheme,')
    (515, '                                    regions_covered):')
    (516, '            """run illumina artic nextflow pipeline"""')
    (517, '            if not os.path.exists(forward_rd + ".gz"):')
    (518, '                shell("gzip " + forward_rd + " " + reverse_rd)')
    (519, '            forward_rd = forward_rd + ".gz"')
    (520, '            reverse_rd = reverse_rd + ".gz"')
    (521, '            shell_command = "venv/bin/python scripts/run_connor_pipeline.py --sif " + sif_file + " "')
    (522, '            shell_command += "--main_nf " + main_nf + " --outdir " + output_dir + " "')
    (523, '            shell_command += "--scheme_url " + scheme_url + " --scheme_version " + primer_scheme + " "')
    (524, '            shell_command += "--ilm1 " + forward_rd + " --ilm2 " + reverse_rd + " --nextflow_path " + nextflow_path + " "')
    (525, '            shell_command += "--sample_name " + os.path.basename(output_dir)')
    (526, '            if not os.path.exists(forward_rd.replace(".gz", "")):')
    (527, '                shell_command += " && gunzip " + forward_rd + " " + reverse_rd')
    (528, '            shell(shell_command)')
    (529, '            # cut off ends of assemblies that lie outside of the amplicon scheme')
    (530, '            filename = os.path.join(output_dir, "consensus.fa")')
    (531, '            sample_name, sample_sequence = clean_genome(filename)')
    (532, '            sample_sequence = list(sample_sequence)')
    (533, '            sample_sequence = sample_sequence[regions_covered["scheme_start"]: regions_covered["scheme_end"]]')
    (534, '            # write out the masked simulated sequence')
    (535, '            with open(filename.replace("consensus", "consensus_trimmed"), "w") as outGen:')
    (536, '                outGen.write("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['if not os.path.exists(os.path.dirname(output[0]))']
    (507, '        def illumina_artic_assemble(forward_rd,')
    (508, '                                    reverse_rd,')
    (509, '                                    sif_file,')
    (510, '                                    main_nf,')
    (511, '                                    scheme_url,')
    (512, '                                    output_dir,')
    (513, '                                    nextflow_path,')
    (514, '                                    primer_scheme,')
    (515, '                                    regions_covered):')
    (516, '            """run illumina artic nextflow pipeline"""')
    (517, '            if not os.path.exists(forward_rd + ".gz"):')
    (518, '                shell("gzip " + forward_rd + " " + reverse_rd)')
    (519, '            forward_rd = forward_rd + ".gz"')
    (520, '            reverse_rd = reverse_rd + ".gz"')
    (521, '            shell_command = "venv/bin/python scripts/run_connor_pipeline.py --sif " + sif_file + " "')
    (522, '            shell_command += "--main_nf " + main_nf + " --outdir " + output_dir + " "')
    (523, '            shell_command += "--scheme_url " + scheme_url + " --scheme_version " + primer_scheme + " "')
    (524, '            shell_command += "--ilm1 " + forward_rd + " --ilm2 " + reverse_rd + " --nextflow_path " + nextflow_path + " "')
    (525, '            shell_command += "--sample_name " + os.path.basename(output_dir)')
    (526, '            if not os.path.exists(forward_rd.replace(".gz", "")):')
    (527, '                shell_command += " && gunzip " + forward_rd + " " + reverse_rd')
    (528, '            shell(shell_command)')
    (529, '            # cut off ends of assemblies that lie outside of the amplicon scheme')
    (530, '            filename = os.path.join(output_dir, "consensus.fa")')
    (531, '            sample_name, sample_sequence = clean_genome(filename)')
    (532, '            sample_sequence = list(sample_sequence)')
    (533, '            sample_sequence = sample_sequence[regions_covered["scheme_start"]: regions_covered["scheme_end"]]')
    (534, '            # write out the masked simulated sequence')
    (535, '            with open(filename.replace("consensus", "consensus_trimmed"), "w") as outGen:')
    (536, '                outGen.write("\\')
    (537, '".join([">" + sample_name, "".join(sample_sequence)]))')
    (538, '        # check the input sample matches the output sample')
    (539, '        assert os.path.basename(input[0]).replace("_1.fastq", "") == os.path.basename(output[0]) \\\\')
    (540, '            and os.path.basename(input[1]).replace("_2.fastq", "") == os.path.basename(output[0])')
    (541, '        # make output directory')
    (542, '        if not os.path.exists(os.path.dirname(output[0])):')
    (543, '            os.mkdir(os.path.dirname(output[0]))')
    (544, '        # import the primer scheme df to get amplicon scheme start and end relative to ref')
    (545, '        primer_df, pool1_primers, pool2_primers = find_primer_scheme(params.primer_scheme,')
    (546, '                                                                    "primer_schemes")')
    (547, "        with open(os.path.join(input[2], \\'amplicon_statistics.pickle\\'), \\'rb\\') as statIn:")
    (548, '            amplicon_stats = pickle.load(statIn)')
    (549, '        regions_covered = {}')
    (550, '        sample_stats = amplicon_stats[os.path.basename(input[0]).replace("_1.fastq", "").replace(".gz", "")]')
    (551, '        amplicons = list(sample_stats.keys())')
    (552, '        scheme_start = int(primer_df.loc[primer_df[\\\'name\\\'] == amplicons[0].split("---")[0]].reset_index(drop=True)["ref_start"][0])')
    (553, '        scheme_end = int(primer_df.loc[primer_df[\\\'name\\\'] == amplicons[len(amplicons)-1].split("---")[1]].reset_index(drop=True)["ref_start"][0]) + 1')
    (554, '        regions_covered = {"scheme_start": scheme_start,')
    (555, '                            "scheme_end": scheme_end}')
    (556, '        del amplicon_stats')
    (557, '        # run artic assembly pipeline')
    (558, '        illumina_artic_assemble(input[0],')
    (559, '                                input[1],')
    (560, '                                params.artic_illumina_container,')
    (561, '                                params.main_nf,')
    (562, '                                params.scheme_dir,')
    (563, '                                output[0],')
    (564, '                                params.nextflow_path,')
    (565, '                                params.primer_scheme,')
    (566, '                                regions_covered)')
    (567, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['log', 'run', 'if not os.path.exists(read_file + ".gz")']
    (585, '        def epi2me_artic_assemble(main_nf,')
    (586, '                                output_dir,')
    (587, '                                scheme,')
    (588, '                                read_file,')
    (589, '                                regions_covered,')
    (590, '                                nextflow_path,')
    (591, '                                nextflow_cache):')
    (592, '            """run nanopore artic nextflow pipeline"""')
    (593, '            if not os.path.exists(read_file + ".gz"):')
    (594, '                shell("gzip " + read_file)')
    (595, '            read_file = read_file + ".gz"')
    (596, '            shell_command = "venv/bin/python scripts/run_epi2me.py --main_nf " + main_nf + " "')
    (597, '            shell_command += "--force --sample_name " + os.path.basename(output_dir) + " "')
    (598, '            shell_command += "--work_root_dir " + output_dir + " --outdir " + output_dir + " "')
    (599, '            shell_command += "--nxf_sing_cache " + nextflow_cache + " "')
    (600, '            shell_command += "--scheme_version ARTIC/" + scheme + " "')
    (601, '            shell_command += "--reads " + read_file + " --nextflow_path " + nextflow_path')
    (602, '            if not os.path.exists(read_file.replace(".gz", "")):')
    (603, '                shell_command += " && gunzip " + read_file')
    (604, '            print(shell_command)')
    (605, '            shell(shell_command)')
    (606, '            # cut off ends of assemblies that lie outside of the amplicon scheme')
    (607, '            filename = os.path.join(output_dir, "consensus.fa")')
    (608, '            sample_name, sample_sequence = clean_genome(filename)')
    (609, '            sample_sequence = list(sample_sequence)')
    (610, '            sample_sequence = sample_sequence[regions_covered["scheme_start"]: regions_covered["scheme_end"]]')
    (611, '            # write out the masked simulated sequence')
    (612, '            with open(filename.replace("consensus", "consensus_trimmed"), "w") as outGen:')
    (613, '                outGen.write("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['if not os.path.exists(os.path.dirname(output[0]))']
    (585, '        def epi2me_artic_assemble(main_nf,')
    (586, '                                output_dir,')
    (587, '                                scheme,')
    (588, '                                read_file,')
    (589, '                                regions_covered,')
    (590, '                                nextflow_path,')
    (591, '                                nextflow_cache):')
    (592, '            """run nanopore artic nextflow pipeline"""')
    (593, '            if not os.path.exists(read_file + ".gz"):')
    (594, '                shell("gzip " + read_file)')
    (595, '            read_file = read_file + ".gz"')
    (596, '            shell_command = "venv/bin/python scripts/run_epi2me.py --main_nf " + main_nf + " "')
    (597, '            shell_command += "--force --sample_name " + os.path.basename(output_dir) + " "')
    (598, '            shell_command += "--work_root_dir " + output_dir + " --outdir " + output_dir + " "')
    (599, '            shell_command += "--nxf_sing_cache " + nextflow_cache + " "')
    (600, '            shell_command += "--scheme_version ARTIC/" + scheme + " "')
    (601, '            shell_command += "--reads " + read_file + " --nextflow_path " + nextflow_path')
    (602, '            if not os.path.exists(read_file.replace(".gz", "")):')
    (603, '                shell_command += " && gunzip " + read_file')
    (604, '            print(shell_command)')
    (605, '            shell(shell_command)')
    (606, '            # cut off ends of assemblies that lie outside of the amplicon scheme')
    (607, '            filename = os.path.join(output_dir, "consensus.fa")')
    (608, '            sample_name, sample_sequence = clean_genome(filename)')
    (609, '            sample_sequence = list(sample_sequence)')
    (610, '            sample_sequence = sample_sequence[regions_covered["scheme_start"]: regions_covered["scheme_end"]]')
    (611, '            # write out the masked simulated sequence')
    (612, '            with open(filename.replace("consensus", "consensus_trimmed"), "w") as outGen:')
    (613, '                outGen.write("\\')
    (614, '".join([">" + sample_name, "".join(sample_sequence)]))')
    (615, '        # check the input sample matches the output sample')
    (616, '        assert os.path.basename(input[0]).replace(".fastq", "") == os.path.basename(output[0])')
    (617, '        # make output directory')
    (618, '        if not os.path.exists(os.path.dirname(output[0])):')
    (619, '            os.mkdir(os.path.dirname(output[0]))')
    (620, '        # import the primer scheme df to get amplicon scheme start and end relative to ref')
    (621, '        primer_df, pool1_primers, pool2_primers = find_primer_scheme(params.primer_scheme,')
    (622, '                                                                    "primer_schemes")')
    (623, "        with open(os.path.join(input[1], \\'amplicon_statistics.pickle\\'), \\'rb\\') as statIn:")
    (624, '            amplicon_stats = pickle.load(statIn)')
    (625, '        regions_covered = {}')
    (626, '        sample_stats = amplicon_stats[os.path.basename(input[0]).replace(".fastq", "").replace(".gz", "")]')
    (627, '        amplicons = list(sample_stats.keys())')
    (628, '        scheme_start = int(primer_df.loc[primer_df[\\\'name\\\'] == amplicons[0].split("---")[0]].reset_index(drop=True)["ref_start"][0])')
    (629, '        scheme_end = int(primer_df.loc[primer_df[\\\'name\\\'] == amplicons[len(amplicons)-1].split("---")[1]].reset_index(drop=True)["ref_start"][0]) + 1')
    (630, '        regions_covered = {"scheme_start": scheme_start,')
    (631, '                            "scheme_end": scheme_end}')
    (632, '        del amplicon_stats')
    (633, '        # run artic assembly pipeline')
    (634, '        epi2me_artic_assemble(params.main_nf,')
    (635, '                            output[0],')
    (636, '                            params.primer_scheme,')
    (637, '                            input[0],')
    (638, '                            regions_covered,')
    (639, '                            params.nextflow_path,')
    (640, '                            params.cache_dir)')
    (641, '')
    (642, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=apoire27/BodennmillerPipeline, file=workflow/Snakefile
context_key: ['if _sample_dict is None']
    (363, 'def get_sample_dict():')
    (364, '    global _sample_dict')
    (365, '    if _sample_dict is None:')
    (366, '        checkpoints.generate_sample_list.get()')
    (367, '        dat_samples = pd.read_csv(file_path_samples)')
    (368, '        _sample_dict = {pathlib.Path(x).stem: x for x in dat_samples[')
    (369, "            \\'mcd_folders\\']}")
    (370, '    return _sample_dict')
    (371, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/Pore-C-Snakemake, file=rules/mapping.smk
context_key: ['if vcf']
    (30, 'def is_phased(wildcards):')
    (31, '    vcf = lookup_value("vcf_path", mapping_df)(wildcards).strip()')
    (32, '    if vcf:')
    (33, '        return True')
    (34, '    else:')
    (35, '        return False')
    (36, '')
    (37, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/Pore-C-Snakemake, file=rules/common.smk
context_key: ['if Path(config["phased_vcfs"]).exists()']
    (17, 'def create_config_dataframes():')
    (18, '    # one row per reference genome')
    (19, '    reference_df = pd.read_table(config["references"], comment="#").set_index(["refgenome_id"], drop=False)')
    (20, '')
    (21, '    # one row per input pore-c fastq')
    (22, '    basecall_df = pd.read_table(config["basecalls"], comment="#").set_index(["run_id", "enzyme"], drop=False)')
    (23, '')
    (24, "    # easier for dataentry to have list of refgenomes on basecall table, here we stack so that there\\'s one row per (basecall, regenome) pair")
    (25, '    run_to_ref_df = (')
    (26, '        basecall_df["refgenome_ids"]')
    (27, '        .str.split(",", expand=True)')
    (28, '        .stack()')
    (29, '        .rename("refgenome_id")')
    (30, '        .to_frame()')
    (31, '        .droplevel(-1, axis=0)')
    (32, '    )')
    (33, '    # now join this to the refgenome table')
    (34, '    mapping_df = basecall_df.join(run_to_ref_df).drop(["refgenome_ids"], axis=1)')
    (35, '')
    (36, '    if Path(config["phased_vcfs"]).exists():')
    (37, '        # if we have a phased VCF for some biospecimen, regenome combination we add it to the mapping table')
    (38, '        phased_vcf_df = pd.read_table(config["phased_vcfs"], comment="#").set_index(["refgenome_id", "biospecimen"])')
    (39, '        mapping_df = pd.merge(')
    (40, '            mapping_df, phased_vcf_df, left_on=["refgenome_id", "biospecimen"], right_index=True, how="left"')
    (41, '        ).fillna({"phase_set_id": "unphased", "vcf_path": ""})')
    (42, '    else:')
    (43, '        mapping_df["phase_set_id"] = "unphased"')
    (44, '        mapping_df["vcf_path"] = ""')
    (45, '')
    (46, '    mapping_df = mapping_df.set_index(["refgenome_id", "phase_set_id"], append=True, drop=False)')
    (47, '')
    (48, '    # can subset the mapping')
    (49, '    if config["mapping_query"]:')
    (50, '        mapping_df = mapping_df.query(config["mapping_query"])')
    (51, '    return basecall_df, reference_df, mapping_df')
    (52, '')
    (53, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/Pore-C-Snakemake, file=rules/common.smk
context_key: ['if df.index.nlevels == 1']
    (94, '    def _inner(wildcards):')
    (95, '        # print(df)')
    (96, '        if df.index.nlevels == 1:')
    (97, '            res = df.loc[wildcards[index_names[0]], :]')
    (98, '            return res[column]')
    (99, '        else:')
    (100, '            row = df.xs(tuple(wildcards[k] for k in index_names), level=index_names, drop_level=True)')
    (101, '            assert len(row) == 1, row')
    (102, '            return row[column].values[0]')
    (103, '')
    (104, '    return _inner')
    (105, '')
    (106, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/Pore-C-Snakemake, file=rules/common.smk
context_key: ['if df.index.nlevels == 1']
    (116, '    def _inner(wildcards):')
    (117, '        # print(df)')
    (118, '        if df.index.nlevels == 1:')
    (119, '            row = df.loc[wildcards[index_names[0]], :]')
    (120, '        else:')
    (121, '            row = df.xs(tuple(wildcards[k] for k in index_names), level=index_names, drop_level=True)')
    (122, '        assert len(row) == 1, row')
    (123, '        return row.iloc[0, :][columns].to_json()')
    (124, '')
    (125, '    return _inner')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dorinemerlat/EXOGAP, file=workflow/rules/prepare_data.smk
context_key: ['if keep_path is True', "if directory[-1] != \\'/\\'"]
    (131, 'def get_data(directory, keep_path = False):')
    (132, '    files = [f for f in listdir(directory) if isfile(join(directory, f))]')
    (133, '')
    (134, '    if keep_path is True:')
    (135, "        if directory[-1] != \\'/\\':")
    (136, "            directory = directory + \\'/\\'")
    (137, '        for i in range(0,len(files)):')
    (138, '            files[i] = directory + files[i]')
    (139, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dorinemerlat/EXOGAP, file=workflow/rules/prepare_data.smk
context_key: ['if keep_path is True']
    (131, 'def get_data(directory, keep_path = False):')
    (132, '    files = [f for f in listdir(directory) if isfile(join(directory, f))]')
    (133, '')
    (134, '    if keep_path is True:')
    (135, "        if directory[-1] != \\'/\\':")
    (136, "            directory = directory + \\'/\\'")
    (137, '        for i in range(0,len(files)):')
    (138, '            files[i] = directory + files[i]')
    (139, '')
    (140, '    return files')
    (141, '')
    (142, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dorinemerlat/CompEvo, file=workflow/Snakefile
context_key: ['if len(i) == 1']
    (18, 'def get_part():')
    (19, '    res = list()')
    (20, '    for i in list(range(1,10001)):')
    (21, '        i = str(i)')
    (22, '        if len(i) == 1:')
    (23, "            i = \\'0000\\' + i")
    (24, '        elif len(i) == 2:')
    (25, "            i = \\'000\\' + i")
    (26, '        elif len(i) == 3:')
    (27, "            i = \\'00\\' + i ")
    (28, '        elif len(i) == 4:')
    (29, "            i = \\'0\\' + i   ")
    (30, '        res.append(i)')
    (31, '    return res')
    (32, '')
    (33, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dorinemerlat/CompEvo, file=workflow/rules/run_orthomcl.smk
context_key: ["if \\'outgroup\\' in value", "if value[\\'outgroup\\'] == True"]
    (253, 'def get_outgroup(INFOS):')
    (254, '    outgroup = ""')
    (255, '    for key, value in INFOS.items():')
    (256, "        if \\'outgroup\\' in value:")
    (257, "            if value[\\'outgroup\\'] == True:")
    (258, "                outgroup += value[\\'code\\'] + \\'_\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dorinemerlat/CompEvo, file=workflow/rules/run_orthomcl.smk
context_key: ["if \\'outgroup\\' in value"]
    (253, 'def get_outgroup(INFOS):')
    (254, '    outgroup = ""')
    (255, '    for key, value in INFOS.items():')
    (256, "        if \\'outgroup\\' in value:")
    (257, "            if value[\\'outgroup\\'] == True:")
    (258, "                outgroup += value[\\'code\\'] + \\'_\\'")
    (259, "    outgroup += \\'_\\'")
    (260, "    outgroup = outgroup.replace(\\'__\\',\\'\\')")
    (261, '    return outgroup')
    (262, '')
    (263, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=frankier/vocabaqdata, file=workflow/Snakefile
context_key: ['if isinstance(src_name, str)']
    (19, '    def __init__(self, name, out, src_name=None):')
    (20, '        self.name = name')
    (21, '        if isinstance(src_name, str):')
    (22, '            src_name = [src_name]')
    (23, '        elif src_name is None:')
    (24, '            src_name = []')
    (25, '        self.src_name = src_name')
    (26, '        self.src = [globals()[src_name] for src_name in src_name]')
    (27, '        self._out = out')
    (28, '')
    (29, '    def is_available(self):')
    (30, '        return all((src != "/does/not/exist" for src in self.src))')
    (31, '')
    (32, '    @property')
    (33, '    def available_out(self) -> List[str]:')
    (34, '        if self.is_available():')
    (35, '            return self._out')
    (36, '        else:')
    (37, '            return []')
    (38, '')
    (39, '    @property')
    (40, '    def out(self):')
    (41, '        return self._out')
    (42, '')
    (43, '    def __str__(self):')
    (44, '        if self.is_available():')
    (45, '            stat = "YES"')
    (46, '        else:')
    (47, '            names = "; ".join(self.src_name)')
    (48, '            stat = f"NO (pass -C {names} to build)"')
    (49, '        return "{:>30} {}".format(self.name, stat)')
    (50, '')
    (51, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tedil/genecart, file=workflow/rules/common.smk
context_key: ['if max_len']
    (26, 'def get_max_len(wildcards):')
    (27, '    max_len = config["genecart"].get("frequent_sets", dict()).get("max_len", None)')
    (28, '    if max_len:')
    (29, '        return int(max_len)')
    (30, '    else:')
    (31, '        return None')
    (32, '')
    (33, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/kraken2_classification, file=Snakefile
context_key: ["if len(s) == 1 or s[0] == \\'Sample\\' or s[0] == \\'#Sample\\' or s[0].startswith(\\'#\\')"]
    (46, 'def get_sample_reads(sample_file):')
    (47, '    sample_reads = {}')
    (48, "    paired_end = \\'\\'")
    (49, '    with open(sample_file) as sf:')
    (50, '        for l in sf.readlines():')
    (51, '            s = l.strip().split("\\\\t")')
    (52, "            if len(s) == 1 or s[0] == \\'Sample\\' or s[0] == \\'#Sample\\' or s[0].startswith(\\'#\\'):")
    (53, '                continue')
    (54, '            sample = s[0]')
    (55, '            # paired end specified')
    (56, '            if (len(s)==3):')
    (57, '                reads = [s[1],s[2]]')
    (58, "                if paired_end != \\'\\' and not paired_end:")
    (59, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (60, '                paired_end = True')
    (61, '            # single end specified')
    (62, '            elif len(s)==2:')
    (63, '                reads=s[1]')
    (64, "                if paired_end != \\'\\' and paired_end:")
    (65, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (66, '                paired_end = False')
    (67, '            if sample in sample_reads:')
    (68, '                raise ValueError("Non-unique sample encountered!")')
    (69, '            sample_reads[sample] = reads')
    (70, '    return (sample_reads, paired_end)')
    (71, '')
    (72, '')
    (73, '# read in sample info and reads from the sample_file')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/kraken2_classification, file=Snakefile
context_key: ['if paired_end']
    (46, 'def get_sample_reads(sample_file):')
    (47, '    sample_reads = {}')
    (48, "    paired_end = \\'\\'")
    (49, '    with open(sample_file) as sf:')
    (50, '        for l in sf.readlines():')
    (51, '            s = l.strip().split("\\\\t")')
    (52, "            if len(s) == 1 or s[0] == \\'Sample\\' or s[0] == \\'#Sample\\' or s[0].startswith(\\'#\\'):")
    (53, '                continue')
    (54, '            sample = s[0]')
    (55, '            # paired end specified')
    (56, '            if (len(s)==3):')
    (57, '                reads = [s[1],s[2]]')
    (58, "                if paired_end != \\'\\' and not paired_end:")
    (59, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (60, '                paired_end = True')
    (61, '            # single end specified')
    (62, '            elif len(s)==2:')
    (63, '                reads=s[1]')
    (64, "                if paired_end != \\'\\' and paired_end:")
    (65, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (66, '                paired_end = False')
    (67, '            if sample in sample_reads:')
    (68, '                raise ValueError("Non-unique sample encountered!")')
    (69, '            sample_reads[sample] = reads')
    (70, '    return (sample_reads, paired_end)')
    (71, '')
    (72, '')
    (73, '# read in sample info and reads from the sample_file')
    (74, "sample_reads, paired_end = get_sample_reads(config[\\'sample_file\\'])")
    (75, 'if paired_end:')
    (76, "    paired_string = \\'--paired\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/kraken2_classification, file=Snakefile
context_key: ["if not \\'confidence_threshold\\' in config"]
    (46, 'def get_sample_reads(sample_file):')
    (47, '    sample_reads = {}')
    (48, "    paired_end = \\'\\'")
    (49, '    with open(sample_file) as sf:')
    (50, '        for l in sf.readlines():')
    (51, '            s = l.strip().split("\\\\t")')
    (52, "            if len(s) == 1 or s[0] == \\'Sample\\' or s[0] == \\'#Sample\\' or s[0].startswith(\\'#\\'):")
    (53, '                continue')
    (54, '            sample = s[0]')
    (55, '            # paired end specified')
    (56, '            if (len(s)==3):')
    (57, '                reads = [s[1],s[2]]')
    (58, "                if paired_end != \\'\\' and not paired_end:")
    (59, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (60, '                paired_end = True')
    (61, '            # single end specified')
    (62, '            elif len(s)==2:')
    (63, '                reads=s[1]')
    (64, "                if paired_end != \\'\\' and paired_end:")
    (65, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (66, '                paired_end = False')
    (67, '            if sample in sample_reads:')
    (68, '                raise ValueError("Non-unique sample encountered!")')
    (69, '            sample_reads[sample] = reads')
    (70, '    return (sample_reads, paired_end)')
    (71, '')
    (72, '')
    (73, '# read in sample info and reads from the sample_file')
    (74, "sample_reads, paired_end = get_sample_reads(config[\\'sample_file\\'])")
    (75, 'if paired_end:')
    (76, "    paired_string = \\'--paired\\'")
    (77, 'else:')
    (78, "    paired_string = \\'\\'")
    (79, 'sample_names = sample_reads.keys()')
    (80, '')
    (81, '# also read in desired confidence threshold for Kraken')
    (82, "if not \\'confidence_threshold\\' in config:")
    (83, "    config[\\'confidence_threshold\\'] = 0.0")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if alg_conf["id"] in [mcmc_traj_conf["id"] for mcmc_traj_conf in config["benchmark_setup"]["evaluation"]["mcmc_traj_plots"]']
    (59, 'def traj_plots():')
    (60, '    ret = [[[[expand("{output_dir}/" \\\\')
    (61, '            "evaluation=/{evaluation_string}/"\\\\')
    (62, '            "adjmat=/{adjmat_string}/"\\\\')
    (63, '            "parameters=/{param_string}/"\\\\')
    (64, '            "data=/{data_string}/"\\\\')
    (65, '            "algorithm=/{alg_string}/" \\\\')
    (66, '            "seed={seed}/"')
    (67, '            "traj_plot.eps",')
    (68, '            output_dir="results",')
    (69, '            alg_string=json_string_mcmc_noest[alg_conf["id"]],')
    (70, '            **alg_conf,')
    (71, '            seed=seed,')
    (72, '            evaluation_string=gen_evaluation_string_from_conf("mcmc_traj_plots", alg_conf["id"]),')
    (73, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (74, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (75, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (76, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (77, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (78, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (79, '                if alg_conf["id"] in [mcmc_traj_conf["id"] for mcmc_traj_conf in config["benchmark_setup"]["evaluation"]["mcmc_traj_plots"] ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if ("active" not in mcmc_traj_conf) or (mcmc_traj_conf["active"] == True)]']
    (59, 'def traj_plots():')
    (60, '    ret = [[[[expand("{output_dir}/" \\\\')
    (61, '            "evaluation=/{evaluation_string}/"\\\\')
    (62, '            "adjmat=/{adjmat_string}/"\\\\')
    (63, '            "parameters=/{param_string}/"\\\\')
    (64, '            "data=/{data_string}/"\\\\')
    (65, '            "algorithm=/{alg_string}/" \\\\')
    (66, '            "seed={seed}/"')
    (67, '            "traj_plot.eps",')
    (68, '            output_dir="results",')
    (69, '            alg_string=json_string_mcmc_noest[alg_conf["id"]],')
    (70, '            **alg_conf,')
    (71, '            seed=seed,')
    (72, '            evaluation_string=gen_evaluation_string_from_conf("mcmc_traj_plots", alg_conf["id"]),')
    (73, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (74, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (75, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (76, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (77, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (78, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (79, '                if alg_conf["id"] in [mcmc_traj_conf["id"] for mcmc_traj_conf in config["benchmark_setup"]["evaluation"]["mcmc_traj_plots"] ')
    (80, '                                                            if ("active" not in mcmc_traj_conf) or (mcmc_traj_conf["active"] == True)] ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if alg_conf["id"] in [mcmc_traj_conf["id"] for mcmc_traj_conf in config["benchmark_setup"]["evaluation"][mcmc_module]']
    (84, 'def processed_trajs(mcmc_module):')
    (85, '    ret = [[[[expand("{output_dir}/" \\\\')
    (86, '            "evaluation=/{evaluation_string}/"\\\\')
    (87, '            "adjmat=/{adjmat_string}/"\\\\')
    (88, '            "parameters=/{param_string}/"\\\\')
    (89, '            "data=/{data_string}/"\\\\')
    (90, '            "algorithm=/{alg_string}/id={id}/"\\\\')
    (91, '            "seed={seed}/"\\\\')
    (92, '            "processed_graphtraj.csv",')
    (93, '            output_dir="results",')
    (94, '            alg_string=json_string_mcmc_noest[alg_conf["id"]],')
    (95, '            **alg_conf, # contains e.g. id')
    (96, '            seed=seed,')
    (97, '            evaluation_string=gen_evaluation_string_from_conf(mcmc_module, alg_conf["id"]),')
    (98, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (99, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (100, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (101, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (102, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (103, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (104, '                if alg_conf["id"] in [mcmc_traj_conf["id"] for mcmc_traj_conf in config["benchmark_setup"]["evaluation"][mcmc_module] ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if ("active" not in mcmc_traj_conf) or (mcmc_traj_conf["active"] == True)]']
    (84, 'def processed_trajs(mcmc_module):')
    (85, '    ret = [[[[expand("{output_dir}/" \\\\')
    (86, '            "evaluation=/{evaluation_string}/"\\\\')
    (87, '            "adjmat=/{adjmat_string}/"\\\\')
    (88, '            "parameters=/{param_string}/"\\\\')
    (89, '            "data=/{data_string}/"\\\\')
    (90, '            "algorithm=/{alg_string}/id={id}/"\\\\')
    (91, '            "seed={seed}/"\\\\')
    (92, '            "processed_graphtraj.csv",')
    (93, '            output_dir="results",')
    (94, '            alg_string=json_string_mcmc_noest[alg_conf["id"]],')
    (95, '            **alg_conf, # contains e.g. id')
    (96, '            seed=seed,')
    (97, '            evaluation_string=gen_evaluation_string_from_conf(mcmc_module, alg_conf["id"]),')
    (98, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (99, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (100, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (101, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (102, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (103, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (104, '                if alg_conf["id"] in [mcmc_traj_conf["id"] for mcmc_traj_conf in config["benchmark_setup"]["evaluation"][mcmc_module] ')
    (105, '                                                            if ("active" not in mcmc_traj_conf) or (mcmc_traj_conf["active"] == True)] ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if alg_conf["id"] in config["benchmark_setup"]["evaluation"]["graph_plots"]']
    (111, 'def bnlearn_graphvizcompare_plots(filename="graphvizcompare",ext="pdf"):')
    (112, '    ret = [[[[expand("{output_dir}/" \\\\')
    (113, '            "evaluation=/{evaluation_string}/"\\\\')
    (114, '            "adjmat=/{adjmat_string}/"\\\\')
    (115, '            "parameters=/{param_string}/"\\\\')
    (116, '            "data=/{data_string}/"\\\\')
    (117, '            "algorithm=/{alg_string}/" \\\\')
    (118, '            "seed={seed}/" + \\\\')
    (119, '            filename + "." + ext,')
    (120, '            output_dir="results",')
    (121, '            alg_string=json_string[alg_conf["id"]],')
    (122, '            **alg_conf,')
    (123, '            seed=seed,')
    (124, '            evaluation_string="graphvizcompare/layout=True",')
    (125, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (126, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (127, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (128, '            for seed in get_seed_range(sim_setup["seed_range"]) if sim_setup["graph_id"] != None]')
    (129, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (130, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (131, '                 if alg_conf["id"] in config["benchmark_setup"]["evaluation"]["graph_plots"]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if alg_conf["id"] in config["benchmark_setup"]["evaluation"]["graph_plots"]']
    (135, 'def adjmat_diffplots(filename="adjmat_diffplot",ext="png"):')
    (136, '    ret = [[[[expand("{output_dir}/" \\\\')
    (137, '            "evaluation=/{evaluation_string}/"\\\\')
    (138, '            "adjmat=/{adjmat_string}/"\\\\')
    (139, '            "parameters=/{param_string}/"\\\\')
    (140, '            "data=/{data_string}/"\\\\')
    (141, '            "algorithm=/{alg_string}/" \\\\')
    (142, '            "seed={seed}/" + \\\\')
    (143, '            filename + "." + ext,')
    (144, '            output_dir="results",')
    (145, '            alg_string=json_string[alg_conf["id"]],')
    (146, '            **alg_conf,')
    (147, '            seed=seed,')
    (148, '            evaluation_string="adjmat_diffplot",')
    (149, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (150, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (151, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (152, '            for seed in get_seed_range(sim_setup["seed_range"]) if sim_setup["graph_id"] != None]')
    (153, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (154, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (155, '                if alg_conf["id"] in config["benchmark_setup"]["evaluation"]["graph_plots"]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if alg_conf["id"] in config["benchmark_setup"]["evaluation"]["graph_plots"]']
    (175, 'def adjmat_plots():')
    (176, '    ret = [[[[expand("{output_dir}/adjmat_estimate/"\\\\               ')
    (177, '            "adjmat=/{adjmat_string}/"\\\\            ')
    (178, '            "parameters=/{param_string}/"\\\\')
    (179, '            "data=/{data_string}/"\\\\            ')
    (180, '            "algorithm=/{alg_string}/"\\\\                            ')
    (181, '            "seed={seed}/"')
    (182, '            "adjmat.eps",')
    (183, '            output_dir="results",')
    (184, '            alg_string=json_string[alg_conf["id"]],')
    (185, '            **alg_conf,')
    (186, '            seed=seed,')
    (187, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (188, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (189, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (190, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (191, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (192, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (193, '                if alg_conf["id"] in config["benchmark_setup"]["evaluation"]["graph_plots"]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if alg_conf["id"] in config["benchmark_setup"]["evaluation"]["graph_plots"]']
    (197, 'def adjmats():')
    (198, '    ret = [[[[expand("{output_dir}/adjmat_estimate/"\\\\               ')
    (199, '            "adjmat=/{adjmat_string}/"\\\\            ')
    (200, '            "parameters=/{param_string}/"\\\\')
    (201, '            "data=/{data_string}/"\\\\            ')
    (202, '            "algorithm=/{alg_string}/"\\\\                            ')
    (203, '            "seed={seed}/"')
    (204, '            "adjmat.csv",')
    (205, '            output_dir="results",')
    (206, '            alg_string=json_string[alg_conf["id"]],')
    (207, '            **alg_conf,')
    (208, '            seed=seed,')
    (209, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (210, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (211, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (212, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (213, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (214, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (215, '                if alg_conf["id"] in config["benchmark_setup"]["evaluation"]["graph_plots"]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if alg_conf["id"] in config["benchmark_setup"]["evaluation"]["graph_plots"]']
    (244, 'def graph_plots():')
    (245, '    ret = [[[[expand("{output_dir}/graph_plot/"\\\\               ')
    (246, '            "adjmat=/{adjmat_string}/"\\\\            ')
    (247, '            "parameters=/{param_string}/"\\\\')
    (248, '            "data=/{data_string}/"\\\\            ')
    (249, '            "algorithm=/{alg_string}/"\\\\                            ')
    (250, '            "seed={seed}.png",')
    (251, '            output_dir="results",')
    (252, '            alg_string=json_string[alg_conf["id"]],')
    (253, '            **alg_conf,')
    (254, '            seed=seed,')
    (255, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (256, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (257, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (258, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (259, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (260, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (261, '                if alg_conf["id"] in config["benchmark_setup"]["evaluation"]["graph_plots"]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if alg_conf["id"] in [conf["id"] for conf in config["benchmark_setup"]["evaluation"]["mcmc_autocorr_plots"]']
    (265, 'def autocorr_plots():')
    (266, '    ret = [[[[expand("{output_dir}/" \\\\')
    (267, '            "evaluation=/{evaluation_string}/"\\\\')
    (268, '            "adjmat=/{adjmat_string}/"\\\\')
    (269, '            "parameters=/{param_string}/"\\\\')
    (270, '            "data=/{data_string}/"\\\\')
    (271, '            "algorithm=/{alg_string}/" \\\\')
    (272, '            "seed={seed}/"')
    (273, '            "autocorr_plot.png",')
    (274, '            output_dir="results",')
    (275, '            alg_string=json_string_mcmc_noest[alg_conf["id"]],')
    (276, '            **alg_conf,')
    (277, '            seed=seed,')
    (278, '            evaluation_string=gen_evaluation_string_from_conf("mcmc_autocorr_plots", alg_conf["id"]),')
    (279, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (280, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (281, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (282, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (283, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (284, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (285, '                if alg_conf["id"] in [conf["id"] for conf in config["benchmark_setup"]["evaluation"]["mcmc_autocorr_plots"] ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if ("active" not in conf) or (conf["active"] == True)]']
    (265, 'def autocorr_plots():')
    (266, '    ret = [[[[expand("{output_dir}/" \\\\')
    (267, '            "evaluation=/{evaluation_string}/"\\\\')
    (268, '            "adjmat=/{adjmat_string}/"\\\\')
    (269, '            "parameters=/{param_string}/"\\\\')
    (270, '            "data=/{data_string}/"\\\\')
    (271, '            "algorithm=/{alg_string}/" \\\\')
    (272, '            "seed={seed}/"')
    (273, '            "autocorr_plot.png",')
    (274, '            output_dir="results",')
    (275, '            alg_string=json_string_mcmc_noest[alg_conf["id"]],')
    (276, '            **alg_conf,')
    (277, '            seed=seed,')
    (278, '            evaluation_string=gen_evaluation_string_from_conf("mcmc_autocorr_plots", alg_conf["id"]),')
    (279, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (280, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (281, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (282, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (283, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (284, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (285, '                if alg_conf["id"] in [conf["id"] for conf in config["benchmark_setup"]["evaluation"]["mcmc_autocorr_plots"] ')
    (286, '                                                    if ("active" not in conf) or (conf["active"] == True)] ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if alg_conf["id"] in [conf["id"] for conf in config["benchmark_setup"]["evaluation"]["mcmc_heatmaps"]']
    (290, 'def heatmap_plots():')
    (291, '    ret = [[[[expand("{output_dir}/" \\\\')
    (292, '            "evaluation=/{evaluation_string}/"\\\\')
    (293, '            "adjmat=/{adjmat_string}/"\\\\')
    (294, '            "parameters=/{param_string}/"\\\\')
    (295, '            "data=/{data_string}/"\\\\')
    (296, '            "algorithm=/{alg_string}/" \\\\')
    (297, '            "seed={seed}/"')
    (298, '            "heatmap_plot.png",')
    (299, '            output_dir="results",')
    (300, '            alg_string=json_string_mcmc_noest[alg_conf["id"]],')
    (301, '            **alg_conf,')
    (302, '            seed=seed,')
    (303, '            evaluation_string=gen_evaluation_string_from_conf("mcmc_heatmaps", alg_conf["id"]),')
    (304, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (305, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (306, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (307, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (308, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (309, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (310, '                if alg_conf["id"] in [conf["id"] for conf in config["benchmark_setup"]["evaluation"]["mcmc_heatmaps"] ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if ("active" not in conf) or (conf["active"] == True)]']
    (290, 'def heatmap_plots():')
    (291, '    ret = [[[[expand("{output_dir}/" \\\\')
    (292, '            "evaluation=/{evaluation_string}/"\\\\')
    (293, '            "adjmat=/{adjmat_string}/"\\\\')
    (294, '            "parameters=/{param_string}/"\\\\')
    (295, '            "data=/{data_string}/"\\\\')
    (296, '            "algorithm=/{alg_string}/" \\\\')
    (297, '            "seed={seed}/"')
    (298, '            "heatmap_plot.png",')
    (299, '            output_dir="results",')
    (300, '            alg_string=json_string_mcmc_noest[alg_conf["id"]],')
    (301, '            **alg_conf,')
    (302, '            seed=seed,')
    (303, '            evaluation_string=gen_evaluation_string_from_conf("mcmc_heatmaps", alg_conf["id"]),')
    (304, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (305, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (306, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (307, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (308, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (309, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (310, '                if alg_conf["id"] in [conf["id"] for conf in config["benchmark_setup"]["evaluation"]["mcmc_heatmaps"] ')
    (311, '                                                    if ("active" not in conf) or (conf["active"] == True)] ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if algid in ["bidag_order_mcmc", "parallelDG", "trilearn_pgibbs", "gg99_singlepair", "gt13_multipair"]', 'rule', 'input']
    (290, 'def heatmap_plots():')
    (291, '    ret = [[[[expand("{output_dir}/" \\\\')
    (292, '            "evaluation=/{evaluation_string}/"\\\\')
    (293, '            "adjmat=/{adjmat_string}/"\\\\')
    (294, '            "parameters=/{param_string}/"\\\\')
    (295, '            "data=/{data_string}/"\\\\')
    (296, '            "algorithm=/{alg_string}/" \\\\')
    (297, '            "seed={seed}/"')
    (298, '            "heatmap_plot.png",')
    (299, '            output_dir="results",')
    (300, '            alg_string=json_string_mcmc_noest[alg_conf["id"]],')
    (301, '            **alg_conf,')
    (302, '            seed=seed,')
    (303, '            evaluation_string=gen_evaluation_string_from_conf("mcmc_heatmaps", alg_conf["id"]),')
    (304, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (305, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (306, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (307, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (308, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (309, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (310, '                if alg_conf["id"] in [conf["id"] for conf in config["benchmark_setup"]["evaluation"]["mcmc_heatmaps"] ')
    (311, '                                                    if ("active" not in conf) or (conf["active"] == True)] ]')
    (312, '            for alg in active_algorithms("mcmc_heatmaps")]')
    (313, '    return ret')
    (314, '')
    (315, '')
    (316, '# From the alg id we could easily determine the varying paramter by checking which key has')
    (317, '# a list instead of a single value. But we need to match the id and we need to match the ')
    (318, '# parameters so that we can get the varying parameter value.')
    (319, '')
    (320, '')
    (321, '# Only get the pattern strings for the actual mcmc algorithms')
    (322, 'mcmc_alg_ids = set()')
    (323, '')
    (324, 'for mcmc_dict in config["benchmark_setup"]["evaluation"]["mcmc_traj_plots"]:')
    (325, '    # get the actual conf')
    (326, '    ')
    (327, '    alg_conf = None')
    (328, '    curalg = None')
    (329, '    for alg, algconfs in config["resources"]["structure_learning_algorithms"].items():  ')
    (330, '        mcmc_alg_ids.add(alg)')
    (331, '# Create adapted anonymous MCMC rules where the algorithm parameters are matched.')
    (332, 'for algid in mcmc_alg_ids:')
    (333, '    if algid in ["bidag_order_mcmc", "parallelDG", "trilearn_pgibbs", "gg99_singlepair", "gt13_multipair"]:')
    (334, '        # Processed graph trajectory')
    (335, '        rule:')
    (336, '            input:                 ')
    (337, '                configfilename, # the variyng param might change')
    (338, '                "workflow/scripts/evaluation/write_graph_traj.py",                ')
    (339, '                traj="{output_dir}/adjvecs/"\\\\               ')
    (340, '                    "adjmat=/{adjmat_string}/"\\\\            ')
    (341, '                    "parameters=/{param_string}/"\\\\')
    (342, '                    "data=/{data_string}/"\\\\')
    (343, '                    "algorithm=/"+pattern_strings[algid]+"/"\\\\                            ')
    (344, '                    "seed={seed}/"')
    (345, '                    "adjvecs.csv"        ')
    (346, '            output:')
    (347, '                traj="{output_dir}/"\\\\')
    (348, '                "evaluation=/" + pattern_strings["mcmc_traj_plots"] + "/"\\\\ ')
    (349, '                "adjmat=/{adjmat_string}/"\\\\            ')
    (350, '                "parameters=/{param_string}/"\\\\')
    (351, '                "data=/{data_string}/"\\\\            ')
    (352, '                "algorithm=/"+pattern_strings[algid]+"/id={id}/"\\\\')
    (353, '                "seed={seed}/" \\\\')
    (354, '                "processed_graphtraj.csv"')
    (355, '            params:')
    (356, '                alg=algid, # Maybe this should be matched in the pattern string instead')
    (357, '                data_string="{data_string}",')
    (358, '                adjmat_string="{adjmat_string}",')
    (359, '                param_string="{param_string}",')
    (360, '                alg_string=pattern_strings[algid],')
    (361, '                eval_string=pattern_strings["mcmc_traj_plots"],')
    (362, '                conf=configfilename')
    (363, '            container:')
    (364, '                docker_image("networkx")')
    (365, '            script:')
    (366, '                "../scripts/evaluation/write_graph_traj.py"')
    (367, '')
    (368, '        # Auto correlations')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if algid in ["bidag_order_mcmc", "parallelDG", "trilearn_pgibbs", "gg99_singlepair", "gt13_multipair"]', 'rule', 'rule', 'input']
    (290, 'def heatmap_plots():')
    (291, '    ret = [[[[expand("{output_dir}/" \\\\')
    (292, '            "evaluation=/{evaluation_string}/"\\\\')
    (293, '            "adjmat=/{adjmat_string}/"\\\\')
    (294, '            "parameters=/{param_string}/"\\\\')
    (295, '            "data=/{data_string}/"\\\\')
    (296, '            "algorithm=/{alg_string}/" \\\\')
    (297, '            "seed={seed}/"')
    (298, '            "heatmap_plot.png",')
    (299, '            output_dir="results",')
    (300, '            alg_string=json_string_mcmc_noest[alg_conf["id"]],')
    (301, '            **alg_conf,')
    (302, '            seed=seed,')
    (303, '            evaluation_string=gen_evaluation_string_from_conf("mcmc_heatmaps", alg_conf["id"]),')
    (304, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (305, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (306, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (307, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (308, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (309, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (310, '                if alg_conf["id"] in [conf["id"] for conf in config["benchmark_setup"]["evaluation"]["mcmc_heatmaps"] ')
    (311, '                                                    if ("active" not in conf) or (conf["active"] == True)] ]')
    (312, '            for alg in active_algorithms("mcmc_heatmaps")]')
    (313, '    return ret')
    (314, '')
    (315, '')
    (316, '# From the alg id we could easily determine the varying paramter by checking which key has')
    (317, '# a list instead of a single value. But we need to match the id and we need to match the ')
    (318, '# parameters so that we can get the varying parameter value.')
    (319, '')
    (320, '')
    (321, '# Only get the pattern strings for the actual mcmc algorithms')
    (322, 'mcmc_alg_ids = set()')
    (323, '')
    (324, 'for mcmc_dict in config["benchmark_setup"]["evaluation"]["mcmc_traj_plots"]:')
    (325, '    # get the actual conf')
    (326, '    ')
    (327, '    alg_conf = None')
    (328, '    curalg = None')
    (329, '    for alg, algconfs in config["resources"]["structure_learning_algorithms"].items():  ')
    (330, '        mcmc_alg_ids.add(alg)')
    (331, '# Create adapted anonymous MCMC rules where the algorithm parameters are matched.')
    (332, 'for algid in mcmc_alg_ids:')
    (333, '    if algid in ["bidag_order_mcmc", "parallelDG", "trilearn_pgibbs", "gg99_singlepair", "gt13_multipair"]:')
    (334, '        # Processed graph trajectory')
    (335, '        rule:')
    (336, '            input:                 ')
    (337, '                configfilename, # the variyng param might change')
    (338, '                "workflow/scripts/evaluation/write_graph_traj.py",                ')
    (339, '                traj="{output_dir}/adjvecs/"\\\\               ')
    (340, '                    "adjmat=/{adjmat_string}/"\\\\            ')
    (341, '                    "parameters=/{param_string}/"\\\\')
    (342, '                    "data=/{data_string}/"\\\\')
    (343, '                    "algorithm=/"+pattern_strings[algid]+"/"\\\\                            ')
    (344, '                    "seed={seed}/"')
    (345, '                    "adjvecs.csv"        ')
    (346, '            output:')
    (347, '                traj="{output_dir}/"\\\\')
    (348, '                "evaluation=/" + pattern_strings["mcmc_traj_plots"] + "/"\\\\ ')
    (349, '                "adjmat=/{adjmat_string}/"\\\\            ')
    (350, '                "parameters=/{param_string}/"\\\\')
    (351, '                "data=/{data_string}/"\\\\            ')
    (352, '                "algorithm=/"+pattern_strings[algid]+"/id={id}/"\\\\')
    (353, '                "seed={seed}/" \\\\')
    (354, '                "processed_graphtraj.csv"')
    (355, '            params:')
    (356, '                alg=algid, # Maybe this should be matched in the pattern string instead')
    (357, '                data_string="{data_string}",')
    (358, '                adjmat_string="{adjmat_string}",')
    (359, '                param_string="{param_string}",')
    (360, '                alg_string=pattern_strings[algid],')
    (361, '                eval_string=pattern_strings["mcmc_traj_plots"],')
    (362, '                conf=configfilename')
    (363, '            container:')
    (364, '                docker_image("networkx")')
    (365, '            script:')
    (366, '                "../scripts/evaluation/write_graph_traj.py"')
    (367, '')
    (368, '        # Auto correlations')
    (369, '        rule:')
    (370, '            input:                 ')
    (371, '                "workflow/scripts/evaluation/write_graph_traj.py",                ')
    (372, '                traj="{output_dir}/"\\\\')
    (373, '                "evaluation=/" + pattern_strings["mcmc_traj_plots"] + "/"\\\\ ')
    (374, '                "adjmat=/{adjmat_string}/"\\\\            ')
    (375, '                "parameters=/{param_string}/"\\\\')
    (376, '                "data=/{data_string}/"\\\\            ')
    (377, '                "algorithm=/"+pattern_strings[algid]+"/id={id}/"\\\\')
    (378, '                "seed={seed}/" \\\\')
    (379, '                "processed_graphtraj.csv"    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if algid in ["bidag_order_mcmc", "parallelDG", "trilearn_pgibbs", "gg99_singlepair", "gt13_multipair"]', 'rule', 'rule', 'output']
    (290, 'def heatmap_plots():')
    (291, '    ret = [[[[expand("{output_dir}/" \\\\')
    (292, '            "evaluation=/{evaluation_string}/"\\\\')
    (293, '            "adjmat=/{adjmat_string}/"\\\\')
    (294, '            "parameters=/{param_string}/"\\\\')
    (295, '            "data=/{data_string}/"\\\\')
    (296, '            "algorithm=/{alg_string}/" \\\\')
    (297, '            "seed={seed}/"')
    (298, '            "heatmap_plot.png",')
    (299, '            output_dir="results",')
    (300, '            alg_string=json_string_mcmc_noest[alg_conf["id"]],')
    (301, '            **alg_conf,')
    (302, '            seed=seed,')
    (303, '            evaluation_string=gen_evaluation_string_from_conf("mcmc_heatmaps", alg_conf["id"]),')
    (304, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (305, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (306, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (307, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (308, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (309, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (310, '                if alg_conf["id"] in [conf["id"] for conf in config["benchmark_setup"]["evaluation"]["mcmc_heatmaps"] ')
    (311, '                                                    if ("active" not in conf) or (conf["active"] == True)] ]')
    (312, '            for alg in active_algorithms("mcmc_heatmaps")]')
    (313, '    return ret')
    (314, '')
    (315, '')
    (316, '# From the alg id we could easily determine the varying paramter by checking which key has')
    (317, '# a list instead of a single value. But we need to match the id and we need to match the ')
    (318, '# parameters so that we can get the varying parameter value.')
    (319, '')
    (320, '')
    (321, '# Only get the pattern strings for the actual mcmc algorithms')
    (322, 'mcmc_alg_ids = set()')
    (323, '')
    (324, 'for mcmc_dict in config["benchmark_setup"]["evaluation"]["mcmc_traj_plots"]:')
    (325, '    # get the actual conf')
    (326, '    ')
    (327, '    alg_conf = None')
    (328, '    curalg = None')
    (329, '    for alg, algconfs in config["resources"]["structure_learning_algorithms"].items():  ')
    (330, '        mcmc_alg_ids.add(alg)')
    (331, '# Create adapted anonymous MCMC rules where the algorithm parameters are matched.')
    (332, 'for algid in mcmc_alg_ids:')
    (333, '    if algid in ["bidag_order_mcmc", "parallelDG", "trilearn_pgibbs", "gg99_singlepair", "gt13_multipair"]:')
    (334, '        # Processed graph trajectory')
    (335, '        rule:')
    (336, '            input:                 ')
    (337, '                configfilename, # the variyng param might change')
    (338, '                "workflow/scripts/evaluation/write_graph_traj.py",                ')
    (339, '                traj="{output_dir}/adjvecs/"\\\\               ')
    (340, '                    "adjmat=/{adjmat_string}/"\\\\            ')
    (341, '                    "parameters=/{param_string}/"\\\\')
    (342, '                    "data=/{data_string}/"\\\\')
    (343, '                    "algorithm=/"+pattern_strings[algid]+"/"\\\\                            ')
    (344, '                    "seed={seed}/"')
    (345, '                    "adjvecs.csv"        ')
    (346, '            output:')
    (347, '                traj="{output_dir}/"\\\\')
    (348, '                "evaluation=/" + pattern_strings["mcmc_traj_plots"] + "/"\\\\ ')
    (349, '                "adjmat=/{adjmat_string}/"\\\\            ')
    (350, '                "parameters=/{param_string}/"\\\\')
    (351, '                "data=/{data_string}/"\\\\            ')
    (352, '                "algorithm=/"+pattern_strings[algid]+"/id={id}/"\\\\')
    (353, '                "seed={seed}/" \\\\')
    (354, '                "processed_graphtraj.csv"')
    (355, '            params:')
    (356, '                alg=algid, # Maybe this should be matched in the pattern string instead')
    (357, '                data_string="{data_string}",')
    (358, '                adjmat_string="{adjmat_string}",')
    (359, '                param_string="{param_string}",')
    (360, '                alg_string=pattern_strings[algid],')
    (361, '                eval_string=pattern_strings["mcmc_traj_plots"],')
    (362, '                conf=configfilename')
    (363, '            container:')
    (364, '                docker_image("networkx")')
    (365, '            script:')
    (366, '                "../scripts/evaluation/write_graph_traj.py"')
    (367, '')
    (368, '        # Auto correlations')
    (369, '        rule:')
    (370, '            input:                 ')
    (371, '                "workflow/scripts/evaluation/write_graph_traj.py",                ')
    (372, '                traj="{output_dir}/"\\\\')
    (373, '                "evaluation=/" + pattern_strings["mcmc_traj_plots"] + "/"\\\\ ')
    (374, '                "adjmat=/{adjmat_string}/"\\\\            ')
    (375, '                "parameters=/{param_string}/"\\\\')
    (376, '                "data=/{data_string}/"\\\\            ')
    (377, '                "algorithm=/"+pattern_strings[algid]+"/id={id}/"\\\\')
    (378, '                "seed={seed}/" \\\\')
    (379, '                "processed_graphtraj.csv"    ')
    (380, '            output:')
    (381, '                traj="{output_dir}/"\\\\')
    (382, '                "evaluation=/" + pattern_strings["mcmc_autocorr_plots"] + "/"\\\\ ')
    (383, '                "adjmat=/{adjmat_string}/"\\\\            ')
    (384, '                "parameters=/{param_string}/"\\\\')
    (385, '                "data=/{data_string}/"\\\\            ')
    (386, '                "algorithm=/"+pattern_strings[algid]+"/id={id}/"\\\\')
    (387, '                "seed={seed}/" \\\\')
    (388, '                "processed_graphtraj.csv"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if algid in ["bidag_order_mcmc", "parallelDG", "trilearn_pgibbs", "gg99_singlepair", "gt13_multipair"]', 'rule', 'rule', 'params']
    (290, 'def heatmap_plots():')
    (291, '    ret = [[[[expand("{output_dir}/" \\\\')
    (292, '            "evaluation=/{evaluation_string}/"\\\\')
    (293, '            "adjmat=/{adjmat_string}/"\\\\')
    (294, '            "parameters=/{param_string}/"\\\\')
    (295, '            "data=/{data_string}/"\\\\')
    (296, '            "algorithm=/{alg_string}/" \\\\')
    (297, '            "seed={seed}/"')
    (298, '            "heatmap_plot.png",')
    (299, '            output_dir="results",')
    (300, '            alg_string=json_string_mcmc_noest[alg_conf["id"]],')
    (301, '            **alg_conf,')
    (302, '            seed=seed,')
    (303, '            evaluation_string=gen_evaluation_string_from_conf("mcmc_heatmaps", alg_conf["id"]),')
    (304, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (305, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (306, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (307, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (308, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (309, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (310, '                if alg_conf["id"] in [conf["id"] for conf in config["benchmark_setup"]["evaluation"]["mcmc_heatmaps"] ')
    (311, '                                                    if ("active" not in conf) or (conf["active"] == True)] ]')
    (312, '            for alg in active_algorithms("mcmc_heatmaps")]')
    (313, '    return ret')
    (314, '')
    (315, '')
    (316, '# From the alg id we could easily determine the varying paramter by checking which key has')
    (317, '# a list instead of a single value. But we need to match the id and we need to match the ')
    (318, '# parameters so that we can get the varying parameter value.')
    (319, '')
    (320, '')
    (321, '# Only get the pattern strings for the actual mcmc algorithms')
    (322, 'mcmc_alg_ids = set()')
    (323, '')
    (324, 'for mcmc_dict in config["benchmark_setup"]["evaluation"]["mcmc_traj_plots"]:')
    (325, '    # get the actual conf')
    (326, '    ')
    (327, '    alg_conf = None')
    (328, '    curalg = None')
    (329, '    for alg, algconfs in config["resources"]["structure_learning_algorithms"].items():  ')
    (330, '        mcmc_alg_ids.add(alg)')
    (331, '# Create adapted anonymous MCMC rules where the algorithm parameters are matched.')
    (332, 'for algid in mcmc_alg_ids:')
    (333, '    if algid in ["bidag_order_mcmc", "parallelDG", "trilearn_pgibbs", "gg99_singlepair", "gt13_multipair"]:')
    (334, '        # Processed graph trajectory')
    (335, '        rule:')
    (336, '            input:                 ')
    (337, '                configfilename, # the variyng param might change')
    (338, '                "workflow/scripts/evaluation/write_graph_traj.py",                ')
    (339, '                traj="{output_dir}/adjvecs/"\\\\               ')
    (340, '                    "adjmat=/{adjmat_string}/"\\\\            ')
    (341, '                    "parameters=/{param_string}/"\\\\')
    (342, '                    "data=/{data_string}/"\\\\')
    (343, '                    "algorithm=/"+pattern_strings[algid]+"/"\\\\                            ')
    (344, '                    "seed={seed}/"')
    (345, '                    "adjvecs.csv"        ')
    (346, '            output:')
    (347, '                traj="{output_dir}/"\\\\')
    (348, '                "evaluation=/" + pattern_strings["mcmc_traj_plots"] + "/"\\\\ ')
    (349, '                "adjmat=/{adjmat_string}/"\\\\            ')
    (350, '                "parameters=/{param_string}/"\\\\')
    (351, '                "data=/{data_string}/"\\\\            ')
    (352, '                "algorithm=/"+pattern_strings[algid]+"/id={id}/"\\\\')
    (353, '                "seed={seed}/" \\\\')
    (354, '                "processed_graphtraj.csv"')
    (355, '            params:')
    (356, '                alg=algid, # Maybe this should be matched in the pattern string instead')
    (357, '                data_string="{data_string}",')
    (358, '                adjmat_string="{adjmat_string}",')
    (359, '                param_string="{param_string}",')
    (360, '                alg_string=pattern_strings[algid],')
    (361, '                eval_string=pattern_strings["mcmc_traj_plots"],')
    (362, '                conf=configfilename')
    (363, '            container:')
    (364, '                docker_image("networkx")')
    (365, '            script:')
    (366, '                "../scripts/evaluation/write_graph_traj.py"')
    (367, '')
    (368, '        # Auto correlations')
    (369, '        rule:')
    (370, '            input:                 ')
    (371, '                "workflow/scripts/evaluation/write_graph_traj.py",                ')
    (372, '                traj="{output_dir}/"\\\\')
    (373, '                "evaluation=/" + pattern_strings["mcmc_traj_plots"] + "/"\\\\ ')
    (374, '                "adjmat=/{adjmat_string}/"\\\\            ')
    (375, '                "parameters=/{param_string}/"\\\\')
    (376, '                "data=/{data_string}/"\\\\            ')
    (377, '                "algorithm=/"+pattern_strings[algid]+"/id={id}/"\\\\')
    (378, '                "seed={seed}/" \\\\')
    (379, '                "processed_graphtraj.csv"    ')
    (380, '            output:')
    (381, '                traj="{output_dir}/"\\\\')
    (382, '                "evaluation=/" + pattern_strings["mcmc_autocorr_plots"] + "/"\\\\ ')
    (383, '                "adjmat=/{adjmat_string}/"\\\\            ')
    (384, '                "parameters=/{param_string}/"\\\\')
    (385, '                "data=/{data_string}/"\\\\            ')
    (386, '                "algorithm=/"+pattern_strings[algid]+"/id={id}/"\\\\')
    (387, '                "seed={seed}/" \\\\')
    (388, '                "processed_graphtraj.csv"')
    (389, '            params:')
    (390, '                alg=algid, # Maybe this should be matched in the pattern string instead')
    (391, '                data_string="{data_string}",')
    (392, '                adjmat_string="{adjmat_string}",')
    (393, '                param_string="{param_string}",')
    (394, '                alg_string=pattern_strings[algid],')
    (395, '                eval_string=pattern_strings["mcmc_autocorr_plots"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if algid in ["bidag_order_mcmc", "parallelDG", "trilearn_pgibbs", "gg99_singlepair", "gt13_multipair"]', 'rule', 'rule', 'container']
    (290, 'def heatmap_plots():')
    (291, '    ret = [[[[expand("{output_dir}/" \\\\')
    (292, '            "evaluation=/{evaluation_string}/"\\\\')
    (293, '            "adjmat=/{adjmat_string}/"\\\\')
    (294, '            "parameters=/{param_string}/"\\\\')
    (295, '            "data=/{data_string}/"\\\\')
    (296, '            "algorithm=/{alg_string}/" \\\\')
    (297, '            "seed={seed}/"')
    (298, '            "heatmap_plot.png",')
    (299, '            output_dir="results",')
    (300, '            alg_string=json_string_mcmc_noest[alg_conf["id"]],')
    (301, '            **alg_conf,')
    (302, '            seed=seed,')
    (303, '            evaluation_string=gen_evaluation_string_from_conf("mcmc_heatmaps", alg_conf["id"]),')
    (304, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (305, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (306, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (307, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (308, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (309, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (310, '                if alg_conf["id"] in [conf["id"] for conf in config["benchmark_setup"]["evaluation"]["mcmc_heatmaps"] ')
    (311, '                                                    if ("active" not in conf) or (conf["active"] == True)] ]')
    (312, '            for alg in active_algorithms("mcmc_heatmaps")]')
    (313, '    return ret')
    (314, '')
    (315, '')
    (316, '# From the alg id we could easily determine the varying paramter by checking which key has')
    (317, '# a list instead of a single value. But we need to match the id and we need to match the ')
    (318, '# parameters so that we can get the varying parameter value.')
    (319, '')
    (320, '')
    (321, '# Only get the pattern strings for the actual mcmc algorithms')
    (322, 'mcmc_alg_ids = set()')
    (323, '')
    (324, 'for mcmc_dict in config["benchmark_setup"]["evaluation"]["mcmc_traj_plots"]:')
    (325, '    # get the actual conf')
    (326, '    ')
    (327, '    alg_conf = None')
    (328, '    curalg = None')
    (329, '    for alg, algconfs in config["resources"]["structure_learning_algorithms"].items():  ')
    (330, '        mcmc_alg_ids.add(alg)')
    (331, '# Create adapted anonymous MCMC rules where the algorithm parameters are matched.')
    (332, 'for algid in mcmc_alg_ids:')
    (333, '    if algid in ["bidag_order_mcmc", "parallelDG", "trilearn_pgibbs", "gg99_singlepair", "gt13_multipair"]:')
    (334, '        # Processed graph trajectory')
    (335, '        rule:')
    (336, '            input:                 ')
    (337, '                configfilename, # the variyng param might change')
    (338, '                "workflow/scripts/evaluation/write_graph_traj.py",                ')
    (339, '                traj="{output_dir}/adjvecs/"\\\\               ')
    (340, '                    "adjmat=/{adjmat_string}/"\\\\            ')
    (341, '                    "parameters=/{param_string}/"\\\\')
    (342, '                    "data=/{data_string}/"\\\\')
    (343, '                    "algorithm=/"+pattern_strings[algid]+"/"\\\\                            ')
    (344, '                    "seed={seed}/"')
    (345, '                    "adjvecs.csv"        ')
    (346, '            output:')
    (347, '                traj="{output_dir}/"\\\\')
    (348, '                "evaluation=/" + pattern_strings["mcmc_traj_plots"] + "/"\\\\ ')
    (349, '                "adjmat=/{adjmat_string}/"\\\\            ')
    (350, '                "parameters=/{param_string}/"\\\\')
    (351, '                "data=/{data_string}/"\\\\            ')
    (352, '                "algorithm=/"+pattern_strings[algid]+"/id={id}/"\\\\')
    (353, '                "seed={seed}/" \\\\')
    (354, '                "processed_graphtraj.csv"')
    (355, '            params:')
    (356, '                alg=algid, # Maybe this should be matched in the pattern string instead')
    (357, '                data_string="{data_string}",')
    (358, '                adjmat_string="{adjmat_string}",')
    (359, '                param_string="{param_string}",')
    (360, '                alg_string=pattern_strings[algid],')
    (361, '                eval_string=pattern_strings["mcmc_traj_plots"],')
    (362, '                conf=configfilename')
    (363, '            container:')
    (364, '                docker_image("networkx")')
    (365, '            script:')
    (366, '                "../scripts/evaluation/write_graph_traj.py"')
    (367, '')
    (368, '        # Auto correlations')
    (369, '        rule:')
    (370, '            input:                 ')
    (371, '                "workflow/scripts/evaluation/write_graph_traj.py",                ')
    (372, '                traj="{output_dir}/"\\\\')
    (373, '                "evaluation=/" + pattern_strings["mcmc_traj_plots"] + "/"\\\\ ')
    (374, '                "adjmat=/{adjmat_string}/"\\\\            ')
    (375, '                "parameters=/{param_string}/"\\\\')
    (376, '                "data=/{data_string}/"\\\\            ')
    (377, '                "algorithm=/"+pattern_strings[algid]+"/id={id}/"\\\\')
    (378, '                "seed={seed}/" \\\\')
    (379, '                "processed_graphtraj.csv"    ')
    (380, '            output:')
    (381, '                traj="{output_dir}/"\\\\')
    (382, '                "evaluation=/" + pattern_strings["mcmc_autocorr_plots"] + "/"\\\\ ')
    (383, '                "adjmat=/{adjmat_string}/"\\\\            ')
    (384, '                "parameters=/{param_string}/"\\\\')
    (385, '                "data=/{data_string}/"\\\\            ')
    (386, '                "algorithm=/"+pattern_strings[algid]+"/id={id}/"\\\\')
    (387, '                "seed={seed}/" \\\\')
    (388, '                "processed_graphtraj.csv"')
    (389, '            params:')
    (390, '                alg=algid, # Maybe this should be matched in the pattern string instead')
    (391, '                data_string="{data_string}",')
    (392, '                adjmat_string="{adjmat_string}",')
    (393, '                param_string="{param_string}",')
    (394, '                alg_string=pattern_strings[algid],')
    (395, '                eval_string=pattern_strings["mcmc_autocorr_plots"]')
    (396, '            container:')
    (397, '                docker_image("networkx")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/evaluation.smk
context_key: ['if algid in ["bidag_order_mcmc", "parallelDG", "trilearn_pgibbs", "gg99_singlepair", "gt13_multipair"]', 'rule', 'rule', 'script']
    (290, 'def heatmap_plots():')
    (291, '    ret = [[[[expand("{output_dir}/" \\\\')
    (292, '            "evaluation=/{evaluation_string}/"\\\\')
    (293, '            "adjmat=/{adjmat_string}/"\\\\')
    (294, '            "parameters=/{param_string}/"\\\\')
    (295, '            "data=/{data_string}/"\\\\')
    (296, '            "algorithm=/{alg_string}/" \\\\')
    (297, '            "seed={seed}/"')
    (298, '            "heatmap_plot.png",')
    (299, '            output_dir="results",')
    (300, '            alg_string=json_string_mcmc_noest[alg_conf["id"]],')
    (301, '            **alg_conf,')
    (302, '            seed=seed,')
    (303, '            evaluation_string=gen_evaluation_string_from_conf("mcmc_heatmaps", alg_conf["id"]),')
    (304, '            adjmat_string=gen_adjmat_string_from_conf(sim_setup["graph_id"], seed), ')
    (305, '            param_string=gen_parameter_string_from_conf(sim_setup["parameters_id"], seed),')
    (306, '            data_string=gen_data_string_from_conf(sim_setup["data_id"], seed, seed_in_path=False))')
    (307, '            for seed in get_seed_range(sim_setup["seed_range"])]')
    (308, '            for sim_setup in config["benchmark_setup"]["data"]]')
    (309, '            for alg_conf in config["resources"]["structure_learning_algorithms"][alg] ')
    (310, '                if alg_conf["id"] in [conf["id"] for conf in config["benchmark_setup"]["evaluation"]["mcmc_heatmaps"] ')
    (311, '                                                    if ("active" not in conf) or (conf["active"] == True)] ]')
    (312, '            for alg in active_algorithms("mcmc_heatmaps")]')
    (313, '    return ret')
    (314, '')
    (315, '')
    (316, '# From the alg id we could easily determine the varying paramter by checking which key has')
    (317, '# a list instead of a single value. But we need to match the id and we need to match the ')
    (318, '# parameters so that we can get the varying parameter value.')
    (319, '')
    (320, '')
    (321, '# Only get the pattern strings for the actual mcmc algorithms')
    (322, 'mcmc_alg_ids = set()')
    (323, '')
    (324, 'for mcmc_dict in config["benchmark_setup"]["evaluation"]["mcmc_traj_plots"]:')
    (325, '    # get the actual conf')
    (326, '    ')
    (327, '    alg_conf = None')
    (328, '    curalg = None')
    (329, '    for alg, algconfs in config["resources"]["structure_learning_algorithms"].items():  ')
    (330, '        mcmc_alg_ids.add(alg)')
    (331, '# Create adapted anonymous MCMC rules where the algorithm parameters are matched.')
    (332, 'for algid in mcmc_alg_ids:')
    (333, '    if algid in ["bidag_order_mcmc", "parallelDG", "trilearn_pgibbs", "gg99_singlepair", "gt13_multipair"]:')
    (334, '        # Processed graph trajectory')
    (335, '        rule:')
    (336, '            input:                 ')
    (337, '                configfilename, # the variyng param might change')
    (338, '                "workflow/scripts/evaluation/write_graph_traj.py",                ')
    (339, '                traj="{output_dir}/adjvecs/"\\\\               ')
    (340, '                    "adjmat=/{adjmat_string}/"\\\\            ')
    (341, '                    "parameters=/{param_string}/"\\\\')
    (342, '                    "data=/{data_string}/"\\\\')
    (343, '                    "algorithm=/"+pattern_strings[algid]+"/"\\\\                            ')
    (344, '                    "seed={seed}/"')
    (345, '                    "adjvecs.csv"        ')
    (346, '            output:')
    (347, '                traj="{output_dir}/"\\\\')
    (348, '                "evaluation=/" + pattern_strings["mcmc_traj_plots"] + "/"\\\\ ')
    (349, '                "adjmat=/{adjmat_string}/"\\\\            ')
    (350, '                "parameters=/{param_string}/"\\\\')
    (351, '                "data=/{data_string}/"\\\\            ')
    (352, '                "algorithm=/"+pattern_strings[algid]+"/id={id}/"\\\\')
    (353, '                "seed={seed}/" \\\\')
    (354, '                "processed_graphtraj.csv"')
    (355, '            params:')
    (356, '                alg=algid, # Maybe this should be matched in the pattern string instead')
    (357, '                data_string="{data_string}",')
    (358, '                adjmat_string="{adjmat_string}",')
    (359, '                param_string="{param_string}",')
    (360, '                alg_string=pattern_strings[algid],')
    (361, '                eval_string=pattern_strings["mcmc_traj_plots"],')
    (362, '                conf=configfilename')
    (363, '            container:')
    (364, '                docker_image("networkx")')
    (365, '            script:')
    (366, '                "../scripts/evaluation/write_graph_traj.py"')
    (367, '')
    (368, '        # Auto correlations')
    (369, '        rule:')
    (370, '            input:                 ')
    (371, '                "workflow/scripts/evaluation/write_graph_traj.py",                ')
    (372, '                traj="{output_dir}/"\\\\')
    (373, '                "evaluation=/" + pattern_strings["mcmc_traj_plots"] + "/"\\\\ ')
    (374, '                "adjmat=/{adjmat_string}/"\\\\            ')
    (375, '                "parameters=/{param_string}/"\\\\')
    (376, '                "data=/{data_string}/"\\\\            ')
    (377, '                "algorithm=/"+pattern_strings[algid]+"/id={id}/"\\\\')
    (378, '                "seed={seed}/" \\\\')
    (379, '                "processed_graphtraj.csv"    ')
    (380, '            output:')
    (381, '                traj="{output_dir}/"\\\\')
    (382, '                "evaluation=/" + pattern_strings["mcmc_autocorr_plots"] + "/"\\\\ ')
    (383, '                "adjmat=/{adjmat_string}/"\\\\            ')
    (384, '                "parameters=/{param_string}/"\\\\')
    (385, '                "data=/{data_string}/"\\\\            ')
    (386, '                "algorithm=/"+pattern_strings[algid]+"/id={id}/"\\\\')
    (387, '                "seed={seed}/" \\\\')
    (388, '                "processed_graphtraj.csv"')
    (389, '            params:')
    (390, '                alg=algid, # Maybe this should be matched in the pattern string instead')
    (391, '                data_string="{data_string}",')
    (392, '                adjmat_string="{adjmat_string}",')
    (393, '                param_string="{param_string}",')
    (394, '                alg_string=pattern_strings[algid],')
    (395, '                eval_string=pattern_strings["mcmc_autocorr_plots"]')
    (396, '            container:')
    (397, '                docker_image("networkx")')
    (398, '            script:')
    (399, '                "../scripts/evaluation/write_autocorr_traj.py"')
    (400, '')
    (401, '')
    (402, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_summary_shell_command.smk
context_key: ['if algorithm == "mylib_myalg"']
    (9, 'def summarise_alg_shell(algorithm):')
    (10, '    if algorithm == "mylib_myalg":        ')
    (11, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (12, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (13, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (14, '                "--filename {output.res} " \\\\                 ')
    (15, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (16, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (17, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (18, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (19, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (20, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (21, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval `cat {input.ntests}` " ')
    (22, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (23, '        return ret')
    (24, '')
    (25, '    elif algorithm == "causaldag_gsp":        ')
    (26, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (27, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (28, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (29, '                "--filename {output.res} " \\\\                 ')
    (30, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (31, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (32, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (33, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (34, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (35, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (36, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval `cat {input.ntests}` " ')
    (37, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (38, '        return ret')
    (39, '')
    (40, '    elif algorithm == "gcastle_pc":        ')
    (41, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (42, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (43, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (44, '                "--filename {output.res} " \\\\                 ')
    (45, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (46, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (47, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (48, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (49, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (50, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (51, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval `cat {input.ntests}` " ')
    (52, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (53, '        return ret')
    (54, '    elif algorithm == "gcastle_anm":        ')
    (55, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (56, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (57, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (58, '                "--filename {output.res} " \\\\                 ')
    (59, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (60, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (61, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (62, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (63, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (64, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (65, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval `cat {input.ntests}` " ')
    (66, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (67, '        return ret')
    (68, '')
    (69, '    elif algorithm == "gcastle_direct_lingam":        ')
    (70, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (71, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (72, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (73, '                "--filename {output.res} " \\\\                 ')
    (74, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (75, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (76, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (77, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (78, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (79, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (80, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval `cat {input.ntests}` " ')
    (81, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (82, '        return ret')
    (83, '')
    (84, '    elif algorithm == "gcastle_ica_lingam":        ')
    (85, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (86, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (87, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (88, '                "--filename {output.res} " \\\\                 ')
    (89, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (90, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (91, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (92, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (93, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (94, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (95, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval `cat {input.ntests}` " ')
    (96, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (97, '        return ret')
    (98, '')
    (99, '    elif algorithm == "gcastle_notears_nonlinear":        ')
    (100, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (101, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (102, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (103, '                "--filename {output.res} " \\\\                 ')
    (104, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (105, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (106, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (107, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (108, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (109, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (110, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval `cat {input.ntests}` " ')
    (111, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (112, '        return ret')
    (113, '')
    (114, '    elif algorithm == "gcastle_notears_low_rank":        ')
    (115, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (116, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (117, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (118, '                "--filename {output.res} " \\\\                 ')
    (119, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (120, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (121, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (122, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (123, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (124, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (125, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval `cat {input.ntests}` " ')
    (126, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (127, '        return ret')
    (128, '')
    (129, '    elif algorithm == "gcastle_golem":        ')
    (130, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (131, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (132, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (133, '                "--filename {output.res} " \\\\                 ')
    (134, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (135, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (136, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (137, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (138, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (139, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (140, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval `cat {input.ntests}` " ')
    (141, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (142, '        return ret')
    (143, '')
    (144, '    elif algorithm == "gcastle_grandag":        ')
    (145, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (146, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (147, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (148, '                "--filename {output.res} " \\\\                 ')
    (149, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (150, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (151, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (152, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (153, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (154, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (155, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval `cat {input.ntests}` " ')
    (156, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (157, '        return ret')
    (158, '')
    (159, '    elif algorithm == "gcastle_mcsl":        ')
    (160, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (161, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (162, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (163, '                "--filename {output.res} " \\\\                 ')
    (164, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (165, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (166, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (167, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (168, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (169, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (170, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval `cat {input.ntests}` " ')
    (171, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (172, '        return ret')
    (173, '')
    (174, '    elif algorithm == "gcastle_gae":        ')
    (175, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (176, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (177, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (178, '                "--filename {output.res} " \\\\                 ')
    (179, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (180, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (181, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (182, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (183, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (184, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (185, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval `cat {input.ntests}` " ')
    (186, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (187, '        return ret')
    (188, '')
    (189, '    elif algorithm == "gcastle_rl":        ')
    (190, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (191, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (192, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (193, '                "--filename {output.res} " \\\\                 ')
    (194, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (195, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (196, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (197, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (198, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (199, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (200, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval `cat {input.ntests}` " ')
    (201, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (202, '        return ret')
    (203, '')
    (204, '    elif algorithm == "gcastle_corl":        ')
    (205, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (206, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (207, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (208, '                "--filename {output.res} " \\\\                 ')
    (209, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (210, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (211, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (212, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (213, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (214, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (215, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval `cat {input.ntests}` " ')
    (216, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (217, '        return ret')
    (218, '')
    (219, '    elif algorithm == "gcastle_notears":        ')
    (220, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (221, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (222, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (223, '                "--filename {output.res} " \\\\                 ')
    (224, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (225, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (226, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (227, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (228, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (229, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (230, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval `cat {input.ntests}` " ')
    (231, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (232, '        return ret')
    (233, '')
    (234, '    # TODO: use dict_to_summary for the algorithms below as well.')
    (235, '    elif algorithm == "gt13_multipair":')
    (236, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (237, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (238, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (239, '                "--filename {output.res} " \\\\ ')
    (240, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (241, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (242, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval gt13_multipair " \\\\')
    (243, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (244, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (245, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (246, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname n_samples       --colval {wildcards.n_samples} " \\\\')
    (247, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname randomits       --colval {wildcards.randomits} " \\\\')
    (248, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname penalty         --colval {wildcards.penalty} " \\\\ ')
    (249, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname prior           --colval {wildcards.prior} " \\\\')
    (250, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ascore          --colval {wildcards.ascore} " \\\\')
    (251, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname bscore          --colval {wildcards.bscore} " \\\\')
    (252, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname clq             --colval {wildcards.clq} " \\\\')
    (253, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname sep             --colval {wildcards.sep} " \\\\')
    (254, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname mcmc_seed       --colval {wildcards.mcmc_seed} " \\\\               ')
    (255, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname datatype       --colval {wildcards.datatype} " \\\\')
    (256, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} " \\\\               ')
    (257, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (258, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (259, '')
    (260, '    elif algorithm == "gg99_singlepair":')
    (261, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (262, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (263, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (264, '                "--filename {output.res} " \\\\ ')
    (265, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (266, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (267, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval gg99_singlepair " \\\\')
    (268, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (269, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (270, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (271, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname n_samples       --colval {wildcards.n_samples} " \\\\')
    (272, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname randomits       --colval {wildcards.randomits} " \\\\')
    (273, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname penalty         --colval {wildcards.penalty} " \\\\')
    (274, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname prior           --colval {wildcards.prior} " \\\\')
    (275, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ascore          --colval {wildcards.ascore} " \\\\')
    (276, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname bscore          --colval {wildcards.bscore} " \\\\')
    (277, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname clq             --colval {wildcards.clq} " \\\\')
    (278, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname sep             --colval {wildcards.sep} " \\\\')
    (279, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname mcmc_seed       --colval {wildcards.mcmc_seed} " \\\\')
    (280, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname datatype       --colval {wildcards.datatype} " \\\\')
    (281, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} " \\\\               ')
    (282, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (283, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (284, '')
    (285, '    elif algorithm == "bnlearn_tabu":')
    (286, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (287, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (288, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (289, '                "--filename {output.res} " \\\\ ')
    (290, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (291, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (292, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval bnlearn_tabu " \\\\')
    (293, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (294, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (295, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (296, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname score           --colval {wildcards.score} " \\\\')
    (297, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname iss             --colval {wildcards.iss} " \\\\')
    (298, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname issmu          --colval {wildcards.issmu} " \\\\')
    (299, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname issw          --colval {wildcards.issw} " \\\\')
    (300, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname l               --colval {wildcards.l} " \\\\')
    (301, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname k               --colval {wildcards.k} " \\\\')
    (302, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname prior           --colval {wildcards.prior} " \\\\')
    (303, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname beta            --colval {wildcards.beta} " \\\\')
    (304, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname start           --colval null " \\\\')
    (305, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname whitelist       --colval null " \\\\')
    (306, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname blacklist       --colval null " \\\\')
    (307, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname debug           --colval false " \\\\ ')
    (308, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname tabu            --colval 10 " \\\\')
    (309, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maxtabu        --colval tabu " \\\\')
    (310, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maxiter        --colval Inf " \\\\')
    (311, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maxp            --colval Inf " \\\\')
    (312, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname optimized       --colval true " \\\\')
    (313, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (314, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (315, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval `cat {input.ntests}` " \\\\')
    (316, '')
    (317, '    elif algorithm == "notears":')
    (318, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (319, '        "--adjmat_true {input.adjmat_true} " \\\\')
    (320, '        "--adjmat_est {input.adjmat_est} " \\\\')
    (321, '        "--filename {output.res} " \\\\ ')
    (322, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (323, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (324, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval notears " \\\\')
    (325, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (326, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (327, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (328, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname min_rate_of_progress        --colval {wildcards.min_rate_of_progress} " \\\\')
    (329, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname penalty_growth_rate        --colval {wildcards.penalty_growth_rate} " \\\\')
    (330, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname optimation_accuracy         --colval {wildcards.optimation_accuracy} " \\\\')
    (331, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname loss                        --colval {wildcards.loss} " \\\\ ')
    (332, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname loss_grad           --colval {wildcards.loss_grad} " \\\\')
    (333, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (334, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (335, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (336, '')
    (337, '    elif algorithm == "sklearn_glasso":')
    (338, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (339, '        "--adjmat_true {input.adjmat_true} " \\\\')
    (340, '        "--adjmat_est {input.adjmat_est} " \\\\')
    (341, '        "--filename {output.res} " \\\\ ')
    (342, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (343, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (344, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval sklearn_glasso " \\\\')
    (345, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (346, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (347, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (348, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha           --colval {wildcards.alpha} " \\\\')
    (349, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname mode            --colval {wildcards.mode} " \\\\')
    (350, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname tol             --colval {wildcards.tol} " \\\\')
    (351, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname enet_tol        --colval {wildcards.enet_tol} " \\\\ ')
    (352, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname verbose         --colval {wildcards.verbose} " \\\\')
    (353, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname precmat_threshold        --colval {wildcards.precmat_threshold} " \\\\ ')
    (354, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname assume_centered --colval {wildcards.assume_centered} " \\\\')
    (355, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (356, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (357, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (358, '')
    (359, '    if algorithm == "bnlearn_hc":')
    (360, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (361, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (362, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (363, '                "--filename {output.res} " \\\\ ')
    (364, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (365, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (366, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval bnlearn_hc " \\\\')
    (367, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (368, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (369, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (370, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname score           --colval {wildcards.score} " \\\\')
    (371, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname iss             --colval {wildcards.iss} " \\\\')
    (372, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname issmu          --colval {wildcards.issmu} " \\\\')
    (373, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname issw          --colval {wildcards.issw} " \\\\')
    (374, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname l               --colval {wildcards.l} " \\\\')
    (375, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname k               --colval {wildcards.k} " \\\\')
    (376, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname prior           --colval {wildcards.prior} " \\\\')
    (377, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname beta            --colval {wildcards.beta} " \\\\')
    (378, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname start           --colval null " \\\\')
    (379, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname whitelist       --colval null " \\\\')
    (380, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname blacklist       --colval null " \\\\')
    (381, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname debug           --colval false " \\\\ ')
    (382, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname perturb         --colval {wildcards.perturb} " \\\\')
    (383, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname restart         --colval {wildcards.restart} " \\\\')
    (384, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maxiter        --colval Inf " \\\\')
    (385, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maxp            --colval Inf " \\\\')
    (386, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname optimized       --colval true " \\\\')
    (387, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (388, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (389, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval `cat {input.ntests}` " \\\\')
    (390, '')
    (391, '')
    (392, '    elif algorithm == "rblip_asobs":')
    (393, '        return "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (394, '        "--adjmat_true {input.adjmat_true} " \\\\')
    (395, '        "--adjmat_est {input.adjmat_est} " \\\\')
    (396, '        "--filename {output.res} " \\\\ ')
    (397, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (398, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (399, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval rblip_asobs " \\\\')
    (400, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (401, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (402, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (403, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout            --colval {wildcards.timeout} " \\\\')
    (404, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname scorermethod   --colval {wildcards.scorermethod} " \\\\')
    (405, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname solvermethod   --colval {wildcards.solvermethod} " \\\\')
    (406, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname indeg           --colval {wildcards.indeg} " \\\\ ')
    (407, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname cores           --colval {wildcards.cores} " \\\\')
    (408, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname allocated       --colval {wildcards.allocated} " \\\\')
    (409, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname scorefunction   --colval {wildcards.scorefunction} " \\\\')
    (410, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha           --colval {wildcards.alpha} " \\\\')
    (411, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname verbose_level         --colval {wildcards.verbose_level} " \\\\')
    (412, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (413, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (414, '')
    (415, '')
    (416, '    elif algorithm == "bidag_itsearch":')
    (417, '        return "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (418, '        "--adjmat_true {input.adjmat_true} " \\\\')
    (419, '        "--adjmat_est {input.adjmat_est} " \\\\')
    (420, '        "--filename {output} " \\\\ ')
    (421, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (422, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate     --colval {wildcards.replicate} "\\\\')
    (423, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm     --colval bidag_itsearch "\\\\')
    (424, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (425, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (426, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\')
    (427, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname plus1it       --colval {wildcards.plus1it} " \\\\')
    (428, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname moveprobs     --colval null " \\\\')
    (429, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname MAP           --colval {wildcards.MAP} " \\\\            ')
    (430, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname posterior     --colval {wildcards.posterior} " \\\\')
    (431, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname iterations    --colval null " \\\\')
    (432, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname stepsave      --colval null " \\\\')
    (433, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname softlimit     --colval {wildcards.softlimit} " \\\\ ')
    (434, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname hardlimit     --colval {wildcards.hardlimit} " \\\\ ')
    (435, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha         --colval {wildcards.alpha} " \\\\ ')
    (436, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname gamma         --colval {wildcards.gamma}  " \\\\ ')
    (437, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname startspace    --colval null " \\\\ ')
    (438, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname blacklist     --colval null " \\\\ ')
    (439, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname verbose       --colval true " \\\\')
    (440, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname chainout      --colval true " \\\\')
    (441, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname scoreout      --colval true " \\\\')
    (442, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname cpdag         --colval {wildcards.cpdag} " \\\\')
    (443, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname mergetype     --colval {wildcards.mergetype} " \\\\')
    (444, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname addspace      --colval null " \\\\')
    (445, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname scoretable    --colval null " \\\\')
    (446, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname startorder    --colval null " \\\\')
    (447, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname estimate          --colval {wildcards.estimate} " \\\\')
    (448, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname scoretype    --colval {wildcards.scoretype} " \\\\')
    (449, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname chi          --colval {wildcards.chi} " \\\\')
    (450, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname edgepf       --colval {wildcards.edgepf} " \\\\')
    (451, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname am           --colval {wildcards.am} " \\\\')
    (452, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname aw           --colval {wildcards.aw} " \\\\')
    (453, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout      --colval {wildcards.timeout} " \\\\')
    (454, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname time         --colval `cat {input.time}` "  \\\\')
    (455, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (456, '')
    (457, '    elif algorithm == "pcalg_pc":')
    (458, '        return "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (459, '        "--adjmat_true {input.adjmat_true} " \\\\')
    (460, '        "--adjmat_est {input.adjmat_est} " \\\\')
    (461, '        "--filename {output} " \\\\ ')
    (462, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (463, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (464, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval pcalg_pc "\\\\')
    (465, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (466, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (467, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (468, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha       --colval {wildcards.alpha} "\\\\')
    (469, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname fixedGaps   --colval null " \\\\')
    (470, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname fixedEdges  --colval null " \\\\')
    (471, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname NAdelete    --colval {wildcards.NAdelete} " \\\\')
    (472, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname mmax       --colval {wildcards.mmax} " \\\\')
    (473, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname conservative --colval {wildcards.conservative} " \\\\')
    (474, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname majrule     --colval {wildcards.majrule} " \\\\')
    (475, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname solveconfl  --colval {wildcards.solveconfl} " \\\\')
    (476, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname numCores     --colval {wildcards.numCores} " \\\\')
    (477, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname verbose      --colval {wildcards.verbose} " \\\\')
    (478, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (479, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname time          --colval `cat {input.time}` "  \\\\')
    (480, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (481, '')
    (482, '    elif algorithm == "dualpc":')
    (483, '        return "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (484, '        "--adjmat_true {input.adjmat_true} " \\\\')
    (485, '        "--adjmat_est {input.adjmat_est} " \\\\')
    (486, '        "--filename {output} " \\\\ ')
    (487, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (488, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (489, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval "+algorithm+" "\\\\')
    (490, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (491, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (492, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (493, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha       --colval {wildcards.alpha} "\\\\')
    (494, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname max_ord       --colval {wildcards.max_ord} " \\\\')
    (495, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname skeleton  --colval {wildcards.skeleton} " \\\\')
    (496, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (497, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname time          --colval `cat {input.time}` "  \\\\')
    (498, '        " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (499, '')
    (500, '    elif algorithm == "bnlearn_mmhc":')
    (501, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (502, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (503, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (504, '                "--filename {output} " \\\\ ')
    (505, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (506, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (507, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval bnlearn_mmhc "\\\\')
    (508, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (509, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (510, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (511, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha       --colval {wildcards.alpha} " \\\\')
    (512, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test       --colval {wildcards.test} " \\\\')
    (513, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname score           --colval {wildcards.score} " \\\\')
    (514, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname iss             --colval {wildcards.iss} " \\\\')
    (515, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname issmu          --colval {wildcards.issmu} " \\\\')
    (516, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname issw          --colval {wildcards.issw} " \\\\')
    (517, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname l               --colval {wildcards.l} " \\\\')
    (518, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname k               --colval {wildcards.k} " \\\\')
    (519, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname prior           --colval {wildcards.prior} " \\\\')
    (520, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname beta            --colval {wildcards.beta} " \\\\')
    (521, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname whitelist   --colval null " \\\\')
    (522, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname debug       --colval false " \\\\')
    (523, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maximizeargs   --colval null " \\\\')
    (524, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (525, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time          --colval `cat {input.time}` " \\\\')
    (526, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval `cat {input.ntests}` " \\\\')
    (527, '')
    (528, '    elif algorithm == "bnlearn_h2pc":')
    (529, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (530, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (531, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (532, '                "--filename {output} " \\\\ ')
    (533, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (534, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (535, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval " + algorithm +" "\\\\')
    (536, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (537, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (538, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (539, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha       --colval {wildcards.alpha} " \\\\')
    (540, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test       --colval {wildcards.test} " \\\\')
    (541, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname score           --colval {wildcards.score} " \\\\')
    (542, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname iss             --colval {wildcards.iss} " \\\\')
    (543, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname issmu          --colval {wildcards.issmu} " \\\\')
    (544, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname issw          --colval {wildcards.issw} " \\\\')
    (545, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname l               --colval {wildcards.l} " \\\\')
    (546, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname k               --colval {wildcards.k} " \\\\')
    (547, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname prior           --colval {wildcards.prior} " \\\\')
    (548, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname beta            --colval {wildcards.beta} " \\\\')
    (549, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname whitelist   --colval null " \\\\')
    (550, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname debug       --colval false " \\\\')
    (551, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maximizeargs   --colval null " \\\\')
    (552, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (553, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time          --colval `cat {input.time}` " \\\\')
    (554, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval `cat {input.ntests}` " \\\\')
    (555, '')
    (556, '    elif algorithm == "bnlearn_rsmax2":')
    (557, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (558, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (559, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (560, '                "--filename {output} " \\\\ ')
    (561, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (562, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (563, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval " + algorithm +" "\\\\')
    (564, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (565, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (566, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (567, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha       --colval {wildcards.alpha} " \\\\')
    (568, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test       --colval {wildcards.test} " \\\\')
    (569, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname score           --colval {wildcards.score} " \\\\')
    (570, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname iss             --colval {wildcards.iss} " \\\\')
    (571, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname issmu          --colval {wildcards.issmu} " \\\\')
    (572, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname issw          --colval {wildcards.issw} " \\\\')
    (573, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname l               --colval {wildcards.l} " \\\\')
    (574, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname k               --colval {wildcards.k} " \\\\')
    (575, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname prior           --colval {wildcards.prior} " \\\\')
    (576, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname beta            --colval {wildcards.beta} " \\\\')
    (577, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname whitelist   --colval null " \\\\')
    (578, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname debug       --colval false " \\\\')
    (579, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maximizeargs   --colval null " \\\\')
    (580, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (581, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time          --colval `cat {input.time}` " \\\\')
    (582, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval `cat {input.ntests}` " \\\\')
    (583, '')
    (584, '    elif algorithm == "bnlearn_interiamb":')
    (585, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (586, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (587, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (588, '                "--filename {output} " \\\\ ')
    (589, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (590, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate        --colval {wildcards.replicate} "\\\\')
    (591, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm        --colval bnlearn_interiamb "\\\\')
    (592, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} "  \\\\       ')
    (593, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters               --colval {wildcards.bn} "  \\\\       ')
    (594, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data             --colval {wildcards.data} "  \\\\       ')
    (595, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha            --colval {wildcards.alpha} " \\\\')
    (596, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname cluster          --colval null " \\\\')
    (597, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname whitelist        --colval null " \\\\')
    (598, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname blacklist        --colval null " \\\\')
    (599, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test             --colval {wildcards.test} " \\\\')
    (600, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname B                --colval {wildcards.B} " \\\\')
    (601, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname debug            --colval {wildcards.debug} " \\\\')
    (602, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maxsx           --colval {wildcards.maxsx} " \\\\')
    (603, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname undirected       --colval {wildcards.undirected} " \\\\')
    (604, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (605, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time             --colval `cat {input.time}` " \\\\')
    (606, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval `cat {input.ntests}` " \\\\')
    (607, '')
    (608, '                ')
    (609, '    elif algorithm == "bnlearn_gs":')
    (610, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (611, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (612, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (613, '                "--filename {output} " \\\\ ')
    (614, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (615, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate        --colval {wildcards.replicate} "\\\\')
    (616, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm        --colval " +algorithm+" "\\\\')
    (617, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} "  \\\\       ')
    (618, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters               --colval {wildcards.bn} "  \\\\       ')
    (619, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data             --colval {wildcards.data} "  \\\\       ')
    (620, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha            --colval {wildcards.alpha} " \\\\')
    (621, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname cluster          --colval null " \\\\')
    (622, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname whitelist        --colval null " \\\\')
    (623, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname blacklist        --colval null " \\\\')
    (624, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test             --colval {wildcards.test} " \\\\')
    (625, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname B                --colval {wildcards.B} " \\\\')
    (626, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname debug            --colval {wildcards.debug} " \\\\')
    (627, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maxsx           --colval {wildcards.maxsx} " \\\\')
    (628, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname undirected       --colval {wildcards.undirected} " \\\\')
    (629, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (630, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time             --colval `cat {input.time}` " \\\\')
    (631, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval `cat {input.ntests}` " \\\\')
    (632, '')
    (633, '    elif algorithm == "bnlearn_pcstable":')
    (634, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (635, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (636, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (637, '                "--filename {output} " \\\\ ')
    (638, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (639, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate        --colval {wildcards.replicate} "\\\\')
    (640, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm        --colval " +algorithm+" "\\\\')
    (641, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} "  \\\\       ')
    (642, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters               --colval {wildcards.bn} "  \\\\       ')
    (643, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data             --colval {wildcards.data} "  \\\\       ')
    (644, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha            --colval {wildcards.alpha} " \\\\')
    (645, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname cluster          --colval null " \\\\')
    (646, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname whitelist        --colval null " \\\\')
    (647, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname blacklist        --colval null " \\\\')
    (648, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test             --colval {wildcards.test} " \\\\')
    (649, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname B                --colval {wildcards.B} " \\\\')
    (650, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname debug            --colval {wildcards.debug} " \\\\')
    (651, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maxsx           --colval {wildcards.maxsx} " \\\\')
    (652, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname undirected       --colval {wildcards.undirected} " \\\\')
    (653, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (654, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time             --colval `cat {input.time}` " \\\\')
    (655, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval `cat {input.ntests}` " \\\\')
    (656, '')
    (657, '    elif algorithm == "bnlearn_iamb":')
    (658, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (659, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (660, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (661, '                "--filename {output} " \\\\ ')
    (662, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (663, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate        --colval {wildcards.replicate} "\\\\')
    (664, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm        --colval " +algorithm+" "\\\\')
    (665, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} "  \\\\       ')
    (666, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters               --colval {wildcards.bn} "  \\\\       ')
    (667, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data             --colval {wildcards.data} "  \\\\       ')
    (668, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha            --colval {wildcards.alpha} " \\\\')
    (669, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname cluster          --colval null " \\\\')
    (670, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname whitelist        --colval null " \\\\')
    (671, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname blacklist        --colval null " \\\\')
    (672, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test             --colval {wildcards.test} " \\\\')
    (673, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname B                --colval {wildcards.B} " \\\\')
    (674, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname debug            --colval {wildcards.debug} " \\\\')
    (675, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maxsx           --colval {wildcards.maxsx} " \\\\')
    (676, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname undirected       --colval {wildcards.undirected} " \\\\')
    (677, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (678, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time             --colval `cat {input.time}` " \\\\')
    (679, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval `cat {input.ntests}` " \\\\')
    (680, '')
    (681, '    elif algorithm == "bnlearn_fastiamb":')
    (682, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (683, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (684, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (685, '                "--filename {output} " \\\\ ')
    (686, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (687, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate        --colval {wildcards.replicate} "\\\\')
    (688, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm        --colval " +algorithm+" "\\\\')
    (689, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} "  \\\\       ')
    (690, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters               --colval {wildcards.bn} "  \\\\       ')
    (691, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data             --colval {wildcards.data} "  \\\\       ')
    (692, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha            --colval {wildcards.alpha} " \\\\')
    (693, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname cluster          --colval null " \\\\')
    (694, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname whitelist        --colval null " \\\\')
    (695, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname blacklist        --colval null " \\\\')
    (696, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test             --colval {wildcards.test} " \\\\')
    (697, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname B                --colval {wildcards.B} " \\\\')
    (698, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname debug            --colval {wildcards.debug} " \\\\')
    (699, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maxsx           --colval {wildcards.maxsx} " \\\\')
    (700, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname undirected       --colval {wildcards.undirected} " \\\\')
    (701, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (702, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time             --colval `cat {input.time}` " \\\\')
    (703, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval `cat {input.ntests}` " \\\\')
    (704, '')
    (705, '    elif algorithm == "bnlearn_iambfdr":')
    (706, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (707, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (708, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (709, '                "--filename {output} " \\\\ ')
    (710, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (711, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate        --colval {wildcards.replicate} "\\\\')
    (712, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm        --colval " +algorithm+" "\\\\')
    (713, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} "  \\\\       ')
    (714, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters               --colval {wildcards.bn} "  \\\\       ')
    (715, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data             --colval {wildcards.data} "  \\\\       ')
    (716, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha            --colval {wildcards.alpha} " \\\\')
    (717, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname cluster          --colval null " \\\\')
    (718, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname whitelist        --colval null " \\\\')
    (719, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname blacklist        --colval null " \\\\')
    (720, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test             --colval {wildcards.test} " \\\\')
    (721, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname B                --colval {wildcards.B} " \\\\')
    (722, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname debug            --colval {wildcards.debug} " \\\\')
    (723, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maxsx           --colval {wildcards.maxsx} " \\\\')
    (724, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname undirected       --colval {wildcards.undirected} " \\\\')
    (725, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (726, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time             --colval `cat {input.time}` " \\\\')
    (727, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval `cat {input.ntests}` " \\\\')
    (728, '')
    (729, '    elif algorithm == "bnlearn_mmpc":')
    (730, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (731, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (732, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (733, '                "--filename {output} " \\\\ ')
    (734, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (735, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate        --colval {wildcards.replicate} "\\\\')
    (736, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm        --colval " +algorithm+" "\\\\')
    (737, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} "  \\\\       ')
    (738, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters               --colval {wildcards.bn} "  \\\\       ')
    (739, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data             --colval {wildcards.data} "  \\\\       ')
    (740, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha            --colval {wildcards.alpha} " \\\\')
    (741, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname cluster          --colval null " \\\\')
    (742, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname whitelist        --colval null " \\\\')
    (743, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname blacklist        --colval null " \\\\')
    (744, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test             --colval {wildcards.test} " \\\\')
    (745, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname B                --colval {wildcards.B} " \\\\')
    (746, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname debug            --colval {wildcards.debug} " \\\\')
    (747, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maxsx           --colval {wildcards.maxsx} " \\\\')
    (748, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname undirected       --colval {wildcards.undirected} " \\\\')
    (749, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (750, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time             --colval `cat {input.time}` " \\\\')
    (751, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval `cat {input.ntests}` " \\\\')
    (752, '')
    (753, '    elif algorithm == "bnlearn_sihitonpc":')
    (754, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (755, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (756, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (757, '                "--filename {output} " \\\\ ')
    (758, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (759, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate        --colval {wildcards.replicate} "\\\\')
    (760, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm        --colval " +algorithm+" "\\\\')
    (761, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} "  \\\\       ')
    (762, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters               --colval {wildcards.bn} "  \\\\       ')
    (763, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data             --colval {wildcards.data} "  \\\\       ')
    (764, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha            --colval {wildcards.alpha} " \\\\')
    (765, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname cluster          --colval null " \\\\')
    (766, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname whitelist        --colval null " \\\\')
    (767, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname blacklist        --colval null " \\\\')
    (768, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test             --colval {wildcards.test} " \\\\')
    (769, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname B                --colval {wildcards.B} " \\\\')
    (770, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname debug            --colval {wildcards.debug} " \\\\')
    (771, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maxsx           --colval {wildcards.maxsx} " \\\\')
    (772, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname undirected       --colval {wildcards.undirected} " \\\\')
    (773, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (774, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time             --colval `cat {input.time}` " \\\\')
    (775, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval `cat {input.ntests}` " \\\\')
    (776, '')
    (777, '    elif algorithm == "bnlearn_hpc":')
    (778, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (779, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (780, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (781, '                "--filename {output} " \\\\ ')
    (782, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (783, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate        --colval {wildcards.replicate} "\\\\')
    (784, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm        --colval " +algorithm+" "\\\\')
    (785, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} "  \\\\       ')
    (786, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters               --colval {wildcards.bn} "  \\\\       ')
    (787, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data             --colval {wildcards.data} "  \\\\       ')
    (788, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha            --colval {wildcards.alpha} " \\\\')
    (789, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname cluster          --colval null " \\\\')
    (790, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname whitelist        --colval null " \\\\')
    (791, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname blacklist        --colval null " \\\\')
    (792, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test             --colval {wildcards.test} " \\\\')
    (793, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname B                --colval {wildcards.B} " \\\\')
    (794, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname debug            --colval {wildcards.debug} " \\\\')
    (795, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname maxsx           --colval {wildcards.maxsx} " \\\\')
    (796, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname undirected       --colval {wildcards.undirected} " \\\\')
    (797, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (798, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time             --colval `cat {input.time}` " \\\\')
    (799, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval `cat {input.ntests}` " \\\\')
    (800, '')
    (801, '    elif algorithm == "tetrad_fges":')
    (802, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (803, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (804, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (805, '                "--filename {output.res} " \\\\ ')
    (806, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (807, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (808, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval tetrad_fges "\\\\')
    (809, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} " \\\\')
    (810, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} " \\\\')
    (811, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data           --colval {wildcards.data} " \\\\')
    (812, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname faithfulnessAssumed       --colval {wildcards.faithfulnessAssumed} "\\\\')
    (813, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname samplePrior       --colval {wildcards.samplePrior} "\\\\')
    (814, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname structurePrior       --colval {wildcards.structurePrior} "\\\\')
    (815, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname penaltyDiscount       --colval {wildcards.penaltyDiscount} "\\\\')
    (816, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (817, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time        --colval `cat {input.time}` " \\\\')
    (818, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (819, '')
    (820, '')
    (821, '    elif algorithm == "tetrad_fci":')
    (822, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (823, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (824, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (825, '                "--filename {output.res} " \\\\ ')
    (826, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (827, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (828, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval tetrad_fci "\\\\')
    (829, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} " \\\\')
    (830, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} " \\\\')
    (831, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data           --colval {wildcards.data} " \\\\')
    (832, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha       --colval {wildcards.alpha} "\\\\')
    (833, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test       --colval {wildcards.test} "\\\\')
    (834, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (835, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time        --colval `cat {input.time}` " \\\\')
    (836, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (837, '')
    (838, '')
    (839, '    elif algorithm == "tetrad_gfci":')
    (840, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (841, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (842, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (843, '                "--filename {output.res} " \\\\ ')
    (844, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (845, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (846, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval tetrad_gfci "\\\\')
    (847, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} " \\\\')
    (848, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} " \\\\')
    (849, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data           --colval {wildcards.data} " \\\\')
    (850, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha       --colval {wildcards.alpha} "\\\\')
    (851, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname score       --colval {wildcards.score} "\\\\')
    (852, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test       --colval {wildcards.test} "\\\\      ')
    (853, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\          ')
    (854, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time        --colval `cat {input.time}` " \\\\')
    (855, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname penaltyDiscount       --colval {wildcards.penaltyDiscount} "\\\\')
    (856, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname samplePrior       --colval {wildcards.samplePrior} "\\\\')
    (857, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname structurePrior       --colval {wildcards.structurePrior} " \\\\')
    (858, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (859, '')
    (860, '    elif algorithm == "tetrad_rfci":')
    (861, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (862, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (863, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (864, '                "--filename {output.res} " \\\\ ')
    (865, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (866, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (867, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval tetrad_rfci "\\\\')
    (868, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} " \\\\')
    (869, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} " \\\\')
    (870, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data           --colval {wildcards.data} " \\\\')
    (871, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha       --colval {wildcards.alpha} "\\\\')
    (872, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test       --colval {wildcards.test} "\\\\')
    (873, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (874, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time        --colval `cat {input.time}` " \\\\')
    (875, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (876, '')
    (877, '    elif algorithm=="gobnilp":')
    (878, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (879, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (880, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (881, '                "--filename {output.res} " \\\\ ')
    (882, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (883, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (884, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval gobnilp "\\\\')
    (885, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (886, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (887, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\')
    (888, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname palim       --colval {wildcards.palim} "\\\\')
    (889, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha_mu       --colval {wildcards.alpha_mu} "\\\\')
    (890, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha_omega_minus_nvars       --colval {wildcards.alpha_omega_minus_nvars} "\\\\')
    (891, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha       --colval {wildcards.alpha} "\\\\')
    (892, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname continuous       --colval {wildcards.continuous} "\\\\')
    (893, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname score_type       --colval {wildcards.score_type} "\\\\')
    (894, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time_limit       --colval {wildcards.time_limit} "\\\\')
    (895, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname gap_limit       --colval {wildcards.gap_limit} "\\\\')
    (896, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname prune       --colval {wildcards.prune} "\\\\')
    (897, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (898, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname constraints       --colval {wildcards.constraints} "\\\\')
    (899, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname extra_args       --colval {wildcards.extra_args} "\\\\')
    (900, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time        --colval `cat {input.time}` " \\\\')
    (901, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (902, '')
    (903, '    elif algorithm == "trilearn_pgibbs":')
    (904, '        return "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (905, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (906, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (907, '                "--filename {output} " \\\\ ')
    (908, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (909, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (910, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval trilearn_pgibbs "\\\\')
    (911, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat      --colval {wildcards.adjmat} "  \\\\       ')
    (912, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters          --colval {wildcards.bn} "  \\\\       ')
    (913, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data        --colval {wildcards.data} "  \\\\')
    (914, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha       --colval {wildcards.alpha} "\\\\')
    (915, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname beta        --colval {wildcards.beta} "\\\\')
    (916, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname radii       --colval {wildcards.radii} "\\\\')
    (917, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname n_particles           --colval {wildcards.n_particles} "\\\\')
    (918, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname M           --colval {wildcards.M} "\\\\')
    (919, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname pseudo_obs  --colval {wildcards.pseudo_obs} "\\\\')
    (920, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha       --colval {wildcards.alpha} "\\\\')
    (921, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname mcmc_seed   --colval {wildcards.mcmc_seed} "\\\\')
    (922, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname datatype   --colval {wildcards.datatype} "\\\\')
    (923, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (924, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time        --colval `cat {input.time}` " \\\\ ')
    (925, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (926, '')
    (927, '    elif algorithm == "parallelDG":')
    (928, '        ret = "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (929, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (930, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (931, '                "--filename {output.res} " \\\\                 ')
    (932, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate       --colval {wildcards.replicate} " \\\\')
    (933, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm       --colval "+ algorithm+" " \\\\')
    (934, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (935, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (936, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\       ')
    (937, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time            --colval `cat {input.time}` " \\\\')
    (938, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests          --colval None " ')
    (939, '        ret += dict_to_summary(config["resources"]["structure_learning_algorithms"][algorithm][0])')
    (940, '        return ret')
    (941, '')
    (942, '    elif algorithm == "bidag_order_mcmc":')
    (943, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (944, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (945, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (946, '                "--filename {output} " \\\\ ')
    (947, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (948, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (949, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval bidag_order_mcmc "\\\\')
    (950, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (951, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (952, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\')
    (953, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname scoretype    --colval {wildcards.scoretype} " \\\\')
    (954, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname chi          --colval {wildcards.chi} " \\\\')
    (955, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname edgepf       --colval {wildcards.edgepf} " \\\\')
    (956, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname am           --colval {wildcards.am} " \\\\')
    (957, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname aw           --colval {wildcards.aw} " \\\\')
    (958, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname map         --colval {wildcards.MAP} "\\\\')
    (959, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname plus1         --colval {wildcards.plus1} "\\\\')
    (960, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname blacklist   --colval null "\\\\')
    (961, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname startorder  --colval null "\\\\')
    (962, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname scoretable  --colval null "\\\\')
    (963, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname moveprobs   --colval null "\\\\')
    (964, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname iterations  --colval {wildcards.iterations} "\\\\')
    (965, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname stepsave    --colval {wildcards.stepsave} "\\\\')
    (966, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname alpha       --colval {wildcards.alpha} "\\\\')
    (967, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname cpdag       --colval {wildcards.cpdag} "\\\\   ')
    (968, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname gamma       --colval {wildcards.gamma} "\\\\')
    (969, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname hardlimit   --colval 15 "\\\\      ')
    (970, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname chainout    --colval true "\\\\   ')
    (971, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname scoreout    --colval true "\\\\   ')
    (972, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname verbose     --colval false "\\\\')
    (973, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname threshold   --colval {wildcards.threshold} "\\\\')
    (974, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname startspace_algorithm   --colval {wildcards.startspace_algorithm} "\\\\')
    (975, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname mcmc_seed   --colval {wildcards.mcmc_seed} "\\\\')
    (976, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout      --colval {wildcards.timeout} " \\\\')
    (977, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time        --colval `cat {input.time}` " \\\\ ')
    (978, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (979, '')
    (980, '    elif algorithm == "bidag_partition_mcmc":')
    (981, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (982, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (983, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (984, '                "--filename {output} " \\\\ ')
    (985, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (986, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (987, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval bidag_partition_mcmc "\\\\')
    (988, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat          --colval {wildcards.adjmat} "  \\\\       ')
    (989, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} "  \\\\       ')
    (990, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data            --colval {wildcards.data} "  \\\\')
    (991, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname scoretype    --colval {wildcards.scoretype} " \\\\')
    (992, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname chi          --colval {wildcards.chi} " \\\\')
    (993, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname edgepf       --colval {wildcards.edgepf} " \\\\')
    (994, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname am           --colval {wildcards.am} " \\\\')
    (995, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname aw           --colval {wildcards.aw} " \\\\')
    (996, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname blacklist   --colval None "\\\\')
    (997, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname scoretable  --colval None "\\\\')
    (998, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname moveprobs   --colval None "\\\\')
    (999, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname startDAG    --colval None "\\\\                ')
    (1000, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname iterations  --colval {wildcards.iterations} "\\\\')
    (1001, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname stepsave    --colval {wildcards.stepsave} "\\\\')
    (1002, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname gamma       --colval {wildcards.gamma} "\\\\')
    (1003, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname scoreout    --colval true "\\\\   ')
    (1004, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname verbose     --colval {wildcards.verbose} "\\\\')
    (1005, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname startspace_algorithm   --colval {wildcards.startspace_algorithm} "\\\\')
    (1006, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname mcmc_seed   --colval {wildcards.mcmc_seed} "\\\\')
    (1007, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout      --colval {wildcards.timeout} " \\\\')
    (1008, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time        --colval `cat {input.time}` " \\\\ ')
    (1009, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (1010, '')
    (1011, '    elif algorithm == "tetrad_fofc":')
    (1012, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (1013, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (1014, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (1015, '                "--filename {output.res} " \\\\')
    (1016, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (1017, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (1018, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval tetrad_fofc "\\\\')
    (1019, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} " \\\\')
    (1020, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} " \\\\')
    (1021, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data           --colval {wildcards.data} " \\\\')
    (1022, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (1023, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time        --colval `cat {input.time}` " \\\\')
    (1024, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (1025, '')
    (1026, '')
    (1027, '    elif algorithm == "tetrad_ftfc":')
    (1028, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (1029, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (1030, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (1031, '                "--filename {output.res} " \\\\')
    (1032, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (1033, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (1034, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval tetrad_ftfc "\\\\')
    (1035, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} " \\\\')
    (1036, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} " \\\\')
    (1037, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data           --colval {wildcards.data} " \\\\')
    (1038, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (1039, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time        --colval `cat {input.time}` " \\\\')
    (1040, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (1041, '')
    (1042, '')
    (1043, '    elif algorithm == "tetrad_fas":')
    (1044, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (1045, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (1046, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (1047, '                "--filename {output.res} " \\\\')
    (1048, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (1049, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (1050, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval tetrad_fas "\\\\')
    (1051, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} " \\\\')
    (1052, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} " \\\\')
    (1053, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data           --colval {wildcards.data} " \\\\')
    (1054, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (1055, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time        --colval `cat {input.time}` " \\\\')
    (1056, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (1057, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test       --colval {wildcards.test} "\\\\')
    (1058, '')
    (1059, '')
    (1060, '')
    (1061, '    elif algorithm == "tetrad_fask":')
    (1062, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (1063, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (1064, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (1065, '                "--filename {output.res} " \\\\')
    (1066, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (1067, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (1068, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval tetrad_fask "\\\\')
    (1069, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} " \\\\')
    (1070, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} " \\\\')
    (1071, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data           --colval {wildcards.data} " \\\\')
    (1072, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (1073, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time        --colval `cat {input.time}` " \\\\')
    (1074, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (1075, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test       --colval {wildcards.test} "\\\\')
    (1076, '')
    (1077, '')
    (1078, '')
    (1079, '    elif algorithm == "tetrad_pc-all":')
    (1080, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (1081, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (1082, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (1083, '                "--filename {output.res} " \\\\')
    (1084, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (1085, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (1086, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval tetrad_pc-all "\\\\')
    (1087, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} " \\\\')
    (1088, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} " \\\\')
    (1089, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data           --colval {wildcards.data} " \\\\')
    (1090, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname test       --colval {wildcards.test} "\\\\')
    (1091, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (1092, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time        --colval `cat {input.time}` " \\\\')
    (1093, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (1094, '')
    (1095, '')
    (1096, '')
    (1097, '    elif algorithm == "tetrad_lingam":')
    (1098, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (1099, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (1100, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (1101, '                "--filename {output.res} " \\\\')
    (1102, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (1103, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (1104, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval tetrad_lingam "\\\\')
    (1105, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} " \\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/docker_images.smk
context_key: ['if name == "trilearn"']
    (2, 'def docker_image(name):')
    (3, '    if name == "trilearn":')
    (4, '        return "docker://onceltuca/trilearn:1.25"')
    (5, '    elif name == "gobnilp":')
    (6, '        return "docker://onceltuca/gobnilp:4347c64"')
    (7, '    elif name == "thomasjava":')
    (8, '        return "docker://onceltuca/thomasgreen:1.19-bp"')
    (9, '    elif name == "notears":')
    (10, '        return "docker://onceltuca/jmoss20notears:1.4-bp"')
    (11, '    elif name == "greenfortran":')
    (12, '        return "docker://onceltuca/guidicigreen1999"')
    (13, '    elif name == "pydatascience":')
    (14, '        return "docker://onceltuca/datascience-python"')
    (15, '    elif name == "bidag":')
    (16, '        return "docker://onceltuca/bidag:2.0.3"')
    (17, '    elif name == "bnlearn":')
    (18, '        return "docker://onceltuca/bnlearn:4.7"')
    (19, '    elif name == "pcalg":')
    (20, '        return "docker://onceltuca/pcalg:2.7-3"')
    (21, '    elif name == "benchmark":')
    (22, '        return "docker://onceltuca/benchpress:1.2.0"')
    (23, '    elif name == "networkx":')
    (24, '        return "docker://onceltuca/networkx:2.5.1"')
    (25, '    elif name == "bdgraph":')
    (26, '        return "docker://onceltuca/bdgraph:2.64"')
    (27, '    elif name == "tetrad":')
    (28, '        return "docker://onceltuca/causal-cmd:1.1.3"')
    (29, '    elif name == "dualpc":')
    (30, '        return "docker://onceltuca/dualpc"')
    (31, '    elif name == "rbase":')
    (32, '        return "docker://r-base"')
    (33, '    elif name == "parallelDG":')
    (34, '        return "docker://hallawalla/paralleldg:0.8"')
    (35, '    elif name == "gcastle":')
    (36, '        return "docker://onceltuca/gcastle:1.0.3"')
    (37, '    elif name == "tidyverse":')
    (38, '        return "docker://onceltuca/tidyverse"')
    (39, '    elif name == "causaldag":')
    (40, '        return "docker://onceltuca/causaldag:0.1a163"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if isinstance(mylist, list)']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gt13_multipair" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gg99_singlepair" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bidag_itsearch" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "tetrad_fges" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "notears" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "tetrad_fci" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "tetrad_gfci" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "tetrad_rfci" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "tetrad_fofc" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "tetrad_ftfc" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "tetrad_fas" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "tetrad_fask" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "tetrad_pc-all" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "tetrad_lingam" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "tetrad_imgscont" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "pcalg_pc" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "dualpc" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bnlearn_mmhc" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bnlearn_interiamb" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bnlearn_gs" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bnlearn_tabu" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bnlearn_hc" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bnlearn_pcstable" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bnlearn_iamb" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bnlearn_fastiamb" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bnlearn_iambfdr" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bnlearn_mmpc" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bnlearn_sihitonpc" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bnlearn_hpc" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bnlearn_h2pc" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bnlearn_rsmax2" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "sklearn_glasso" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gobnilp" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "parallelDG" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "rblip_asobs" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gcastle_notears" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gcastle_pc" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gcastle_anm" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gcastle_direct_lingam" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gcastle_ica_lingam" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gcastle_notears_nonlinear" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gcastle_notears_low_rank" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gcastle_golem" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gcastle_grandag" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
    (149, 'if "gcastle_grandag" in pattern_strings:')
    (150, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_grandag"], **val)')
    (151, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_grandag"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gcastle_mcsl" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
    (149, 'if "gcastle_grandag" in pattern_strings:')
    (150, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_grandag"], **val)')
    (151, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_grandag"]})')
    (152, 'if "gcastle_mcsl" in pattern_strings:')
    (153, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_mcsl"], **val)')
    (154, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_mcsl"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gcastle_gae" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
    (149, 'if "gcastle_grandag" in pattern_strings:')
    (150, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_grandag"], **val)')
    (151, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_grandag"]})')
    (152, 'if "gcastle_mcsl" in pattern_strings:')
    (153, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_mcsl"], **val)')
    (154, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_mcsl"]})')
    (155, 'if "gcastle_gae" in pattern_strings:')
    (156, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_gae"], **val)')
    (157, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_gae"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gcastle_rl" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
    (149, 'if "gcastle_grandag" in pattern_strings:')
    (150, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_grandag"], **val)')
    (151, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_grandag"]})')
    (152, 'if "gcastle_mcsl" in pattern_strings:')
    (153, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_mcsl"], **val)')
    (154, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_mcsl"]})')
    (155, 'if "gcastle_gae" in pattern_strings:')
    (156, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_gae"], **val)')
    (157, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_gae"]})')
    (158, 'if "gcastle_rl" in pattern_strings:')
    (159, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_rl"], **val)')
    (160, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_rl"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gcastle_corl" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
    (149, 'if "gcastle_grandag" in pattern_strings:')
    (150, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_grandag"], **val)')
    (151, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_grandag"]})')
    (152, 'if "gcastle_mcsl" in pattern_strings:')
    (153, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_mcsl"], **val)')
    (154, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_mcsl"]})')
    (155, 'if "gcastle_gae" in pattern_strings:')
    (156, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_gae"], **val)')
    (157, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_gae"]})')
    (158, 'if "gcastle_rl" in pattern_strings:')
    (159, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_rl"], **val)')
    (160, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_rl"]})')
    (161, 'if "gcastle_corl" in pattern_strings:')
    (162, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_corl"], **val)')
    (163, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_corl"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "causaldag_gsp" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
    (149, 'if "gcastle_grandag" in pattern_strings:')
    (150, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_grandag"], **val)')
    (151, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_grandag"]})')
    (152, 'if "gcastle_mcsl" in pattern_strings:')
    (153, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_mcsl"], **val)')
    (154, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_mcsl"]})')
    (155, 'if "gcastle_gae" in pattern_strings:')
    (156, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_gae"], **val)')
    (157, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_gae"]})')
    (158, 'if "gcastle_rl" in pattern_strings:')
    (159, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_rl"], **val)')
    (160, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_rl"]})')
    (161, 'if "gcastle_corl" in pattern_strings:')
    (162, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_corl"], **val)')
    (163, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_corl"]})')
    (164, 'if "causaldag_gsp" in pattern_strings:')
    (165, '    json_string.update({val["id"]: expand(pattern_strings["causaldag_gsp"], **val)')
    (166, '                        for val in config["resources"]["structure_learning_algorithms"]["causaldag_gsp"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "mylib_myalg" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
    (149, 'if "gcastle_grandag" in pattern_strings:')
    (150, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_grandag"], **val)')
    (151, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_grandag"]})')
    (152, 'if "gcastle_mcsl" in pattern_strings:')
    (153, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_mcsl"], **val)')
    (154, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_mcsl"]})')
    (155, 'if "gcastle_gae" in pattern_strings:')
    (156, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_gae"], **val)')
    (157, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_gae"]})')
    (158, 'if "gcastle_rl" in pattern_strings:')
    (159, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_rl"], **val)')
    (160, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_rl"]})')
    (161, 'if "gcastle_corl" in pattern_strings:')
    (162, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_corl"], **val)')
    (163, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_corl"]})')
    (164, 'if "causaldag_gsp" in pattern_strings:')
    (165, '    json_string.update({val["id"]: expand(pattern_strings["causaldag_gsp"], **val)')
    (166, '                        for val in config["resources"]["structure_learning_algorithms"]["causaldag_gsp"]})')
    (167, 'if "mylib_myalg" in pattern_strings:')
    (168, '    json_string.update({val["id"]: expand(pattern_strings["mylib_myalg"], **val)')
    (169, '                        for val in config["resources"]["structure_learning_algorithms"]["mylib_myalg"]})')
    (170, '')
    (171, '')
    (172, '# Order mcmc is special and has to be the last one since it takes input strings as start space.')
    (173, '# Also, the start space path has to be extracted first.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bidag_order_mcmc" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
    (149, 'if "gcastle_grandag" in pattern_strings:')
    (150, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_grandag"], **val)')
    (151, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_grandag"]})')
    (152, 'if "gcastle_mcsl" in pattern_strings:')
    (153, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_mcsl"], **val)')
    (154, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_mcsl"]})')
    (155, 'if "gcastle_gae" in pattern_strings:')
    (156, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_gae"], **val)')
    (157, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_gae"]})')
    (158, 'if "gcastle_rl" in pattern_strings:')
    (159, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_rl"], **val)')
    (160, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_rl"]})')
    (161, 'if "gcastle_corl" in pattern_strings:')
    (162, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_corl"], **val)')
    (163, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_corl"]})')
    (164, 'if "causaldag_gsp" in pattern_strings:')
    (165, '    json_string.update({val["id"]: expand(pattern_strings["causaldag_gsp"], **val)')
    (166, '                        for val in config["resources"]["structure_learning_algorithms"]["causaldag_gsp"]})')
    (167, 'if "mylib_myalg" in pattern_strings:')
    (168, '    json_string.update({val["id"]: expand(pattern_strings["mylib_myalg"], **val)')
    (169, '                        for val in config["resources"]["structure_learning_algorithms"]["mylib_myalg"]})')
    (170, '')
    (171, '')
    (172, '# Order mcmc is special and has to be the last one since it takes input strings as start space.')
    (173, '# Also, the start space path has to be extracted first.')
    (174, 'if "bidag_order_mcmc" in pattern_strings:')
    (175, '    order_mcmc_list = config["resources"]["structure_learning_algorithms"]["bidag_order_mcmc"]')
    (176, '    for items in order_mcmc_list:    ')
    (177, '        items["startspace_algorithm"] = idtopath(items["startspace_algorithm"], json_string)')
    (178, '')
    (179, '    json_string.update({val["id"]: expand(pattern_strings["bidag_order_mcmc"]+"/"+pattern_strings["mcmc_est"], **val,) ')
    (180, '                        for val in order_mcmc_list } )')
    (181, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
    (149, 'if "gcastle_grandag" in pattern_strings:')
    (150, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_grandag"], **val)')
    (151, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_grandag"]})')
    (152, 'if "gcastle_mcsl" in pattern_strings:')
    (153, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_mcsl"], **val)')
    (154, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_mcsl"]})')
    (155, 'if "gcastle_gae" in pattern_strings:')
    (156, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_gae"], **val)')
    (157, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_gae"]})')
    (158, 'if "gcastle_rl" in pattern_strings:')
    (159, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_rl"], **val)')
    (160, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_rl"]})')
    (161, 'if "gcastle_corl" in pattern_strings:')
    (162, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_corl"], **val)')
    (163, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_corl"]})')
    (164, 'if "causaldag_gsp" in pattern_strings:')
    (165, '    json_string.update({val["id"]: expand(pattern_strings["causaldag_gsp"], **val)')
    (166, '                        for val in config["resources"]["structure_learning_algorithms"]["causaldag_gsp"]})')
    (167, 'if "mylib_myalg" in pattern_strings:')
    (168, '    json_string.update({val["id"]: expand(pattern_strings["mylib_myalg"], **val)')
    (169, '                        for val in config["resources"]["structure_learning_algorithms"]["mylib_myalg"]})')
    (170, '')
    (171, '')
    (172, '# Order mcmc is special and has to be the last one since it takes input strings as start space.')
    (173, '# Also, the start space path has to be extracted first.')
    (174, 'if "bidag_order_mcmc" in pattern_strings:')
    (175, '    order_mcmc_list = config["resources"]["structure_learning_algorithms"]["bidag_order_mcmc"]')
    (176, '    for items in order_mcmc_list:    ')
    (177, '        items["startspace_algorithm"] = idtopath(items["startspace_algorithm"], json_string)')
    (178, '')
    (179, '    json_string.update({val["id"]: expand(pattern_strings["bidag_order_mcmc"]+"/"+pattern_strings["mcmc_est"], **val,) ')
    (180, '                        for val in order_mcmc_list } )')
    (181, '')
    (182, 'if "bidag_partition_mcmc" in pattern_strings:')
    (183, '    bidag_partition_mcmc_list = config["resources"]["structure_learning_algorithms"]["bidag_partition_mcmc"]')
    (184, '    # The path to the startspace algorithm is extended here')
    (185, '    for items in bidag_partition_mcmc_list:    ')
    (186, '        items["startspace_algorithm"] = idtopath(items["startspace_algorithm"], json_string)')
    (187, '')
    (188, '    json_string.update({val["id"]: expand(pattern_strings["bidag_partition_mcmc"], **val,) ')
    (189, '                        for val in bidag_partition_mcmc_list } )')
    (190, '')
    (191, '# Since we dont want the mcmc_est when we call the trajectory directly.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bidag_order_mcmc" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
    (149, 'if "gcastle_grandag" in pattern_strings:')
    (150, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_grandag"], **val)')
    (151, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_grandag"]})')
    (152, 'if "gcastle_mcsl" in pattern_strings:')
    (153, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_mcsl"], **val)')
    (154, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_mcsl"]})')
    (155, 'if "gcastle_gae" in pattern_strings:')
    (156, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_gae"], **val)')
    (157, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_gae"]})')
    (158, 'if "gcastle_rl" in pattern_strings:')
    (159, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_rl"], **val)')
    (160, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_rl"]})')
    (161, 'if "gcastle_corl" in pattern_strings:')
    (162, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_corl"], **val)')
    (163, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_corl"]})')
    (164, 'if "causaldag_gsp" in pattern_strings:')
    (165, '    json_string.update({val["id"]: expand(pattern_strings["causaldag_gsp"], **val)')
    (166, '                        for val in config["resources"]["structure_learning_algorithms"]["causaldag_gsp"]})')
    (167, 'if "mylib_myalg" in pattern_strings:')
    (168, '    json_string.update({val["id"]: expand(pattern_strings["mylib_myalg"], **val)')
    (169, '                        for val in config["resources"]["structure_learning_algorithms"]["mylib_myalg"]})')
    (170, '')
    (171, '')
    (172, '# Order mcmc is special and has to be the last one since it takes input strings as start space.')
    (173, '# Also, the start space path has to be extracted first.')
    (174, 'if "bidag_order_mcmc" in pattern_strings:')
    (175, '    order_mcmc_list = config["resources"]["structure_learning_algorithms"]["bidag_order_mcmc"]')
    (176, '    for items in order_mcmc_list:    ')
    (177, '        items["startspace_algorithm"] = idtopath(items["startspace_algorithm"], json_string)')
    (178, '')
    (179, '    json_string.update({val["id"]: expand(pattern_strings["bidag_order_mcmc"]+"/"+pattern_strings["mcmc_est"], **val,) ')
    (180, '                        for val in order_mcmc_list } )')
    (181, '')
    (182, 'if "bidag_partition_mcmc" in pattern_strings:')
    (183, '    bidag_partition_mcmc_list = config["resources"]["structure_learning_algorithms"]["bidag_partition_mcmc"]')
    (184, '    # The path to the startspace algorithm is extended here')
    (185, '    for items in bidag_partition_mcmc_list:    ')
    (186, '        items["startspace_algorithm"] = idtopath(items["startspace_algorithm"], json_string)')
    (187, '')
    (188, '    json_string.update({val["id"]: expand(pattern_strings["bidag_partition_mcmc"], **val,) ')
    (189, '                        for val in bidag_partition_mcmc_list } )')
    (190, '')
    (191, '# Since we dont want the mcmc_est when we call the trajectory directly.')
    (192, 'json_string_mcmc_noest = {}')
    (193, 'if "bidag_order_mcmc" in pattern_strings:')
    (194, '    json_string_mcmc_noest.update({val["id"]: expand(pattern_strings["bidag_order_mcmc"], **val,) ')
    (195, '                        for val in order_mcmc_list } )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
    (149, 'if "gcastle_grandag" in pattern_strings:')
    (150, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_grandag"], **val)')
    (151, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_grandag"]})')
    (152, 'if "gcastle_mcsl" in pattern_strings:')
    (153, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_mcsl"], **val)')
    (154, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_mcsl"]})')
    (155, 'if "gcastle_gae" in pattern_strings:')
    (156, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_gae"], **val)')
    (157, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_gae"]})')
    (158, 'if "gcastle_rl" in pattern_strings:')
    (159, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_rl"], **val)')
    (160, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_rl"]})')
    (161, 'if "gcastle_corl" in pattern_strings:')
    (162, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_corl"], **val)')
    (163, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_corl"]})')
    (164, 'if "causaldag_gsp" in pattern_strings:')
    (165, '    json_string.update({val["id"]: expand(pattern_strings["causaldag_gsp"], **val)')
    (166, '                        for val in config["resources"]["structure_learning_algorithms"]["causaldag_gsp"]})')
    (167, 'if "mylib_myalg" in pattern_strings:')
    (168, '    json_string.update({val["id"]: expand(pattern_strings["mylib_myalg"], **val)')
    (169, '                        for val in config["resources"]["structure_learning_algorithms"]["mylib_myalg"]})')
    (170, '')
    (171, '')
    (172, '# Order mcmc is special and has to be the last one since it takes input strings as start space.')
    (173, '# Also, the start space path has to be extracted first.')
    (174, 'if "bidag_order_mcmc" in pattern_strings:')
    (175, '    order_mcmc_list = config["resources"]["structure_learning_algorithms"]["bidag_order_mcmc"]')
    (176, '    for items in order_mcmc_list:    ')
    (177, '        items["startspace_algorithm"] = idtopath(items["startspace_algorithm"], json_string)')
    (178, '')
    (179, '    json_string.update({val["id"]: expand(pattern_strings["bidag_order_mcmc"]+"/"+pattern_strings["mcmc_est"], **val,) ')
    (180, '                        for val in order_mcmc_list } )')
    (181, '')
    (182, 'if "bidag_partition_mcmc" in pattern_strings:')
    (183, '    bidag_partition_mcmc_list = config["resources"]["structure_learning_algorithms"]["bidag_partition_mcmc"]')
    (184, '    # The path to the startspace algorithm is extended here')
    (185, '    for items in bidag_partition_mcmc_list:    ')
    (186, '        items["startspace_algorithm"] = idtopath(items["startspace_algorithm"], json_string)')
    (187, '')
    (188, '    json_string.update({val["id"]: expand(pattern_strings["bidag_partition_mcmc"], **val,) ')
    (189, '                        for val in bidag_partition_mcmc_list } )')
    (190, '')
    (191, '# Since we dont want the mcmc_est when we call the trajectory directly.')
    (192, 'json_string_mcmc_noest = {}')
    (193, 'if "bidag_order_mcmc" in pattern_strings:')
    (194, '    json_string_mcmc_noest.update({val["id"]: expand(pattern_strings["bidag_order_mcmc"], **val,) ')
    (195, '                        for val in order_mcmc_list } )')
    (196, 'if "bidag_partition_mcmc" in pattern_strings:')
    (197, '    json_string_mcmc_noest.update({val["id"]: expand(pattern_strings["bidag_partition_mcmc"], **val,) ')
    (198, '                        for val in bidag_partition_mcmc_list } )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
    (149, 'if "gcastle_grandag" in pattern_strings:')
    (150, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_grandag"], **val)')
    (151, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_grandag"]})')
    (152, 'if "gcastle_mcsl" in pattern_strings:')
    (153, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_mcsl"], **val)')
    (154, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_mcsl"]})')
    (155, 'if "gcastle_gae" in pattern_strings:')
    (156, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_gae"], **val)')
    (157, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_gae"]})')
    (158, 'if "gcastle_rl" in pattern_strings:')
    (159, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_rl"], **val)')
    (160, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_rl"]})')
    (161, 'if "gcastle_corl" in pattern_strings:')
    (162, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_corl"], **val)')
    (163, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_corl"]})')
    (164, 'if "causaldag_gsp" in pattern_strings:')
    (165, '    json_string.update({val["id"]: expand(pattern_strings["causaldag_gsp"], **val)')
    (166, '                        for val in config["resources"]["structure_learning_algorithms"]["causaldag_gsp"]})')
    (167, 'if "mylib_myalg" in pattern_strings:')
    (168, '    json_string.update({val["id"]: expand(pattern_strings["mylib_myalg"], **val)')
    (169, '                        for val in config["resources"]["structure_learning_algorithms"]["mylib_myalg"]})')
    (170, '')
    (171, '')
    (172, '# Order mcmc is special and has to be the last one since it takes input strings as start space.')
    (173, '# Also, the start space path has to be extracted first.')
    (174, 'if "bidag_order_mcmc" in pattern_strings:')
    (175, '    order_mcmc_list = config["resources"]["structure_learning_algorithms"]["bidag_order_mcmc"]')
    (176, '    for items in order_mcmc_list:    ')
    (177, '        items["startspace_algorithm"] = idtopath(items["startspace_algorithm"], json_string)')
    (178, '')
    (179, '    json_string.update({val["id"]: expand(pattern_strings["bidag_order_mcmc"]+"/"+pattern_strings["mcmc_est"], **val,) ')
    (180, '                        for val in order_mcmc_list } )')
    (181, '')
    (182, 'if "bidag_partition_mcmc" in pattern_strings:')
    (183, '    bidag_partition_mcmc_list = config["resources"]["structure_learning_algorithms"]["bidag_partition_mcmc"]')
    (184, '    # The path to the startspace algorithm is extended here')
    (185, '    for items in bidag_partition_mcmc_list:    ')
    (186, '        items["startspace_algorithm"] = idtopath(items["startspace_algorithm"], json_string)')
    (187, '')
    (188, '    json_string.update({val["id"]: expand(pattern_strings["bidag_partition_mcmc"], **val,) ')
    (189, '                        for val in bidag_partition_mcmc_list } )')
    (190, '')
    (191, '# Since we dont want the mcmc_est when we call the trajectory directly.')
    (192, 'json_string_mcmc_noest = {}')
    (193, 'if "bidag_order_mcmc" in pattern_strings:')
    (194, '    json_string_mcmc_noest.update({val["id"]: expand(pattern_strings["bidag_order_mcmc"], **val,) ')
    (195, '                        for val in order_mcmc_list } )')
    (196, 'if "bidag_partition_mcmc" in pattern_strings:')
    (197, '    json_string_mcmc_noest.update({val["id"]: expand(pattern_strings["bidag_partition_mcmc"], **val,) ')
    (198, '                        for val in bidag_partition_mcmc_list } )')
    (199, 'if "trilearn_pgibbs" in pattern_strings:')
    (200, '    json_string_mcmc_noest.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (201, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "parallelDG" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
    (149, 'if "gcastle_grandag" in pattern_strings:')
    (150, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_grandag"], **val)')
    (151, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_grandag"]})')
    (152, 'if "gcastle_mcsl" in pattern_strings:')
    (153, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_mcsl"], **val)')
    (154, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_mcsl"]})')
    (155, 'if "gcastle_gae" in pattern_strings:')
    (156, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_gae"], **val)')
    (157, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_gae"]})')
    (158, 'if "gcastle_rl" in pattern_strings:')
    (159, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_rl"], **val)')
    (160, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_rl"]})')
    (161, 'if "gcastle_corl" in pattern_strings:')
    (162, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_corl"], **val)')
    (163, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_corl"]})')
    (164, 'if "causaldag_gsp" in pattern_strings:')
    (165, '    json_string.update({val["id"]: expand(pattern_strings["causaldag_gsp"], **val)')
    (166, '                        for val in config["resources"]["structure_learning_algorithms"]["causaldag_gsp"]})')
    (167, 'if "mylib_myalg" in pattern_strings:')
    (168, '    json_string.update({val["id"]: expand(pattern_strings["mylib_myalg"], **val)')
    (169, '                        for val in config["resources"]["structure_learning_algorithms"]["mylib_myalg"]})')
    (170, '')
    (171, '')
    (172, '# Order mcmc is special and has to be the last one since it takes input strings as start space.')
    (173, '# Also, the start space path has to be extracted first.')
    (174, 'if "bidag_order_mcmc" in pattern_strings:')
    (175, '    order_mcmc_list = config["resources"]["structure_learning_algorithms"]["bidag_order_mcmc"]')
    (176, '    for items in order_mcmc_list:    ')
    (177, '        items["startspace_algorithm"] = idtopath(items["startspace_algorithm"], json_string)')
    (178, '')
    (179, '    json_string.update({val["id"]: expand(pattern_strings["bidag_order_mcmc"]+"/"+pattern_strings["mcmc_est"], **val,) ')
    (180, '                        for val in order_mcmc_list } )')
    (181, '')
    (182, 'if "bidag_partition_mcmc" in pattern_strings:')
    (183, '    bidag_partition_mcmc_list = config["resources"]["structure_learning_algorithms"]["bidag_partition_mcmc"]')
    (184, '    # The path to the startspace algorithm is extended here')
    (185, '    for items in bidag_partition_mcmc_list:    ')
    (186, '        items["startspace_algorithm"] = idtopath(items["startspace_algorithm"], json_string)')
    (187, '')
    (188, '    json_string.update({val["id"]: expand(pattern_strings["bidag_partition_mcmc"], **val,) ')
    (189, '                        for val in bidag_partition_mcmc_list } )')
    (190, '')
    (191, '# Since we dont want the mcmc_est when we call the trajectory directly.')
    (192, 'json_string_mcmc_noest = {}')
    (193, 'if "bidag_order_mcmc" in pattern_strings:')
    (194, '    json_string_mcmc_noest.update({val["id"]: expand(pattern_strings["bidag_order_mcmc"], **val,) ')
    (195, '                        for val in order_mcmc_list } )')
    (196, 'if "bidag_partition_mcmc" in pattern_strings:')
    (197, '    json_string_mcmc_noest.update({val["id"]: expand(pattern_strings["bidag_partition_mcmc"], **val,) ')
    (198, '                        for val in bidag_partition_mcmc_list } )')
    (199, 'if "trilearn_pgibbs" in pattern_strings:')
    (200, '    json_string_mcmc_noest.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (201, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (202, 'if "parallelDG" in pattern_strings:')
    (203, '    json_string_mcmc_noest.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (204, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gt13_multipair" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
    (149, 'if "gcastle_grandag" in pattern_strings:')
    (150, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_grandag"], **val)')
    (151, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_grandag"]})')
    (152, 'if "gcastle_mcsl" in pattern_strings:')
    (153, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_mcsl"], **val)')
    (154, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_mcsl"]})')
    (155, 'if "gcastle_gae" in pattern_strings:')
    (156, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_gae"], **val)')
    (157, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_gae"]})')
    (158, 'if "gcastle_rl" in pattern_strings:')
    (159, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_rl"], **val)')
    (160, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_rl"]})')
    (161, 'if "gcastle_corl" in pattern_strings:')
    (162, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_corl"], **val)')
    (163, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_corl"]})')
    (164, 'if "causaldag_gsp" in pattern_strings:')
    (165, '    json_string.update({val["id"]: expand(pattern_strings["causaldag_gsp"], **val)')
    (166, '                        for val in config["resources"]["structure_learning_algorithms"]["causaldag_gsp"]})')
    (167, 'if "mylib_myalg" in pattern_strings:')
    (168, '    json_string.update({val["id"]: expand(pattern_strings["mylib_myalg"], **val)')
    (169, '                        for val in config["resources"]["structure_learning_algorithms"]["mylib_myalg"]})')
    (170, '')
    (171, '')
    (172, '# Order mcmc is special and has to be the last one since it takes input strings as start space.')
    (173, '# Also, the start space path has to be extracted first.')
    (174, 'if "bidag_order_mcmc" in pattern_strings:')
    (175, '    order_mcmc_list = config["resources"]["structure_learning_algorithms"]["bidag_order_mcmc"]')
    (176, '    for items in order_mcmc_list:    ')
    (177, '        items["startspace_algorithm"] = idtopath(items["startspace_algorithm"], json_string)')
    (178, '')
    (179, '    json_string.update({val["id"]: expand(pattern_strings["bidag_order_mcmc"]+"/"+pattern_strings["mcmc_est"], **val,) ')
    (180, '                        for val in order_mcmc_list } )')
    (181, '')
    (182, 'if "bidag_partition_mcmc" in pattern_strings:')
    (183, '    bidag_partition_mcmc_list = config["resources"]["structure_learning_algorithms"]["bidag_partition_mcmc"]')
    (184, '    # The path to the startspace algorithm is extended here')
    (185, '    for items in bidag_partition_mcmc_list:    ')
    (186, '        items["startspace_algorithm"] = idtopath(items["startspace_algorithm"], json_string)')
    (187, '')
    (188, '    json_string.update({val["id"]: expand(pattern_strings["bidag_partition_mcmc"], **val,) ')
    (189, '                        for val in bidag_partition_mcmc_list } )')
    (190, '')
    (191, '# Since we dont want the mcmc_est when we call the trajectory directly.')
    (192, 'json_string_mcmc_noest = {}')
    (193, 'if "bidag_order_mcmc" in pattern_strings:')
    (194, '    json_string_mcmc_noest.update({val["id"]: expand(pattern_strings["bidag_order_mcmc"], **val,) ')
    (195, '                        for val in order_mcmc_list } )')
    (196, 'if "bidag_partition_mcmc" in pattern_strings:')
    (197, '    json_string_mcmc_noest.update({val["id"]: expand(pattern_strings["bidag_partition_mcmc"], **val,) ')
    (198, '                        for val in bidag_partition_mcmc_list } )')
    (199, 'if "trilearn_pgibbs" in pattern_strings:')
    (200, '    json_string_mcmc_noest.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (201, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (202, 'if "parallelDG" in pattern_strings:')
    (203, '    json_string_mcmc_noest.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (204, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (205, 'if "gt13_multipair" in pattern_strings:')
    (206, '    json_string_mcmc_noest.update({val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (207, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_strings.smk
context_key: ['if "gg99_singlepair" in pattern_strings']
    (9, 'def idtopath(mylist, json_string):')
    (10, '    if isinstance(mylist, list):')
    (11, '        return [json_string[startalg][0] for startalg in mylist]')
    (12, '    else:')
    (13, '        return json_string[str(mylist)]')
    (14, '')
    (15, 'json_string = {}')
    (16, '')
    (17, 'if "gt13_multipair" in pattern_strings:')
    (18, '    json_string = {val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (19, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]}')
    (20, 'if "gg99_singlepair" in pattern_strings:')
    (21, '    json_string.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (22, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
    (23, 'if "bidag_itsearch" in pattern_strings:')
    (24, '    json_string.update({val["id"]: expand(pattern_strings["bidag_itsearch"], **val)')
    (25, '                        for val in config["resources"]["structure_learning_algorithms"]["bidag_itsearch"]})')
    (26, 'if "tetrad_fges" in pattern_strings:')
    (27, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fges"], **val)')
    (28, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fges"]})')
    (29, 'if "notears" in pattern_strings:')
    (30, '    json_string.update({val["id"]: expand(pattern_strings["notears"], **val) ')
    (31, '                        for val in config["resources"]["structure_learning_algorithms"]["notears"]})')
    (32, 'if "tetrad_fci" in pattern_strings:')
    (33, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fci"], **val) ')
    (34, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fci"]})')
    (35, 'if "tetrad_gfci" in pattern_strings:')
    (36, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_gfci"], **val) ')
    (37, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_gfci"]})')
    (38, 'if "tetrad_rfci" in pattern_strings:')
    (39, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_rfci"], **val) ')
    (40, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_rfci"]})')
    (41, 'if "tetrad_fofc" in pattern_strings:')
    (42, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fofc"], **val)')
    (43, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fofc"]})')
    (44, 'if "tetrad_ftfc" in pattern_strings:')
    (45, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_ftfc"], **val)')
    (46, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_ftfc"]})')
    (47, 'if "tetrad_fas" in pattern_strings:')
    (48, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fas"], **val)')
    (49, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fas"]})')
    (50, 'if "tetrad_fask" in pattern_strings:')
    (51, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_fask"], **val)')
    (52, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_fask"]})')
    (53, 'if "tetrad_pc-all" in pattern_strings:')
    (54, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_pc-all"], **val)')
    (55, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_pc-all"]})')
    (56, 'if "tetrad_lingam" in pattern_strings:')
    (57, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_lingam"], **val)')
    (58, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_lingam"]})')
    (59, 'if "tetrad_imgscont" in pattern_strings:')
    (60, '    json_string.update({val["id"]: expand(pattern_strings["tetrad_imgscont"], **val)')
    (61, '                        for val in config["resources"]["structure_learning_algorithms"]["tetrad_imgscont"]})')
    (62, 'if "pcalg_pc" in pattern_strings:')
    (63, '    json_string.update({val["id"]: expand(pattern_strings["pcalg_pc"], **val)  ')
    (64, '                        for val in config["resources"]["structure_learning_algorithms"]["pcalg_pc"]})')
    (65, 'if "dualpc" in pattern_strings:')
    (66, '    json_string.update({val["id"]: expand(pattern_strings["dualpc"], **val)  ')
    (67, '                        for val in config["resources"]["structure_learning_algorithms"]["dualpc"]})')
    (68, 'if "bnlearn_mmhc" in pattern_strings:')
    (69, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_mmhc"], **val)')
    (70, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmhc"]} )')
    (71, 'if "bnlearn_interiamb" in pattern_strings:')
    (72, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_interiamb"], **val)')
    (73, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_interiamb"]} )')
    (74, 'if "bnlearn_gs" in pattern_strings:')
    (75, '    json_string.update({val["id"]: expand(pattern_strings["bnlearn_gs"], **val)')
    (76, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_gs"]} )')
    (77, 'if "bnlearn_tabu" in pattern_strings:')
    (78, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_tabu"], **val)')
    (79, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_tabu"]})')
    (80, 'if "bnlearn_hc" in pattern_strings:')
    (81, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hc"], **val)')
    (82, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hc"]})')
    (83, 'if "bnlearn_pcstable" in pattern_strings:')
    (84, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_pcstable"], **val)')
    (85, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_pcstable"]})')
    (86, 'if "bnlearn_iamb" in pattern_strings:')
    (87, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iamb"], **val)')
    (88, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iamb"]})')
    (89, 'if "bnlearn_fastiamb" in pattern_strings:')
    (90, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_fastiamb"], **val)')
    (91, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_fastiamb"]})')
    (92, 'if "bnlearn_iambfdr" in pattern_strings:')
    (93, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_iambfdr"], **val)')
    (94, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_iambfdr"]})')
    (95, 'if "bnlearn_mmpc" in pattern_strings:')
    (96, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_mmpc"], **val)')
    (97, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_mmpc"]})')
    (98, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (99, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_sihitonpc"], **val)')
    (100, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_sihitonpc"]})')
    (101, 'if "bnlearn_hpc" in pattern_strings:')
    (102, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_hpc"], **val)')
    (103, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_hpc"]})')
    (104, 'if "bnlearn_h2pc" in pattern_strings:')
    (105, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_h2pc"], **val)')
    (106, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_h2pc"]})')
    (107, 'if "bnlearn_rsmax2" in pattern_strings:')
    (108, '    json_string.update({val["id"]:  expand(pattern_strings["bnlearn_rsmax2"], **val)')
    (109, '                        for val in config["resources"]["structure_learning_algorithms"]["bnlearn_rsmax2"]})')
    (110, 'if "sklearn_glasso" in pattern_strings:')
    (111, '    json_string.update({val["id"]:  expand(pattern_strings["sklearn_glasso"], **val)')
    (112, '                        for val in config["resources"]["structure_learning_algorithms"]["sklearn_glasso"]})')
    (113, 'if "gobnilp" in pattern_strings:')
    (114, '    json_string.update({val["id"]: expand(pattern_strings["gobnilp"], **val)')
    (115, '                        for val in config["resources"]["structure_learning_algorithms"]["gobnilp"]})                      ')
    (116, 'if "trilearn_pgibbs" in pattern_strings:')
    (117, '    json_string.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (118, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (119, 'if "parallelDG" in pattern_strings:')
    (120, '    json_string.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (121, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (122, 'if "rblip_asobs" in pattern_strings:')
    (123, '    json_string.update({val["id"]: expand(pattern_strings["rblip_asobs"], **val)')
    (124, '                        for val in config["resources"]["structure_learning_algorithms"]["rblip_asobs"]})')
    (125, 'if "gcastle_notears" in pattern_strings:')
    (126, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears"], **val)')
    (127, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears"]})')
    (128, 'if "gcastle_pc" in pattern_strings:')
    (129, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_pc"], **val)')
    (130, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_pc"]})')
    (131, 'if "gcastle_anm" in pattern_strings:')
    (132, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_anm"], **val)')
    (133, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_anm"]})')
    (134, 'if "gcastle_direct_lingam" in pattern_strings:')
    (135, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_direct_lingam"], **val)')
    (136, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_direct_lingam"]})')
    (137, 'if "gcastle_ica_lingam" in pattern_strings:')
    (138, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_ica_lingam"], **val)')
    (139, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_ica_lingam"]})')
    (140, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (141, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_nonlinear"], **val)')
    (142, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_nonlinear"]})')
    (143, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (144, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_notears_low_rank"], **val)')
    (145, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_notears_low_rank"]})')
    (146, 'if "gcastle_golem" in pattern_strings:')
    (147, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_golem"], **val)')
    (148, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_golem"]})')
    (149, 'if "gcastle_grandag" in pattern_strings:')
    (150, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_grandag"], **val)')
    (151, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_grandag"]})')
    (152, 'if "gcastle_mcsl" in pattern_strings:')
    (153, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_mcsl"], **val)')
    (154, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_mcsl"]})')
    (155, 'if "gcastle_gae" in pattern_strings:')
    (156, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_gae"], **val)')
    (157, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_gae"]})')
    (158, 'if "gcastle_rl" in pattern_strings:')
    (159, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_rl"], **val)')
    (160, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_rl"]})')
    (161, 'if "gcastle_corl" in pattern_strings:')
    (162, '    json_string.update({val["id"]: expand(pattern_strings["gcastle_corl"], **val)')
    (163, '                        for val in config["resources"]["structure_learning_algorithms"]["gcastle_corl"]})')
    (164, 'if "causaldag_gsp" in pattern_strings:')
    (165, '    json_string.update({val["id"]: expand(pattern_strings["causaldag_gsp"], **val)')
    (166, '                        for val in config["resources"]["structure_learning_algorithms"]["causaldag_gsp"]})')
    (167, 'if "mylib_myalg" in pattern_strings:')
    (168, '    json_string.update({val["id"]: expand(pattern_strings["mylib_myalg"], **val)')
    (169, '                        for val in config["resources"]["structure_learning_algorithms"]["mylib_myalg"]})')
    (170, '')
    (171, '')
    (172, '# Order mcmc is special and has to be the last one since it takes input strings as start space.')
    (173, '# Also, the start space path has to be extracted first.')
    (174, 'if "bidag_order_mcmc" in pattern_strings:')
    (175, '    order_mcmc_list = config["resources"]["structure_learning_algorithms"]["bidag_order_mcmc"]')
    (176, '    for items in order_mcmc_list:    ')
    (177, '        items["startspace_algorithm"] = idtopath(items["startspace_algorithm"], json_string)')
    (178, '')
    (179, '    json_string.update({val["id"]: expand(pattern_strings["bidag_order_mcmc"]+"/"+pattern_strings["mcmc_est"], **val,) ')
    (180, '                        for val in order_mcmc_list } )')
    (181, '')
    (182, 'if "bidag_partition_mcmc" in pattern_strings:')
    (183, '    bidag_partition_mcmc_list = config["resources"]["structure_learning_algorithms"]["bidag_partition_mcmc"]')
    (184, '    # The path to the startspace algorithm is extended here')
    (185, '    for items in bidag_partition_mcmc_list:    ')
    (186, '        items["startspace_algorithm"] = idtopath(items["startspace_algorithm"], json_string)')
    (187, '')
    (188, '    json_string.update({val["id"]: expand(pattern_strings["bidag_partition_mcmc"], **val,) ')
    (189, '                        for val in bidag_partition_mcmc_list } )')
    (190, '')
    (191, '# Since we dont want the mcmc_est when we call the trajectory directly.')
    (192, 'json_string_mcmc_noest = {}')
    (193, 'if "bidag_order_mcmc" in pattern_strings:')
    (194, '    json_string_mcmc_noest.update({val["id"]: expand(pattern_strings["bidag_order_mcmc"], **val,) ')
    (195, '                        for val in order_mcmc_list } )')
    (196, 'if "bidag_partition_mcmc" in pattern_strings:')
    (197, '    json_string_mcmc_noest.update({val["id"]: expand(pattern_strings["bidag_partition_mcmc"], **val,) ')
    (198, '                        for val in bidag_partition_mcmc_list } )')
    (199, 'if "trilearn_pgibbs" in pattern_strings:')
    (200, '    json_string_mcmc_noest.update({val["id"]:  expand(pattern_strings["trilearn_pgibbs"], **val)')
    (201, '                        for val in config["resources"]["structure_learning_algorithms"]["trilearn_pgibbs"]})')
    (202, 'if "parallelDG" in pattern_strings:')
    (203, '    json_string_mcmc_noest.update({val["id"]:  expand(pattern_strings["parallelDG"], **val)')
    (204, '                        for val in config["resources"]["structure_learning_algorithms"]["parallelDG"]})')
    (205, 'if "gt13_multipair" in pattern_strings:')
    (206, '    json_string_mcmc_noest.update({val["id"]: expand(pattern_strings["gt13_multipair"], **val)')
    (207, '                        for val in config["resources"]["structure_learning_algorithms"]["gt13_multipair"]})')
    (208, 'if "gg99_singlepair" in pattern_strings:')
    (209, '    json_string_mcmc_noest.update({val["id"]: expand(pattern_strings["gg99_singlepair"], **val)')
    (210, '                        for val in config["resources"]["structure_learning_algorithms"]["gg99_singlepair"]})')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/Snakefile
context_key: ['if isinstance(val, dict) and val["ids"] != []']
    (54, 'def get_active_rules(wildcards):')
    (55, '    rules = []')
    (56, '    for key, val in config["benchmark_setup"]["evaluation"].items():')
    (57, '        # Check if boolean or list or object wirh nonempty ids field. ')
    (58, '        if isinstance(val, dict) and val["ids"] != []: # TODO: this was OrderedDict, so might have to impose order somewhere.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/Snakefile
context_key: ['if isinstance(val, bool) and val is True']
    (54, 'def get_active_rules(wildcards):')
    (55, '    rules = []')
    (56, '    for key, val in config["benchmark_setup"]["evaluation"].items():')
    (57, '        # Check if boolean or list or object wirh nonempty ids field. ')
    (58, '        if isinstance(val, dict) and val["ids"] != []: # TODO: this was OrderedDict, so might have to impose order somewhere.')
    (59, '            rules.append("results/output/"+key+"/"+key+".done")')
    (60, '        if isinstance(val, bool) and val is True:')
    (61, '            rules.append("results/output/"+key+"/"+key+".done")')
    (62, '        if isinstance(val, list) and val != []:')
    (63, '            if key == "mcmc_traj_plots" or key == "mcmc_heatmaps" or key == "mcmc_autocorr_plots":')
    (64, '                for item in val:')
    (65, '                    if ("active" not in item) or item["active"] == True:')
    (66, '                        rules.append("results/output/"+key+"/"+key+".done")')
    (67, '                        break')
    (68, '            else:')
    (69, '                rules.append("results/output/"+key+"/"+key+".done")    ')
    (70, '    return rules')
    (71, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/helper_functions.smk
context_key: ['if seed_range == None']
    (126, 'def get_seed_range(seed_range):')
    (127, '    if seed_range == None:')
    (128, '        return [1]')
    (129, '    else:')
    (130, '        return range(seed_range[0], seed_range[1]+1)')
    (131, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/helper_functions.smk
context_key: ['if "pcalg_randdag" in pattern_strings and adjmat_id in [c["id"] for c in config["resources"]["graph"]["pcalg_randdag"]]']
    (172, 'def gen_adjmat_string_from_conf(adjmat_id, seed):')
    (173, '    # find the adjmat_gen_method from adjmat_gen_id')
    (174, '    # Maybe fill up a dict as for structure learning algortihms')
    (175, '    # Then we would loose the seed.')
    (176, '    ')
    (177, '    if "pcalg_randdag" in pattern_strings and adjmat_id in [c["id"] for c in config["resources"]["graph"]["pcalg_randdag"]]:')
    (178, '        adjmat_dict = next(item for item in config["resources"]["graph"]["pcalg_randdag"] if item["id"] == adjmat_id)')
    (179, '        return expand(pattern_strings["pcalg_randdag"] + "/seed={seed}", **adjmat_dict, seed=seed)')
    (180, '')
    (181, '    elif "trilearn_cta" in pattern_strings and adjmat_id in [c["id"] for c in config["resources"]["graph"]["trilearn_cta"]]:')
    (182, '        adjmat_dict = next(item for item in config["resources"]["graph"]["trilearn_cta"] if item["id"] == adjmat_id)')
    (183, '        return expand(pattern_strings["trilearn_cta"] + "/seed={seed}", **adjmat_dict, seed=seed)')
    (184, '')
    (185, '    elif "bandmat" in pattern_strings and adjmat_id in [c["id"] for c in config["resources"]["graph"]["bandmat"]]:')
    (186, '        adjmat_dict = next(item for item in config["resources"]["graph"]["bandmat"] if item["id"] == adjmat_id)')
    (187, '        return expand(pattern_strings["bandmat"] + "/seed={seed}", **adjmat_dict, seed=seed)')
    (188, '')
    (189, '    elif "rand_bandmat" in pattern_strings and adjmat_id in [c["id"] for c in config["resources"]["graph"]["rand_bandmat"]]:')
    (190, '        adjmat_dict = next(item for item in config["resources"]["graph"]["rand_bandmat"] if item["id"] == adjmat_id)')
    (191, '        return expand(pattern_strings["rand_bandmat"] + "/seed={seed}", **adjmat_dict, seed=seed)')
    (192, '')
    (193, '    elif adjmat_id is not None and Path("resources/adjmat/myadjmats/"+adjmat_id).is_file():')
    (194, '        filename_no_ext = os.path.splitext(os.path.basename(adjmat_id))[0]')
    (195, '        return  "myadjmats/" + filename_no_ext # this could be hepar2.csv e.g.')
    (196, '')
    (197, '    elif "bdgraph_graphsim" in pattern_strings and adjmat_id in [c["id"] for c in config["resources"]["graph"]["bdgraph_graphsim"]]:')
    (198, '        adjmat_dict = next(item for item in config["resources"]["graph"]["bdgraph_graphsim"] if item["id"] == adjmat_id)')
    (199, '        return expand(pattern_strings["bdgraph_graphsim"] + "/seed={seed}", **adjmat_dict, seed=seed)')
    (200, '')
    (201, '    elif adjmat_id is None:')
    (202, '        return None')
    (203, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/helper_functions.smk
context_key: ['if "bin_bn" in pattern_strings and gen_method_id in [c["id"] for c in config["resources"]["parameters"]["bin_bn"]]']
    (204, 'def gen_parameter_string_from_conf(gen_method_id, seed):')
    (205, '    if "bin_bn" in pattern_strings and gen_method_id in [c["id"] for c in config["resources"]["parameters"]["bin_bn"]]:        ')
    (206, '        curconf = next(item for item in config["resources"]["parameters"]["bin_bn"] if item["id"] == gen_method_id)')
    (207, '        return expand(pattern_strings["bin_bn"] + "/seed={seed}", **curconf, seed=seed)')
    (208, '')
    (209, '    elif "bdgraph_rgwish" in pattern_strings and gen_method_id in [c["id"] for c in config["resources"]["parameters"]["bdgraph_rgwish"]]:        ')
    (210, '        curconf = next(item for item in config["resources"]["parameters"]["bdgraph_rgwish"] if item["id"] == gen_method_id)')
    (211, '        return expand(pattern_strings["bdgraph_rgwish"] + "/seed={seed}", **curconf, seed=seed)')
    (212, '')
    (213, '    elif "sem_params" in pattern_strings and gen_method_id in [c["id"] for c in config["resources"]["parameters"]["sem_params"]]:     ')
    (214, '        curconf = next(item for item in config["resources"]["parameters"]["sem_params"] if item["id"] == gen_method_id)')
    (215, '        return expand(pattern_strings["sem_params"] + "/seed={seed}", **curconf, seed=seed)')
    (216, '')
    (217, '    elif "trilearn_hyper-dir" in pattern_strings and gen_method_id in [c["id"] for c in config["resources"]["parameters"]["trilearn_hyper-dir"]]:        ')
    (218, '        curconf = next(item for item in config["resources"]["parameters"]["trilearn_hyper-dir"] if item["id"] == gen_method_id)')
    (219, '        return expand(pattern_strings["trilearn_hyper-dir"] + "/seed={seed}", **curconf, seed=seed)')
    (220, '')
    (221, '    elif "trilearn_intra-class" in pattern_strings and gen_method_id in [c["id"] for c in config["resources"]["parameters"]["trilearn_intra-class"]]:        ')
    (222, '        curconf = next(item for item in config["resources"]["parameters"]["trilearn_intra-class"] if item["id"] == gen_method_id)')
    (223, '        return expand(pattern_strings["trilearn_intra-class"] + "/seed={seed}", **curconf, seed=seed)')
    (224, '')
    (225, '    elif "trilearn_g_inv_wishart" in pattern_strings and gen_method_id in [c["id"] for c in config["resources"]["parameters"]["trilearn_g_inv_wishart"]]:        ')
    (226, '        curconf = next(item for item in config["resources"]["parameters"]["trilearn_g_inv_wishart"] if item["id"] == gen_method_id)')
    (227, '        return expand(pattern_strings["trilearn_g_inv_wishart"] + "/seed={seed}", **curconf, seed=seed)')
    (228, '')
    (229, '    elif Path("resources/parameters/myparams/bn.fit_networks/"+str(gen_method_id)).is_file():')
    (230, '        return  "bn.fit_networks/" + gen_method_id # gen_method_id could be hepar2.rds e.g.')
    (231, '')
    (232, '    elif Path("resources/parameters/myparams/sem_params/"+str(gen_method_id)).is_file():')
    (233, '        return  "sem_params/" + gen_method_id # gen_method_id could be hepar2.rds e.g.')
    (234, '')
    (235, '    elif gen_method_id is None:')
    (236, '        return None')
    (237, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/helper_functions.smk
context_key: ['if Path("resources/data/mydatasets/"+data_id).is_file()']
    (238, 'def gen_data_string_from_conf(data_id, seed,seed_in_path=True):')
    (239, '    ')
    (240, '    if Path("resources/data/mydatasets/"+data_id).is_file():')
    (241, '        num_lines = sum(1 for line in open("resources/data/mydatasets/"+data_id)) - 1')
    (242, '        return "fixed" + \\\\')
    (243, '            "/filename="+data_id + \\\\')
    (244, '            "/n="+str(num_lines) + \\\\ ')
    (245, '            "/seed="+str(seed) ')
    (246, '')
    (247, '    elif Path("resources/data/mydatasets/"+data_id).exists():        ')
    (248, '        paths = Path("resources/data/mydatasets/").glob(data_id+\\\'/*.csv\\\')')
    (249, '        files = [x.name for x in paths if x.is_file()]')
    (250, '')
    (251, '        return ["fixed" + \\\\')
    (252, '                "/filename="+data_id + "/"+ f + \\\\')
    (253, '                "/n="+str(None) + \\\\ ')
    (254, '                "/seed="+str(seed) for f in files]')
    (255, '')
    (256, '    elif data_id in [c["id"] for c in config["resources"]["data"]["iid"]]:')
    (257, '        # Find the data entry from the resources')
    (258, '        data = next(item for item in config["resources"]["data"]["iid"] if item["id"] == data_id)')
    (259, '        if seed_in_path: # seed_in_path is a hack..')
    (260, '            return expand("iid" +\\\\')
    (261, '                            "/standardized={standardized}" + \\\\ ')
    (262, '                            "/n={n}" + \\\\')
    (263, '                            "/seed={seed}", ')
    (264, '                            n = data["sample_sizes"],')
    (265, '                            standardized = data["standardized"],')
    (266, '                            seed = seed)')
    (267, '        else:')
    (268, '            return expand("iid" +\\\\')
    (269, '                        "/standardized={standardized}" + \\\\ ')
    (270, '                            "/n={n}",')
    (271, '                            standardized = data["standardized"],')
    (272, '                            n = data["sample_sizes"])')
    (273, '')
    (274, '    elif data_id in [c["id"] for c in config["resources"]["data"]["gcastle_iidsimulation"]]:')
    (275, '        # Find the data entry from the resources')
    (276, '        data = next(item for item in config["resources"]["data"]["gcastle_iidsimulation"] if item["id"] == data_id)')
    (277, '        if seed_in_path:')
    (278, '            return expand("gcastle_iidsimulation" +\\\\')
    (279, '                            "/standardized={standardized}/" + \\\\ ')
    (280, '                            "method={method}/" + \\\\')
    (281, '                            "sem_type={sem_type}/" + \\\\')
    (282, '                            "noise_scale={noise_scale}/" + \\\\')
    (283, '                            "n={n}" + \\\\')
    (284, '                            "/seed={seed}", ')
    (285, '                            method = data["method"],')
    (286, '                            sem_type = data["sem_type"],')
    (287, '                            noise_scale = data["noise_scale"],')
    (288, '                            n = data["n"],')
    (289, '                            standardized = data["standardized"],')
    (290, '                            seed = seed)')
    (291, '        else:')
    (292, '            return expand("gcastle_iidsimulation" +\\\\')
    (293, '                            "/standardized={standardized}/" + \\\\ ')
    (294, '                            "method={method}/" + \\\\')
    (295, '                            "sem_type={sem_type}/" + \\\\')
    (296, '                            "noise_scale={noise_scale}/" + \\\\')
    (297, '                            "n={n}",')
    (298, '                            method = data["method"],')
    (299, '                            sem_type = data["sem_type"],')
    (300, '                            noise_scale = data["noise_scale"],')
    (301, '                            standardized = data["standardized"],')
    (302, '                            n = data["n"])')
    (303, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/helper_functions.smk
context_key: ['if (eval_method == "mcmc_traj_plots") or (eval_method == "mcmc_autocorr_plots") or (eval_method == "mcmc_heatmaps")', 'if alg_conf_id in [ac["id"] for ac in alg_conf_list]']
    (312, 'def active_algorithms(eval_method="benchmarks"):')
    (313, '    with open(configfilename) as json_file:')
    (314, '        conf = json.load(json_file)')
    (315, '')
    (316, '    algs = []')
    (317, '    ')
    (318, '    if (eval_method == "mcmc_traj_plots") or (eval_method == "mcmc_autocorr_plots") or (eval_method == "mcmc_heatmaps"):')
    (319, '        benchmarks_alg_ids = [benchmarks_dict["id"]  for benchmarks_dict in config["benchmark_setup"]["evaluation"][eval_method] if benchmarks_dict["active"] == True]')
    (320, '        for alg, alg_conf_list in config["resources"]["structure_learning_algorithms"].items():     ')
    (321, '            for alg_conf_id in benchmarks_alg_ids: ')
    (322, '                #print(alg_conf_id)')
    (323, '                if alg_conf_id in [ac["id"] for ac in alg_conf_list]:')
    (324, '                    algs.append( alg )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/helper_functions.smk
context_key: ['if (eval_method == "mcmc_traj_plots") or (eval_method == "mcmc_autocorr_plots") or (eval_method == "mcmc_heatmaps")', 'elif eval_method == "benchmarks"', 'if alg_conf_id in [ac["id"] for ac in alg_conf_list]']
    (312, 'def active_algorithms(eval_method="benchmarks"):')
    (313, '    with open(configfilename) as json_file:')
    (314, '        conf = json.load(json_file)')
    (315, '')
    (316, '    algs = []')
    (317, '    ')
    (318, '    if (eval_method == "mcmc_traj_plots") or (eval_method == "mcmc_autocorr_plots") or (eval_method == "mcmc_heatmaps"):')
    (319, '        benchmarks_alg_ids = [benchmarks_dict["id"]  for benchmarks_dict in config["benchmark_setup"]["evaluation"][eval_method] if benchmarks_dict["active"] == True]')
    (320, '        for alg, alg_conf_list in config["resources"]["structure_learning_algorithms"].items():     ')
    (321, '            for alg_conf_id in benchmarks_alg_ids: ')
    (322, '                #print(alg_conf_id)')
    (323, '                if alg_conf_id in [ac["id"] for ac in alg_conf_list]:')
    (324, '                    algs.append( alg )')
    (325, '    elif eval_method == "benchmarks":')
    (326, '        benchmarks_alg_ids = config["benchmark_setup"]["evaluation"]["benchmarks"]["ids"]')
    (327, '        for alg, alg_conf_list in config["resources"]["structure_learning_algorithms"].items():     ')
    (328, '            for alg_conf_id in benchmarks_alg_ids:        ')
    (329, '                if alg_conf_id in [ac["id"] for ac in alg_conf_list]:')
    (330, '                    algs.append( alg )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/helper_functions.smk
context_key: ['if (eval_method == "mcmc_traj_plots") or (eval_method == "mcmc_autocorr_plots") or (eval_method == "mcmc_heatmaps")', 'else', 'if alg_conf_id in [ac["id"] for ac in alg_conf_list]']
    (312, 'def active_algorithms(eval_method="benchmarks"):')
    (313, '    with open(configfilename) as json_file:')
    (314, '        conf = json.load(json_file)')
    (315, '')
    (316, '    algs = []')
    (317, '    ')
    (318, '    if (eval_method == "mcmc_traj_plots") or (eval_method == "mcmc_autocorr_plots") or (eval_method == "mcmc_heatmaps"):')
    (319, '        benchmarks_alg_ids = [benchmarks_dict["id"]  for benchmarks_dict in config["benchmark_setup"]["evaluation"][eval_method] if benchmarks_dict["active"] == True]')
    (320, '        for alg, alg_conf_list in config["resources"]["structure_learning_algorithms"].items():     ')
    (321, '            for alg_conf_id in benchmarks_alg_ids: ')
    (322, '                #print(alg_conf_id)')
    (323, '                if alg_conf_id in [ac["id"] for ac in alg_conf_list]:')
    (324, '                    algs.append( alg )')
    (325, '    elif eval_method == "benchmarks":')
    (326, '        benchmarks_alg_ids = config["benchmark_setup"]["evaluation"]["benchmarks"]["ids"]')
    (327, '        for alg, alg_conf_list in config["resources"]["structure_learning_algorithms"].items():     ')
    (328, '            for alg_conf_id in benchmarks_alg_ids:        ')
    (329, '                if alg_conf_id in [ac["id"] for ac in alg_conf_list]:')
    (330, '                    algs.append( alg )')
    (331, '    else:')
    (332, '        benchmarks_alg_ids = [benchmarks_dict for benchmarks_dict in config["benchmark_setup"]["evaluation"][eval_method]]')
    (333, '        for alg, alg_conf_list in config["resources"]["structure_learning_algorithms"].items():     ')
    (334, '            for alg_conf_id in benchmarks_alg_ids:        ')
    (335, '                if alg_conf_id in [ac["id"] for ac in alg_conf_list]:')
    (336, '                    algs.append( alg )')
    (337, '    ')
    (338, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/helper_functions.smk
context_key: ['if (eval_method == "mcmc_traj_plots") or (eval_method == "mcmc_autocorr_plots") or (eval_method == "mcmc_heatmaps")']
    (312, 'def active_algorithms(eval_method="benchmarks"):')
    (313, '    with open(configfilename) as json_file:')
    (314, '        conf = json.load(json_file)')
    (315, '')
    (316, '    algs = []')
    (317, '    ')
    (318, '    if (eval_method == "mcmc_traj_plots") or (eval_method == "mcmc_autocorr_plots") or (eval_method == "mcmc_heatmaps"):')
    (319, '        benchmarks_alg_ids = [benchmarks_dict["id"]  for benchmarks_dict in config["benchmark_setup"]["evaluation"][eval_method] if benchmarks_dict["active"] == True]')
    (320, '        for alg, alg_conf_list in config["resources"]["structure_learning_algorithms"].items():     ')
    (321, '            for alg_conf_id in benchmarks_alg_ids: ')
    (322, '                #print(alg_conf_id)')
    (323, '                if alg_conf_id in [ac["id"] for ac in alg_conf_list]:')
    (324, '                    algs.append( alg )')
    (325, '    elif eval_method == "benchmarks":')
    (326, '        benchmarks_alg_ids = config["benchmark_setup"]["evaluation"]["benchmarks"]["ids"]')
    (327, '        for alg, alg_conf_list in config["resources"]["structure_learning_algorithms"].items():     ')
    (328, '            for alg_conf_id in benchmarks_alg_ids:        ')
    (329, '                if alg_conf_id in [ac["id"] for ac in alg_conf_list]:')
    (330, '                    algs.append( alg )')
    (331, '    else:')
    (332, '        benchmarks_alg_ids = [benchmarks_dict for benchmarks_dict in config["benchmark_setup"]["evaluation"][eval_method]]')
    (333, '        for alg, alg_conf_list in config["resources"]["structure_learning_algorithms"].items():     ')
    (334, '            for alg_conf_id in benchmarks_alg_ids:        ')
    (335, '                if alg_conf_id in [ac["id"] for ac in alg_conf_list]:')
    (336, '                    algs.append( alg )')
    (337, '    ')
    (338, '')
    (339, '    return list(set(algs))')
    (340, '')
    (341, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=DKFZ-ODCF/reference-data, file=workflow/Snakefile
context_key: ['if re.search(id_type, str(resp))']
    (18, 'def get_id_type():')
    (19, '        possible_id_types = ["toplevel", "primary_assembly"]')
    (20, '')
    (21, '        species=config["name"]')
    (22, '        datatype="dna"')
    (23, '        build=config["assembly_default"]')
    (24, '        release=config["ensembl_release"]')
    (25, '')
    (26, '        url = f"ftp://ftp.ensembl.org/pub/release-{release}/fasta/{species}/{datatype}/"')
    (27, '        print(url)')
    (28, '        resp = request.urlopen(url).read()')
    (29, '')
    (30, '        for id_type in possible_id_types:')
    (31, '            if re.search(id_type, str(resp)):')
    (32, '                return id_type')
    (33, '        else:')
    (34, '            return None')
    (35, '')
    (36, '')
    (37, '')
    (38, '')
    (39, '')
    (40, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=DKFZ-ODCF/reference-data, file=workflow/Snakefile
context_key: ['if licensed']
    (41, 'def get_assembly_path(l3_variant = "", l4_variant = "unmodified", tracking = False, licensed = False, as_list = False):')
    (42, '')
    (43, '    """Generate assembly path depending on supplied parameters.')
    (44, '')
    (45, '    :param')
    (46, '    l3_variant : string Identifies the third-level variant, i.e. variants with added / remove sequences .e.g PhiX.')
    (47, '    l4_variant : string Identifies the fourth-level variant i.e. changed in the sequences names. Must not inlcude changes to the genomic coordiantes.')
    (48, '    licensed  : bool Flag for data under proprietary license to be stored in seperate non-public repository.')
    (49, '')
    (50, '    :returns')
    (51, '    assembly_path (str) Path to directory where data is stored.')
    (52, '    """')
    (53, '')
    (54, '    if licensed:')
    (55, '        basedir = path_join(config["data_repo_dir"], config["licensed_data_dir"])')
    (56, '    else:')
    (57, '        basedir = path_join(config["data_repo_dir"], config["public_data_dir"])')
    (58, '')
    (59, '')
    (60, '    assembly_path_primary = path_join(basedir, SPECIES, config["assembly_name"])')
    (61, '')
    (62, '    l3_variant = f"_{l3_variant}" if l3_variant != "" else ""')
    (63, '')
    (64, '    path_elements = [basedir,')
    (65, '            SPECIES,')
    (66, '            config["assembly_name"],')
    (67, '            config["assembly_name"]  + l3_variant,')
    (68, '            l4_variant]')
    (69, '')
    (70, '    if as_list:')
    (71, '        return path_elements')
    (72, '    else:')
    (73, '        assembly_path = path_join(*path_elements)')
    (74, '        return assembly_path')
    (75, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=DKFZ-ODCF/reference-data, file=workflow/Snakefile
context_key: ['if "phix" in L3_VARIANT.lower()']
    (41, 'def get_assembly_path(l3_variant = "", l4_variant = "unmodified", tracking = False, licensed = False, as_list = False):')
    (42, '')
    (43, '    """Generate assembly path depending on supplied parameters.')
    (44, '')
    (45, '    :param')
    (46, '    l3_variant : string Identifies the third-level variant, i.e. variants with added / remove sequences .e.g PhiX.')
    (47, '    l4_variant : string Identifies the fourth-level variant i.e. changed in the sequences names. Must not inlcude changes to the genomic coordiantes.')
    (48, '    licensed  : bool Flag for data under proprietary license to be stored in seperate non-public repository.')
    (49, '')
    (50, '    :returns')
    (51, '    assembly_path (str) Path to directory where data is stored.')
    (52, '    """')
    (53, '')
    (54, '    if licensed:')
    (55, '        basedir = path_join(config["data_repo_dir"], config["licensed_data_dir"])')
    (56, '    else:')
    (57, '        basedir = path_join(config["data_repo_dir"], config["public_data_dir"])')
    (58, '')
    (59, '')
    (60, '    assembly_path_primary = path_join(basedir, SPECIES, config["assembly_name"])')
    (61, '')
    (62, '    l3_variant = f"_{l3_variant}" if l3_variant != "" else ""')
    (63, '')
    (64, '    path_elements = [basedir,')
    (65, '            SPECIES,')
    (66, '            config["assembly_name"],')
    (67, '            config["assembly_name"]  + l3_variant,')
    (68, '            l4_variant]')
    (69, '')
    (70, '    if as_list:')
    (71, '        return path_elements')
    (72, '    else:')
    (73, '        assembly_path = path_join(*path_elements)')
    (74, '        return assembly_path')
    (75, '')
    (76, 'LOGS_DIR            = config.get("logs_dir")')
    (77, 'SPECIES             = config["scientific_name"].replace(" ", "_")')
    (78, 'L3_VARIANT          = config.get("l3_variant", "") # "PhiX"')
    (79, 'ENSEMBL_RELEASE     = config["ensembl_release"]')
    (80, 'ENSEMBL_ID_TYPE     = get_id_type()')
    (81, 'FILENAME_BASE       = config["url_name"] + "." + config["assembly_default"]')
    (82, 'ASSEMBLYDIR_VARIANT = get_assembly_path(l3_variant=L3_VARIANT,')
    (83, '                                        l4_variant="unmodified",')
    (84, '                                        licensed=False)')
    (85, 'assembly_dir_path = get_assembly_path(l3_variant=L3_VARIANT,')
    (86, '                                        l4_variant="unmodified",')
    (87, '                                        licensed=False, as_list=True)')
    (88, '')
    (89, '')
    (90, '# define the order of rules to be executed depending on the level3-variant')
    (91, 'if "phix" in L3_VARIANT.lower():')
    (92, '     FILENAME_BASE_ADD = FILENAME_BASE + "_phiX"')
    (93, '#     ruleorder: download_fasta > download_gff3 > download_gtf > unzip > download_PhiX > add_PhiX > fasta_link > bwa_index > star_index')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jpcsmith/qcsd-experiments, file=workflow/rules/ml-eval-conn.smk
context_key: ['if flatten']
    (10, 'def ml_eval_conn__plot__inputs(wildcards, flatten: bool = False):')
    (11, '    defence = wildcards["defence"]')
    (12, '    hyperparams = ml_ec_config["hyperparams"]')
    (13, '    base = "results/ml-eval-conn"')
    (14, '')
    (15, '    result = {')
    (16, '        "$k$-FP": {')
    (17, '            "QCSD": f"{base}/defence~{defence}/classifier~kfp/predictions.csv",')
    (18, '            "Simulated": f"{base}/defence~simulated-{defence}/classifier~kfp/predictions.csv",')
    (19, '            "Undef.": f"{base}/defence~undefended/classifier~kfp/predictions.csv",')
    (20, '        },')
    (21, '        "DF": {')
    (22, '            "QCSD": f"{base}/defence~{defence}/classifier~dfnet/hyperparams~{hyperparams[defence][\\\'dfnet\\\']}/predictions.csv",')
    (23, '            "Simulated": f"{base}/defence~simulated-{defence}/classifier~dfnet/hyperparams~{hyperparams[defence][\\\'dfnet\\\']}/predictions.csv",')
    (24, '            "Undef.": f"{base}/defence~undefended/classifier~dfnet/hyperparams~{hyperparams[\\\'undefended\\\'][\\\'dfnet\\\']}/predictions.csv",')
    (25, '        },')
    (26, '        "Var-CNN": {')
    (27, '            "QCSD": f"{base}/defence~{defence}/classifier~varcnn/predictions.csv",')
    (28, '            "Simulated": f"{base}/defence~simulated-{defence}/classifier~varcnn/predictions.csv",')
    (29, '            "Undef.": f"{base}/defence~undefended/classifier~varcnn/predictions.csv"')
    (30, '        }')
    (31, '    }')
    (32, '    if flatten:')
    (33, '        result = [v for values in result.values() for v in values.values()]')
    (34, '    return result')
    (35, '')
    (36, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jpcsmith/qcsd-experiments, file=workflow/rules/ml-eval-conn.smk
context_key: ['if "undefended" in wildcards["path"]']
    (53, 'def ml_eval_conn__combine_varcnn__inputs(wildcards):')
    (54, '    base = "results/ml-eval-conn/" + wildcards["path"]')
    (55, '    if "undefended" in wildcards["path"]:')
    (56, '        hparams = ml_ec_config["hyperparams"]["undefended"]')
    (57, '    elif "front" in wildcards["path"]:')
    (58, '        hparams = ml_ec_config["hyperparams"]["front"]')
    (59, '    elif "tamaraw" in wildcards["path"]:')
    (60, '        hparams = ml_ec_config["hyperparams"]["tamaraw"]')
    (61, '    else:')
    (62, '        raise ValueError(f"Unsupported defence: {wildcards[\\\'path\\\']}")')
    (63, '')
    (64, '    return {')
    (65, '        feature_type: f"{base}/classifier~{tag}/hyperparams~{hparams[tag]}/predictions.csv"')
    (66, '        for feature_type, tag in [("times", "varcnn-time"), ("sizes", "varcnn-sizes")]')
    (67, '    }')
    (68, '')
    (69, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pughlab/cfMeDIP-seq-analysis-pipeline, file=Snakefile
context_key: ["if \\'settings\\' in config[\\'data\\'][\\'cohorts\\'][cohort]", 'if key in config_data']
    (17, 'def get_cohort_config(cohort):')
    (18, '    """Returns the configuration details for specific cohort.')
    (19, '    Importantly, retrieving a cohort using this function (rather than directly')
    (20, '    accessing it via the config dictionary) will also inject the default settings')
    (21, '    (config > data > defaults) wherever a specific setting is not specified.')
    (22, '    """')
    (23, "    config_data = dict(config[\\'data\\'][\\'defaults\\'])")
    (24, "    if \\'settings\\' in config[\\'data\\'][\\'cohorts\\'][cohort]:")
    (25, "        for key in config[\\'data\\'][\\'cohorts\\'][cohort][\\'settings\\']:")
    (26, '            if key in config_data:')
    (27, "                config_data[key] = config[\\'data\\'][\\'cohorts\\'][cohort][\\'settings\\'][key]")
    (28, '            else:')
    (29, "                raise(Exception(\\'Setting {} does not exist for cohort {}. Available settings: {}\\'.format(")
    (30, "                    key, cohort, \\', \\'.join(config_data.keys())")
    (31, '                )))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pughlab/cfMeDIP-seq-analysis-pipeline, file=Snakefile
context_key: ["if \\'settings\\' in config[\\'data\\'][\\'cohorts\\'][cohort]"]
    (17, 'def get_cohort_config(cohort):')
    (18, '    """Returns the configuration details for specific cohort.')
    (19, '    Importantly, retrieving a cohort using this function (rather than directly')
    (20, '    accessing it via the config dictionary) will also inject the default settings')
    (21, '    (config > data > defaults) wherever a specific setting is not specified.')
    (22, '    """')
    (23, "    config_data = dict(config[\\'data\\'][\\'defaults\\'])")
    (24, "    if \\'settings\\' in config[\\'data\\'][\\'cohorts\\'][cohort]:")
    (25, "        for key in config[\\'data\\'][\\'cohorts\\'][cohort][\\'settings\\']:")
    (26, '            if key in config_data:')
    (27, "                config_data[key] = config[\\'data\\'][\\'cohorts\\'][cohort][\\'settings\\'][key]")
    (28, '            else:')
    (29, "                raise(Exception(\\'Setting {} does not exist for cohort {}. Available settings: {}\\'.format(")
    (30, "                    key, cohort, \\', \\'.join(config_data.keys())")
    (31, '                )))')
    (32, '    return(config_data)')
    (33, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pughlab/cfMeDIP-seq-analysis-pipeline, file=Snakefile
context_key: ["if \\'exclude\\' in cohort_config and cohort_config[\\'exclude\\'] is not None"]
    (38, 'def get_cohort_data(cohort):')
    (39, '    """Parses the samplesheet for a specific cohort.')
    (40, '    Also removes any excluded_cases from the samplesheet before it is returned.')
    (41, '    """')
    (42, '    cohort_config = get_cohort_config(cohort);')
    (43, "    samplesheet = pd.read_csv(config[\\'data\\'][\\'cohorts\\'][cohort][\\'samplesheet\\'], comment=\\'#\\').drop_duplicates()")
    (44, "    if \\'exclude\\' in cohort_config and cohort_config[\\'exclude\\'] is not None:")
    (45, "        samplesheet = samplesheet[~samplesheet.sample_name.isin(cohort_config[\\'exclude\\'])]")
    (46, '    return(samplesheet)')
    (47, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pughlab/cfMeDIP-seq-analysis-pipeline, file=Snakefile
context_key: ['if cohort is None']
    (66, 'def get_all_samples(cohort=None):')
    (67, '    """Retrieves all samples to be processed.')
    (68, '    Does so by calling get_cohort_data, and therefore filters out excluded_cases.')
    (69, '')
    (70, '    Keyword arguments:')
    (71, '        cohort -- Name of a cohort, OPTIONAL. If not specified, returns all samples')
    (72, '                  across all cohorts.')
    (73, '    """')
    (74, '    all_samples = pd.concat([')
    (75, '        get_cohort_data(cohort_name).assign(cohort_name = cohort_name)')
    (76, '        for cohort_name')
    (77, "        in config[\\'data\\'][\\'cohorts\\']")
    (78, "        if config[\\'data\\'][\\'cohorts\\'][cohort_name][\\'active\\']")
    (79, '        ])')
    (80, '')
    (81, '    if cohort is None:')
    (82, '        return(all_samples)')
    (83, '    else:')
    (84, '        return(all_samples[all_samples.cohort_name == cohort])')
    (85, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/rules/utilities.smk
context_key: ["if f.endswith(ref_exts) and type == \\'fasta\\'", "if f.endswith(\\'.gz\\')"]
    (18, 'def get_resource_file(ref_dir,type=None):')
    (19, "    ref_exts = tuple([\\'.fasta\\',\\'.fa\\',\\'.fna\\'])")
    (20, "    ann_exts = tuple([\\'.gff\\',\\'.gff3\\',\\'.gtf\\'])")
    (21, '    for f in os.listdir(ref_dir):')
    (22, "        if f.endswith(ref_exts) and type == \\'fasta\\':")
    (23, "            if f.endswith(\\'.gz\\'):")
    (24, "                sys.exit(\\'Reference file is compressed. Decompress file before restarting pipeline. Exiting...\\')")
    (25, '            else:')
    (26, '                return f')
    (27, "        elif f.endswith(ann_exts) and type == \\'gtf\\':")
    (28, "            if f.endswith(\\'.gz\\'):")
    (29, "                sys.exit(\\'Annotation file is compressed. Decompress file before restarting pipeline. Exiting...\\')")
    (30, '            return f')
    (31, '        else:')
    (32, '            continue')
    (33, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/rules/utilities.smk
context_key: ["if f.endswith(ref_exts) and type == \\'fasta\\'", "if type == \\'fasta\\'"]
    (18, 'def get_resource_file(ref_dir,type=None):')
    (19, "    ref_exts = tuple([\\'.fasta\\',\\'.fa\\',\\'.fna\\'])")
    (20, "    ann_exts = tuple([\\'.gff\\',\\'.gff3\\',\\'.gtf\\'])")
    (21, '    for f in os.listdir(ref_dir):')
    (22, "        if f.endswith(ref_exts) and type == \\'fasta\\':")
    (23, "            if f.endswith(\\'.gz\\'):")
    (24, "                sys.exit(\\'Reference file is compressed. Decompress file before restarting pipeline. Exiting...\\')")
    (25, '            else:')
    (26, '                return f')
    (27, "        elif f.endswith(ann_exts) and type == \\'gtf\\':")
    (28, "            if f.endswith(\\'.gz\\'):")
    (29, "                sys.exit(\\'Annotation file is compressed. Decompress file before restarting pipeline. Exiting...\\')")
    (30, '            return f')
    (31, '        else:')
    (32, '            continue')
    (33, '    ')
    (34, "    if type == \\'fasta\\':")
    (35, '        sys.exit(\\\'Reference fasta file not detected. Ensure file extension is either of ".fa" or ".fasta" and \\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/rules/utilities.smk
context_key: ["elif type == \\'gtf\\'"]
    (18, 'def get_resource_file(ref_dir,type=None):')
    (19, "    ref_exts = tuple([\\'.fasta\\',\\'.fa\\',\\'.fna\\'])")
    (20, "    ann_exts = tuple([\\'.gff\\',\\'.gff3\\',\\'.gtf\\'])")
    (21, '    for f in os.listdir(ref_dir):')
    (22, "        if f.endswith(ref_exts) and type == \\'fasta\\':")
    (23, "            if f.endswith(\\'.gz\\'):")
    (24, "                sys.exit(\\'Reference file is compressed. Decompress file before restarting pipeline. Exiting...\\')")
    (25, '            else:')
    (26, '                return f')
    (27, "        elif f.endswith(ann_exts) and type == \\'gtf\\':")
    (28, "            if f.endswith(\\'.gz\\'):")
    (29, "                sys.exit(\\'Annotation file is compressed. Decompress file before restarting pipeline. Exiting...\\')")
    (30, '            return f')
    (31, '        else:')
    (32, '            continue')
    (33, '    ')
    (34, "    if type == \\'fasta\\':")
    (35, '        sys.exit(\\\'Reference fasta file not detected. Ensure file extension is either of ".fa" or ".fasta" and \\\\')
    (36, "exists within the specified reference directory. Exiting...\\')")
    (37, "    elif type == \\'gtf\\':")
    (38, '        sys.exit(\\\'Gene annotation file not detected. Ensure file extension is either of ".gff(3)" or ".gtf" and \\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/rules/utilities.smk
context_key: ['if pd.isna(R1) | pd.isna(R2)']
    (52, 'def is_paired(wildcards,R1,R2):')
    (53, '    if pd.isna(R1) | pd.isna(R2):')
    (54, '        return False')
    (55, '    else:')
    (56, '        return True')
    (57, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/rules/utilities.smk
context_key: ['if is_paired(wildcards,R1,R2)', "if aligner == \\'star\\' or aligner == \\'bwa\\'"]
    (66, 'def get_aligner_input(wildcards,aligner):')
    (67, "    target_dir = f\\'../../{sample_dir}\\' if aligner == \\'bwa\\' else f\\'../../../{sample_dir}\\'")
    (68, '    R1,R2 = get_fastq_filenames(wildcards)')
    (69, '    if is_paired(wildcards,R1,R2):')
    (70, "        if aligner == \\'star\\' or aligner == \\'bwa\\':")
    (71, '            return f"{target_dir}/{R1}",f"{target_dir}/{R2}"')
    (72, '        else:')
    (73, '            return f"-1 {target_dir}/{R1} -2 {target_dir}/{R2}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/rules/utilities.smk
context_key: ['if is_paired(wildcards,R1,R2)', 'else', "if aligner == \\'star\\' or aligner == \\'bwa\\'"]
    (66, 'def get_aligner_input(wildcards,aligner):')
    (67, "    target_dir = f\\'../../{sample_dir}\\' if aligner == \\'bwa\\' else f\\'../../../{sample_dir}\\'")
    (68, '    R1,R2 = get_fastq_filenames(wildcards)')
    (69, '    if is_paired(wildcards,R1,R2):')
    (70, "        if aligner == \\'star\\' or aligner == \\'bwa\\':")
    (71, '            return f"{target_dir}/{R1}",f"{target_dir}/{R2}"')
    (72, '        else:')
    (73, '            return f"-1 {target_dir}/{R1} -2 {target_dir}/{R2}"')
    (74, '    else:')
    (75, "        if aligner == \\'star\\' or aligner == \\'bwa\\':")
    (76, '            return f"{target_dir}/{R1}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/rules/utilities.smk
context_key: ['if is_paired(wildcards,R1,R2)', 'else', 'else']
    (66, 'def get_aligner_input(wildcards,aligner):')
    (67, "    target_dir = f\\'../../{sample_dir}\\' if aligner == \\'bwa\\' else f\\'../../../{sample_dir}\\'")
    (68, '    R1,R2 = get_fastq_filenames(wildcards)')
    (69, '    if is_paired(wildcards,R1,R2):')
    (70, "        if aligner == \\'star\\' or aligner == \\'bwa\\':")
    (71, '            return f"{target_dir}/{R1}",f"{target_dir}/{R2}"')
    (72, '        else:')
    (73, '            return f"-1 {target_dir}/{R1} -2 {target_dir}/{R2}"')
    (74, '    else:')
    (75, "        if aligner == \\'star\\' or aligner == \\'bwa\\':")
    (76, '            return f"{target_dir}/{R1}"')
    (77, '        else:')
    (78, '            return f"-U {target_dir}/{R1}"')
    (79, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/rules/utilities.smk
context_key: ["if R1.endswith(\\'.gz\\')"]
    (80, 'def get_star_readfile_command(wildcards):')
    (81, '    R1 = get_fastq_filenames(wildcards)[0]')
    (82, "    if R1.endswith(\\'.gz\\'):")
    (83, "        return \\'--readFilesCommand zcat\\'")
    (84, '    else:')
    (85, "        return \\'\\'")
    (86, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/rules/utilities.smk
context_key: ["if mode == \\'RNA\\'"]
    (87, 'def get_aligner_directory(wildcards):')
    (88, "    prefix = \\'../results/\\' + wildcards.sample + \\'/\\' + wildcards.aligner + \\'/\\'")
    (89, "    if mode == \\'RNA\\':")
    (90, "        return prefix + wildcards.aligner + \\'_output/\\'")
    (91, '    else:')
    (92, '        return prefix')
    (93, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/rules/utilities.smk
context_key: ["if calling and config[\\'apply_bqsr\\']"]
    (94, 'def get_input_bam(wildcards,calling=False,ind=False):')
    (95, "    ext = \\'.bai\\' if ind else \\'.bam\\'")
    (96, "    prefix = \\'../results/\\' + wildcards.sample + \\'/\\' + wildcards.aligner")
    (97, '    suffix = wildcards.sample + ext')
    (98, "    if calling and config[\\'apply_bqsr\\']:")
    (99, "        return prefix + \\'/bqsr/recal_reads_\\' + suffix")
    (100, "    elif mode == \\'RNA\\':")
    (101, "        return prefix + \\'/bam_preprocessing/dedup_rna_reads_\\' + suffix ")
    (102, '    else:')
    (103, "        return prefix + \\'/bam_preprocessing/dedup_reads_\\' + suffix")
    (104, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/rules/utilities.smk
context_key: ['if mode == "RNA"']
    (105, 'def get_hapcaller_rna_params(wildcards):')
    (106, '    if mode == "RNA":')
    (107, "        return \\'-stand-call-conf \\' + str(config[\\'stand_call_conf\\']) + \\' --dont-use-soft-clipped-bases\\'")
    (108, '    else:')
    (109, "        return \\'\\'")
    (110, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/rules/utilities.smk
context_key: ['if mode == "DNA"']
    (111, 'def get_intersection_num(wildcards):')
    (112, '    if mode == "DNA":')
    (113, "        return config[\\'nfiles_DNA_mode\\']")
    (114, '    else:')
    (115, "        return config[\\'nfiles_RNA_mode\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akhanf/hcp-lifespan-downloader, file=Snakefile
context_key: ["if \\'filters\\' in config[\\'packages\\'][wildcards.package]"]
    (44, 'def get_s3_txt(wildcards):')
    (45, "    if \\'filters\\' in config[\\'packages\\'][wildcards.package]:")
    (46, "        return \\'results/s3_lists/{package}.filtered/{subject}_{package}.txt\\'.format(**wildcards)")
    (47, '    else:')
    (48, "        return \\'results/s3_lists/{package}/{subject}_{package}.txt\\'.format(**wildcards)")
    (49, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akhanf/nnunet-fetalbrain, file=Snakefile
context_key: ['if os.path.exists(output.latest_model)']
    (172, 'def get_checkpoint_opt(wildcards, output):')
    (173, '    if os.path.exists(output.latest_model):')
    (174, "        return \\'--continue_training\\'")
    (175, '    else:')
    (176, "        return \\'\\' ")
    (177, '      ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sabiqali/strline, file=Snakefile
context_key: ['if bk == "none"']
    (52, 'def get_basecalled_root_dir_for_sample(wildcards):')
    (53, "    dt = config[wildcards.sample][\\'data_type\\']")
    (54, "    bk = config[dt][\\'barcoding_kit\\']")
    (55, '    # if barcoding is not enabled, merged all fastqs in the basecalled directory')
    (56, '    if bk == "none":')
    (57, '        p = get_basecalled_dir(wildcards)')
    (58, '    else:')
    (59, '        # barcoding enabled')
    (60, '        p = get_barcoded_dir(wildcards)')
    (61, '    return p')
    (62, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sabiqali/strline, file=Snakefile
context_key: ['if bc == "none"']
    (63, 'def get_basecalled_subdir_for_sample(wildcards):')
    (64, '    bc = get_barcode_id_for_sample(wildcards)')
    (65, '    if bc == "none":')
    (66, '        return ""')
    (67, '    else:')
    (68, '        return "barcode" + bc')
    (69, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=troy-layouni/CellSegTest, file=workflow/Snakefile
context_key: ['if _sample_dict is None']
    (363, 'def get_sample_dict():')
    (364, '    global _sample_dict')
    (365, '    if _sample_dict is None:')
    (366, '        checkpoints.generate_sample_list.get()')
    (367, '        dat_samples = pd.read_csv(file_path_samples)')
    (368, '        _sample_dict = {pathlib.Path(x).stem: x for x in dat_samples[')
    (369, "            \\'mcd_folders\\']}")
    (370, '    return _sample_dict')
    (371, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['if os.path.exists(VCF)']
    (90, 'def get_all_inputs(wildcards): #Primary rule whose input values determine which outputs (and corresponding rules) will be run by snakemake')
    (91, '    input_list = ["accessory/.sHWE_pass.txt", f"sHWE/.optimum_popnum.txt", f"{BASE}-rulegraph.png",')
    (92, '                  f"plink/{BASE}_LDp_sHWE.bed", f"{QUERY}.bed", f"plink/{BASE}.eigenvec",')
    (93, '                  f"accessory/samples.txt", f"input/{BASE}_CtlMat.bed", f"plink/{BASE}.genome.gz",')
    (94, '                  f"{BASE}.genesis.txt", f"{BASE}-Mapping-report.pdf"] #, f"{BASE}.genesis.sig.annt.txt"')
    (95, '    #input_list += [f"{BASE}.blink.txt", f"{BASE}.blink.sig.annt.txt"] #Uncomment to enable GWAS with BLINK.  Newer version of BLINK was causing issues with singularity.')
    (96, "    if config[\\'admixMapping\\'][\\'skip\\'] != \\'true\\': #Do not include admixture mapping in desired results, unless requested.")
    (97, '        input_list += [f"{BASE}.admixmap.txt", f"{BASE}.admixmap.txt"] #, f"{BASE}.admixmap.annt.txt"')
    (98, '    if os.path.exists(VCF): input_list += [f"{BASE}.dos.genesis.txt"] #Requests dosage GWAS to be run if a dosage VCF is provided.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['if os.path.exists(VCF)']
    (116, 'def get_report_input(wildcards):')
    (117, '    input_list = [f"{BASE}-rulegraph.png", f"{BASE}.genesis.txt"]')
    (118, '    if config[\\\'admixMapping\\\'][\\\'skip\\\'] != \\\'true\\\': input_list += [f"{BASE}.admixmap.txt", f"{BASE}.globalancestry.txt"]')
    (119, '    if os.path.exists(VCF): input_list += [f"{BASE}.dos.genesis.txt"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ["if os.path.exists(config[\\'conditional_analysis\\'][\\'snp_list\\'])"]
    (122, 'def get_conditional_analysis_input(wildcards):')
    (123, '    inputList = []')
    (124, "    if os.path.exists(config[\\'conditional_analysis\\'][\\'snp_list\\']):")
    (125, "        with open(config[\\'conditional_analysis\\'][\\'snp_list\\'],\\'r\\') as snp_list: # One column with snpIDs")
    (126, '            for line in snp_list:')
    (127, '                marker = line.strip().replace(":", "-")')
    (128, '                inputList.append(f"conditional-analysis/{BASE}-{marker}.genesis.txt")')
    (129, '    else: print("Unable to locate \\\'snp_list\\\' file specified in config file under \\\'conditional_analysis")')
    (130, '    return(inputList)')
    (131, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=EmilySNichols/nnunet-fetalbrain, file=Snakefile
context_key: ['if os.path.exists(output.latest_model)']
    (172, 'def get_checkpoint_opt(wildcards, output):')
    (173, '    if os.path.exists(output.latest_model):')
    (174, "        return \\'--continue_training\\'")
    (175, '    else:')
    (176, "        return \\'\\' ")
    (177, '      ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/nanopore-assembly, file=Snakefile
context_key: ['if wildcards.busco_cat == "before"']
    (185, 'def busco_input(wildcards):')
    (186, "    \\'\\'\\'")
    (187, '    Function to give the correct input for busco before and after polishing')
    (188, "    \\'\\'\\'")
    (189, '    if wildcards.busco_cat == "before":')
    (190, '        return(f"{wildcards.prefix}_oneline.k32.w100.ntLink-arks.longstitch-scaffolds.fa")')
    (191, '    elif wildcards.busco_cat == "after":')
    (192, '        return(f"{wildcards.prefix}_oneline.k32.w100.ntLink-arks.longstitch-scaffolds.fa.PolcaCorrected.fa")')
    (193, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/population-structural-var-calling-smoove, file=Snakefile
context_key: ['if BWA_MEM_M == "Y"']
    (37, 'def preprocessing(wildcards):')
    (38, "    \\'\\'\\'")
    (39, '    If mapping has been done with bwa mem -M option, then run an extra step fo creating bam files with split and discordant reads before smoove_call')
    (40, "    \\'\\'\\'")
    (41, '    makedirs("1_call")')
    (42, '    if BWA_MEM_M == "Y":')
    (43, '        return("1_call/{sample}.done")')
    (44, '    elif BWA_MEM_M == "N":')
    (45, '        return([])')
    (46, '')
    (47, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-sorting-hat
context_key: ["if \\'lsf_config\\' in config"]
    (10, 'def get_lsf_config_file():')
    (11, '    """')
    (12, '    Returns the rule-specific LSF resource settings file to be copied to')
    (13, '    acc working directory before the snakemake call')
    (14, '    """')
    (15, "    if \\'lsf_config\\' in config:")
    (16, "        return config[\\'lsf_config\\']")
    (17, '    else:')
    (18, '        return None')
    (19, '')
    (20, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-sorting-hat
context_key: ["if \\'loghandler\\' in config"]
    (25, 'def get_log_handler_script():')
    (26, "    if \\'loghandler\\' in config:")
    (27, '        return f"--log-handler-script {config[\\\'loghandler\\\']} &>/dev/null"')
    (28, '    return ""')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-sorting-hat
context_key: ["if \\'tool\\' in config"]
    (30, 'def get_tool(acc):')
    (31, '    """')
    (32, '    If a tool is given in the config, then that tool is preferred,')
    (33, '    otherwise it is determined by the experiment type.')
    (34, '    """')
    (35, "    if \\'tool\\' in config:")
    (36, "        return config[\\'tool\\']")
    (37, '    if acc not in metadata:')
    (38, '        fill_metadata(acc)')
    (39, "    if \\'experiment_type\\' not in metadata[acc]:")
    (40, '        print(f"Experiment type not found for {acc}")')
    (41, '        return None')
    (42, "    if \\'differential\\' in metadata[acc][\\'experiment_type\\']:")
    (43, '        return "all-diff"')
    (44, "    if \\'baseline\\' in metadata[acc][\\'experiment_type\\']:")
    (45, '        return "all-baseline"')
    (46, '    print(f"Could not determine tool for {acc}")')
    (47, '    return None')
    (48, '')
    (49, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-sorting-hat
context_key: ['if acc not in metadata']
    (50, 'def get_organism(acc):')
    (51, '    if acc not in metadata:')
    (52, '        fill_metadata(acc)')
    (53, "    return metadata[acc][\\'organism\\']")
    (54, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-sorting-hat
context_key: ['if acc not in metadata']
    (55, 'def get_exp_type(acc):')
    (56, '    if acc not in metadata:')
    (57, '        fill_metadata(acc)')
    (58, "    return metadata[acc][\\'experiment_type\\']")
    (59, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-sorting-hat
context_key: ['if acc not in metadata']
    (60, 'def get_assay_contrast_line(acc):')
    (61, '    if acc not in metadata:')
    (62, '        fill_metadata(acc)')
    (63, "    if \\'contrasts\\' in metadata[acc]:")
    (64, '        contrast_ids = "::".join(metadata[acc][\\\'contrasts\\\'].keys())')
    (65, '        contrast_labels = "&&".join(metadata[acc][\\\'contrasts\\\'].values())')
    (66, '        return f"contrast_ids={contrast_ids} contrast_labels=\\\\"{contrast_labels}\\\\""')
    (67, "    elif \\'assays\\' in metadata[acc]:")
    (68, '        assay_ids = "::".join(metadata[acc][\\\'assays\\\'].keys())')
    (69, '        assay_labels = "&&".join(metadata[acc][\\\'assays\\\'].values())')
    (70, '        return f"assay_ids={assay_ids} assay_labels=\\\\"{assay_labels}\\\\""')
    (71, '    return None')
    (72, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-sorting-hat
context_key: ['if acc not in metadata']
    (73, 'def get_gff_path(acc):')
    (74, '    if acc not in metadata:')
    (75, '        fill_metadata(acc)')
    (76, "    return metadata[acc][\\'gff\\']")
    (77, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-sorting-hat
context_key: ["if \\'skip_steps_file\\' in config"]
    (78, 'def get_skip_steps_file():')
    (79, "    if \\'skip_steps_file\\' in config:")
    (80, "        return config[\\'skip_steps_file\\']")
    (81, '    else:')
    (82, '        return None')
    (83, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-sorting-hat
context_key: ["if \\'atlas_meta_config\\' in config"]
    (84, 'def get_meta_config():')
    (85, "    if \\'atlas_meta_config\\' in config:")
    (86, "        return config[\\'atlas_meta_config\\']")
    (87, '    else:')
    (88, '        return None')
    (89, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-sorting-hat
context_key: ["if \\'species\\' in config"]
    (90, 'def get_user_species(wildcards):')
    (91, "    if \\'species\\' in config:")
    (92, '        if get_organism( wildcards[\\\'accession\\\'] ) in config[\\\'species\\\'].split(":"):')
    (93, "            return \\'True\\'")
    (94, '        else:')
    (95, "            return \\'False\\'")
    (96, '    else:')
    (97, "        return \\'True\\'")
    (98, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-sorting-hat
context_key: ['if param in config']
    (99, 'def get_db_params(param):')
    (100, '    """Return config parameters to establish connection with isl db."""')
    (101, '    if param in config:')
    (102, '        return config[param]')
    (103, '    else:')
    (104, '        sys.exit(1)')
    (105, '')
    (106, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-sorting-hat
context_key: ["if \\'accessions\\' in config"]
    (107, 'def get_outputs():')
    (108, '    import os')
    (109, '    ACC=[]')
    (110, "    if \\'accessions\\' in config:")
    (111, '        ACC=config[\\\'accessions\\\'].split(":")')
    (112, '    else:')
    (113, "        ACC=glob.glob(\\'E-*\\')")
    (114, '')
    (115, "    if \\'goal\\' in config:")
    (116, "        if config[\\'goal\\'] == \\'recalculations\\':")
    (117, '')
    (118, "            if \\'atlas_exps\\' in config:")
    (119, '                # filter out ACC based on accessions that are available in atlas_exps')
    (120, "                valid_acc_dir = next(os.walk( config[\\'atlas_exps\\'] ))[1]")
    (121, '                # intersection of the two lists')
    (122, '                pass_ACC = list(set(ACC) & set(valid_acc_dir))')
    (123, '                # filter E-PROT-* as no recalculations are needed for these. ')
    (124, '                # In any case, Snakemake-recalculations will exit with error for proteomics experiments')
    (125, "                nonprot_ACC = list(filter(lambda k: not \\'E-PROT\\' in k, pass_ACC))")
    (126, '                return expand("{accession}/{accession}.recalculations.done", accession=nonprot_ACC)')
    (127, '            else:')
    (128, '                # filter E-PROT-* as no recalculations are needed for these. ')
    (129, '                # In any case, Snakemake-recalculations will exit with error for proteomics experiments')
    (130, "                nonprot_ACC = list(filter(lambda k: not \\'E-PROT\\' in k, ACC))")
    (131, '                return expand("{accession}/{accession}.recalculations.done", accession=nonprot_ACC)')
    (132, "        elif config[\\'goal\\'] == \\'reprocess\\':")
    (133, '            return expand("{accession}/{accession}.reprocess.done", accession=ACC)')
    (134, '        else:')
    (135, '            return None')
    (136, '    else:')
    (137, '        return None')
    (138, '')
    (139, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ['if not metadata_summary']
    (10, 'def read_metadata_summary():')
    (11, '    global metadata_summary')
    (12, '    if not metadata_summary:')
    (13, "        with open(config[\\'metadata_summary\\'], \\'r\\') as fh:")
    (14, '            metadata_summary = yaml.safe_load(fh)')
    (15, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'methods_base\\' in config"]
    (18, 'def get_methods_template_baseline():')
    (19, "    if \\'methods_base\\' in config:")
    (20, "        return config[\\'methods_base\\']")
    (21, '    else:')
    (22, '        return None')
    (23, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'methods_dif\\' in config"]
    (24, 'def get_methods_template_differential():')
    (25, "    if \\'methods_dif\\' in config:")
    (26, "        return config[\\'methods_dif\\']")
    (27, '    else:')
    (28, '        return None')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'zooma_exclusions\\' in config"]
    (30, 'def get_zooma_exclusions():')
    (31, "    if \\'zooma_exclusions\\' in config:")
    (32, "        return config[\\'zooma_exclusions\\']")
    (33, '    else:')
    (34, '        return None')
    (35, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'isl_dir\\' in config"]
    (36, 'def get_isl_dir():')
    (37, "    if \\'isl_dir\\' in config:")
    (38, "        return config[\\'isl_dir\\']")
    (39, '    else:')
    (40, '        return None')
    (41, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'isl_genomes\\' in config"]
    (42, 'def get_isl_genomes():')
    (43, "    if \\'isl_genomes\\' in config:")
    (44, "        return config[\\'isl_genomes\\']")
    (45, '    else:')
    (46, '        return None')
    (47, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'irap_versions\\' in config"]
    (48, 'def get_irap_versions():')
    (49, "    if \\'irap_versions\\' in config:")
    (50, "        return config[\\'irap_versions\\']")
    (51, '    else:')
    (52, '        return None')
    (53, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'irap_container\\' in config"]
    (54, 'def get_irap_container():')
    (55, "    if \\'irap_container\\' in config:")
    (56, "        return config[\\'irap_container\\']")
    (57, '    else:')
    (58, '        return None')
    (59, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'tmp_dir\\' in config"]
    (60, 'def get_tmp_dir():')
    (61, "    if \\'tmp_dir\\' in config:")
    (62, "        return config[\\'tmp_dir\\']")
    (63, '    else:')
    (64, '        return None')
    (65, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'skip_steps_file\\' in config", 'try']
    (66, 'def read_skip_steps_file():')
    (67, "    if \\'skip_steps_file\\' in config:")
    (68, '        global skip_steps')
    (69, "        with open(config[\\'skip_steps_file\\'], \\'r\\') as stream:")
    (70, '            try:')
    (71, '                skip_steps=yaml.safe_load(stream)')
    (72, '            except yaml.YAMLError as exc:')
    (73, '                print(exc)')
    (74, '        return skip_steps')
    (75, '')
    (76, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ['if label in config']
    (77, 'def get_from_config_or_metadata_summary(label):')
    (78, '    if label in config:')
    (79, '        return config[label]')
    (80, '    else:')
    (81, '        return metadata_summary[label]')
    (82, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'contrast_labels\\' in config"]
    (92, 'def get_contrast_labels():')
    (93, "    if \\'contrast_labels\\' in config:")
    (94, '        return f"{config[\\\'contrast_labels\\\']}".split("&&")')
    (95, '    else:')
    (96, '        read_metadata_summary()')
    (97, "        return [metadata_summary[\\'contrasts\\'][x] for x in metadata_summary[\\'contrasts\\']]")
    (98, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'contrast_ids\\' in config"]
    (99, 'def get_contrast_ids():')
    (100, "    if \\'contrast_ids\\' in config:")
    (101, '        return f"{config[\\\'contrast_ids\\\']}".split("::")')
    (102, '    else:')
    (103, '        read_metadata_summary()')
    (104, "        return [x for x in metadata_summary[\\'contrasts\\']]")
    (105, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'assay_labels\\' in config"]
    (106, 'def get_assay_labels():')
    (107, "    if \\'assay_labels\\' in config:")
    (108, '        return f"{config[\\\'assay_labels\\\']}".split("&&")')
    (109, '    else:')
    (110, '        read_metadata_summary()')
    (111, "        return [metadata_summary[\\'assays\\'][x] for x in metadata_summary[\\'assays\\']]")
    (112, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'assay_ids\\' in config"]
    (113, 'def get_assay_ids():')
    (114, "    if \\'assay_ids\\' in config:")
    (115, '        return f"{config[\\\'assay_ids\\\']}".split("::")')
    (116, '    else:')
    (117, "        return [x for x in metadata_summary[\\'assays\\']]")
    (118, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'ext\\' in config"]
    (119, 'def get_ext_db():')
    (120, "    if \\'ext\\' in config:")
    (121, '        return f"{config[\\\'ext\\\']}".split(":")')
    (122, '    else:')
    (123, '        return ["go", "reactome", "interpro"]')
    (124, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'metric\\' in config"]
    (125, 'def get_metrics_recalculations():')
    (126, '    """')
    (127, '    The logic is based on atlas production files. ')
    (128, '    """')
    (129, '    import glob')
    (130, "    if \\'metric\\' in config:")
    (131, '        return config[\\\'metric\\\'].split(":")')
    (132, '    else:')
    (133, '        metric_grabbed = []')
    (134, "        for metric in [\\'tpms\\', \\'fpkms\\']:")
    (135, '            # glob returns a list of matching paths, so if the metric is available a non-empty list is returned.')
    (136, '            if glob.glob(f"{config[\\\'accession\\\']}-{metric}.tsv"):')
    (137, '                metric_grabbed.append(metric)')
    (138, '        if (len(metric_grabbed )>0):')
    (139, '            return metric_grabbed     # ideally: [\\\'tpms\\\', "fpkms"]')
    (140, '        else:')
    (141, '            sys.exit("No metric available for baseline analyses.")')
    (142, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'metric\\' in config"]
    (143, 'def get_metrics_reprocess():')
    (144, '    """')
    (145, '    The logic is based on files processed by iRAP/ISL.')
    (146, '    """')
    (147, '    import os')
    (148, "    if \\'metric\\' in config:")
    (149, '        return config[\\\'metric\\\'].split(":")')
    (150, '    else:')
    (151, '        metrics_reprocess = []')
    (152, '        isl_dir = get_isl_dir()')
    (153, '        organism = get_organism()')
    (154, "        acc = config[\\'accession\\']")
    (155, '        files_ils_dir = os.listdir( f"{isl_dir}/{acc}/{organism}" )')
    (156, '        # we only need one match, no need to traverse the full list')
    (157, "        if next((s for s in files_ils_dir if \\'.tpm.\\' in s), None):")
    (158, "            metrics_reprocess.append(\\'tpms\\')")
    (159, "        if next((s for s in files_ils_dir if \\'.fpkm.\\' in s), None):")
    (160, "            metrics_reprocess.append(\\'fpkms\\')")
    (161, '')
    (162, '        if not metrics_reprocess:')
    (163, '            sys.exit("No metrics for reprocessing found in isl path.")')
    (164, '        else:')
    (165, '            return metrics_reprocess')
    (166, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if \\'atlas_meta_config\\' in config"]
    (167, 'def get_meta_config():')
    (168, "    if \\'atlas_meta_config\\' in config:")
    (169, "        return config[\\'atlas_meta_config\\']")
    (170, '    else:')
    (171, '        return None')
    (172, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ['if i in config']
    (173, 'def get_db_params():')
    (174, '    """')
    (175, '    When goal is reprocess, return config parameters to establish connection with isl db.')
    (176, '    """')
    (177, "    isl_vars=[ \\'oracle_home\\', \\'python_user\\', \\'python_connect_string\\', \\'python_password\\' ] ")
    (178, '    db_params = []')
    (179, '    for i in isl_vars:')
    (180, '        if i in config:')
    (181, '            db_params.append( config[i] )')
    (182, '        else:')
    (183, '            print(f" Missing ISL db param: {i}")')
    (184, '            sys.exit(1)')
    (185, '    return db_params')
    (186, '')
    (187, '')
    (188, '#metrics = get_metrics()')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ['if f not in config', 'if method']
    (197, 'def check_config_required(fields, method=""):')
    (198, '    ex=False')
    (199, '    for f in fields:')
    (200, '        if f not in config:')
    (201, '            print(f"{f} required to be set in config")')
    (202, '            ex=True')
    (203, '            if method:')
    (204, '                print(f" for method {method}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ['if f not in config', 'if ex']
    (197, 'def check_config_required(fields, method=""):')
    (198, '    ex=False')
    (199, '    for f in fields:')
    (200, '        if f not in config:')
    (201, '            print(f"{f} required to be set in config")')
    (202, '            ex=True')
    (203, '            if method:')
    (204, '                print(f" for method {method}")')
    (205, '    if ex:')
    (206, '        exit(2)')
    (207, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if skip_accession != None and acc in skip_accession[\\'skips\\'][tool]"]
    (208, 'def skip(acc, tool):')
    (209, "    if skip_accession != None and acc in skip_accession[\\'skips\\'][tool]:")
    (210, '        return False')
    (211, '    else:')
    (212, '        return True')
    (213, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if wildcards[\\'contrast_id\\'] in metadata_summary[\\'contrasts\\']"]
    (222, 'def get_contrast_label(wildcards):')
    (223, '    """')
    (224, '    Meant to be used within a rule, where a specific contrast_id is set in')
    (225, '    the wildcards. It retrieves the contrast id to label from the')
    (226, '    metadata_summary.')
    (227, '    """')
    (228, '    global metadata_summary')
    (229, "    if wildcards[\\'contrast_id\\'] in metadata_summary[\\'contrasts\\']:")
    (230, "        return metadata_summary[\\'contrasts\\'][wildcards[\\'contrast_id\\']]")
    (231, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ['if attempt > len(mem_avail)']
    (248, 'def get_mem_mb(wildcards, attempt):')
    (249, '    """')
    (250, '    To adjust resources in the rules ')
    (251, '    attemps = reiterations + 1')
    (252, '    Max number attemps = 8')
    (253, '    """')
    (254, '    mem_avail = [ 2, 2, 4, 8, 16, 64, 128, 256 ]  ')
    (255, '    if attempt > len(mem_avail):')
    (256, '        print(f"Attemps {attempt} exceeds the maximum number of attemps: {len(mem_avail)}")')
    (257, '        print(f"modify value of --restart-times or adjust mem_avail resources accordingly")')
    (258, '        sys.exit(1)')
    (259, '    else:')
    (260, '        return mem_avail[attempt-1] * 1000')
    (261, '')
    (262, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ["if experiment_type == \\'proteomics_differential\\'"]
    (360, 'def input_round_log2_fold_changes(wildcards):')
    (361, '    """')
    (362, '    Ensure rename files has been run before rounding log2 fold changes, for differential proteomics')
    (363, '    """')
    (364, '    inputs_files = [ f"{wildcards[\\\'accession\\\']}-analytics.tsv.undecorated" ]')
    (365, "    if experiment_type == \\'proteomics_differential\\':")
    (366, '        inputs_files.append( f"logs/{wildcards[\\\'accession\\\']}-rename_differential_proteomics_files.done" )')
    (367, '    return inputs_files')
    (368, '')
    (369, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/Pacbio-Alternative-Splicing, file=Snakefile
context_key: ['if targeted != ""']
    (23, 'def parse_sampleID(fname):')
    (24, "    return fname.split(\\'/\\')[-1].split(\\'.bam\\')[0]")
    (25, '')
    (26, "file = sorted(glob.glob(subread + \\'*.bam\\'), key=parse_sampleID)")
    (27, "m = subread + \\'merged.bam\\'")
    (28, 'while m in file:file.remove(m)')
    (29, '')
    (30, 'd = {}')
    (31, 'for key, value in itertools.groupby(file, parse_sampleID):')
    (32, '    d[key] = list(value)')
    (33, 'samples = d.keys()')
    (34, '')
    (35, '# path to apps')
    (36, 'tofu = config["tofu"]')
    (37, 'sqanti3 = config["sqanti3"]')
    (38, '')
    (39, '#pythonimport = config["pythonimport"]')
    (40, '')
    (41, '# reference')
    (42, 'ref = config["ref"]')
    (43, 'genome = config["genome"]')
    (44, 'gtf = config["gtf"]')
    (45, 'gref = config["gmap_ref"]')
    (46, 'gff3 = config["gff3"]')
    (47, 'tappas_gff3 = config["tappas_gff3"]')
    (48, '')
    (49, 'cn = ""')
    (50, 'st = 0')
    (51, 'targeted = config["targeted"]')
    (52, 'if targeted != "":')
    (53, '   cn = targeted.split(":")[0]')
    (54, '   st = int(targeted.split(":")[1].split("-")[0])')
    (55, '   ed = int(targeted.split(":")[1].split("-")[1])')
    (56, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=raivivek/ginkgo_pipeline_ex, file=Snakefile
context_key: ['if library']
    (12, 'def iterate_all_fastqs(library=None):')
    (13, '    for _library in iterate_all_libraries():')
    (14, '        if library:')
    (15, '            yield config["input"]["libraries"][_library]["fastqs"]')
    (16, '        else:')
    (17, '            for fastq in config["input"]["libraries"][_library]["fastqs"]:')
    (18, '                yield fastq')
    (19, '')
    (20, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jbloomlab/SARS-CoV-2-RBD_B.1.351, file=Snakefile
context_key: ["if any(barcode_runs_expandR1[\\'n_R1\\'] < 1)"]
    (24, 'def nb_markdown(nb):')
    (25, '    """Return path to Markdown results of notebook `nb`."""')
    (26, "    return os.path.join(config[\\'summary_dir\\'],")
    (27, "                        os.path.basename(os.path.splitext(nb)[0]) + \\'.md\\')")
    (28, '')
    (29, '# Global variables extracted from config --------------------------------------')
    (30, "pacbio_runs = (pd.read_csv(config[\\'pacbio_runs\\'], dtype = str)")
    (31, "               .assign(pacbioRun=lambda x: x[\\'library\\'] + \\'_\\' + x[\\'run\\'])")
    (32, '               )')
    (33, "assert len(pacbio_runs[\\'pacbioRun\\'].unique()) == len(pacbio_runs[\\'pacbioRun\\'])")
    (34, '')
    (35, '# Information on samples and barcode runs -------------------------------------')
    (36, "barcode_runs = pd.read_csv(config[\\'barcode_runs\\'])")
    (37, '')
    (38, '# combination of the *library* and *sample* columns should be unique.')
    (39, "assert len(barcode_runs.groupby([\\'library\\', \\'sample\\'])) == len(barcode_runs)")
    (40, '')
    (41, '# *sample* should be the hyphen separated concatenation of')
    (42, '# *experiment*, *antibody*, *concentration*, and *sort_bin*.')
    (43, 'sample_vs_expect = (')
    (44, '    barcode_runs')
    (45, "    .assign(expect=lambda x: x[[\\'experiment\\', \\'antibody\\', \\'concentration\\',")
    (46, "                                \\'sort_bin\\']]")
    (47, "                             .apply(lambda r: \\'-\\'.join(r.values.astype(str)),")
    (48, '                                    axis=1),')
    (49, "            equal=lambda x: x[\\'sample\\'] == x[\\'expect\\'],")
    (50, '            )')
    (51, '    )')
    (52, "assert sample_vs_expect[\\'equal\\'].all(), sample_vs_expect.query(\\'equal != True\\')")
    (53, '')
    (54, '# barcode runs with R1 files expanded by glob')
    (55, 'barcode_runs_expandR1 = (')
    (56, '    barcode_runs')
    (57, "    .assign(R1=lambda x: x[\\'R1\\'].str.split(\\'; \\').map(")
    (58, '                    lambda y: list(itertools.chain(*map(glob.glob, y)))),')
    (59, "            n_R1=lambda x: x[\\'R1\\'].map(len),")
    (60, "            sample_lib=lambda x: x[\\'sample\\'] + \\'_\\' + x[\\'library\\'],")
    (61, '            )')
    (62, '    )')
    (63, '')
    (64, "assert barcode_runs_expandR1[\\'sample_lib\\'].nunique() == len(barcode_runs_expandR1)")
    (65, "if any(barcode_runs_expandR1[\\'n_R1\\'] < 1):")
    (66, '    raise ValueError(f"no R1 for {barcode_runs_expandR1.query(\\\'n_R1 < 1\\\')}")')
    (67, '')
    (68, '# Rules -----------------------------------------------------------------------')
    (69, '')
    (70, '# this is the target rule (in place of `all`) since it first rule listed')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nextstrain/monkeypox, file=ingest/Snakefile
context_key: ['if len(files_to_upload) != len(remote_file_names)']
    (8, 'def _get_all_targets(wildcards):')
    (9, '    # Default targets are the metadata TSV and sequences FASTA files')
    (10, '    all_targets = ["data/sequences.fasta", "data/metadata.tsv"]')
    (11, '')
    (12, '    # Add additional targets based on upload config')
    (13, '    upload_config = config.get("upload", {})')
    (14, '')
    (15, '    for target, params in upload_config.items():')
    (16, '        files_to_upload = params.get("files_to_upload", [])')
    (17, '        remote_file_names = params.get("remote_file_names", [])')
    (18, '')
    (19, '        if len(files_to_upload) != len(remote_file_names):')
    (20, '            print(')
    (21, '                f"Skipping file upload for {target!r} because the number of",')
    (22, '                "files to upload does not match the number of remote file names.",')
    (23, '            )')
    (24, '        elif len(remote_file_names) != len(set(remote_file_names)):')
    (25, '            print(')
    (26, '                f"Skipping file upload for {target!r} because there are duplicate remote file names."')
    (27, '            )')
    (28, '        elif not params.get("dst"):')
    (29, '            print(')
    (30, '                f"Skipping file upload for {target!r} because the destintion was not defined."')
    (31, '            )')
    (32, '        else:')
    (33, '            all_targets.extend(')
    (34, '                expand(')
    (35, '                    [')
    (36, '                        f"data/upload/{target}/{{file_to_upload}}-to-{{remote_file_name}}.done"')
    (37, '                    ],')
    (38, '                    zip,')
    (39, '                    file_to_upload=files_to_upload,')
    (40, '                    remote_file_name=remote_file_names,')
    (41, '                )')
    (42, '            )')
    (43, '')
    (44, "    # Add additional targets for Nextstrain\\'s internal Slack notifications")
    (45, '    if send_slack_notifications:')
    (46, '        all_targets.extend(')
    (47, '            [')
    (48, '                "data/notify/genbank-record-change.done",')
    (49, '                "data/notify/metadata-diff.done",')
    (50, '            ]')
    (51, '        )')
    (52, '')
    (53, '    if config.get("trigger_rebuild"):')
    (54, '        all_targets.append("data/trigger/rebuild.done")')
    (55, '')
    (56, '    return all_targets')
    (57, '')
    (58, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dica, file=workflow/rules/vqsr.smk
context_key: ['if wildcards.type == "snp"']
    (2, 'def _get_recal_params(wildcards):')
    (3, '    known_variants = resolve_multi_filepath(config["resources"]["known_variants"])')
    (4, '    if wildcards.type == "snp":')
    (5, '        return (')
    (6, '            "-mode SNP "')
    (7, '            "--max-gaussians 4 "')
    (8, '            "-an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR "')
    (9, '            "-resource:hapmap,known=false,training=true,truth=true,prior=15.0 {hapmap} "')
    (10, '            "-resource:omni,known=false,training=true,truth=true,prior=12.0 {omni} "')
    (11, '            "-resource:1000G,known=false,training=true,truth=false,prior=10.0 {g1k} "')
    (12, '            "-resource:dbsnp,known=true,training=false,truth=false,prior=2.0 {dbsnp}"')
    (13, '        ).format(**known_variants)')
    (14, '    else:')
    (15, '        return (')
    (16, '            "-mode INDEL " ')
    (17, '            "-an QD -an FS -an SOR -an MQRankSum -an ReadPosRankSum "')
    (18, '            "--max-gaussians 4 "')
    (19, '            "-resource:mills,known=false,training=true,truth=true,prior=12.0 {mills} "')
    (20, '            "-resource:dbsnp,known=true,training=false,truth=false,prior=2.0 {dbsnp}"')
    (21, '        ).format(**known_variants)')
    (22, '')
    (23, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dica, file=workflow/rules/common.smk
context_key: ['if read_type == "se"', 'if conservative_cpu_count(reserve_cores=2, max_cores=99) > 2']
    (37, 'def threads_calculator(read_type="pe"):')
    (38, '    if read_type == "se":')
    (39, '        if conservative_cpu_count(reserve_cores=2, max_cores=99) > 2:')
    (40, '            return conservative_cpu_count(reserve_cores=2, max_cores=99) / 2')
    (41, '        else:')
    (42, '            return 1')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dica, file=workflow/rules/common.smk
context_key: ['if read_type == "se"', 'else', 'if conservative_cpu_count(reserve_cores=2, max_cores=99) > 4']
    (37, 'def threads_calculator(read_type="pe"):')
    (38, '    if read_type == "se":')
    (39, '        if conservative_cpu_count(reserve_cores=2, max_cores=99) > 2:')
    (40, '            return conservative_cpu_count(reserve_cores=2, max_cores=99) / 2')
    (41, '        else:')
    (42, '            return 1')
    (43, '    else:')
    (44, '        if conservative_cpu_count(reserve_cores=2, max_cores=99) > 4:')
    (45, '            return conservative_cpu_count(reserve_cores=2, max_cores=99) / 4')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dica, file=workflow/rules/common.smk
context_key: ['if read_type == "se"', 'else', 'else']
    (37, 'def threads_calculator(read_type="pe"):')
    (38, '    if read_type == "se":')
    (39, '        if conservative_cpu_count(reserve_cores=2, max_cores=99) > 2:')
    (40, '            return conservative_cpu_count(reserve_cores=2, max_cores=99) / 2')
    (41, '        else:')
    (42, '            return 1')
    (43, '    else:')
    (44, '        if conservative_cpu_count(reserve_cores=2, max_cores=99) > 4:')
    (45, '            return conservative_cpu_count(reserve_cores=2, max_cores=99) / 4')
    (46, '        else:')
    (47, '            return 1')
    (48, '')
    (49, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dica, file=workflow/rules/common.smk
context_key: ['if not os.path.isabs(filepath)']
    (50, 'def expand_filepath(filepath):')
    (51, '    filepath = os.path.expandvars(os.path.expanduser(filepath))')
    (52, '    if not os.path.isabs(filepath):')
    (53, '        raise FileNotFoundError(')
    (54, '            errno.ENOENT,')
    (55, '            os.strerror(errno.ENOENT) + " (path must be absolute)",')
    (56, '            filepath,')
    (57, '        )')
    (58, '    return filepath')
    (59, '')
    (60, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dica, file=workflow/rules/common.smk
context_key: ['if does not exists, create path and return it. If any errors, retur']
    (71, 'def tmp_path(path=""):')
    (72, '    """')
    (73, '    if does not exists, create path and return it. If any errors, return')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dica, file=workflow/rules/common.smk
context_key: ['if path', 'try']
    (71, 'def tmp_path(path=""):')
    (72, '    """')
    (73, '    if does not exists, create path and return it. If any errors, return')
    (74, '    default path')
    (75, '    :param path: path')
    (76, '    :return: path')
    (77, '    """')
    (78, '    default_path = os.getenv("TMPDIR", config.get("paths").get("tmp_dir"))')
    (79, '    if path:')
    (80, '        try:')
    (81, '            os.makedirs(path)')
    (82, '        except OSError as e:')
    (83, '            if e.errno != errno.EEXIST:')
    (84, '                return default_path')
    (85, '        return path')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dica, file=workflow/rules/common.smk
context_key: ['if path']
    (71, 'def tmp_path(path=""):')
    (72, '    """')
    (73, '    if does not exists, create path and return it. If any errors, return')
    (74, '    default path')
    (75, '    :param path: path')
    (76, '    :return: path')
    (77, '    """')
    (78, '    default_path = os.getenv("TMPDIR", config.get("paths").get("tmp_dir"))')
    (79, '    if path:')
    (80, '        try:')
    (81, '            os.makedirs(path)')
    (82, '        except OSError as e:')
    (83, '            if e.errno != errno.EEXIST:')
    (84, '                return default_path')
    (85, '        return path')
    (86, '    return default_path')
    (87, '')
    (88, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dica, file=workflow/rules/common.smk
context_key: ['if n >= prefix[s]']
    (106, '    def bytes2human(n):')
    (107, '        # http://code.activestate.com/recipes/578019')
    (108, '        # >>> bytes2human(10000)')
    (109, "        # \\'9.8K\\'")
    (110, '        # >>> bytes2human(100001221)')
    (111, "        # \\'95.4M\\'")
    (112, '        symbols = ("K", "M", "G", "T", "P", "E", "Z", "Y")')
    (113, '        prefix = {}')
    (114, '        for i, s in enumerate(symbols):')
    (115, '            prefix[s] = 1 << (i + 1) * 10')
    (116, '        for s in reversed(symbols):')
    (117, '            if n >= prefix[s]:')
    (118, '                value = float(n) / prefix[s]')
    (119, '                return "%.0f%s" % (value, s)')
    (120, '        return "%sB" % n')
    (121, '')
    (122, '    def preserve(resource, percentage, stock):')
    (123, '        preserved = resource - max(resource * percentage // 100, stock)')
    (124, '        return preserved if preserved != 0 else stock')
    (125, '')
    (126, '    # def preserve(resource, percentage, stock):')
    (127, '    #     return resource - max(resource * percentage // 100, stock)')
    (128, '')
    (129, '    params_template = "\\\'-Xms{} -Xmx{} -XX:ParallelGCThreads={} " "-Djava.io.tmpdir={}\\\'"')
    (130, '')
    (131, '    mem_min = 1024**3 * 2  # 2GB')
    (132, '')
    (133, '    mem_size = preserve(total_physical_mem_size(), percentage_to_preserve, stock_mem)')
    (134, '    cpu_nums = preserve(cpu_count(), percentage_to_preserve, stock_cpu)')
    (135, '    tmpdir = tmp_path(tmp_dir)')
    (136, '')
    (137, '    return params_template.format(')
    (138, '        bytes2human(mem_min).lower(),')
    (139, '        bytes2human(max(mem_size // cpu_nums * multiply_by, mem_min)).lower(),')
    (140, '        min(cpu_nums, multiply_by),')
    (141, '        tmpdir,')
    (142, '    )')
    (143, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/dica, file=workflow/rules/common.smk
context_key: ['if arguments']
    (144, 'def multi_flag_dbi(flag, arguments):')
    (145, '    if arguments:')
    (146, '        return " ".join(flag + " " + arg for arg in arguments)')
    (147, "    return \\'\\'")
    (148, '')
    (149, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yoshihikosuzuki/purge_tips_smk, file=workflow/Snakefile
context_key: ['if not os.path.exists(FINAL_OUTPUT)']
    (11, 'def is_converged(wildcards):')
    (12, '    global FINAL_OUTPUT, COUNTER')
    (13, '    if not os.path.exists(FINAL_OUTPUT):')
    (14, '        checkpoints.purge_tips.get(dir=COUNTER, base=BASE)')
    (15, '        COUNTER += 1')
    (16, '        return f"{COUNTER}/{BASE}.filtered.fastq"')
    (17, '    else:')
    (18, '        return FINAL_OUTPUT')
    (19, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/strelka2.smk
context_key: ['if "controls" in config["groups"] and "normals" in config["groups"] and "bulks" in config["groups"]', 'if bulk in config["groups"]["controls"] and bulk in config["groups"]["normals"]']
    (49, 'def _get_control_normal_bulk_bam(wildcards):')
    (50, '    if "controls" in config["groups"] and "normals" in config["groups"] and "bulks" in config["groups"]:')
    (51, '        for bulk in config["groups"]["bulks"]:')
    (52, '            if bulk in config["groups"]["controls"] and bulk in config["groups"]["normals"]:')
    (53, '                return "data/reads/mapped/" + wildcards.project + "/" + bulk + "." + wildcards.reference + "." + wildcards.mapper + ".bam"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/strelka2.smk
context_key: ['if "controls" in config["groups"] and "normals" in config["groups"] and "bulks" in config["groups"]']
    (49, 'def _get_control_normal_bulk_bam(wildcards):')
    (50, '    if "controls" in config["groups"] and "normals" in config["groups"] and "bulks" in config["groups"]:')
    (51, '        for bulk in config["groups"]["bulks"]:')
    (52, '            if bulk in config["groups"]["controls"] and bulk in config["groups"]["normals"]:')
    (53, '                return "data/reads/mapped/" + wildcards.project + "/" + bulk + "." + wildcards.reference + "." + wildcards.mapper + ".bam"')
    (54, '    return None')
    (55, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/strelka2.smk
context_key: ['if "controls" in config["groups"] and "normals" in config["groups"] and "bulks" in config["groups"]', 'if bulk in config["groups"]["controls"] and bulk not in config["groups"]["normals"]']
    (56, 'def _get_control_tumour_bulk_bam(wildcards):')
    (57, '    if "controls" in config["groups"] and "normals" in config["groups"] and "bulks" in config["groups"]:')
    (58, '        for bulk in config["groups"]["bulks"]:')
    (59, '            if bulk in config["groups"]["controls"] and bulk not in config["groups"]["normals"]:')
    (60, '                return "data/reads/mapped/" + wildcards.project + "/" + bulk + "." + wildcards.reference + "." + wildcards.mapper + ".bam"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/strelka2.smk
context_key: ['if "controls" in config["groups"] and "normals" in config["groups"] and "bulks" in config["groups"]']
    (56, 'def _get_control_tumour_bulk_bam(wildcards):')
    (57, '    if "controls" in config["groups"] and "normals" in config["groups"] and "bulks" in config["groups"]:')
    (58, '        for bulk in config["groups"]["bulks"]:')
    (59, '            if bulk in config["groups"]["controls"] and bulk not in config["groups"]["normals"]:')
    (60, '                return "data/reads/mapped/" + wildcards.project + "/" + bulk + "." + wildcards.reference + "." + wildcards.mapper + ".bam"')
    (61, '    return None')
    (62, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/mapping.smk
context_key: ['if "sample_annotations" in config', 'if wildcards.sample in config["sample_annotations"]', 'if "layout" in config["sample_annotations"][wildcards.sample]', 'if config["sample_annotations"][wildcards.sample]["layout"] == "SINGLE"']
    (17, 'def _get_fastqs(wildcards):')
    (18, '    if "sample_annotations" in config:')
    (19, '        if wildcards.sample in config["sample_annotations"]:')
    (20, '            if "layout" in config["sample_annotations"][wildcards.sample]:')
    (21, '                if config["sample_annotations"][wildcards.sample]["layout"] == "SINGLE":')
    (22, '                    return "data/reads/raw/" + wildcards.project + "/" + wildcards.sample + ".fastq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/mapping.smk
context_key: ['if "sample_annotations" in config']
    (17, 'def _get_fastqs(wildcards):')
    (18, '    if "sample_annotations" in config:')
    (19, '        if wildcards.sample in config["sample_annotations"]:')
    (20, '            if "layout" in config["sample_annotations"][wildcards.sample]:')
    (21, '                if config["sample_annotations"][wildcards.sample]["layout"] == "SINGLE":')
    (22, '                    return "data/reads/raw/" + wildcards.project + "/" + wildcards.sample + ".fastq.gz"')
    (23, '    return ["data/reads/raw/" + wildcards.project + "/" + wildcards.sample + ".R1.fastq.gz",')
    (24, '            "data/reads/raw/" + wildcards.project + "/" + wildcards.sample + ".R2.fastq.gz"]')
    (25, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/octopus.smk
context_key: ['if sample not in config["groups"]["cells"]']
    (7, 'def _get_sample_dropout_option(wildcards):')
    (8, '    concentrations = []')
    (9, '    for sample in config["groups"][wildcards.group]:')
    (10, '        if sample not in config["groups"]["cells"]:')
    (11, '            concentrations.append(sample + "=50")')
    (12, '    if len(concentrations) > 0:')
    (13, '        return "--sample-dropout " + " ".join(concentrations)')
    (14, '    else:')
    (15, '        return ""')
    (16, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/octopus.smk
context_key: ['if "normals" in config["groups"]', 'if sample in config["groups"]["normals"]']
    (17, 'def _get_normal_samples(wildcards):')
    (18, '    result = []')
    (19, '    if "normals" in config["groups"]:')
    (20, '        for sample in config["groups"][wildcards.group]:')
    (21, '            if sample in config["groups"]["normals"]:')
    (22, '                result.append(sample)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/octopus.smk
context_key: ['if "normals" in config["groups"]']
    (17, 'def _get_normal_samples(wildcards):')
    (18, '    result = []')
    (19, '    if "normals" in config["groups"]:')
    (20, '        for sample in config["groups"][wildcards.group]:')
    (21, '            if sample in config["groups"]["normals"]:')
    (22, '                result.append(sample)')
    (23, '    return result')
    (24, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/octopus.smk
context_key: ['if len(normals) > 0']
    (25, 'def _get_normal_samples_option(wildcards):')
    (26, '    normals = _get_normal_samples(wildcards)')
    (27, '    if len(normals) > 0:')
    (28, '        return "--normal-samples " + " ".join(normals)')
    (29, '    else:')
    (30, '        return ""')
    (31, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/octopus.smk
context_key: ['if any(sample in config["groups"]["cells"] for sample in config["groups"][wildcards.group])']
    (32, 'def _get_calling_model(wildcards):')
    (33, '    if any(sample in config["groups"]["cells"] for sample in config["groups"][wildcards.group]):')
    (34, '        return "cell"')
    (35, '    elif len(_get_normal_samples(wildcards)) > 0:')
    (36, '        return "cancer"')
    (37, '    elif len(config["groups"][wildcards.group]) == 1:')
    (38, '        return "individual"')
    (39, '    else:')
    (40, '        return "population"')
    (41, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/octopus.smk
context_key: ['if any(sample in config["groups"]["cells"] for sample in config["groups"][wildcards.group])']
    (42, 'def _get_sequence_error_model(wildcards):')
    (43, '    if any(sample in config["groups"]["cells"] for sample in config["groups"][wildcards.group]):')
    (44, '        return "10X"')
    (45, '    else:')
    (46, '        return "PCRF"')
    (47, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/evaluate.smk
context_key: ['if wildcards.match in fields']
    (33, 'def _get_caller_score_field(wildcards):')
    (34, '    fields = config["caller_score_fields"][wildcards.caller]')
    (35, '    if wildcards.match in fields:')
    (36, '        return fields[wildcards.match]')
    (37, '    else:')
    (38, '        return fields')
    (39, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/sccaller.smk
context_key: ['if sample in config["groups"]["cells"]']
    (120, 'def _get_sccaller_vcfs(wildcards):')
    (121, '    cells, bulks = [], []')
    (122, '    for sample in config["groups"][wildcards.group]:')
    (123, '        if sample in config["groups"]["cells"]:')
    (124, '            cells.append(sample)')
    (125, '        else:')
    (126, '            bulks.append(sample)')
    (127, '    bulk_group = None')
    (128, '    for group, samples in config["groups"].items():')
    (129, '        if set(samples) == set(bulks):')
    (130, '            bulk_group = group')
    (131, '            break')
    (132, '    if bulk_group is None:')
    (133, '        normal_cells = []')
    (134, '        if "normals" in config["groups"]:')
    (135, '            for cell in cells:')
    (136, '                if cell in config["groups"]["normals"]:')
    (137, '                    normal_cells.append(cell)')
    (138, '            for group, samples in config["groups"].items():')
    (139, '                if set(samples) == set(normal_cells):')
    (140, '                    bulk_group = group')
    (141, '                    break')
    (142, '        if bulk_group is None:')
    (143, '            raise Exception("No valid bulk group")')
    (144, '    res = []')
    (145, '    for cell in cells:')
    (146, '        res.append("results/calls/" + wildcards.project + "/" + cell + "+" + bulk_group + "." + wildcards.reference + "." + wildcards.mapper + ".SCcaller.vcf.gz")')
    (147, '    return res')
    (148, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/sccaller.smk
context_key: ['if sample in config["groups"]["cells"]']
    (178, 'def _get_sccaller_somatic_vcfs(wildcards):')
    (179, '    cells, bulks = [], []')
    (180, '    for sample in config["groups"][wildcards.group]:')
    (181, '        if sample in config["groups"]["cells"]:')
    (182, '            cells.append(sample)')
    (183, '        else:')
    (184, '            bulks.append(sample)')
    (185, '    bulk_group = None')
    (186, '    for group, samples in config["groups"].items():')
    (187, '        if set(samples) == set(bulks):')
    (188, '            bulk_group = group')
    (189, '            break')
    (190, '    if bulk_group is None:')
    (191, '        normal_cells = []')
    (192, '        if "normals" in config["groups"]:')
    (193, '            for cell in cells:')
    (194, '                if cell in config["groups"]["normals"]:')
    (195, '                    normal_cells.append(cell)')
    (196, '            for group, samples in config["groups"].items():')
    (197, '                if set(samples) == set(normal_cells):')
    (198, '                    bulk_group = group')
    (199, '                    break')
    (200, '        if bulk_group is None:')
    (201, '            raise Exception("No valid bulk group")')
    (202, '    res = []')
    (203, '    for cell in cells:')
    (204, '        res.append("results/calls/" + wildcards.project + "/" + cell + "+" + bulk_group + "." + wildcards.reference + "." + wildcards.mapper + ".SCcaller.somatics.vcf.gz")')
    (205, '    return res')
    (206, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=luntergroup/single-cell, file=workflow/rules/prosolo.smk
context_key: ['if sample in config["groups"]["cells"]']
    (78, 'def _get_prosolo_vcfs(wildcards):')
    (79, '    cells, bulks = [], []')
    (80, '    for sample in config["groups"][wildcards.group]:')
    (81, '        if sample in config["groups"]["cells"]:')
    (82, '            cells.append(sample)')
    (83, '        else:')
    (84, '            bulks.append(sample)')
    (85, '    bulk_group = None')
    (86, '    for group, samples in config["groups"].items():')
    (87, '        if set(samples) == set(bulks):')
    (88, '            bulk_group = group')
    (89, '            break')
    (90, '    if bulk_group is None:')
    (91, '        normal_cells = []')
    (92, '        if "normals" in config["groups"]:')
    (93, '            for cell in cells:')
    (94, '                if cell in config["groups"]["normals"]:')
    (95, '                    normal_cells.append(cell)')
    (96, '            for group, samples in config["groups"].items():')
    (97, '                if set(samples) == set(normal_cells):')
    (98, '                    bulk_group = group')
    (99, '                    break')
    (100, '        if bulk_group is None:')
    (101, '            raise Exception("No valid bulk group")')
    (102, '    res = []')
    (103, '    for cell in cells:')
    (104, '        res.append("results/calls/" + wildcards.project + "/" + cell + "+" + bulk_group + "." + wildcards.reference + "." + wildcards.mapper + ".Prosolo.vcf.gz")')
    (105, '    return res')
    (106, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab, file=workflow/rules/common.smk
context_key: ["if not Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], sub_dir).exists()", 'if config[sub_dir_config_keys[sub_dir]] is None']
    (36, 'def init_paths(sub_dir):')
    (37, "    if not Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], sub_dir).exists():")
    (38, '        if config[sub_dir_config_keys[sub_dir]] is None:')
    (39, "            print(\\'\\\\\\'\\'+sub_dir_config_keys[sub_dir]+\\'\\\\\\'\\'+\\' not specified in config file. Creating \\\\\\'\\'+sub_dir+\\'\\\\\\' in local folder.\\')")
    (40, "            Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], sub_dir).mkdir(parents=True, exist_ok=True)")
    (41, "            config[sub_dir_config_keys[sub_dir]] = Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], sub_dir)")
    (42, '        else:')
    (43, '            Path(config[sub_dir_config_keys[sub_dir]]).mkdir(parents=True, exist_ok=True)')
    (44, "            Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], sub_dir).symlink_to(config[sub_dir_config_keys[sub_dir]])")
    (45, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab, file=workflow/rules/common.smk
context_key: ['if len(fq)>1']
    (50, 'def rget_fq_by_target_ext(src_dir, target, extensions):')
    (51, '    # fq = []')
    (52, '    # for ext in extensions:')
    (53, "        # fq = glob.glob(src_dir+\\'/**/\\'+target+\\'**\\'+ext, recursive=True)")
    (54, '        # fq.extend(Path(src_dir).rglob(target + "*" +ext))## pathlib.Path.rglob doesn\\\'t work for symlink')
    (55, "    fq = list(set(chain(*[glob.glob(src_dir+\\'/**/\\'+target+\\'/**\\'+ext, recursive=True) for ext in extensions])))")
    (56, '    if len(fq)>1:')
    (57, "        raise ValueError(\\'More than one fastq found for target: \\' + target + \\'! Execution halted.\\')")
    (58, '    elif len(fq)==0:')
    (59, "        raise ValueError(\\'No fastq found for target: \\' + target + \\'! Execution halted.\\')")
    (60, '    return str(fq[0])')
    (61, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab, file=workflow/rules/common.smk
context_key: ['if len(lib)>1']
    (62, 'def rget_path_by_lib(src_dir, target, extensions):')
    (63, "    lib = sorted(Path(src_dir).resolve().rglob(target+\\'/\\'))")
    (64, '    if len(lib)>1:')
    (65, "        raise ValueError(\\'More than one fastq found for target: \\' + target + \\'! Execution halted.\\')")
    (66, '    elif len(lib)==0:')
    (67, "        raise ValueError(\\'No fastq found for target: \\' + target + \\'! Execution halted.\\')")
    (68, '    return str(lib[0])')
    (69, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab, file=workflow/rules/common.smk
context_key: ['if len(fq)>1']
    (70, 'def rget_fq_by_ext(src_dir, extensions):')
    (71, "    fq = list(set(chain(*[glob.glob(src_dir+\\'/**\\'+ext, recursive=True) for ext in extensions])))")
    (72, '    if len(fq)>1:')
    (73, "        raise ValueError(\\'More than one fastq found for target: \\' + target + \\'! Execution halted.\\')")
    (74, '    elif len(fq)==0:')
    (75, "        raise ValueError(\\'No fastq found for target: \\' + target + \\'! Execution halted.\\')")
    (76, '    return str(fq[0])')
    (77, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab, file=workflow/rules/common.smk
context_key: ['if not os.path.exists(f)']
    (229, 'def parse_dynamic_output(rule):')
    (230, '    # all_out: list of all files supposed to be generated in a given rule')
    (231, '    # Parse a list of output files. If a file was already produced in past run, remove that file from list.')
    (232, '    # Used to prevent redundant jobs on later reruns.')
    (233, '    all_out = get_files(rule)')
    (234, '    out_files = []')
    (235, '    for f in all_out: ')
    (236, '        if not os.path.exists(f):')
    (237, '            out_files.append(f)')
    (238, '    return out_files')
    (239, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab, file=workflow/rules/common.smk
context_key: ['if os.path.exists(output)']
    (246, 'def parse_STAR_dummy(wc):')
    (247, '    output = os.path.join("workflow", "data", wc.user, wc.project, "alignments", wc.sample, wc.sample) + "_Aligned.sortedByCoord.out.bam"')
    (248, '    if os.path.exists(output):')
    (249, '        return ""')
    (250, '    else:')
    (251, '        return "tmp/STARload.done"')
    (252, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab, file=workflow/rules/common.smk
context_key: ['if os.path.exists(output)']
    (259, 'def parse_fc_dummy(wc):')
    (260, '    output = os.path.join("workflow", "data", wc.user, wc.project, "alignments", wc.sample, wc.sample) + "_gene_assigned"')
    (261, '    if os.path.exists(output):')
    (262, '        return ""')
    (263, '    else:')
    (264, '        return "tmp/STARunload.done"')
    (265, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Vyoming/Immune_Tumor_Bulk_RNA, file=rules/functions.smk
context_key: ['if "strandedness" in samples.columns']
    (38, 'def get_strandness(samples):')
    (39, '    if "strandedness" in samples.columns:')
    (40, '        return samples["strandedness"].tolist()')
    (41, '    else:')
    (42, '        strand_list=["none"]')
    (43, '        return strand_list*samples.shape[0]')
    (44, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=frankier/vocabirt, file=workflow/Snakefile
context_key: ['if wc.split_mode in ("both", "word")']
    (91, 'def irt_input(wc):')
    (92, '    if wc.split_mode in ("both", "word"):')
    (93, '        return pjoin(IRTNN, wc.split_mode, "pred")')
    (94, '    else:')
    (95, '        return pjoin(IRTS, wc.split_mode)')
    (96, '')
    (97, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=frankier/vocabirt, file=workflow/Snakefile
context_key: ['if wc.patched == "wordfreq"']
    (98, 'def eval_irt_input(wc):')
    (99, '    if wc.patched == "wordfreq":')
    (100, '        return pjoin(PATCHED_IRT, wc.split_mode)')
    (101, '    else:')
    (102, '        return irt_input(wc)')
    (103, '')
    (104, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crs4/fair-crcc-img-convert, file=workflow/rules/common.smk
context_key: ['if any(p.is_absolute() for p in source_paths)']
    (28, 'def glob_source_paths() -> Iterable[Path]:')
    (29, '    base_dir = get_repository_path()')
    (30, "    source_paths = [ Path(p) for p in config[\\'sources\\'][\\'items\\'] ]")
    (31, '    if any(p.is_absolute() for p in source_paths):')
    (32, '        raise ValueError("Source paths must be relative to repository.path (absolute paths found).")')
    (33, '    # glob any directories for files that end with any of the Extensions')
    (34, '    try:')
    (35, '        cwd = Path.cwd()')
    (36, '        os.chdir(base_dir)')
    (37, '        # We glob with os.walk rather than Path.rglob to catch any of the supported')
    (38, '        # in a single directory traversal (with rglob we can only scan for one extension)')
    (39, '        source_files = \\\\')
    (40, '            [ Path(root, slide)')
    (41, '                for p in source_paths if p.is_dir()')
    (42, '                for root, _, files in os.walk(p)')
    (43, '                for slide in files if any(slide.endswith(ext) for ext in Extensions) ] + \\\\')
    (44, '            [ Path(p) for p in source_paths if p.is_file() and any(p.suffix == ext for ext in Extensions) ]')
    (45, '    finally:')
    (46, '        os.chdir(cwd)')
    (47, '    return source_files')
    (48, '')
    (49, '')
    (50, '###### Input functions ######')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crs4/fair-crcc-img-convert, file=workflow/rules/common.smk
context_key: ['if path.exists()']
    (51, 'def gen_rule_input_path(wildcard):')
    (52, '    """')
    (53, '    Given the wildcard, tries all Extensions until it')
    (54, '    finds one that matches a file name in the repository.')
    (55, '    """')
    (56, '    for suffix in Extensions:')
    (57, '        path = get_repository_path() / f"{wildcard.slide}{suffix}"')
    (58, '        if path.exists():')
    (59, '            return path')
    (60, "    return \\'\\'")
    (61, '')
    (62, '')
    (63, '###### Environment configuration functions ######')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crs4/fair-crcc-img-convert, file=workflow/rules/common.smk
context_key: ['if workflow.use_singularity', 'if repository == work_dir or repository in work_dir.parents']
    (64, 'def configure_environment():')
    (65, '    shell.prefix("set -o pipefail; ")')
    (66, '')
    (67, '    if workflow.use_singularity:')
    (68, '        # Bind mount the repository path into container.')
    (69, '        # Ideally we want to mount the repository in read-only mode.')
    (70, '        # To avoid making the working directory read-only should it be inside')
    (71, '        # or the same path as the working directory, we check for this case')
    (72, '        # and if true we mount read-write.')
    (73, "        repository = Path(config[\\'repository\\'][\\'path\\']).resolve()")
    (74, '        work_dir = Path.cwd()')
    (75, '        if repository == work_dir or repository in work_dir.parents:')
    (76, "            mount_options = \\'rw\\'")
    (77, '        else:')
    (78, "            mount_options = \\'ro\\'")
    (79, "        workflow.singularity_args += \\' \\'.join([")
    (80, '            # Use --cleanenv to work around singularity exec overriding env')
    (81, '            # vars from docker image with host values (https://github.com/sylabs/singularity/issues/533)')
    (82, '            " --cleanenv",')
    (83, '            f" --bind {repository}:{repository}:{mount_options}"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=merfishtools/merfishtools-evaluation, file=Snakefile
context_key: ['if isinstance(codebooks, str)']
    (163, 'def get_codebook(wildcards):')
    (164, '    codebooks = config["codebooks"][wildcards.dataset]')
    (165, '    if isinstance(codebooks, str):')
    (166, '        return codebooks')
    (167, '    return codebooks[int(wildcards.experiment)]')
    (168, '')
    (169, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=merfishtools/merfishtools-evaluation, file=Snakefile
context_key: ['if wildcards.dataset.startswith("simulated")']
    (389, 'def get_experiments(wildcards):')
    (390, '    if wildcards.dataset.startswith("simulated"):')
    (391, '        return means')
    (392, '    elif wildcards.dataset == "140genesData":')
    (393, '        return [e for e in experiments(wildcards.dataset) ')
    (394, '                if config["codebooks"][wildcards.dataset][e] == ')
    (395, '                   "codebook/140genesData.{}.txt".format(wildcards.codebook)]')
    (396, '    else:')
    (397, '        return experiments(wildcards.dataset)')
    (398, '    ')
    (399, '')
    (400, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MLKaufman/rna-seq-star-deseq2, file=rules/common.smk
context_key: ['if "strandedness" in units.columns']
    (23, 'def get_strandedness(units):')
    (24, '    if "strandedness" in units.columns:')
    (25, '        return units["strandedness"].tolist()')
    (26, '    else:')
    (27, '        strand_list = ["none"]')
    (28, '        return strand_list * units.shape[0]')
    (29, '')
    (30, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/cluster_rnaseq, file=Snakefile
context_key: ['if any(is_reference)']
    (66, 'def get_reference_level(x):')
    (67, "    \\'\\'\\'")
    (68, '\\tGets the reference level for a column.')
    (69, "\\t\\'\\'\\'")
    (70, '    levels = x.unique()')
    (71, '    is_reference = [lvl.startswith("*") for lvl in levels]')
    (72, '    if any(is_reference):    \\t\\t\\t\\t')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snakerTest, file=workflow/Snakefile
context_key: ['if _sample_dict is None']
    (363, 'def get_sample_dict():')
    (364, '    global _sample_dict')
    (365, '    if _sample_dict is None:')
    (366, '        checkpoints.generate_sample_list.get()')
    (367, '        dat_samples = pd.read_csv(file_path_samples)')
    (368, '        _sample_dict = {pathlib.Path(x).stem: x for x in dat_samples[')
    (369, "            \\'mcd_folders\\']}")
    (370, '    return _sample_dict')
    (371, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=argschwind/TAPseq_workflow, file=rules/extract_dge.smk
context_key: ['if isinstance(whitelist, str)']
    (8, 'def get_whitelist_arg(whitelist):')
    (9, '  if isinstance(whitelist, str):')
    (10, '    whitelist_arg = "-w " + whitelist + " "')
    (11, '  else:')
    (12, '    whitelist_arg = ""')
    (13, '  return whitelist_arg')
    (14, '')
    (15, '# workflow rules -----------------------------------------------------------------------------------')
    (16, '')
    (17, '# count UMI observations per cell barcode and gene tag')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlexanderLabWHOI/EukHeist, file=Snakefile
context_key: ['if FORWARD']
    (40, 'def identify_read_groups(assembly_group_name, STUDY, FORWARD=True):')
    (41, '    outlist=[] ')
    (42, "    ERR_list = METAG_SAMPLELIST.loc[assembly_group_name, \\'ERR_list\\'].split(\\', \\')")
    (43, '    if FORWARD: ')
    (44, '        num = 1')
    (45, '    else: ')
    (46, '        num = 2')
    (47, '    for E in ERR_list: ')
    (48, '        outlist.append(SCRATCHDIR + "/trimmed/{}/{}_{}.trimmed.fastq.gz".format(STUDY,E, num)) ')
    (49, '    return(outlist)')
    (50, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_AmpliconSeq, file=workflow/rules/init.smk
context_key: ['try', 'try', 'if not os.path.exists(join(WORKDIR,"fastqs"))', 'if not os.path.exists(join(WORKDIR,"results"))']
    (30, 'def check_writeaccess(filename):')
    (31, '\\t"""Checks permissions to see if user can write to a file')
    (32, '\\t:param filename <str>: Name of file to check')
    (33, '\\t"""')
    (34, '\\tfilename=filename.strip()')
    (35, '\\tcheck_existence(filename)')
    (36, '\\tif not os.access(filename,os.W_OK):')
    (37, '\\t\\tsys.exit("File: {} exists, but user cannot write to file due to permissions!".format(filename))')
    (38, '\\treturn True')
    (39, '')
    (40, '')
    (41, '#')
    (42, 'MEMORY="100"')
    (43, '# get working dir from config')
    (44, 'WORKDIR = config["workdir"]')
    (45, '')
    (46, '# get resources folder')
    (47, 'try:')
    (48, '\\tRESOURCESDIR = config["resourcesdir"]')
    (49, 'except KeyError:')
    (50, '\\tRESOURCESDIR = join(WORKDIR,"resources")')
    (51, '')
    (52, '# get scripts folder')
    (53, 'try:')
    (54, '\\tSCRIPTSDIR = config["scriptsdir"]')
    (55, 'except KeyError:')
    (56, '\\tSCRIPTSDIR = join(WORKDIR,"scripts")')
    (57, '')
    (58, '## Load tools from YAML file')
    (59, 'with open(config["tools"]) as f:')
    (60, '\\tTOOLS = yaml.safe_load(f)')
    (61, '')
    (62, 'if not os.path.exists(join(WORKDIR,"fastqs")):')
    (63, '\\tos.mkdir(join(WORKDIR,"fastqs"))')
    (64, 'if not os.path.exists(join(WORKDIR,"results")):')
    (65, '\\tos.mkdir(join(WORKDIR,"results"))')
    (66, 'for f in ["samples", "tools", "cluster"]:')
    (67, '\\tcheck_readaccess(config[f])')
    (68, '')
    (69, 'SAMPLESDF = pd.read_csv(config["samples"],sep="\\\\t",header=0,index_col="sampleName")')
    (70, 'SAMPLES = list(SAMPLESDF.index)')
    (71, 'SAMPLESDF["R1"]=join(RESOURCESDIR,"dummy")')
    (72, 'SAMPLESDF["R2"]=join(RESOURCESDIR,"dummy")')
    (73, 'SAMPLESDF["PEorSE"]="PE"')
    (74, '')
    (75, 'for sample in SAMPLES:')
    (76, '\\tR1file=SAMPLESDF["path_to_R1_fastq"][sample]')
    (77, '\\tR2file=SAMPLESDF["path_to_R2_fastq"][sample]')
    (78, '\\t# print(sample,R1file,R2file)')
    (79, '\\tcheck_readaccess(R1file)')
    (80, '\\tR1filenewname=join(WORKDIR,"fastqs",sample+".R1.fastq.gz")')
    (81, '\\tif not os.path.exists(R1filenewname):')
    (82, '\\t\\tos.symlink(R1file,R1filenewname)')
    (83, '\\tSAMPLESDF.loc[[sample],"R1"]=R1filenewname')
    (84, "\\tif str(R2file)!=\\'nan\\':")
    (85, '\\t\\tcheck_readaccess(R2file)')
    (86, '\\t\\tR2filenewname=join(WORKDIR,"fastqs",sample+".R2.fastq.gz")')
    (87, '\\t\\tif not os.path.exists(R2filenewname):')
    (88, '\\t\\t\\tos.symlink(R2file,R2filenewname)')
    (89, '\\t\\tSAMPLESDF.loc[[sample],"R2"]=R2filenewname')
    (90, '\\telse:')
    (91, '\\t\\tSAMPLESDF.loc[[sample],"PEorSE"]="SE"')
    (92, '# print(SAMPLESDF)')
    (93, '# sys.exit()')
    (94, '')
    (95, '')
    (96, "'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hivlab/blast-virome, file=rules/common.smk
context_key: ['if cols_to_integer']
    (11, 'def concatenate_tables(input, sep="\\\\s+", cols_to_integer=None):')
    (12, '    frames = [safely_read_csv(f, sep=sep) for f in input]')
    (13, '    frames_concatenated = pd.concat(frames, keys=input, sort=False)')
    (14, '    if cols_to_integer:')
    (15, '        frames_concatenated[cols_to_integer] = frames_concatenated[')
    (16, '            cols_to_integer')
    (17, '        ].apply(lambda x: pd.Series(x, dtype="Int64"))')
    (18, '    return frames_concatenated')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/chipseq, file=workflow/rules/common.smk
context_key: ['if isinstance(fq2_present, pd.core.series.Series)']
    (36, 'def is_single_end(sample, unit):')
    (37, '    """Determine whether unit is single-end."""')
    (38, '    fq2_present = pd.isnull(units.loc[(sample, unit), "fq2"])')
    (39, '    if isinstance(fq2_present, pd.core.series.Series):')
    (40, '        # if this is the case, get_fastqs cannot work properly')
    (41, '        raise ValueError(')
    (42, '            f"Multiple fq2 entries found for sample-unit combination {sample}-{unit}.\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/chipseq, file=workflow/rules/common.smk
context_key: ['if ( wildcards.read == "0" or wildcards.read == "1" )', 'if is_sra_se(wildcards.sample, wildcards.unit)']
    (67, 'def get_individual_fastq(wildcards):')
    (68, '    """Get individual raw FASTQ files from unit sheet, based on a read (end) wildcard"""')
    (69, '    if ( wildcards.read == "0" or wildcards.read == "1" ):')
    (70, '        if is_sra_se(wildcards.sample, wildcards.unit):')
    (71, '            return expand("resources/ref/sra-se-reads/{accession}.fastq",')
    (72, '                              accession=units.loc[ (wildcards.sample, wildcards.unit), "sra_accession" ])')
    (73, '        elif is_sra_pe(wildcards.sample, wildcards.unit):')
    (74, '            return expand("resources/ref/sra-pe-reads/{accession}.1.fastq",')
    (75, '                              accession=units.loc[ (wildcards.sample, wildcards.unit), "sra_accession" ])')
    (76, '        else:')
    (77, '            return units.loc[ (wildcards.sample, wildcards.unit), "fq1" ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/chipseq, file=workflow/rules/common.smk
context_key: ['if ( wildcards.read == "0" or wildcards.read == "1" )', 'elif wildcards.read == "2"', 'if is_sra_pe(wildcards.sample, wildcards.unit)']
    (67, 'def get_individual_fastq(wildcards):')
    (68, '    """Get individual raw FASTQ files from unit sheet, based on a read (end) wildcard"""')
    (69, '    if ( wildcards.read == "0" or wildcards.read == "1" ):')
    (70, '        if is_sra_se(wildcards.sample, wildcards.unit):')
    (71, '            return expand("resources/ref/sra-se-reads/{accession}.fastq",')
    (72, '                              accession=units.loc[ (wildcards.sample, wildcards.unit), "sra_accession" ])')
    (73, '        elif is_sra_pe(wildcards.sample, wildcards.unit):')
    (74, '            return expand("resources/ref/sra-pe-reads/{accession}.1.fastq",')
    (75, '                              accession=units.loc[ (wildcards.sample, wildcards.unit), "sra_accession" ])')
    (76, '        else:')
    (77, '            return units.loc[ (wildcards.sample, wildcards.unit), "fq1" ]')
    (78, '    elif wildcards.read == "2":')
    (79, '        if is_sra_pe(wildcards.sample, wildcards.unit):')
    (80, '            return expand("resources/ref/sra-pe-reads/{accession}.2.fastq",')
    (81, '                          accession=units.loc[ (wildcards.sample, wildcards.unit), "sra_accession" ])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/chipseq, file=workflow/rules/common.smk
context_key: ['if ( wildcards.read == "0" or wildcards.read == "1" )', 'elif wildcards.read == "2"', 'else']
    (67, 'def get_individual_fastq(wildcards):')
    (68, '    """Get individual raw FASTQ files from unit sheet, based on a read (end) wildcard"""')
    (69, '    if ( wildcards.read == "0" or wildcards.read == "1" ):')
    (70, '        if is_sra_se(wildcards.sample, wildcards.unit):')
    (71, '            return expand("resources/ref/sra-se-reads/{accession}.fastq",')
    (72, '                              accession=units.loc[ (wildcards.sample, wildcards.unit), "sra_accession" ])')
    (73, '        elif is_sra_pe(wildcards.sample, wildcards.unit):')
    (74, '            return expand("resources/ref/sra-pe-reads/{accession}.1.fastq",')
    (75, '                              accession=units.loc[ (wildcards.sample, wildcards.unit), "sra_accession" ])')
    (76, '        else:')
    (77, '            return units.loc[ (wildcards.sample, wildcards.unit), "fq1" ]')
    (78, '    elif wildcards.read == "2":')
    (79, '        if is_sra_pe(wildcards.sample, wildcards.unit):')
    (80, '            return expand("resources/ref/sra-pe-reads/{accession}.2.fastq",')
    (81, '                          accession=units.loc[ (wildcards.sample, wildcards.unit), "sra_accession" ])')
    (82, '        else:')
    (83, '            return units.loc[ (wildcards.sample, wildcards.unit), "fq2" ]')
    (84, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/chipseq, file=workflow/rules/common.smk
context_key: ['if is_sra_se(wildcards.sample, wildcards.unit)']
    (85, 'def get_fastqs(wildcards):')
    (86, '    """Get raw FASTQ files from unit sheet."""')
    (87, '    if is_sra_se(wildcards.sample, wildcards.unit):')
    (88, '        return expand("resources/ref/sra-se-reads/{accession}.fastq",')
    (89, '                          accession=units.loc[ (wildcards.sample, wildcards.unit), "sra_accession" ])')
    (90, '    elif is_sra_pe(wildcards.sample, wildcards.unit):')
    (91, '        return expand(["resources/ref/sra-pe-reads/{accession}.1.fastq", "ref/sra-pe-reads/{accession}.2.fastq"],')
    (92, '                          accession=units.loc[ (wildcards.sample, wildcards.unit), "sra_accession" ])')
    (93, '    elif is_single_end(wildcards.sample, wildcards.unit):')
    (94, '        return units.loc[ (wildcards.sample, wildcards.unit), "fq1" ]')
    (95, '    else:')
    (96, '        u = units.loc[ (wildcards.sample, wildcards.unit), ["fq1", "fq2"] ].dropna()')
    (97, '        return [ f"{u.fq1}", f"{u.fq2}" ]')
    (98, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/chipseq, file=workflow/rules/common.smk
context_key: ['if not is_control(sample)']
    (103, 'def get_sample_control_peak_combinations_list():')
    (104, '    sam_contr = []')
    (105, '    for sample in samples.index:')
    (106, '        if not is_control(sample):')
    (107, '            sam_contr.extend(expand(["{sample}-{control}.{peak}"], sample = sample, control = samples.loc[sample]["control"], peak = config["params"]["peak-analysis"]))')
    (108, '    return sam_contr')
    (109, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/chipseq, file=workflow/rules/common.smk
context_key: ['if is_single_end(wildcards.sample, wildcards.unit)']
    (153, 'def get_map_reads_input(wildcards):')
    (154, '    if is_single_end(wildcards.sample, wildcards.unit):')
    (155, '        return "results/trimmed/{sample}-{unit}.fastq.gz"')
    (156, '    return ["results/trimmed/{sample}-{unit}.1.fastq.gz", "results/trimmed/{sample}-{unit}.2.fastq.gz"]')
    (157, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/chipseq, file=workflow/rules/common.smk
context_key: ['if is_single_end(sample, unit)']
    (165, 'def get_multiqc_input(wildcards):')
    (166, '    multiqc_input = []')
    (167, '    for (sample, unit) in units.index:')
    (168, '        reads = [ "1", "2" ]')
    (169, '        if is_single_end(sample, unit):')
    (170, '            reads = [ "0" ]')
    (171, '            multiqc_input.extend(expand (["logs/cutadapt/{sample}-{unit}.se.log"],')
    (172, '            sample = sample, unit = unit))')
    (173, '        else:')
    (174, '            multiqc_input.extend(expand (["logs/cutadapt/{sample}-{unit}.pe.log"],')
    (175, '            sample = sample, unit = unit))')
    (176, '')
    (177, '        multiqc_input.extend(')
    (178, '            expand (')
    (179, '                [')
    (180, '                    "results/qc/fastqc/{sample}.{unit}.{reads}_fastqc.zip",')
    (181, '                    "results/qc/fastqc/{sample}.{unit}.{reads}.html",')
    (182, '                    "results/mapped/{sample}-{unit}.mapped.flagstat",')
    (183, '                    "results/mapped/{sample}-{unit}.mapped.idxstats",')
    (184, '                    "results/mapped/{sample}-{unit}.mapped.stats.txt"')
    (185, '                ],')
    (186, '                sample = sample,')
    (187, '                unit = unit,')
    (188, '                reads = reads')
    (189, '            )')
    (190, '        )')
    (191, '    for sample in samples.index:')
    (192, '        multiqc_input.extend(')
    (193, '            expand (')
    (194, '                [')
    (195, '                    "results/picard_dedup/{sample}.metrics.txt",')
    (196, '                    "results/picard_dedup/{sample}.picard_dedup.flagstat",')
    (197, '                    "results/picard_dedup/{sample}.picard_dedup.idxstats",')
    (198, '                    "results/picard_dedup/{sample}.picard_dedup.stats.txt",')
    (199, '                    "results/bamtools_filtered/{sample}.sorted.bamtools_filtered.flagstat",')
    (200, '                    "results/bamtools_filtered/{sample}.sorted.bamtools_filtered.idxstats",')
    (201, '                    "results/bamtools_filtered/{sample}.sorted.bamtools_filtered.stats.txt",')
    (202, '                    "results/phantompeakqualtools/{sample}.phantompeak.spp.out",')
    (203, '                    "results/phantompeakqualtools/{sample}.spp_correlation_mqc.tsv",')
    (204, '                    "results/phantompeakqualtools/{sample}.spp_nsc_mqc.tsv",')
    (205, '                    "results/phantompeakqualtools/{sample}.spp_rsc_mqc.tsv"')
    (206, '                ],')
    (207, '                sample = sample')
    (208, '            )')
    (209, '        )')
    (210, '        if config["params"]["deeptools-plots"]["activate"]:')
    (211, '            multiqc_input.extend(')
    (212, '                expand(')
    (213, '                    [')
    (214, '                        "results/deeptools/plot_profile_data.tab"')
    (215, '                    ],')
    (216, '                    sample=sample')
    (217, '                )')
    (218, '            )')
    (219, '')
    (220, '        if not config["single_end"]:')
    (221, '            multiqc_input.extend(')
    (222, '                expand (')
    (223, '                    [')
    (224, '                        "results/orph_rm_pe/{sample}.sorted.orph_rm_pe.idxstats",')
    (225, '                        "results/orph_rm_pe/{sample}.sorted.orph_rm_pe.flagstat",')
    (226, '                        "results/orph_rm_pe/{sample}.sorted.orph_rm_pe.stats.txt"')
    (227, '                    ],')
    (228, '                    sample = sample')
    (229, '                )')
    (230, '            )')
    (231, '')
    (232, '        if not is_control(sample):')
    (233, '            multiqc_input.extend(')
    (234, '                expand (')
    (235, '                    [')
    (236, '                        "results/deeptools/{sample}-{control}.fingerprint_qcmetrics.txt",')
    (237, '                        "results/deeptools/{sample}-{control}.fingerprint_counts.txt",')
    (238, '                        "results/macs2_callpeak/{sample}-{control}.{peak}_peaks.xls"')
    (239, '                    ],')
    (240, '                sample = sample,')
    (241, '                control = samples.loc[sample]["control"],')
    (242, '                peak = config["params"]["peak-analysis"]')
    (243, '            )')
    (244, '        )')
    (245, '        if config["params"]["lc_extrap"]["activate"]:')
    (246, '                multiqc_input.extend( expand(["results/preseq/{sample}.lc_extrap"], sample = sample))')
    (247, '        if config["params"]["picard_metrics"]["activate"]:')
    (248, '            if not config["single_end"]:')
    (249, '                multiqc_input.extend(')
    (250, '                    expand(')
    (251, '                        [')
    (252, '                            "results/qc/multiple_metrics/{sample}.insert_size_metrics",')
    (253, '                            "results/qc/multiple_metrics/{sample}.insert_size_histogram.pdf",')
    (254, '                        ],')
    (255, '                    sample = sample')
    (256, '                )')
    (257, '            )')
    (258, '            multiqc_input.extend(')
    (259, '                expand (')
    (260, '                    [                        ')
    (261, '                        "results/qc/multiple_metrics/{sample}.alignment_summary_metrics",')
    (262, '                        "results/qc/multiple_metrics/{sample}.base_distribution_by_cycle_metrics",')
    (263, '                        "results/qc/multiple_metrics/{sample}.base_distribution_by_cycle.pdf",')
    (264, '                        "results/qc/multiple_metrics/{sample}.quality_by_cycle_metrics",')
    (265, '                        "results/qc/multiple_metrics/{sample}.quality_by_cycle.pdf",')
    (266, '                        "results/qc/multiple_metrics/{sample}.quality_distribution_metrics",')
    (267, '                        "results/qc/multiple_metrics/{sample}.quality_distribution.pdf"')
    (268, '                    ], ')
    (269, '                sample = sample')
    (270, '            )')
    (271, '        )')
    (272, '    return multiqc_input')
    (273, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/chipseq, file=workflow/rules/common.smk
context_key: ['if is_single_end(sample, unit)']
    (274, 'def all_input(wildcards):')
    (275, '    do_annot = config["params"]["peak-annotation-analysis"]["activate"]')
    (276, '    do_peak_qc = config["params"]["peak-qc"]["activate"]')
    (277, '    do_consensus_peak = config["params"]["consensus-peak-analysis"]["activate"]')
    (278, '')
    (279, '    wanted_input = []')
    (280, '')
    (281, '    # QC with fastQC and multiQC')
    (282, '    wanted_input.extend([')
    (283, '        "results/qc/multiqc/multiqc.html"')
    (284, '    ])')
    (285, '')
    (286, '    # trimming reads')
    (287, '    for (sample, unit) in units.index:')
    (288, '        if is_single_end(sample, unit):')
    (289, '            wanted_input.extend(expand(')
    (290, '                    [')
    (291, '                        "results/trimmed/{sample}-{unit}.fastq.gz",')
    (292, '                        "results/trimmed/{sample}-{unit}.se.qc.txt"')
    (293, '                    ],')
    (294, '                    sample = sample,')
    (295, '                    unit = unit')
    (296, '                )')
    (297, '            )')
    (298, '        else:')
    (299, '            wanted_input.extend(')
    (300, '                expand (')
    (301, '                    [')
    (302, '                        "results/trimmed/{sample}-{unit}.1.fastq.gz",')
    (303, '                        "results/trimmed/{sample}-{unit}.2.fastq.gz",')
    (304, '                        "results/trimmed/{sample}-{unit}.pe.qc.txt"')
    (305, '                    ],')
    (306, '                    sample = sample,')
    (307, '                    unit = unit')
    (308, '            )')
    (309, '        )')
    (310, '')
    (311, '    # mapping, merging and filtering bam-files')
    (312, '    for sample in samples.index:')
    (313, '        wanted_input.extend(')
    (314, '            expand (')
    (315, '                [')
    (316, '                    "results/IGV/big_wig/merged_library.bigWig.igv.txt",')
    (317, '                    "results/phantompeakqualtools/{sample}.phantompeak.pdf"')
    (318, '                ],')
    (319, '                sample = sample')
    (320, '            )')
    (321, '        )')
    (322, '')
    (323, '        if config["params"]["deeptools-plots"]["activate"]:')
    (324, '            wanted_input.extend(')
    (325, '                expand(')
    (326, '                    [')
    (327, '                        "results/deeptools/plot_profile.pdf",')
    (328, '                        "results/deeptools/heatmap.pdf",')
    (329, '                        "results/deeptools/heatmap_matrix.tab"')
    (330, '                    ],')
    (331, '                    sample=sample')
    (332, '                )')
    (333, '            )')
    (334, '')
    (335, '        if not is_control(sample):')
    (336, '            with checkpoints.get_gsize.get().output[0].open() as f:')
    (337, '                # only produce the following files, if a gsize is specified')
    (338, '                if f.read().strip() != "":')
    (339, '                    if do_annot:')
    (340, '                        wanted_input.extend(')
    (341, '                            expand(')
    (342, '                                [')
    (343, '                                    "results/homer/annotate_peaks/{sample}-{control}.{peak}_peaks.annotatePeaks.txt"')
    (344, '                                ],')
    (345, '                                sample = sample,')
    (346, '                                control = samples.loc[sample]["control"],')
    (347, '                                peak = config["params"]["peak-analysis"]')
    (348, '                            )')
    (349, '                        )')
    (350, '                        if do_peak_qc:')
    (351, '                            wanted_input.extend(')
    (352, '                                expand(')
    (353, '                                    [')
    (354, '                                        "results/homer/plots/plot_{peak}_annotatepeaks_summary.txt",')
    (355, '                                        "results/homer/plots/plot_{peak}_annotatepeaks.pdf"')
    (356, '                                    ],')
    (357, '                                    peak = config["params"]["peak-analysis"]')
    (358, '                                )')
    (359, '                            )')
    (360, '                    if do_consensus_peak:')
    (361, '                        for antibody in samples["antibody"]:')
    (362, '                            if exists_multiple_groups(antibody) or exists_replicates(antibody):')
    (363, '                                wanted_input.extend(')
    (364, '                                    expand(')
    (365, '                                        [')
    (366, '                                            "results/macs2_merged_expand/{antibody}.consensus_{peak}-peaks.boolean.saf",')
    (367, '                                            "results/macs2_merged_expand/plots/{antibody}.consensus_{peak}-peaks.boolean.intersect.plot.pdf",')
    (368, '                                            "results/IGV/consensus/merged_library.{antibody}.consensus_{peak}-peaks.igv.txt"')
    (369, '                                        ],')
    (370, '                                        peak = config["params"]["peak-analysis"],')
    (371, '                                        antibody = antibody')
    (372, '                                    )')
    (373, '                                )')
    (374, '                                if do_annot:')
    (375, '                                    wanted_input.extend(')
    (376, '                                        expand(')
    (377, '                                            [')
    (378, '                                                "results/homer/annotate_consensus_peaks/{antibody}.consensus_{peak}-peaks.annotatePeaks.txt",')
    (379, '                                                "results/homer/annotate_consensus_peaks/{antibody}.consensus_{peak}-peaks.boolean.annotatePeaks.txt",')
    (380, '                                                "results/feature_counts/{antibody}.consensus_{peak}-peaks.featureCounts",')
    (381, '                                                "results/feature_counts/{antibody}.consensus_{peak}-peaks.featureCounts.summary",')
    (382, '                                                "results/feature_counts/{antibody}.consensus_{peak}-peaks.featureCounts.jcounts",')
    (383, '                                                "results/deseq2/dss_rld/{antibody}.consensus_{peak}-peaks.dds.rld.RData",')
    (384, '                                                "results/deseq2/plots/{antibody}.consensus_{peak}-peaks.pca_plot.pdf",')
    (385, '                                                "results/deseq2/plots/{antibody}.consensus_{peak}-peaks.heatmap_plot.pdf",')
    (386, '                                                "results/deseq2/pca_vals/{antibody}.consensus_{peak}-peaks.pca.vals.txt",')
    (387, '                                                "results/deseq2/dists/{antibody}.consensus_{peak}-peaks.sample.dists.txt",')
    (388, '                                                "results/deseq2/sizeFactors/{antibody}.consensus_{peak}-peaks.sizeFactors.RData",')
    (389, '                                                "results/deseq2/sizeFactors/{antibody}.consensus_{peak}-peaks.sizeFactors.sizeFactor.txt",')
    (390, '                                                "results/deseq2/results/{antibody}.consensus_{peak}-peaks.deseq2_results.txt",')
    (391, '                                                "results/deseq2/FDR/{antibody}.consensus_{peak}-peaks.deseq2.FDR_0.01.results.txt",')
    (392, '                                                "results/deseq2/FDR/{antibody}.consensus_{peak}-peaks.deseq2.FDR_0.05.results.txt",')
    (393, '                                                "results/deseq2/FDR/{antibody}.consensus_{peak}-peaks.deseq2.FDR_0.01.results.bed",')
    (394, '                                                "results/deseq2/FDR/{antibody}.consensus_{peak}-peaks.deseq2.FDR_0.05.results.bed",')
    (395, '                                                "results/deseq2/plots/FDR/{antibody}.consensus_{peak}-peaks_FDR_0.01_MA_plot.pdf",')
    (396, '                                                "results/deseq2/plots/FDR/{antibody}.consensus_{peak}-peaks_FDR_0.05_MA_plot.pdf",')
    (397, '                                                "results/deseq2/plots/FDR/{antibody}.consensus_{peak}-peaks_FDR_0.01_volcano_plot.pdf",')
    (398, '                                                "results/deseq2/plots/FDR/{antibody}.consensus_{peak}-peaks_FDR_0.05_volcano_plot.pdf",')
    (399, '                                                "results/deseq2/plots/{antibody}.consensus_{peak}-peaks_sample_corr_heatmap.pdf",')
    (400, '                                                "results/deseq2/plots/{antibody}.consensus_{peak}-peaks_scatter_plots.pdf"')
    (401, '                                            ],')
    (402, '                                            peak = config["params"]["peak-analysis"],')
    (403, '                                            antibody = antibody')
    (404, '                                        )')
    (405, '                                    )')
    (406, '            wanted_input.extend(')
    (407, '                expand(')
    (408, '                    [')
    (409, '                        "results/deeptools/{sample}-{control}.plot_fingerprint.pdf",')
    (410, '                        "results/macs2_callpeak/{sample}-{control}.{peak}_treat_pileup.bdg",')
    (411, '                        "results/macs2_callpeak/{sample}-{control}.{peak}_control_lambda.bdg",')
    (412, '                        "results/macs2_callpeak/{sample}-{control}.{peak}_peaks.{peak}Peak",')
    (413, '                        "results/IGV/macs2_callpeak-{peak}/merged_library.{sample}-{control}.{peak}_peaks.igv.txt",')
    (414, '                        "results/macs2_callpeak/plots/plot_{peak}_peaks_count.pdf",')
    (415, '                        "results/macs2_callpeak/plots/plot_{peak}_peaks_frip_score.pdf",')
    (416, '                        "results/macs2_callpeak/plots/plot_{peak}_peaks_macs2.pdf"')
    (417, '                    ],')
    (418, '                    sample = sample,')
    (419, '                    control = samples.loc[sample]["control"],')
    (420, '                    peak = config["params"]["peak-analysis"],')
    (421, '                    antibody = samples.loc[sample]["antibody"]')
    (422, '                )')
    (423, '            )')
    (424, '            if config["params"]["peak-analysis"] == "broad":')
    (425, '                wanted_input.extend(')
    (426, '                    expand(')
    (427, '                        [')
    (428, '                            "results/macs2_callpeak/{sample}-{control}.{peak}_peaks.gappedPeak"')
    (429, '                        ],')
    (430, '                        sample = sample,')
    (431, '                        control = samples.loc[sample]["control"],')
    (432, '                        peak = config["params"]["peak-analysis"]')
    (433, '                    )')
    (434, '                )')
    (435, '            if config["params"]["peak-analysis"] == "narrow":')
    (436, '                wanted_input.extend(')
    (437, '                    expand(')
    (438, '                        [')
    (439, '                            "results/macs2_callpeak/{sample}-{control}.{peak}_summits.bed"')
    (440, '                        ],')
    (441, '                        sample = sample,')
    (442, '                        control = samples.loc[sample]["control"],')
    (443, '                        peak = config["params"]["peak-analysis"]')
    (444, '                    )')
    (445, '                )')
    (446, '    return wanted_input')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kalenkovich/reproducible-MEEG, file=Snakefile
context_key: ["if LooseVersion(matplotlib.__version__) >= \\'2\\'"]
    (807, 'def _set_matplotlib_defaults():')
    (808, '    import matplotlib.pyplot as plt')
    (809, '    fontsize = 8')
    (810, "    params = {\\'axes.labelsize\\': fontsize,")
    (811, "              \\'legend.fontsize\\': fontsize,")
    (812, "              \\'xtick.labelsize\\': fontsize,")
    (813, "              \\'ytick.labelsize\\': fontsize,")
    (814, "              \\'axes.titlesize\\': fontsize + 2,")
    (815, "              \\'figure.max_open_warning\\': 200,")
    (816, "              \\'axes.spines.top\\': False,")
    (817, "              \\'axes.spines.right\\': False,")
    (818, "              \\'axes.grid\\': True,")
    (819, "              \\'lines.linewidth\\': 1,")
    (820, '              }')
    (821, '    import matplotlib')
    (822, "    if LooseVersion(matplotlib.__version__) >= \\'2\\':")
    (823, "        params[\\'font.size\\'] = fontsize")
    (824, '    else:')
    (825, "        params[\\'text.fontsize\\'] = fontsize")
    (826, '    plt.rcParams.update(params)')
    (827, '')
    (828, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kalenkovich/reproducible-MEEG, file=Snakefile
context_key: ['if l_freq == 1']
    (835, 'def plot_erp(evokeds_path, png_path, properties_path):')
    (836, '    l_freq = EPOCHS_L_FREQ')
    (837, '')
    (838, '    evokeds = mne.read_evokeds(evokeds_path)')
    (839, '    idx = evokeds[0].ch_names.index(ERP_EEG_CHANNEL)')
    (840, '    assert evokeds[1].ch_names[idx] == ERP_EEG_CHANNEL')
    (841, '    assert evokeds[2].ch_names[idx] == ERP_EEG_CHANNEL')
    (842, "    mapping = {\\'Famous\\': evokeds[0], \\'Scrambled\\': evokeds[1],")
    (843, "               \\'Unfamiliar\\': evokeds[2]}")
    (844, "    colors =  {\\'Famous\\': \\'blue\\', \\'Scrambled\\': \\'red\\',")
    (845, "               \\'Unfamiliar\\': \\'green\\'}")
    (846, '')
    (847, '    _set_matplotlib_defaults()')
    (848, '')
    (849, '    fig, ax = plt.subplots(1, figsize=(3.3, 2.3))')
    (850, '    scale = 1e6')
    (851, '    times = evokeds[0].times * 1000')
    (852, "    for condition in (\\'Scrambled\\', \\'Unfamiliar\\', \\'Famous\\'):")
    (853, '        ax.plot(times, mapping[condition].data[idx] * scale,')
    (854, '                colors[condition], label=condition)')
    (855, '    ax.grid(True)')
    (856, "    ax.set(xlim=[-100, 1000 * TMAX], xlabel=\\'Time (in ms after stimulus onset)\\',")
    (857, "           ylim=[-12.5, 5], ylabel=u\\'Potential difference (\\xce\\xbcV)\\')")
    (858, "    ax.axvline(800, ls=\\'--\\', color=\\'k\\')")
    (859, '    if l_freq == 1:')
    (860, "        ax.legend(loc=\\'lower right\\')")
    (861, "    ax.annotate(\\'A\\' if l_freq is None else \\'B\\', (-0.2, 1), **ANNOT_KWARGS)")
    (862, '    fig.tight_layout(pad=0.5)')
    (863, '    # plt.show()')
    (864, '')
    (865, '    fig.savefig(png_path)')
    (866, '')
    (867, '    baseline = tuple(np.round(evokeds[0].baseline, 3))')
    (868, '    properties = dict(')
    (869, '        sensor=ERP_EEG_CHANNEL,')
    (870, '        baseline=baseline,')
    (871, "        baseline_units=\\'s\\',")
    (872, '        colors=colors')
    (873, '    )')
    (874, "    with open(properties_path, \\'w\\', encoding=\\'utf-8\\') as f:")
    (875, '        # code from https://stackoverflow.com/a/12309296')
    (876, '        json.dump(properties, f, ensure_ascii=False, indent=4)')
    (877, '')
    (878, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/biomarker, file=workflow/rules/common.smk
context_key: ['if len(flowcells) > 1']
    (44, 'def get_flowcell(units, wildcards):')
    (45, '    flowcells = set([u.flowcell for u in get_units(units, wildcards)])')
    (46, '    if len(flowcells) > 1:')
    (47, '        raise ValueError("Sample type combination from different sequence flowcells")')
    (48, '    return flowcells.pop()')
    (49, '')
    (50, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=watronfire/monterey, file=workflow/rules/support.smk
context_key: ['if not line.startswith( "#" )']
    (6, 'def get_pair_dict( wildcards ):')
    (7, '    global checkpoints')
    (8, '    with checkpoints.generate_pairs.get( **wildcards ).output[0].open() as pair_file:')
    (9, '        PAIRS = dict()')
    (10, '        for line in pair_file:')
    (11, '            if not line.startswith( "#" ):')
    (12, '                pair = line.strip().split( "," )')
    (13, '                short_name = line.strip().replace( ",", "-" ).replace( " ", "" )')
    (14, '                PAIRS[short_name] = pair')
    (15, '    return PAIRS')
    (16, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leoisl/pandora_workflow, file=Snakefile
context_key: ['if technology == "illumina"']
    (21, 'def get_technology_param(technology):')
    (22, '    assert technology in ["illumina", "nanopore"]')
    (23, '    if technology == "illumina":')
    (24, '        return "--illumina"')
    (25, '    else:')
    (26, '        return ""')
    (27, '########################################################################################################################')
    (28, '########################################################################################################################')
    (29, '')
    (30, '')
    (31, '########################################################################################################################')
    (32, '# setup global vars')
    (33, '########################################################################################################################')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefile
context_key: ['columns', 'if no_variant_calling', 'if len(words) == 5']
    (15, 'def read_samples():')
    (16, '    """Function to get names and dna/rna fastq paths from a sample file')
    (17, '    specified in the configuration. Input file is expected to have five')
    (18, '    columns:')
    (19, '    <unique_sample_id> <fastq1_dna_path> <fastq2_dna_path> <fastq1_rna_path> <fastq2_rna_path>')
    (20, '    Or if the vcf_file config option is provided, the input file is')
    (21, '    expected to have these five columns:')
    (22, '    <vcf_sample_id> <unique_sample_name> <dna_bam_path> <fastq1_rna_path> <fastq2_rna_path>')
    (23, '    or (optionally) just these four:')
    (24, '    <vcf_sample_id> <unique_sample_name> <fastq1_rna_path> <fastq2_rna_path>')
    (25, '    Modify this function as needed to provide the correct dictionary."""')
    (26, '    f = open(config[\\\'sample_file\\\'], "r")')
    (27, '    samp_dict = {}')
    (28, '    for line in f:')
    (29, '        words = line.strip().split("\\\\t")')
    (30, '        if no_variant_calling:')
    (31, '            if len(words) == 5:')
    (32, '                samp_dict[words[1]] = (words[2], (words[3], words[4]))')
    (33, '            elif len(words) == 4:')
    (34, "                # then the dna_bam_path hasn\\'t been specified")
    (35, '                samp_dict[words[1]] = ((words[2], words[3]),)')
    (36, '                # sanity check to make sure that rna_only is set to true')
    (37, "                config[\\'rna_only\\'] = True")
    (38, '            else:')
    (39, '                raise ValueError("The samples file is not formatted correctly for this line: "+line)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefile
context_key: ['columns', 'if no_variant_calling', 'else']
    (15, 'def read_samples():')
    (16, '    """Function to get names and dna/rna fastq paths from a sample file')
    (17, '    specified in the configuration. Input file is expected to have five')
    (18, '    columns:')
    (19, '    <unique_sample_id> <fastq1_dna_path> <fastq2_dna_path> <fastq1_rna_path> <fastq2_rna_path>')
    (20, '    Or if the vcf_file config option is provided, the input file is')
    (21, '    expected to have these five columns:')
    (22, '    <vcf_sample_id> <unique_sample_name> <dna_bam_path> <fastq1_rna_path> <fastq2_rna_path>')
    (23, '    or (optionally) just these four:')
    (24, '    <vcf_sample_id> <unique_sample_name> <fastq1_rna_path> <fastq2_rna_path>')
    (25, '    Modify this function as needed to provide the correct dictionary."""')
    (26, '    f = open(config[\\\'sample_file\\\'], "r")')
    (27, '    samp_dict = {}')
    (28, '    for line in f:')
    (29, '        words = line.strip().split("\\\\t")')
    (30, '        if no_variant_calling:')
    (31, '            if len(words) == 5:')
    (32, '                samp_dict[words[1]] = (words[2], (words[3], words[4]))')
    (33, '            elif len(words) == 4:')
    (34, "                # then the dna_bam_path hasn\\'t been specified")
    (35, '                samp_dict[words[1]] = ((words[2], words[3]),)')
    (36, '                # sanity check to make sure that rna_only is set to true')
    (37, "                config[\\'rna_only\\'] = True")
    (38, '            else:')
    (39, '                raise ValueError("The samples file is not formatted correctly for this line: "+line)')
    (40, '        else:')
    (41, '            samp_dict[words[0]] = ((words[1], words[2]), (words[3], words[4]))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefile
context_key: ["if not check_config(\\'SAMP_NAMES\\')"]
    (15, 'def read_samples():')
    (16, '    """Function to get names and dna/rna fastq paths from a sample file')
    (17, '    specified in the configuration. Input file is expected to have five')
    (18, '    columns:')
    (19, '    <unique_sample_id> <fastq1_dna_path> <fastq2_dna_path> <fastq1_rna_path> <fastq2_rna_path>')
    (20, '    Or if the vcf_file config option is provided, the input file is')
    (21, '    expected to have these five columns:')
    (22, '    <vcf_sample_id> <unique_sample_name> <dna_bam_path> <fastq1_rna_path> <fastq2_rna_path>')
    (23, '    or (optionally) just these four:')
    (24, '    <vcf_sample_id> <unique_sample_name> <fastq1_rna_path> <fastq2_rna_path>')
    (25, '    Modify this function as needed to provide the correct dictionary."""')
    (26, '    f = open(config[\\\'sample_file\\\'], "r")')
    (27, '    samp_dict = {}')
    (28, '    for line in f:')
    (29, '        words = line.strip().split("\\\\t")')
    (30, '        if no_variant_calling:')
    (31, '            if len(words) == 5:')
    (32, '                samp_dict[words[1]] = (words[2], (words[3], words[4]))')
    (33, '            elif len(words) == 4:')
    (34, "                # then the dna_bam_path hasn\\'t been specified")
    (35, '                samp_dict[words[1]] = ((words[2], words[3]),)')
    (36, '                # sanity check to make sure that rna_only is set to true')
    (37, "                config[\\'rna_only\\'] = True")
    (38, '            else:')
    (39, '                raise ValueError("The samples file is not formatted correctly for this line: "+line)')
    (40, '        else:')
    (41, '            samp_dict[words[0]] = ((words[1], words[2]), (words[3], words[4]))')
    (42, '    return samp_dict')
    (43, 'GLOBAL_SAMP = read_samples()')
    (44, '')
    (45, "# the user can define config[\\'SAMP_NAMES\\'] to contain whichever sample names")
    (46, "# they\\'d like to run the pipeline on")
    (47, "if not check_config(\\'SAMP_NAMES\\'):")
    (48, "    config[\\'SAMP_NAMES\\'] = list(GLOBAL_SAMP.keys())")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefile
context_key: ['else', "if len(config[\\'SAMP_NAMES\\']) != user_samps_len"]
    (15, 'def read_samples():')
    (16, '    """Function to get names and dna/rna fastq paths from a sample file')
    (17, '    specified in the configuration. Input file is expected to have five')
    (18, '    columns:')
    (19, '    <unique_sample_id> <fastq1_dna_path> <fastq2_dna_path> <fastq1_rna_path> <fastq2_rna_path>')
    (20, '    Or if the vcf_file config option is provided, the input file is')
    (21, '    expected to have these five columns:')
    (22, '    <vcf_sample_id> <unique_sample_name> <dna_bam_path> <fastq1_rna_path> <fastq2_rna_path>')
    (23, '    or (optionally) just these four:')
    (24, '    <vcf_sample_id> <unique_sample_name> <fastq1_rna_path> <fastq2_rna_path>')
    (25, '    Modify this function as needed to provide the correct dictionary."""')
    (26, '    f = open(config[\\\'sample_file\\\'], "r")')
    (27, '    samp_dict = {}')
    (28, '    for line in f:')
    (29, '        words = line.strip().split("\\\\t")')
    (30, '        if no_variant_calling:')
    (31, '            if len(words) == 5:')
    (32, '                samp_dict[words[1]] = (words[2], (words[3], words[4]))')
    (33, '            elif len(words) == 4:')
    (34, "                # then the dna_bam_path hasn\\'t been specified")
    (35, '                samp_dict[words[1]] = ((words[2], words[3]),)')
    (36, '                # sanity check to make sure that rna_only is set to true')
    (37, "                config[\\'rna_only\\'] = True")
    (38, '            else:')
    (39, '                raise ValueError("The samples file is not formatted correctly for this line: "+line)')
    (40, '        else:')
    (41, '            samp_dict[words[0]] = ((words[1], words[2]), (words[3], words[4]))')
    (42, '    return samp_dict')
    (43, 'GLOBAL_SAMP = read_samples()')
    (44, '')
    (45, "# the user can define config[\\'SAMP_NAMES\\'] to contain whichever sample names")
    (46, "# they\\'d like to run the pipeline on")
    (47, "if not check_config(\\'SAMP_NAMES\\'):")
    (48, "    config[\\'SAMP_NAMES\\'] = list(GLOBAL_SAMP.keys())")
    (49, 'else:')
    (50, "    # double check that the user isn\\'t asking for samples they haven\\'t provided")
    (51, "    user_samps_len = len(config[\\'SAMP_NAMES\\'])")
    (52, "    config[\\'SAMP_NAMES\\'] = list(set(GLOBAL_SAMP.keys()).intersection(config[\\'SAMP_NAMES\\']))")
    (53, "    if len(config[\\'SAMP_NAMES\\']) != user_samps_len:")
    (54, '        warnings.warn("Not all of the samples requested have provided input. Proceeding with as many samples as is possible...")')
    (55, '')
    (56, '')
    (57, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefile
context_key: ['if no_variant_calling', 'if len(GLOBAL_SAMP[samp]) == 2']
    (77, '    def read_vcf_samples():')
    (78, '        f = open(config[\\\'sample_file\\\'], "r")')
    (79, '        samp_dict = {}')
    (80, '        for line in f:')
    (81, '            words = line.strip().split("\\\\t")')
    (82, "            if words[1] in config[\\'SAMP_NAMES\\']:")
    (83, '                samp_dict[words[1]] = words[0]')
    (84, '        return samp_dict')
    (85, '    SAMP_TO_VCF_ID = read_vcf_samples()')
    (86, 'else:')
    (87, "    SAMP2 = {samp: GLOBAL_SAMP[samp][1] for samp in config[\\'SAMP_NAMES\\']}")
    (88, '    SAMP_TO_VCF_ID = {samp: samp for samp, fastqs in GLOBAL_SAMP.items()}')
    (89, 'include: "Snakefiles/Snakefile-WASP"')
    (90, '')
    (91, '# counts analysis pipeline')
    (92, 'SAMP3 = {}')
    (93, "for samp in config[\\'SAMP_NAMES\\']:")
    (94, '    if no_variant_calling:')
    (95, '        if len(GLOBAL_SAMP[samp]) == 2:')
    (96, '            dna = GLOBAL_SAMP[samp][0]')
    (97, '        else:')
    (98, "            config[\\'rna_only\\'] = True")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefile
context_key: ['if no_variant_calling', 'else']
    (77, '    def read_vcf_samples():')
    (78, '        f = open(config[\\\'sample_file\\\'], "r")')
    (79, '        samp_dict = {}')
    (80, '        for line in f:')
    (81, '            words = line.strip().split("\\\\t")')
    (82, "            if words[1] in config[\\'SAMP_NAMES\\']:")
    (83, '                samp_dict[words[1]] = words[0]')
    (84, '        return samp_dict')
    (85, '    SAMP_TO_VCF_ID = read_vcf_samples()')
    (86, 'else:')
    (87, "    SAMP2 = {samp: GLOBAL_SAMP[samp][1] for samp in config[\\'SAMP_NAMES\\']}")
    (88, '    SAMP_TO_VCF_ID = {samp: samp for samp, fastqs in GLOBAL_SAMP.items()}')
    (89, 'include: "Snakefiles/Snakefile-WASP"')
    (90, '')
    (91, '# counts analysis pipeline')
    (92, 'SAMP3 = {}')
    (93, "for samp in config[\\'SAMP_NAMES\\']:")
    (94, '    if no_variant_calling:')
    (95, '        if len(GLOBAL_SAMP[samp]) == 2:')
    (96, '            dna = GLOBAL_SAMP[samp][0]')
    (97, '        else:')
    (98, "            config[\\'rna_only\\'] = True")
    (99, '    else:')
    (100, '        dna = rules.rm_dups.output.final_bam.format(sample=samp)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefile
context_key: ['if no_variant_calling', "if config[\\'rna_only\\']"]
    (77, '    def read_vcf_samples():')
    (78, '        f = open(config[\\\'sample_file\\\'], "r")')
    (79, '        samp_dict = {}')
    (80, '        for line in f:')
    (81, '            words = line.strip().split("\\\\t")')
    (82, "            if words[1] in config[\\'SAMP_NAMES\\']:")
    (83, '                samp_dict[words[1]] = words[0]')
    (84, '        return samp_dict')
    (85, '    SAMP_TO_VCF_ID = read_vcf_samples()')
    (86, 'else:')
    (87, "    SAMP2 = {samp: GLOBAL_SAMP[samp][1] for samp in config[\\'SAMP_NAMES\\']}")
    (88, '    SAMP_TO_VCF_ID = {samp: samp for samp, fastqs in GLOBAL_SAMP.items()}')
    (89, 'include: "Snakefiles/Snakefile-WASP"')
    (90, '')
    (91, '# counts analysis pipeline')
    (92, 'SAMP3 = {}')
    (93, "for samp in config[\\'SAMP_NAMES\\']:")
    (94, '    if no_variant_calling:')
    (95, '        if len(GLOBAL_SAMP[samp]) == 2:')
    (96, '            dna = GLOBAL_SAMP[samp][0]')
    (97, '        else:')
    (98, "            config[\\'rna_only\\'] = True")
    (99, '    else:')
    (100, '        dna = rules.rm_dups.output.final_bam.format(sample=samp)')
    (101, '    rna = rules.rmdup.output.sort.format(sample=samp)')
    (102, "    if config[\\'rna_only\\']:")
    (103, '        SAMP3[samp] = (rna,)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefile
context_key: ['if no_variant_calling', 'else']
    (77, '    def read_vcf_samples():')
    (78, '        f = open(config[\\\'sample_file\\\'], "r")')
    (79, '        samp_dict = {}')
    (80, '        for line in f:')
    (81, '            words = line.strip().split("\\\\t")')
    (82, "            if words[1] in config[\\'SAMP_NAMES\\']:")
    (83, '                samp_dict[words[1]] = words[0]')
    (84, '        return samp_dict')
    (85, '    SAMP_TO_VCF_ID = read_vcf_samples()')
    (86, 'else:')
    (87, "    SAMP2 = {samp: GLOBAL_SAMP[samp][1] for samp in config[\\'SAMP_NAMES\\']}")
    (88, '    SAMP_TO_VCF_ID = {samp: samp for samp, fastqs in GLOBAL_SAMP.items()}')
    (89, 'include: "Snakefiles/Snakefile-WASP"')
    (90, '')
    (91, '# counts analysis pipeline')
    (92, 'SAMP3 = {}')
    (93, "for samp in config[\\'SAMP_NAMES\\']:")
    (94, '    if no_variant_calling:')
    (95, '        if len(GLOBAL_SAMP[samp]) == 2:')
    (96, '            dna = GLOBAL_SAMP[samp][0]')
    (97, '        else:')
    (98, "            config[\\'rna_only\\'] = True")
    (99, '    else:')
    (100, '        dna = rules.rm_dups.output.final_bam.format(sample=samp)')
    (101, '    rna = rules.rmdup.output.sort.format(sample=samp)')
    (102, "    if config[\\'rna_only\\']:")
    (103, '        SAMP3[samp] = (rna,)')
    (104, '    else:')
    (105, '        SAMP3[samp] = (dna, rna)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/Snakefile-counts
context_key: ["if \\'SAMP3\\' not in globals()"]
    (11, '    def read_samples():')
    (12, '        """Function to get names and fastq paths from a sample file specified')
    (13, '        in the configuration. Input file is expected to have 4 columns:')
    (14, '        <vcf_sample_id> <unique_sample_id> <dna_bam_path> <rna_bam_path>. If')
    (15, '        <dna_bam_path> is left out, the pipeline will default to an rna-only')
    (16, "        analysis, which doesn\\'t require dna reads but is more conservative.")
    (17, '        Modify this function as needed to provide a dictionary of sample_id')
    (18, '        keys and (fastq1, fastq1) values."""')
    (19, '        f = open(config[\\\'sample_file\\\'], "r")')
    (20, '        samp_dict = {}')
    (21, '        for line in f:')
    (22, '            words = line.strip().split("\\\\t")')
    (23, "            if config[\\'rna_only\\'] and len(words) == 4:")
    (24, '                samp_dict[words[1]] = (words[3],)')
    (25, '            elif len(words) == 3:')
    (26, '                samp_dict[words[1]] = (words[2],)')
    (27, '                # sanity check to make sure that rna_only is set to true')
    (28, "                config[\\'rna_only\\'] = True")
    (29, '            else:')
    (30, '                samp_dict[words[1]] = (words[2], words[3])')
    (31, '        return samp_dict')
    (32, '    SAMP3 = read_samples()')
    (33, '')
    (34, "# the user can define config[\\'SAMP_NAMES\\'] to contain whichever sample names")
    (35, "# they\\'d like to run the pipeline on")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/Snakefile-counts
context_key: ["if \\'SAMP_NAMES\\' not in config"]
    (11, '    def read_samples():')
    (12, '        """Function to get names and fastq paths from a sample file specified')
    (13, '        in the configuration. Input file is expected to have 4 columns:')
    (14, '        <vcf_sample_id> <unique_sample_id> <dna_bam_path> <rna_bam_path>. If')
    (15, '        <dna_bam_path> is left out, the pipeline will default to an rna-only')
    (16, "        analysis, which doesn\\'t require dna reads but is more conservative.")
    (17, '        Modify this function as needed to provide a dictionary of sample_id')
    (18, '        keys and (fastq1, fastq1) values."""')
    (19, '        f = open(config[\\\'sample_file\\\'], "r")')
    (20, '        samp_dict = {}')
    (21, '        for line in f:')
    (22, '            words = line.strip().split("\\\\t")')
    (23, "            if config[\\'rna_only\\'] and len(words) == 4:")
    (24, '                samp_dict[words[1]] = (words[3],)')
    (25, '            elif len(words) == 3:')
    (26, '                samp_dict[words[1]] = (words[2],)')
    (27, '                # sanity check to make sure that rna_only is set to true')
    (28, "                config[\\'rna_only\\'] = True")
    (29, '            else:')
    (30, '                samp_dict[words[1]] = (words[2], words[3])')
    (31, '        return samp_dict')
    (32, '    SAMP3 = read_samples()')
    (33, '')
    (34, "# the user can define config[\\'SAMP_NAMES\\'] to contain whichever sample names")
    (35, "# they\\'d like to run the pipeline on")
    (36, "if \\'SAMP_NAMES\\' not in config:")
    (37, "    config[\\'SAMP_NAMES\\'] = list(SAMP3.keys())")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/Snakefile-counts
context_key: ['else', "if len(config[\\'SAMP_NAMES\\']) != user_samps_len"]
    (11, '    def read_samples():')
    (12, '        """Function to get names and fastq paths from a sample file specified')
    (13, '        in the configuration. Input file is expected to have 4 columns:')
    (14, '        <vcf_sample_id> <unique_sample_id> <dna_bam_path> <rna_bam_path>. If')
    (15, '        <dna_bam_path> is left out, the pipeline will default to an rna-only')
    (16, "        analysis, which doesn\\'t require dna reads but is more conservative.")
    (17, '        Modify this function as needed to provide a dictionary of sample_id')
    (18, '        keys and (fastq1, fastq1) values."""')
    (19, '        f = open(config[\\\'sample_file\\\'], "r")')
    (20, '        samp_dict = {}')
    (21, '        for line in f:')
    (22, '            words = line.strip().split("\\\\t")')
    (23, "            if config[\\'rna_only\\'] and len(words) == 4:")
    (24, '                samp_dict[words[1]] = (words[3],)')
    (25, '            elif len(words) == 3:')
    (26, '                samp_dict[words[1]] = (words[2],)')
    (27, '                # sanity check to make sure that rna_only is set to true')
    (28, "                config[\\'rna_only\\'] = True")
    (29, '            else:')
    (30, '                samp_dict[words[1]] = (words[2], words[3])')
    (31, '        return samp_dict')
    (32, '    SAMP3 = read_samples()')
    (33, '')
    (34, "# the user can define config[\\'SAMP_NAMES\\'] to contain whichever sample names")
    (35, "# they\\'d like to run the pipeline on")
    (36, "if \\'SAMP_NAMES\\' not in config:")
    (37, "    config[\\'SAMP_NAMES\\'] = list(SAMP3.keys())")
    (38, 'else:')
    (39, "    # double check that the user isn\\'t asking for samples they haven\\'t provided")
    (40, "    user_samps_len = len(config[\\'SAMP_NAMES\\'])")
    (41, "    config[\\'SAMP_NAMES\\'] = list(set(SAMP3.keys()).intersection(config[\\'SAMP_NAMES\\']))")
    (42, "    if len(config[\\'SAMP_NAMES\\']) != user_samps_len:")
    (43, '        warnings.warn("Not all of the samples requested have provided input. Proceeding with as many samples as is possible...")')
    (44, '')
    (45, '# check existence of SAMP_TO_VCF_ID. it may have already been defined if this')
    (46, '# Snakefile is being included from somewhere else')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/Snakefile-counts
context_key: ["if \\'SAMP_TO_VCF_ID\\' not in globals()", "if words[1] in config[\\'SAMP_NAMES\\']"]
    (11, '    def read_samples():')
    (12, '        """Function to get names and fastq paths from a sample file specified')
    (13, '        in the configuration. Input file is expected to have 4 columns:')
    (14, '        <vcf_sample_id> <unique_sample_id> <dna_bam_path> <rna_bam_path>. If')
    (15, '        <dna_bam_path> is left out, the pipeline will default to an rna-only')
    (16, "        analysis, which doesn\\'t require dna reads but is more conservative.")
    (17, '        Modify this function as needed to provide a dictionary of sample_id')
    (18, '        keys and (fastq1, fastq1) values."""')
    (19, '        f = open(config[\\\'sample_file\\\'], "r")')
    (20, '        samp_dict = {}')
    (21, '        for line in f:')
    (22, '            words = line.strip().split("\\\\t")')
    (23, "            if config[\\'rna_only\\'] and len(words) == 4:")
    (24, '                samp_dict[words[1]] = (words[3],)')
    (25, '            elif len(words) == 3:')
    (26, '                samp_dict[words[1]] = (words[2],)')
    (27, '                # sanity check to make sure that rna_only is set to true')
    (28, "                config[\\'rna_only\\'] = True")
    (29, '            else:')
    (30, '                samp_dict[words[1]] = (words[2], words[3])')
    (31, '        return samp_dict')
    (32, '    SAMP3 = read_samples()')
    (33, '')
    (34, "# the user can define config[\\'SAMP_NAMES\\'] to contain whichever sample names")
    (35, "# they\\'d like to run the pipeline on")
    (36, "if \\'SAMP_NAMES\\' not in config:")
    (37, "    config[\\'SAMP_NAMES\\'] = list(SAMP3.keys())")
    (38, 'else:')
    (39, "    # double check that the user isn\\'t asking for samples they haven\\'t provided")
    (40, "    user_samps_len = len(config[\\'SAMP_NAMES\\'])")
    (41, "    config[\\'SAMP_NAMES\\'] = list(set(SAMP3.keys()).intersection(config[\\'SAMP_NAMES\\']))")
    (42, "    if len(config[\\'SAMP_NAMES\\']) != user_samps_len:")
    (43, '        warnings.warn("Not all of the samples requested have provided input. Proceeding with as many samples as is possible...")')
    (44, '')
    (45, '# check existence of SAMP_TO_VCF_ID. it may have already been defined if this')
    (46, '# Snakefile is being included from somewhere else')
    (47, "if \\'SAMP_TO_VCF_ID\\' not in globals():")
    (48, '    def read_vcf_samples():')
    (49, '        f = open(config[\\\'sample_file\\\'], "r")')
    (50, '        samp_dict = {}')
    (51, '        for line in f:')
    (52, '            words = line.strip().split("\\\\t")')
    (53, "            if words[1] in config[\\'SAMP_NAMES\\']:")
    (54, '                samp_dict[words[1]] = words[0]')
    (55, '        return samp_dict')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/Snakefile-counts
context_key: ["if \\'SAMP_TO_VCF_ID\\' not in globals()"]
    (48, '    def read_vcf_samples():')
    (49, '        f = open(config[\\\'sample_file\\\'], "r")')
    (50, '        samp_dict = {}')
    (51, '        for line in f:')
    (52, '            words = line.strip().split("\\\\t")')
    (53, "            if words[1] in config[\\'SAMP_NAMES\\']:")
    (54, '                samp_dict[words[1]] = words[0]')
    (55, '        return samp_dict')
    (56, '    SAMP_TO_VCF_ID = read_vcf_samples()')
    (57, '')
    (58, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dpmerrell/tcga-pipeline, file=Snakefile
context_key: ['if wc["ctype"] in config["barcode_needs_backup"][wc["omic_type"]]']
    (128, 'def get_barcode_backup_file(wc):')
    (129, '    if wc["ctype"] in config["barcode_needs_backup"][wc["omic_type"]]:')
    (130, '        return OMIC_CSVS[wc["ctype"]][config["barcode_files"][wc["omic_type"]][1]]')
    (131, '    else:')
    (132, '        return [] ')
    (133, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skurscheid/mcf10_promoter_profiling, file=scripts/helper.py
context_key: ["if chip_antibody == \\'none\\'"]
    (6, 'def make_targets_from_runTable(runTable, library_type):\\r')
    (7, '    t = []\\r')
    (8, '    for index, row in runTable.iterrows():\\r')
    (9, "        chip_antibody = row[\\'chip_antibody\\'].split()[0]\\r")
    (10, "        if chip_antibody == \\'none\\':\\r")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akhanf/clinical-atlasreg, file=workflow/rules/registration.smk
context_key: ["if wildcards.subject in config['subject_t1w_custom']"]
    (3, 'def get_t1w_filename(wildcards): ')
    (4, "    if wildcards.subject in config['subject_t1w_custom']:")
    (5, "        return config['subject_t1w_custom'][wildcards.subject]")
    (6, '    else:')
    (7, "        return config['subject_t1w']")
    (8, '')
    (9, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=thomasbtf/document-anonymization, file=workflow/rules/common.smk
context_key: ['if "additional_metadata" in pep.sample_table.columns', 'if type(pep.sample_table.loc[wildcards.id]["additional_metadata"]) == str']
    (33, 'def get_additional_metadata(wildcards):')
    (34, '    if "additional_metadata" in pep.sample_table.columns:')
    (35, '        if type(pep.sample_table.loc[wildcards.id]["additional_metadata"]) == str:')
    (36, '            return pep.sample_table.loc[wildcards.id][["additional_metadata"]]')
    (37, '        else:')
    (38, '            return []')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=thomasbtf/document-anonymization, file=workflow/rules/common.smk
context_key: ['if "additional_metadata" in pep.sample_table.columns', 'else']
    (33, 'def get_additional_metadata(wildcards):')
    (34, '    if "additional_metadata" in pep.sample_table.columns:')
    (35, '        if type(pep.sample_table.loc[wildcards.id]["additional_metadata"]) == str:')
    (36, '            return pep.sample_table.loc[wildcards.id][["additional_metadata"]]')
    (37, '        else:')
    (38, '            return []')
    (39, '    else:')
    (40, '        return []')
    (41, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=thomasbtf/document-anonymization, file=workflow/rules/common.smk
context_key: ['if ".snakemake" not in path and ".DS_Store" not in pat']
    (45, 'def get_image_paths_for_id(wildcards):')
    (46, '    with checkpoints.fix_file_ext.get(id=wildcards.id).output[0].open() as f:')
    (47, '        paths = pd.read_csv(f, sep="\\')
    (48, '", header=None, squeeze=True)')
    (49, '')
    (50, '    # TODO')
    (51, '    # This is ugly. We should rewrite the file handling.')
    (52, '    paths = [')
    (53, '        path.removeprefix(')
    (54, '            "results/{id}/tmp/uncompressed-lz4-docs/".format(id=wildcards.id)')
    (55, '        )')
    (56, '        for path in paths')
    (57, '        if ".snakemake" not in path and ".DS_Store" not in path')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=thomasbtf/document-anonymization, file=workflow/rules/common.smk
context_key: ['if case == "no_redaction"']
    (89, 'def get_questionable_imgs(wildcards, case):')
    (90, '    if case == "no_redaction":')
    (91, '        pattern = "results/{id}/to-check/no_redaction/{{img}}".format(id=wildcards.id)')
    (92, '        with checkpoints.create_paths_for_manually_checking.get(')
    (93, '            id=wildcards.id')
    (94, '        ).output.no_redaction.open() as f:')
    (95, '            paths = load_tsv(f)')
    (96, '')
    (97, '    elif case == "high_degree_of_redaction":')
    (98, '        pattern = "results/{id}/to-check/high_degree_of_redaction/{{img}}".format(')
    (99, '            id=wildcards.id')
    (100, '        )')
    (101, '        with checkpoints.create_paths_for_manually_checking.get(')
    (102, '            id=wildcards.id')
    (103, '        ).output.high_degree_of_redaction.open() as f:')
    (104, '            paths = load_tsv(f)')
    (105, '')
    (106, '    elif case == "partly_found_address":')
    (107, '        pattern = "results/{id}/to-check/partly_found_address/{{img}}".format(')
    (108, '            id=wildcards.id')
    (109, '        )')
    (110, '        with checkpoints.create_paths_for_manually_checking.get(')
    (111, '            id=wildcards.id')
    (112, '        ).output.partly_found_address.open() as f:')
    (113, '            paths = load_tsv(f)')
    (114, '')
    (115, '    elif case == "partly_found_name":')
    (116, '        pattern = "results/{id}/to-check/partly_found_name/{{img}}".format(')
    (117, '            id=wildcards.id')
    (118, '        )')
    (119, '        with checkpoints.create_paths_for_manually_checking.get(')
    (120, '            id=wildcards.id')
    (121, '        ).output.partly_found_name.open() as f:')
    (122, '            paths = load_tsv(f)')
    (123, '    elif case == "all":')
    (124, '        with checkpoints.create_paths_for_manually_checking.get(')
    (125, '            id=wildcards.id')
    (126, '        ).output.no_redaction.open() as f:')
    (127, '            no_redaction = load_tsv(f)')
    (128, '        with checkpoints.create_paths_for_manually_checking.get(')
    (129, '            id=wildcards.id')
    (130, '        ).output.high_degree_of_redaction.open() as f:')
    (131, '            high_degree_of_redaction = load_tsv(f)')
    (132, '        with checkpoints.create_paths_for_manually_checking.get(')
    (133, '            id=wildcards.id')
    (134, '        ).output.partly_found_address.open() as f:')
    (135, '            partly_found_address = load_tsv(f)')
    (136, '        with checkpoints.create_paths_for_manually_checking.get(')
    (137, '            id=wildcards.id')
    (138, '        ).output.partly_found_name.open() as f:')
    (139, '            partly_found_name = load_tsv(f)')
    (140, '')
    (141, '        all_paths = pd.Series("results/{id}/tmp/filler".format(id=wildcards.id))')
    (142, '        all_paths = (')
    (143, '            all_paths.append(no_redaction, ignore_index=True)')
    (144, '            .append(high_degree_of_redaction, ignore_index=True)')
    (145, '            .append(partly_found_address, ignore_index=True)')
    (146, '            .append(partly_found_name, ignore_index=True)')
    (147, '        )')
    (148, '        all_paths = all_paths.unique()')
    (149, '')
    (150, '        return all_paths')
    (151, '')
    (152, '    paths = [')
    (153, '        path.replace("results/{id}/processed-docs/".format(id=wildcards.id), "")')
    (154, '        for path in paths')
    (155, '    ]')
    (156, '')
    (157, '    return expand(pattern, img=paths)')
    (158, '')
    (159, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=thomasbtf/document-anonymization, file=workflow/rules/common.smk
context_key: ['if docs.endswith(".docs")']
    (170, 'def get_uncompressed_docs_dir(wildcards):')
    (171, '    docs = get_compressed_docs(wildcards).values[0]')
    (172, '    if docs.endswith(".docs"):')
    (173, '        return ("results/{id}/tmp/uncompressed-zip-docs",)')
    (174, '    elif docs.endswith(".tar.lz4"):')
    (175, '        return ("results/{id}/tmp/uncompressed-lz4-docs",)')
    (176, '')
    (177, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=thomasbtf/document-anonymization, file=workflow/rules/common.smk
context_key: ['if filename.endswith(".zip"']
    (178, 'def get_zip_files_in_dir(wildcards):')
    (179, '    docs = get_compressed_docs(wildcards).values[0]')
    (180, '    filenames = os.listdir(docs)')
    (181, '    return [')
    (182, '        filename.removesuffix(".zip")')
    (183, '        for filename in filenames')
    (184, '        if filename.endswith(".zip")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=rimjhimroy/snakeGATK4, file=Snakefile
context_key: ['if trim']
    (38, 'def get_fastq2(wildcards):')
    (39, '    fastqs = samples.loc[(wildcards.prefix), ["fq1", "fq2"]].dropna()')
    (40, '    if trim:')
    (41, '        return {"r1": os.path.join("data/trimmed",wildcards.prefix + "_R1_001.trim.fastq.gz"), "r2": os.path.join("data/trimmed",wildcards.prefix + "_R2_001.trim.fastq.gz")}')
    (42, '    else:')
    (43, '        return {"r1": fastqs.fq1, "r2": fastqs.fq2}')
    (44, '')
    (45, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JihedC/Snakemake_ChIP_seq_pipeline, file=Snakefile
context_key: ['if "fq2" not in samples.columns']
    (30, 'def sample_is_single_end(sample):')
    (31, '    """This function detect missing value in the column 2 of the units.tsv"""')
    (32, '    if "fq2" not in samples.columns:')
    (33, '        return True')
    (34, '    else:')
    (35, '        return pd.isnull(samples.loc[(sample), "fq2"])')
    (36, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JihedC/Snakemake_ChIP_seq_pipeline, file=Snakefile
context_key: ['if sample_is_single_end(wildcards.sample)']
    (37, 'def get_fastq(wildcards):')
    (38, '    """This function checks if the sample has paired end or single end reads and returns 1 or 2 names of the fastq files"""')
    (39, '    if sample_is_single_end(wildcards.sample):')
    (40, '        return samples.loc[(wildcards.sample), ["fq1"]].dropna()')
    (41, '    else:')
    (42, '        return samples.loc[(wildcards.sample), ["fq1", "fq2"]].dropna()')
    (43, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JihedC/Snakemake_ChIP_seq_pipeline, file=Snakefile
context_key: ['if sample_is_single_end(wildcards.sample)']
    (44, 'def get_trim_names(wildcards):')
    (45, '    """')
    (46, '    This function:')
    (47, '      1. Checks if the sample is paired end or single end')
    (48, '      2. Returns the correct input and output trimmed file names. ')
    (49, '    """')
    (50, '    if sample_is_single_end(wildcards.sample):')
    (51, '        inFile = samples.loc[(wildcards.sample), ["fq1"]].dropna()')
    (52, '        return "--in1 " + inFile[0] + " --out1 " + WORKING_DIR + "trimmed/" + wildcards.sample + "_R1_trimmed.fq.gz" ')
    (53, '    else:')
    (54, '        inFile = samples.loc[(wildcards.sample), ["fq1", "fq2"]].dropna()')
    (55, '        return "--in1 " + inFile[0] + " --in2 " + inFile[1] + " --out1 " + WORKING_DIR + "trimmed/" + wildcards.sample + "_R1_trimmed.fq.gz --out2 "  + WORKING_DIR + "trimmed/" + wildcards.sample + "_R2_trimmed.fq.gz"')
    (56, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JihedC/Snakemake_ChIP_seq_pipeline, file=Snakefile
context_key: ['if sample_is_single_end(wildcards.sample)']
    (57, 'def get_star_names(wildcards):')
    (58, '    """')
    (59, '    This function:')
    (60, '      1. Checks if the sample is paired end or single end.')
    (61, '      2. Returns the correct input file names for STAR mapping step.')
    (62, '    """')
    (63, '    if sample_is_single_end(wildcards.sample):')
    (64, '        return WORKING_DIR + "trimmed/" + wildcards.sample + "_R1_trimmed.fq.gz"     ')
    (65, '    else:')
    (66, '        return WORKING_DIR + "trimmed/" + wildcards.sample + "_R1_trimmed.fq.gz " + WORKING_DIR + "trimmed/" + wildcards.sample + "_R2_trimmed.fq.gz"')
    (67, '')
    (68, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akirosingh/BC1bulk-rnaseq, file=Snakefile
context_key: ['if isPE(wildcards)']
    (36, 'def get_fqs(wildcards):')
    (37, '    if isPE(wildcards):')
    (38, '        return {')
    (39, "            \\'fq1\\' : samples.loc[(wildcards.sample_id), \\'fq1\\'],")
    (40, "            \\'fq2\\' : samples.loc[(wildcards.sample_id), \\'fq2\\']")
    (41, '            }')
    (42, '    else:')
    (43, "        return {\\'fq\\' : samples.loc[(wildcards.sample_id), \\'fq1\\']}")
    (44, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akirosingh/BC1bulk-rnaseq, file=Snakefile
context_key: ["if quant_program == \\'salmon\\'"]
    (45, 'def getStrand(wildcards):')
    (46, "    if quant_program == \\'salmon\\':")
    (47, "        return \\'Not using kallisto\\'")
    (48, '    else:')
    (49, "        if \\'strandedness\\' not in samples.columns:")
    (50, '            raise ValueError("Set to use kallisto for quantification but not stranding specified!")')
    (51, "    s = samples.loc[wildcards.sample_id, \\'strandedness\\'].lower()")
    (52, "    if s == \\'forward\\' or s == \\'stranded\\':")
    (53, "        return \\'--fr-stranded\\'")
    (54, "    elif s == \\'reverse\\':")
    (55, "        return \\'--rf-stranded\\'")
    (56, "    elif s == \\'none\\' or s == \\'unstranded\\':")
    (57, "        return \\'\\'")
    (58, '    else:')
    (59, '        raise ValueError(f"Unrecognized strand type for {wildcards.sample_id}")')
    (60, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akirosingh/BC1bulk-rnaseq, file=Snakefile
context_key: ["if pd.isna(samples.loc[wildcards.sample_id, \\'fq2\\'])"]
    (61, 'def isPE(wildcards):')
    (62, "    if pd.isna(samples.loc[wildcards.sample_id, \\'fq2\\']):")
    (63, '        return False')
    (64, '    else:')
    (65, '        return True')
    (66, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akirosingh/BC1bulk-rnaseq, file=Snakefile
context_key: ["if quant_program == \\'kallisto\\'"]
    (67, 'def get_quants(wildcards):')
    (68, "    if quant_program == \\'kallisto\\':")
    (69, '        quants = [f"kallisto/{sid}" for sid in samples[\\\'id\\\']]')
    (70, "    elif quant_program == \\'salmon\\':")
    (71, '        quants = [f"salmon/{sid}" for sid in samples[\\\'id\\\']]')
    (72, '    else:')
    (73, '        raise ValueError("quant_program must be either \\\'salmon\\\' or \\\'kallisto\\\'")')
    (74, "    return {\\'cts\\' : quants}")
    (75, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akirosingh/BC1bulk-rnaseq, file=Snakefile
context_key: ["if quant_program == \\'kallisto\\'"]
    (79, 'def get_multiqc_input(wildcards):')
    (80, "    if quant_program == \\'kallisto\\':")
    (81, '        logs = [f"logs/kallisto/{sample_id}.log" for sample_id in samples[\\\'id\\\']]')
    (82, '    else:')
    (83, '        logs = [f"salmon/{sample_id}" for sample_id in samples[\\\'id\\\']]')
    (84, '    fastqc = [f"qc/fastqc/{sample_id}" for sample_id in samples[\\\'id\\\']]')
    (85, '    return logs + fastqc')
    (86, '')
    (87, '')
    (88, '###########################################################')
    (89, '### Snakemake Rules')
    (90, '###########################################################')
    (91, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PyPSA/pypsa-eur-mga, file=Snakefile
context_key: ['if m is not None']
    (13, 'def memory(w):')
    (14, '    factor = 1.3')
    (15, "    for o in w.opts.split(\\'-\\'):")
    (16, "        m = re.match(r\\'^(\\\\d+)h$\\', o, re.IGNORECASE)")
    (17, '        if m is not None:')
    (18, '            factor /= int(m.group(1))')
    (19, '            break')
    (20, "    if w.clusters.endswith(\\'m\\'):")
    (21, '        return int(factor * (18000 + 180 * int(w.clusters[:-1])))')
    (22, '    else:')
    (23, '        return int(factor * (10000 + 195 * int(w.clusters)))')
    (24, '')
    (25, '# OPTIMAL SOLUTION')
    (26, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PyPSA/pypsa-eur-mga, file=Snakefile
context_key: ['if int(clusters) == int(w.clusters)']
    (84, 'def input_generate_clusters_alternatives(w):')
    (85, '    wildcard_sets = get_wildcard_sets(config)')
    (86, '    input = []')
    (87, '    for wildcards in wildcard_sets:')
    (88, '        for clusters in wildcards["clusters"]:')
    (89, '            if int(clusters) == int(w.clusters):')
    (90, "                for opts in wildcards[\\'opts\\']:")
    (91, "                    for epsilon in wildcards[\\'epsilon\\']:")
    (92, "                        for category in wildcards[\\'category\\']:")
    (93, '                            alternatives = checkpoints.generate_list_of_alternatives.get(')
    (94, '                                clusters=w.clusters,')
    (95, '                                opts=opts,')
    (96, '                                category=category).output[0]')
    (97, '                            obj_list = []')
    (98, '                            with open(alternatives, "r") as f:  ')
    (99, '                                for line in f:')
    (100, '                                    obj_list.append(line.strip())')
    (101, '                            for obj in obj_list:              ')
    (102, '                                input.append(')
    (103, '                                    "results/networks/elec_s_{clusters}_ec_lcopt_{opts}_tol{epsilon}_cat-{category}_obj-{objective}.nc".format(')
    (104, '                                        clusters=w.clusters,')
    (105, '                                        opts=opts,')
    (106, '                                        epsilon=epsilon,')
    (107, '                                        objective=obj,')
    (108, '                                        category=category)')
    (109, '                                )')
    (110, '    return input')
    (111, '')
    (112, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ccerutti88/circDetector, file=Snakefile
context_key: ["if species not in config[\\'annotations\\']"]
    (22, 'def get_annotation(wildcards):')
    (23, '    species = wildcards.sample.split("_")[0]')
    (24, "    if species not in config[\\'annotations\\']:")
    (25, '        print(wildcards.sample)')
    (26, '        print(species)')
    (27, "        print(config[\\'annotations\\'])")
    (28, '        print("Warning the species %s is absent from aconfig file" % species)')
    (29, '        exit(1)')
    (30, "    return config[\\'annotations\\'][species]")
    (31, '')
    (32, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ["if filename.endswith(\\'.R1.fastq.gz\\') or filename.endswith(\\'.R2.fastq.gz\\')"]
    (12, 'def rename(filename):')
    (13, '    """Dynamically renames FastQ file to have one of the following extensions: *.R1.fastq.gz, *.R2.fastq.gz')
    (14, '    To automatically rename the fastq files, a few assumptions are made. If the extension of the')
    (15, '    FastQ file cannot be infered, an exception is raised telling the user to fix the filename')
    (16, '    of the fastq files.')
    (17, '    @param filename <str>:')
    (18, '        Original name of file to be renamed')
    (19, '    @return filename <str>:')
    (20, '        A renamed FastQ filename')
    (21, '    """')
    (22, '    import re')
    (23, '')
    (24, '    # Covers common extensions from SF, SRA, EBI, TCGA, and external sequencing providers')
    (25, '    # key = regex to match string and value = how it will be renamed')
    (26, '    extensions = {')
    (27, '        # Matches: _S[##]_R[12]_fastq.gz, _S[##]_R[12].fastq.gz, _R[12]_fq.gz; works for exome fqs from SF')
    (28, '        "_S[0-9]+_R1_001.f(ast)?q.gz$": ".R1.fastq.gz",')
    (29, '        "_S[0-9]+_R2_001.f(ast)?q.gz$": ".R2.fastq.gz",')
    (30, '        # Matches: _R[12]_fastq.gz, _R[12].fastq.gz, _R[12]_fq.gz, etc.')
    (31, '        ".R1.f(ast)?q.gz$": ".R1.fastq.gz",')
    (32, '        ".R2.f(ast)?q.gz$": ".R2.fastq.gz",')
    (33, '        # ".R1(?<!\\\\.trimmed).f(ast)?q.gz$": ".R1.fastq.gz",')
    (34, '        # ".R2(?<!\\\\.trimmed).f(ast)?q.gz$": ".R2.fastq.gz",')
    (35, '        # Matches: _R[12]_001_fastq_gz, _R[12].001.fastq.gz, _R[12]_001.fq.gz, etc.')
    (36, '        # Capture lane information as named group')
    (37, '        ".R1.(?P<lane>...).f(ast)?q.gz$": ".R1.fastq.gz",')
    (38, '        ".R2.(?P<lane>...).f(ast)?q.gz$": ".R2.fastq.gz",')
    (39, '        # Matches: _[12].fastq.gz, _[12].fq.gz, _[12]_fastq_gz, etc.')
    (40, '        "_1.f(ast)?q.gz$": ".R1.fastq.gz",')
    (41, '        "_2.f(ast)?q.gz$": ".R2.fastq.gz",')
    (42, '        ####')
    (43, "        # Matches: *.bam if it\\'s not preceded by \\'.recal\\' (i.e. match \\'.bam\\' exactly)")
    (44, '        "(?<!\\\\.recal)\\\\.bam$": ".input.bam",')
    (45, '        "\\\\.recal\\\\.bam$": ".input.bam"')
    (46, '    }')
    (47, '')
    (48, "    if filename.endswith(\\'.R1.fastq.gz\\') or filename.endswith(\\'.R2.fastq.gz\\'):")
    (49, '        # Filename is already in the correct format')
    (50, '        return filename')
    (51, '')
    (52, '    converted = False')
    (53, '    for regex, new_ext in extensions.items():')
    (54, '        matched = re.search(regex, filename)')
    (55, '        if matched:')
    (56, '            # regex matches with a pattern in extensions')
    (57, '            converted = True')
    (58, '            filename = re.sub(regex, new_ext, filename)')
    (59, '            break # only rename once')
    (60, '')
    (61, '    if not converted:')
    (62, '        raise NameError("""\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['if not os.path.exists(renamed)']
    (78, 'def _sym_safe_(input_data, target):')
    (79, '    """Creates re-named symlinks for each FastQ file provided')
    (80, '    as input. If a symlink already exists, it will not try to create a new symlink.')
    (81, '    If relative source PATH is provided, it will be converted to an absolute PATH.')
    (82, '    @param input_data <list[<str>]>:')
    (83, '        List of input files to symlink to target location')
    (84, '    @param target <str>:')
    (85, '        Target path to copy templates and required resources')
    (86, '    @return input_fastqs list[<str>]:')
    (87, '        List of renamed input FastQs')
    (88, '    """')
    (89, '    input_fastqs = [] # store renamed fastq file names')
    (90, '    for file in input_data:')
    (91, '        filename = os.path.basename(file)')
    (92, '        renamed = os.path.join(target, rename(filename))')
    (93, '        input_fastqs.append(renamed)')
    (94, '')
    (95, '        if not os.path.exists(renamed):')
    (96, '            # Create a symlink if it does not already exist')
    (97, '            # Follow source symlinks to resolve any binding issues')
    (98, '            os.symlink(os.path.abspath(os.path.realpath(file)), renamed)')
    (99, '')
    (100, '    return input_fastqs')
    (101, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['if not tn_mode in ["auto","paired","tumor_only"]']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['if os.path.isfile(pairs_filepath)', 'if not "tumor" in df']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['if "normal" in df']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['else', 'if sample_names']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
    (149, '\\\\tFatal: Either a valid pairs file or sample names must be provided.')
    (150, '            Pairs file path provided: {}')
    (151, '            Sample names provided: {}')
    (152, '            """.format(pairs_filepath, sample_names, sys.argv[0])')
    (153, '            )')
    (154, '        else:')
    (155, '            tumor_ids=sample_names')
    (156, '    ')
    (157, '    ## Overlap with given sample names')
    (158, '    if sample_names:')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['else', 'if tn_mode=="paired"']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
    (149, '\\\\tFatal: Either a valid pairs file or sample names must be provided.')
    (150, '            Pairs file path provided: {}')
    (151, '            Sample names provided: {}')
    (152, '            """.format(pairs_filepath, sample_names, sys.argv[0])')
    (153, '            )')
    (154, '        else:')
    (155, '            tumor_ids=sample_names')
    (156, '    ')
    (157, '    ## Overlap with given sample names')
    (158, '    if sample_names:')
    (159, '        overlapped_pairs = {k: paired_ids[k] for k in sample_names if k in paired_ids}')
    (160, '        overlapped_tumors = list(set(tumor_ids) & set(sample_names))')
    (161, '        ')
    (162, '        # print(str(len(overlapped_pairs)) + " of " + str(len(paired_ids)) + " pairs in pairs file matched given sample names")')
    (163, '        # print(str(len(overlapped_tumors)) + " of " + str(len(tumor_ids)) + " tumors in pairs file matched given sample names")')
    (164, '        ')
    (165, '        paired_ids=overlapped_pairs')
    (166, '        tumor_ids=overlapped_tumors')
    (167, '    ')
    (168, '    out_dict={"paired":paired_ids, "tumor_only": dict.fromkeys(set(tumor_ids))}')
    (169, '    ')
    (170, '    if tn_mode=="paired":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['else', 'elif tn_mode=="tumor_only"']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
    (149, '\\\\tFatal: Either a valid pairs file or sample names must be provided.')
    (150, '            Pairs file path provided: {}')
    (151, '            Sample names provided: {}')
    (152, '            """.format(pairs_filepath, sample_names, sys.argv[0])')
    (153, '            )')
    (154, '        else:')
    (155, '            tumor_ids=sample_names')
    (156, '    ')
    (157, '    ## Overlap with given sample names')
    (158, '    if sample_names:')
    (159, '        overlapped_pairs = {k: paired_ids[k] for k in sample_names if k in paired_ids}')
    (160, '        overlapped_tumors = list(set(tumor_ids) & set(sample_names))')
    (161, '        ')
    (162, '        # print(str(len(overlapped_pairs)) + " of " + str(len(paired_ids)) + " pairs in pairs file matched given sample names")')
    (163, '        # print(str(len(overlapped_tumors)) + " of " + str(len(tumor_ids)) + " tumors in pairs file matched given sample names")')
    (164, '        ')
    (165, '        paired_ids=overlapped_pairs')
    (166, '        tumor_ids=overlapped_tumors')
    (167, '    ')
    (168, '    out_dict={"paired":paired_ids, "tumor_only": dict.fromkeys(set(tumor_ids))}')
    (169, '    ')
    (170, '    if tn_mode=="paired":')
    (171, '        out_dict["tumor_only"]=[]')
    (172, '    elif tn_mode=="tumor_only":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['if fqs_found', 'if not os.path.exists(input_fqdir)']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
    (149, '\\\\tFatal: Either a valid pairs file or sample names must be provided.')
    (150, '            Pairs file path provided: {}')
    (151, '            Sample names provided: {}')
    (152, '            """.format(pairs_filepath, sample_names, sys.argv[0])')
    (153, '            )')
    (154, '        else:')
    (155, '            tumor_ids=sample_names')
    (156, '    ')
    (157, '    ## Overlap with given sample names')
    (158, '    if sample_names:')
    (159, '        overlapped_pairs = {k: paired_ids[k] for k in sample_names if k in paired_ids}')
    (160, '        overlapped_tumors = list(set(tumor_ids) & set(sample_names))')
    (161, '        ')
    (162, '        # print(str(len(overlapped_pairs)) + " of " + str(len(paired_ids)) + " pairs in pairs file matched given sample names")')
    (163, '        # print(str(len(overlapped_tumors)) + " of " + str(len(tumor_ids)) + " tumors in pairs file matched given sample names")')
    (164, '        ')
    (165, '        paired_ids=overlapped_pairs')
    (166, '        tumor_ids=overlapped_tumors')
    (167, '    ')
    (168, '    out_dict={"paired":paired_ids, "tumor_only": dict.fromkeys(set(tumor_ids))}')
    (169, '    ')
    (170, '    if tn_mode=="paired":')
    (171, '        out_dict["tumor_only"]=[]')
    (172, '    elif tn_mode=="tumor_only":')
    (173, '        out_dict["paired"]=[]')
    (174, '    ')
    (175, '    return(out_dict)')
    (176, '')
    (177, 'configfile:"config/config.json"')
    (178, '')
    (179, '')
    (180, '######### PARSE CONFIG PARAMS #########')
    (181, "TN_MODE=config[\\'input_params\\'][\\'TN_MODE\\']")
    (182, "exome_targets_bed=config[\\'input_params\\'][\\'EXOME_TARGETS\\']")
    (183, "BASEDIR=os.path.realpath(config[\\'input_params\\'][\\'BASE_OUTDIR\\'])")
    (184, '')
    (185, 'input_fqdir=os.path.join(BASEDIR,"input_files","fastq")')
    (186, "fq_source=config[\\'input_params\\'][\\'FASTQ_SOURCE\\']")
    (187, "fqs_found=glob.glob(os.path.join(fq_source,\\'*.fastq.gz\\'))")
    (188, '')
    (189, 'input_bamdir=os.path.join(BASEDIR,"input_files","bam")')
    (190, "bam_source=os.path.join(config[\\'input_params\\'][\\'BAM_SOURCE\\'])")
    (191, "bams_found=glob.glob(os.path.join(bam_source,\\'*.bam\\'))")
    (192, '')
    (193, "output_fqdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'FASTQ\\'])")
    (194, "output_bamdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'BAM\\'])")
    (195, 'output_qcdir=os.path.join(BASEDIR,"QC")')
    (196, '')
    (197, "VCF2MAF_WRAPPER=config[\\'scripts\\'][\\'vcf2maf_wrapper\\']")
    (198, 'SOBDetector_out=os.path.join(BASEDIR,"ffpe_filter","sobdetector")')
    (199, 'SOBDetector_JARFILE=os.path.join(SOBDetector_out, "jarfile","SOBDetector_v1.0.2.jar")')
    (200, '')
    (201, 'name_symlinks=[]')
    (202, 'if fqs_found:')
    (203, '    name_suffix=".R[1,2].fastq.gz"')
    (204, '    if not os.path.exists(input_fqdir):')
    (205, '        # print("making"+output_fqdir)')
    (206, '        os.makedirs(input_fqdir) ')
    (207, '        name_symlinks=_sym_safe_(fqs_found, input_fqdir)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['if fqs_found', 'else']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
    (149, '\\\\tFatal: Either a valid pairs file or sample names must be provided.')
    (150, '            Pairs file path provided: {}')
    (151, '            Sample names provided: {}')
    (152, '            """.format(pairs_filepath, sample_names, sys.argv[0])')
    (153, '            )')
    (154, '        else:')
    (155, '            tumor_ids=sample_names')
    (156, '    ')
    (157, '    ## Overlap with given sample names')
    (158, '    if sample_names:')
    (159, '        overlapped_pairs = {k: paired_ids[k] for k in sample_names if k in paired_ids}')
    (160, '        overlapped_tumors = list(set(tumor_ids) & set(sample_names))')
    (161, '        ')
    (162, '        # print(str(len(overlapped_pairs)) + " of " + str(len(paired_ids)) + " pairs in pairs file matched given sample names")')
    (163, '        # print(str(len(overlapped_tumors)) + " of " + str(len(tumor_ids)) + " tumors in pairs file matched given sample names")')
    (164, '        ')
    (165, '        paired_ids=overlapped_pairs')
    (166, '        tumor_ids=overlapped_tumors')
    (167, '    ')
    (168, '    out_dict={"paired":paired_ids, "tumor_only": dict.fromkeys(set(tumor_ids))}')
    (169, '    ')
    (170, '    if tn_mode=="paired":')
    (171, '        out_dict["tumor_only"]=[]')
    (172, '    elif tn_mode=="tumor_only":')
    (173, '        out_dict["paired"]=[]')
    (174, '    ')
    (175, '    return(out_dict)')
    (176, '')
    (177, 'configfile:"config/config.json"')
    (178, '')
    (179, '')
    (180, '######### PARSE CONFIG PARAMS #########')
    (181, "TN_MODE=config[\\'input_params\\'][\\'TN_MODE\\']")
    (182, "exome_targets_bed=config[\\'input_params\\'][\\'EXOME_TARGETS\\']")
    (183, "BASEDIR=os.path.realpath(config[\\'input_params\\'][\\'BASE_OUTDIR\\'])")
    (184, '')
    (185, 'input_fqdir=os.path.join(BASEDIR,"input_files","fastq")')
    (186, "fq_source=config[\\'input_params\\'][\\'FASTQ_SOURCE\\']")
    (187, "fqs_found=glob.glob(os.path.join(fq_source,\\'*.fastq.gz\\'))")
    (188, '')
    (189, 'input_bamdir=os.path.join(BASEDIR,"input_files","bam")')
    (190, "bam_source=os.path.join(config[\\'input_params\\'][\\'BAM_SOURCE\\'])")
    (191, "bams_found=glob.glob(os.path.join(bam_source,\\'*.bam\\'))")
    (192, '')
    (193, "output_fqdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'FASTQ\\'])")
    (194, "output_bamdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'BAM\\'])")
    (195, 'output_qcdir=os.path.join(BASEDIR,"QC")')
    (196, '')
    (197, "VCF2MAF_WRAPPER=config[\\'scripts\\'][\\'vcf2maf_wrapper\\']")
    (198, 'SOBDetector_out=os.path.join(BASEDIR,"ffpe_filter","sobdetector")')
    (199, 'SOBDetector_JARFILE=os.path.join(SOBDetector_out, "jarfile","SOBDetector_v1.0.2.jar")')
    (200, '')
    (201, 'name_symlinks=[]')
    (202, 'if fqs_found:')
    (203, '    name_suffix=".R[1,2].fastq.gz"')
    (204, '    if not os.path.exists(input_fqdir):')
    (205, '        # print("making"+output_fqdir)')
    (206, '        os.makedirs(input_fqdir) ')
    (207, '        name_symlinks=_sym_safe_(fqs_found, input_fqdir)')
    (208, '    else:')
    (209, "        name_symlinks=glob.glob(os.path.join(input_fqdir,\\'*.fastq.gz\\'))")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['elif bams_found', 'if not os.path.exists(input_bamdir)']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
    (149, '\\\\tFatal: Either a valid pairs file or sample names must be provided.')
    (150, '            Pairs file path provided: {}')
    (151, '            Sample names provided: {}')
    (152, '            """.format(pairs_filepath, sample_names, sys.argv[0])')
    (153, '            )')
    (154, '        else:')
    (155, '            tumor_ids=sample_names')
    (156, '    ')
    (157, '    ## Overlap with given sample names')
    (158, '    if sample_names:')
    (159, '        overlapped_pairs = {k: paired_ids[k] for k in sample_names if k in paired_ids}')
    (160, '        overlapped_tumors = list(set(tumor_ids) & set(sample_names))')
    (161, '        ')
    (162, '        # print(str(len(overlapped_pairs)) + " of " + str(len(paired_ids)) + " pairs in pairs file matched given sample names")')
    (163, '        # print(str(len(overlapped_tumors)) + " of " + str(len(tumor_ids)) + " tumors in pairs file matched given sample names")')
    (164, '        ')
    (165, '        paired_ids=overlapped_pairs')
    (166, '        tumor_ids=overlapped_tumors')
    (167, '    ')
    (168, '    out_dict={"paired":paired_ids, "tumor_only": dict.fromkeys(set(tumor_ids))}')
    (169, '    ')
    (170, '    if tn_mode=="paired":')
    (171, '        out_dict["tumor_only"]=[]')
    (172, '    elif tn_mode=="tumor_only":')
    (173, '        out_dict["paired"]=[]')
    (174, '    ')
    (175, '    return(out_dict)')
    (176, '')
    (177, 'configfile:"config/config.json"')
    (178, '')
    (179, '')
    (180, '######### PARSE CONFIG PARAMS #########')
    (181, "TN_MODE=config[\\'input_params\\'][\\'TN_MODE\\']")
    (182, "exome_targets_bed=config[\\'input_params\\'][\\'EXOME_TARGETS\\']")
    (183, "BASEDIR=os.path.realpath(config[\\'input_params\\'][\\'BASE_OUTDIR\\'])")
    (184, '')
    (185, 'input_fqdir=os.path.join(BASEDIR,"input_files","fastq")')
    (186, "fq_source=config[\\'input_params\\'][\\'FASTQ_SOURCE\\']")
    (187, "fqs_found=glob.glob(os.path.join(fq_source,\\'*.fastq.gz\\'))")
    (188, '')
    (189, 'input_bamdir=os.path.join(BASEDIR,"input_files","bam")')
    (190, "bam_source=os.path.join(config[\\'input_params\\'][\\'BAM_SOURCE\\'])")
    (191, "bams_found=glob.glob(os.path.join(bam_source,\\'*.bam\\'))")
    (192, '')
    (193, "output_fqdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'FASTQ\\'])")
    (194, "output_bamdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'BAM\\'])")
    (195, 'output_qcdir=os.path.join(BASEDIR,"QC")')
    (196, '')
    (197, "VCF2MAF_WRAPPER=config[\\'scripts\\'][\\'vcf2maf_wrapper\\']")
    (198, 'SOBDetector_out=os.path.join(BASEDIR,"ffpe_filter","sobdetector")')
    (199, 'SOBDetector_JARFILE=os.path.join(SOBDetector_out, "jarfile","SOBDetector_v1.0.2.jar")')
    (200, '')
    (201, 'name_symlinks=[]')
    (202, 'if fqs_found:')
    (203, '    name_suffix=".R[1,2].fastq.gz"')
    (204, '    if not os.path.exists(input_fqdir):')
    (205, '        # print("making"+output_fqdir)')
    (206, '        os.makedirs(input_fqdir) ')
    (207, '        name_symlinks=_sym_safe_(fqs_found, input_fqdir)')
    (208, '    else:')
    (209, "        name_symlinks=glob.glob(os.path.join(input_fqdir,\\'*.fastq.gz\\'))")
    (210, 'elif bams_found:')
    (211, '    name_suffix=".input.bam"')
    (212, '    if not os.path.exists(input_bamdir):')
    (213, '        os.makedirs(input_bamdir) ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['elif bams_found', 'if (len(os.listdir(input_bamdir))==0)']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
    (149, '\\\\tFatal: Either a valid pairs file or sample names must be provided.')
    (150, '            Pairs file path provided: {}')
    (151, '            Sample names provided: {}')
    (152, '            """.format(pairs_filepath, sample_names, sys.argv[0])')
    (153, '            )')
    (154, '        else:')
    (155, '            tumor_ids=sample_names')
    (156, '    ')
    (157, '    ## Overlap with given sample names')
    (158, '    if sample_names:')
    (159, '        overlapped_pairs = {k: paired_ids[k] for k in sample_names if k in paired_ids}')
    (160, '        overlapped_tumors = list(set(tumor_ids) & set(sample_names))')
    (161, '        ')
    (162, '        # print(str(len(overlapped_pairs)) + " of " + str(len(paired_ids)) + " pairs in pairs file matched given sample names")')
    (163, '        # print(str(len(overlapped_tumors)) + " of " + str(len(tumor_ids)) + " tumors in pairs file matched given sample names")')
    (164, '        ')
    (165, '        paired_ids=overlapped_pairs')
    (166, '        tumor_ids=overlapped_tumors')
    (167, '    ')
    (168, '    out_dict={"paired":paired_ids, "tumor_only": dict.fromkeys(set(tumor_ids))}')
    (169, '    ')
    (170, '    if tn_mode=="paired":')
    (171, '        out_dict["tumor_only"]=[]')
    (172, '    elif tn_mode=="tumor_only":')
    (173, '        out_dict["paired"]=[]')
    (174, '    ')
    (175, '    return(out_dict)')
    (176, '')
    (177, 'configfile:"config/config.json"')
    (178, '')
    (179, '')
    (180, '######### PARSE CONFIG PARAMS #########')
    (181, "TN_MODE=config[\\'input_params\\'][\\'TN_MODE\\']")
    (182, "exome_targets_bed=config[\\'input_params\\'][\\'EXOME_TARGETS\\']")
    (183, "BASEDIR=os.path.realpath(config[\\'input_params\\'][\\'BASE_OUTDIR\\'])")
    (184, '')
    (185, 'input_fqdir=os.path.join(BASEDIR,"input_files","fastq")')
    (186, "fq_source=config[\\'input_params\\'][\\'FASTQ_SOURCE\\']")
    (187, "fqs_found=glob.glob(os.path.join(fq_source,\\'*.fastq.gz\\'))")
    (188, '')
    (189, 'input_bamdir=os.path.join(BASEDIR,"input_files","bam")')
    (190, "bam_source=os.path.join(config[\\'input_params\\'][\\'BAM_SOURCE\\'])")
    (191, "bams_found=glob.glob(os.path.join(bam_source,\\'*.bam\\'))")
    (192, '')
    (193, "output_fqdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'FASTQ\\'])")
    (194, "output_bamdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'BAM\\'])")
    (195, 'output_qcdir=os.path.join(BASEDIR,"QC")')
    (196, '')
    (197, "VCF2MAF_WRAPPER=config[\\'scripts\\'][\\'vcf2maf_wrapper\\']")
    (198, 'SOBDetector_out=os.path.join(BASEDIR,"ffpe_filter","sobdetector")')
    (199, 'SOBDetector_JARFILE=os.path.join(SOBDetector_out, "jarfile","SOBDetector_v1.0.2.jar")')
    (200, '')
    (201, 'name_symlinks=[]')
    (202, 'if fqs_found:')
    (203, '    name_suffix=".R[1,2].fastq.gz"')
    (204, '    if not os.path.exists(input_fqdir):')
    (205, '        # print("making"+output_fqdir)')
    (206, '        os.makedirs(input_fqdir) ')
    (207, '        name_symlinks=_sym_safe_(fqs_found, input_fqdir)')
    (208, '    else:')
    (209, "        name_symlinks=glob.glob(os.path.join(input_fqdir,\\'*.fastq.gz\\'))")
    (210, 'elif bams_found:')
    (211, '    name_suffix=".input.bam"')
    (212, '    if not os.path.exists(input_bamdir):')
    (213, '        os.makedirs(input_bamdir) ')
    (214, '    if (len(os.listdir(input_bamdir))==0):')
    (215, '        bam_symlinks=_sym_safe_(bams_found, input_bamdir)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['elif bams_found']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
    (149, '\\\\tFatal: Either a valid pairs file or sample names must be provided.')
    (150, '            Pairs file path provided: {}')
    (151, '            Sample names provided: {}')
    (152, '            """.format(pairs_filepath, sample_names, sys.argv[0])')
    (153, '            )')
    (154, '        else:')
    (155, '            tumor_ids=sample_names')
    (156, '    ')
    (157, '    ## Overlap with given sample names')
    (158, '    if sample_names:')
    (159, '        overlapped_pairs = {k: paired_ids[k] for k in sample_names if k in paired_ids}')
    (160, '        overlapped_tumors = list(set(tumor_ids) & set(sample_names))')
    (161, '        ')
    (162, '        # print(str(len(overlapped_pairs)) + " of " + str(len(paired_ids)) + " pairs in pairs file matched given sample names")')
    (163, '        # print(str(len(overlapped_tumors)) + " of " + str(len(tumor_ids)) + " tumors in pairs file matched given sample names")')
    (164, '        ')
    (165, '        paired_ids=overlapped_pairs')
    (166, '        tumor_ids=overlapped_tumors')
    (167, '    ')
    (168, '    out_dict={"paired":paired_ids, "tumor_only": dict.fromkeys(set(tumor_ids))}')
    (169, '    ')
    (170, '    if tn_mode=="paired":')
    (171, '        out_dict["tumor_only"]=[]')
    (172, '    elif tn_mode=="tumor_only":')
    (173, '        out_dict["paired"]=[]')
    (174, '    ')
    (175, '    return(out_dict)')
    (176, '')
    (177, 'configfile:"config/config.json"')
    (178, '')
    (179, '')
    (180, '######### PARSE CONFIG PARAMS #########')
    (181, "TN_MODE=config[\\'input_params\\'][\\'TN_MODE\\']")
    (182, "exome_targets_bed=config[\\'input_params\\'][\\'EXOME_TARGETS\\']")
    (183, "BASEDIR=os.path.realpath(config[\\'input_params\\'][\\'BASE_OUTDIR\\'])")
    (184, '')
    (185, 'input_fqdir=os.path.join(BASEDIR,"input_files","fastq")')
    (186, "fq_source=config[\\'input_params\\'][\\'FASTQ_SOURCE\\']")
    (187, "fqs_found=glob.glob(os.path.join(fq_source,\\'*.fastq.gz\\'))")
    (188, '')
    (189, 'input_bamdir=os.path.join(BASEDIR,"input_files","bam")')
    (190, "bam_source=os.path.join(config[\\'input_params\\'][\\'BAM_SOURCE\\'])")
    (191, "bams_found=glob.glob(os.path.join(bam_source,\\'*.bam\\'))")
    (192, '')
    (193, "output_fqdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'FASTQ\\'])")
    (194, "output_bamdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'BAM\\'])")
    (195, 'output_qcdir=os.path.join(BASEDIR,"QC")')
    (196, '')
    (197, "VCF2MAF_WRAPPER=config[\\'scripts\\'][\\'vcf2maf_wrapper\\']")
    (198, 'SOBDetector_out=os.path.join(BASEDIR,"ffpe_filter","sobdetector")')
    (199, 'SOBDetector_JARFILE=os.path.join(SOBDetector_out, "jarfile","SOBDetector_v1.0.2.jar")')
    (200, '')
    (201, 'name_symlinks=[]')
    (202, 'if fqs_found:')
    (203, '    name_suffix=".R[1,2].fastq.gz"')
    (204, '    if not os.path.exists(input_fqdir):')
    (205, '        # print("making"+output_fqdir)')
    (206, '        os.makedirs(input_fqdir) ')
    (207, '        name_symlinks=_sym_safe_(fqs_found, input_fqdir)')
    (208, '    else:')
    (209, "        name_symlinks=glob.glob(os.path.join(input_fqdir,\\'*.fastq.gz\\'))")
    (210, 'elif bams_found:')
    (211, '    name_suffix=".input.bam"')
    (212, '    if not os.path.exists(input_bamdir):')
    (213, '        os.makedirs(input_bamdir) ')
    (214, '    if (len(os.listdir(input_bamdir))==0):')
    (215, '        bam_symlinks=_sym_safe_(bams_found, input_bamdir)')
    (216, "    name_symlinks=glob.glob(os.path.join(input_bamdir,\\'*.input.bam\\'))")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['if tn_mode=="auto"', 'if sample_info["paired"]']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
    (149, '\\\\tFatal: Either a valid pairs file or sample names must be provided.')
    (150, '            Pairs file path provided: {}')
    (151, '            Sample names provided: {}')
    (152, '            """.format(pairs_filepath, sample_names, sys.argv[0])')
    (153, '            )')
    (154, '        else:')
    (155, '            tumor_ids=sample_names')
    (156, '    ')
    (157, '    ## Overlap with given sample names')
    (158, '    if sample_names:')
    (159, '        overlapped_pairs = {k: paired_ids[k] for k in sample_names if k in paired_ids}')
    (160, '        overlapped_tumors = list(set(tumor_ids) & set(sample_names))')
    (161, '        ')
    (162, '        # print(str(len(overlapped_pairs)) + " of " + str(len(paired_ids)) + " pairs in pairs file matched given sample names")')
    (163, '        # print(str(len(overlapped_tumors)) + " of " + str(len(tumor_ids)) + " tumors in pairs file matched given sample names")')
    (164, '        ')
    (165, '        paired_ids=overlapped_pairs')
    (166, '        tumor_ids=overlapped_tumors')
    (167, '    ')
    (168, '    out_dict={"paired":paired_ids, "tumor_only": dict.fromkeys(set(tumor_ids))}')
    (169, '    ')
    (170, '    if tn_mode=="paired":')
    (171, '        out_dict["tumor_only"]=[]')
    (172, '    elif tn_mode=="tumor_only":')
    (173, '        out_dict["paired"]=[]')
    (174, '    ')
    (175, '    return(out_dict)')
    (176, '')
    (177, 'configfile:"config/config.json"')
    (178, '')
    (179, '')
    (180, '######### PARSE CONFIG PARAMS #########')
    (181, "TN_MODE=config[\\'input_params\\'][\\'TN_MODE\\']")
    (182, "exome_targets_bed=config[\\'input_params\\'][\\'EXOME_TARGETS\\']")
    (183, "BASEDIR=os.path.realpath(config[\\'input_params\\'][\\'BASE_OUTDIR\\'])")
    (184, '')
    (185, 'input_fqdir=os.path.join(BASEDIR,"input_files","fastq")')
    (186, "fq_source=config[\\'input_params\\'][\\'FASTQ_SOURCE\\']")
    (187, "fqs_found=glob.glob(os.path.join(fq_source,\\'*.fastq.gz\\'))")
    (188, '')
    (189, 'input_bamdir=os.path.join(BASEDIR,"input_files","bam")')
    (190, "bam_source=os.path.join(config[\\'input_params\\'][\\'BAM_SOURCE\\'])")
    (191, "bams_found=glob.glob(os.path.join(bam_source,\\'*.bam\\'))")
    (192, '')
    (193, "output_fqdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'FASTQ\\'])")
    (194, "output_bamdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'BAM\\'])")
    (195, 'output_qcdir=os.path.join(BASEDIR,"QC")')
    (196, '')
    (197, "VCF2MAF_WRAPPER=config[\\'scripts\\'][\\'vcf2maf_wrapper\\']")
    (198, 'SOBDetector_out=os.path.join(BASEDIR,"ffpe_filter","sobdetector")')
    (199, 'SOBDetector_JARFILE=os.path.join(SOBDetector_out, "jarfile","SOBDetector_v1.0.2.jar")')
    (200, '')
    (201, 'name_symlinks=[]')
    (202, 'if fqs_found:')
    (203, '    name_suffix=".R[1,2].fastq.gz"')
    (204, '    if not os.path.exists(input_fqdir):')
    (205, '        # print("making"+output_fqdir)')
    (206, '        os.makedirs(input_fqdir) ')
    (207, '        name_symlinks=_sym_safe_(fqs_found, input_fqdir)')
    (208, '    else:')
    (209, "        name_symlinks=glob.glob(os.path.join(input_fqdir,\\'*.fastq.gz\\'))")
    (210, 'elif bams_found:')
    (211, '    name_suffix=".input.bam"')
    (212, '    if not os.path.exists(input_bamdir):')
    (213, '        os.makedirs(input_bamdir) ')
    (214, '    if (len(os.listdir(input_bamdir))==0):')
    (215, '        bam_symlinks=_sym_safe_(bams_found, input_bamdir)')
    (216, "    name_symlinks=glob.glob(os.path.join(input_bamdir,\\'*.input.bam\\'))")
    (217, 'else:')
    (218, '    raise NameError("""\\')
    (219, '\\\\tFatal: No relevant files found in the BAM or FASTQ directory!')
    (220, '        FASTQ source path provided: {}')
    (221, '        BAM source path provided: {}')
    (222, "        Folders should contain files ending with \\'.fastq.gz\\' or \\'.bam\\' respectively.")
    (223, '        """.format(fq_source, bam_source, sys.argv[0])')
    (224, '    )')
    (225, '')
    (226, 'samples = set([re.sub(name_suffix,"",os.path.basename(fname)) for fname in name_symlinks]) ## Only returns paired fqs')
    (227, '')
    (228, "pairs_file = config[\\'input_params\\'][\\'PAIRS_FILE\\']")
    (229, '')
    (230, '# tn_mode="tumor_only"')
    (231, "tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (232, '# print(tn_mode)')
    (233, "# if \\'TN_MODE\\' in config[\\'input_params\\']:")
    (234, "#     tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (235, '#     print(tn_mode)')
    (236, '#     if tn_mode=="auto":')
    (237, '#         if os.path.isfile(pairs_file):')
    (238, '#             tn_mode="paired"')
    (239, '')
    (240, "tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (241, 'sample_info=read_pairsfile(tn_mode=tn_mode, pairs_filepath=pairs_file, sample_names=samples)')
    (242, '# print(sample_info)')
    (243, '')
    (244, 'if tn_mode=="auto":')
    (245, '    if sample_info["paired"]:')
    (246, '        tn_mode="paired"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['if tn_mode=="auto"', 'else']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
    (149, '\\\\tFatal: Either a valid pairs file or sample names must be provided.')
    (150, '            Pairs file path provided: {}')
    (151, '            Sample names provided: {}')
    (152, '            """.format(pairs_filepath, sample_names, sys.argv[0])')
    (153, '            )')
    (154, '        else:')
    (155, '            tumor_ids=sample_names')
    (156, '    ')
    (157, '    ## Overlap with given sample names')
    (158, '    if sample_names:')
    (159, '        overlapped_pairs = {k: paired_ids[k] for k in sample_names if k in paired_ids}')
    (160, '        overlapped_tumors = list(set(tumor_ids) & set(sample_names))')
    (161, '        ')
    (162, '        # print(str(len(overlapped_pairs)) + " of " + str(len(paired_ids)) + " pairs in pairs file matched given sample names")')
    (163, '        # print(str(len(overlapped_tumors)) + " of " + str(len(tumor_ids)) + " tumors in pairs file matched given sample names")')
    (164, '        ')
    (165, '        paired_ids=overlapped_pairs')
    (166, '        tumor_ids=overlapped_tumors')
    (167, '    ')
    (168, '    out_dict={"paired":paired_ids, "tumor_only": dict.fromkeys(set(tumor_ids))}')
    (169, '    ')
    (170, '    if tn_mode=="paired":')
    (171, '        out_dict["tumor_only"]=[]')
    (172, '    elif tn_mode=="tumor_only":')
    (173, '        out_dict["paired"]=[]')
    (174, '    ')
    (175, '    return(out_dict)')
    (176, '')
    (177, 'configfile:"config/config.json"')
    (178, '')
    (179, '')
    (180, '######### PARSE CONFIG PARAMS #########')
    (181, "TN_MODE=config[\\'input_params\\'][\\'TN_MODE\\']")
    (182, "exome_targets_bed=config[\\'input_params\\'][\\'EXOME_TARGETS\\']")
    (183, "BASEDIR=os.path.realpath(config[\\'input_params\\'][\\'BASE_OUTDIR\\'])")
    (184, '')
    (185, 'input_fqdir=os.path.join(BASEDIR,"input_files","fastq")')
    (186, "fq_source=config[\\'input_params\\'][\\'FASTQ_SOURCE\\']")
    (187, "fqs_found=glob.glob(os.path.join(fq_source,\\'*.fastq.gz\\'))")
    (188, '')
    (189, 'input_bamdir=os.path.join(BASEDIR,"input_files","bam")')
    (190, "bam_source=os.path.join(config[\\'input_params\\'][\\'BAM_SOURCE\\'])")
    (191, "bams_found=glob.glob(os.path.join(bam_source,\\'*.bam\\'))")
    (192, '')
    (193, "output_fqdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'FASTQ\\'])")
    (194, "output_bamdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'BAM\\'])")
    (195, 'output_qcdir=os.path.join(BASEDIR,"QC")')
    (196, '')
    (197, "VCF2MAF_WRAPPER=config[\\'scripts\\'][\\'vcf2maf_wrapper\\']")
    (198, 'SOBDetector_out=os.path.join(BASEDIR,"ffpe_filter","sobdetector")')
    (199, 'SOBDetector_JARFILE=os.path.join(SOBDetector_out, "jarfile","SOBDetector_v1.0.2.jar")')
    (200, '')
    (201, 'name_symlinks=[]')
    (202, 'if fqs_found:')
    (203, '    name_suffix=".R[1,2].fastq.gz"')
    (204, '    if not os.path.exists(input_fqdir):')
    (205, '        # print("making"+output_fqdir)')
    (206, '        os.makedirs(input_fqdir) ')
    (207, '        name_symlinks=_sym_safe_(fqs_found, input_fqdir)')
    (208, '    else:')
    (209, "        name_symlinks=glob.glob(os.path.join(input_fqdir,\\'*.fastq.gz\\'))")
    (210, 'elif bams_found:')
    (211, '    name_suffix=".input.bam"')
    (212, '    if not os.path.exists(input_bamdir):')
    (213, '        os.makedirs(input_bamdir) ')
    (214, '    if (len(os.listdir(input_bamdir))==0):')
    (215, '        bam_symlinks=_sym_safe_(bams_found, input_bamdir)')
    (216, "    name_symlinks=glob.glob(os.path.join(input_bamdir,\\'*.input.bam\\'))")
    (217, 'else:')
    (218, '    raise NameError("""\\')
    (219, '\\\\tFatal: No relevant files found in the BAM or FASTQ directory!')
    (220, '        FASTQ source path provided: {}')
    (221, '        BAM source path provided: {}')
    (222, "        Folders should contain files ending with \\'.fastq.gz\\' or \\'.bam\\' respectively.")
    (223, '        """.format(fq_source, bam_source, sys.argv[0])')
    (224, '    )')
    (225, '')
    (226, 'samples = set([re.sub(name_suffix,"",os.path.basename(fname)) for fname in name_symlinks]) ## Only returns paired fqs')
    (227, '')
    (228, "pairs_file = config[\\'input_params\\'][\\'PAIRS_FILE\\']")
    (229, '')
    (230, '# tn_mode="tumor_only"')
    (231, "tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (232, '# print(tn_mode)')
    (233, "# if \\'TN_MODE\\' in config[\\'input_params\\']:")
    (234, "#     tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (235, '#     print(tn_mode)')
    (236, '#     if tn_mode=="auto":')
    (237, '#         if os.path.isfile(pairs_file):')
    (238, '#             tn_mode="paired"')
    (239, '')
    (240, "tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (241, 'sample_info=read_pairsfile(tn_mode=tn_mode, pairs_filepath=pairs_file, sample_names=samples)')
    (242, '# print(sample_info)')
    (243, '')
    (244, 'if tn_mode=="auto":')
    (245, '    if sample_info["paired"]:')
    (246, '        tn_mode="paired"')
    (247, '    else:')
    (248, '        tn_mode="tumor_only"')
    (249, '')
    (250, '# print(tn_mode)')
    (251, '# exit')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['if not os.path.isfile(intervals_file)']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
    (149, '\\\\tFatal: Either a valid pairs file or sample names must be provided.')
    (150, '            Pairs file path provided: {}')
    (151, '            Sample names provided: {}')
    (152, '            """.format(pairs_filepath, sample_names, sys.argv[0])')
    (153, '            )')
    (154, '        else:')
    (155, '            tumor_ids=sample_names')
    (156, '    ')
    (157, '    ## Overlap with given sample names')
    (158, '    if sample_names:')
    (159, '        overlapped_pairs = {k: paired_ids[k] for k in sample_names if k in paired_ids}')
    (160, '        overlapped_tumors = list(set(tumor_ids) & set(sample_names))')
    (161, '        ')
    (162, '        # print(str(len(overlapped_pairs)) + " of " + str(len(paired_ids)) + " pairs in pairs file matched given sample names")')
    (163, '        # print(str(len(overlapped_tumors)) + " of " + str(len(tumor_ids)) + " tumors in pairs file matched given sample names")')
    (164, '        ')
    (165, '        paired_ids=overlapped_pairs')
    (166, '        tumor_ids=overlapped_tumors')
    (167, '    ')
    (168, '    out_dict={"paired":paired_ids, "tumor_only": dict.fromkeys(set(tumor_ids))}')
    (169, '    ')
    (170, '    if tn_mode=="paired":')
    (171, '        out_dict["tumor_only"]=[]')
    (172, '    elif tn_mode=="tumor_only":')
    (173, '        out_dict["paired"]=[]')
    (174, '    ')
    (175, '    return(out_dict)')
    (176, '')
    (177, 'configfile:"config/config.json"')
    (178, '')
    (179, '')
    (180, '######### PARSE CONFIG PARAMS #########')
    (181, "TN_MODE=config[\\'input_params\\'][\\'TN_MODE\\']")
    (182, "exome_targets_bed=config[\\'input_params\\'][\\'EXOME_TARGETS\\']")
    (183, "BASEDIR=os.path.realpath(config[\\'input_params\\'][\\'BASE_OUTDIR\\'])")
    (184, '')
    (185, 'input_fqdir=os.path.join(BASEDIR,"input_files","fastq")')
    (186, "fq_source=config[\\'input_params\\'][\\'FASTQ_SOURCE\\']")
    (187, "fqs_found=glob.glob(os.path.join(fq_source,\\'*.fastq.gz\\'))")
    (188, '')
    (189, 'input_bamdir=os.path.join(BASEDIR,"input_files","bam")')
    (190, "bam_source=os.path.join(config[\\'input_params\\'][\\'BAM_SOURCE\\'])")
    (191, "bams_found=glob.glob(os.path.join(bam_source,\\'*.bam\\'))")
    (192, '')
    (193, "output_fqdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'FASTQ\\'])")
    (194, "output_bamdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'BAM\\'])")
    (195, 'output_qcdir=os.path.join(BASEDIR,"QC")')
    (196, '')
    (197, "VCF2MAF_WRAPPER=config[\\'scripts\\'][\\'vcf2maf_wrapper\\']")
    (198, 'SOBDetector_out=os.path.join(BASEDIR,"ffpe_filter","sobdetector")')
    (199, 'SOBDetector_JARFILE=os.path.join(SOBDetector_out, "jarfile","SOBDetector_v1.0.2.jar")')
    (200, '')
    (201, 'name_symlinks=[]')
    (202, 'if fqs_found:')
    (203, '    name_suffix=".R[1,2].fastq.gz"')
    (204, '    if not os.path.exists(input_fqdir):')
    (205, '        # print("making"+output_fqdir)')
    (206, '        os.makedirs(input_fqdir) ')
    (207, '        name_symlinks=_sym_safe_(fqs_found, input_fqdir)')
    (208, '    else:')
    (209, "        name_symlinks=glob.glob(os.path.join(input_fqdir,\\'*.fastq.gz\\'))")
    (210, 'elif bams_found:')
    (211, '    name_suffix=".input.bam"')
    (212, '    if not os.path.exists(input_bamdir):')
    (213, '        os.makedirs(input_bamdir) ')
    (214, '    if (len(os.listdir(input_bamdir))==0):')
    (215, '        bam_symlinks=_sym_safe_(bams_found, input_bamdir)')
    (216, "    name_symlinks=glob.glob(os.path.join(input_bamdir,\\'*.input.bam\\'))")
    (217, 'else:')
    (218, '    raise NameError("""\\')
    (219, '\\\\tFatal: No relevant files found in the BAM or FASTQ directory!')
    (220, '        FASTQ source path provided: {}')
    (221, '        BAM source path provided: {}')
    (222, "        Folders should contain files ending with \\'.fastq.gz\\' or \\'.bam\\' respectively.")
    (223, '        """.format(fq_source, bam_source, sys.argv[0])')
    (224, '    )')
    (225, '')
    (226, 'samples = set([re.sub(name_suffix,"",os.path.basename(fname)) for fname in name_symlinks]) ## Only returns paired fqs')
    (227, '')
    (228, "pairs_file = config[\\'input_params\\'][\\'PAIRS_FILE\\']")
    (229, '')
    (230, '# tn_mode="tumor_only"')
    (231, "tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (232, '# print(tn_mode)')
    (233, "# if \\'TN_MODE\\' in config[\\'input_params\\']:")
    (234, "#     tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (235, '#     print(tn_mode)')
    (236, '#     if tn_mode=="auto":')
    (237, '#         if os.path.isfile(pairs_file):')
    (238, '#             tn_mode="paired"')
    (239, '')
    (240, "tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (241, 'sample_info=read_pairsfile(tn_mode=tn_mode, pairs_filepath=pairs_file, sample_names=samples)')
    (242, '# print(sample_info)')
    (243, '')
    (244, 'if tn_mode=="auto":')
    (245, '    if sample_info["paired"]:')
    (246, '        tn_mode="paired"')
    (247, '    else:')
    (248, '        tn_mode="tumor_only"')
    (249, '')
    (250, '# print(tn_mode)')
    (251, '# exit')
    (252, 'pairs_dict=sample_info[tn_mode]')
    (253, 'pairs_ids=list(pairs_dict.keys())')
    (254, '')
    (255, 'output_germline_base=os.path.join(BASEDIR,"germline")')
    (256, 'output_somatic_base=os.path.join(BASEDIR,"somatic_"+tn_mode)')
    (257, 'output_somatic_snpindels=os.path.join(output_somatic_base,"SNP_Indels")')
    (258, 'output_somatic_cnv=os.path.join(output_somatic_base,"CNV")')
    (259, '')
    (260, 'chroms = ["chr1","chr2","chr3","chr4","chr5","chr6","chr7","chr8","chr9","chr10","chr11","chr12","chr13","chr14","chr15","chr16","chr17","chr18","chr19","chr20","chr21","chr22","chrX","chrY","chrM"]')
    (261, 'intervals_file=os.path.join(BASEDIR,"intervals.list")')
    (262, 'if not os.path.isfile(intervals_file):')
    (263, "    with open(intervals_file, \\'w\\') as f:")
    (264, '        f.write("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['if not caller_list']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
    (149, '\\\\tFatal: Either a valid pairs file or sample names must be provided.')
    (150, '            Pairs file path provided: {}')
    (151, '            Sample names provided: {}')
    (152, '            """.format(pairs_filepath, sample_names, sys.argv[0])')
    (153, '            )')
    (154, '        else:')
    (155, '            tumor_ids=sample_names')
    (156, '    ')
    (157, '    ## Overlap with given sample names')
    (158, '    if sample_names:')
    (159, '        overlapped_pairs = {k: paired_ids[k] for k in sample_names if k in paired_ids}')
    (160, '        overlapped_tumors = list(set(tumor_ids) & set(sample_names))')
    (161, '        ')
    (162, '        # print(str(len(overlapped_pairs)) + " of " + str(len(paired_ids)) + " pairs in pairs file matched given sample names")')
    (163, '        # print(str(len(overlapped_tumors)) + " of " + str(len(tumor_ids)) + " tumors in pairs file matched given sample names")')
    (164, '        ')
    (165, '        paired_ids=overlapped_pairs')
    (166, '        tumor_ids=overlapped_tumors')
    (167, '    ')
    (168, '    out_dict={"paired":paired_ids, "tumor_only": dict.fromkeys(set(tumor_ids))}')
    (169, '    ')
    (170, '    if tn_mode=="paired":')
    (171, '        out_dict["tumor_only"]=[]')
    (172, '    elif tn_mode=="tumor_only":')
    (173, '        out_dict["paired"]=[]')
    (174, '    ')
    (175, '    return(out_dict)')
    (176, '')
    (177, 'configfile:"config/config.json"')
    (178, '')
    (179, '')
    (180, '######### PARSE CONFIG PARAMS #########')
    (181, "TN_MODE=config[\\'input_params\\'][\\'TN_MODE\\']")
    (182, "exome_targets_bed=config[\\'input_params\\'][\\'EXOME_TARGETS\\']")
    (183, "BASEDIR=os.path.realpath(config[\\'input_params\\'][\\'BASE_OUTDIR\\'])")
    (184, '')
    (185, 'input_fqdir=os.path.join(BASEDIR,"input_files","fastq")')
    (186, "fq_source=config[\\'input_params\\'][\\'FASTQ_SOURCE\\']")
    (187, "fqs_found=glob.glob(os.path.join(fq_source,\\'*.fastq.gz\\'))")
    (188, '')
    (189, 'input_bamdir=os.path.join(BASEDIR,"input_files","bam")')
    (190, "bam_source=os.path.join(config[\\'input_params\\'][\\'BAM_SOURCE\\'])")
    (191, "bams_found=glob.glob(os.path.join(bam_source,\\'*.bam\\'))")
    (192, '')
    (193, "output_fqdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'FASTQ\\'])")
    (194, "output_bamdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'BAM\\'])")
    (195, 'output_qcdir=os.path.join(BASEDIR,"QC")')
    (196, '')
    (197, "VCF2MAF_WRAPPER=config[\\'scripts\\'][\\'vcf2maf_wrapper\\']")
    (198, 'SOBDetector_out=os.path.join(BASEDIR,"ffpe_filter","sobdetector")')
    (199, 'SOBDetector_JARFILE=os.path.join(SOBDetector_out, "jarfile","SOBDetector_v1.0.2.jar")')
    (200, '')
    (201, 'name_symlinks=[]')
    (202, 'if fqs_found:')
    (203, '    name_suffix=".R[1,2].fastq.gz"')
    (204, '    if not os.path.exists(input_fqdir):')
    (205, '        # print("making"+output_fqdir)')
    (206, '        os.makedirs(input_fqdir) ')
    (207, '        name_symlinks=_sym_safe_(fqs_found, input_fqdir)')
    (208, '    else:')
    (209, "        name_symlinks=glob.glob(os.path.join(input_fqdir,\\'*.fastq.gz\\'))")
    (210, 'elif bams_found:')
    (211, '    name_suffix=".input.bam"')
    (212, '    if not os.path.exists(input_bamdir):')
    (213, '        os.makedirs(input_bamdir) ')
    (214, '    if (len(os.listdir(input_bamdir))==0):')
    (215, '        bam_symlinks=_sym_safe_(bams_found, input_bamdir)')
    (216, "    name_symlinks=glob.glob(os.path.join(input_bamdir,\\'*.input.bam\\'))")
    (217, 'else:')
    (218, '    raise NameError("""\\')
    (219, '\\\\tFatal: No relevant files found in the BAM or FASTQ directory!')
    (220, '        FASTQ source path provided: {}')
    (221, '        BAM source path provided: {}')
    (222, "        Folders should contain files ending with \\'.fastq.gz\\' or \\'.bam\\' respectively.")
    (223, '        """.format(fq_source, bam_source, sys.argv[0])')
    (224, '    )')
    (225, '')
    (226, 'samples = set([re.sub(name_suffix,"",os.path.basename(fname)) for fname in name_symlinks]) ## Only returns paired fqs')
    (227, '')
    (228, "pairs_file = config[\\'input_params\\'][\\'PAIRS_FILE\\']")
    (229, '')
    (230, '# tn_mode="tumor_only"')
    (231, "tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (232, '# print(tn_mode)')
    (233, "# if \\'TN_MODE\\' in config[\\'input_params\\']:")
    (234, "#     tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (235, '#     print(tn_mode)')
    (236, '#     if tn_mode=="auto":')
    (237, '#         if os.path.isfile(pairs_file):')
    (238, '#             tn_mode="paired"')
    (239, '')
    (240, "tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (241, 'sample_info=read_pairsfile(tn_mode=tn_mode, pairs_filepath=pairs_file, sample_names=samples)')
    (242, '# print(sample_info)')
    (243, '')
    (244, 'if tn_mode=="auto":')
    (245, '    if sample_info["paired"]:')
    (246, '        tn_mode="paired"')
    (247, '    else:')
    (248, '        tn_mode="tumor_only"')
    (249, '')
    (250, '# print(tn_mode)')
    (251, '# exit')
    (252, 'pairs_dict=sample_info[tn_mode]')
    (253, 'pairs_ids=list(pairs_dict.keys())')
    (254, '')
    (255, 'output_germline_base=os.path.join(BASEDIR,"germline")')
    (256, 'output_somatic_base=os.path.join(BASEDIR,"somatic_"+tn_mode)')
    (257, 'output_somatic_snpindels=os.path.join(output_somatic_base,"SNP_Indels")')
    (258, 'output_somatic_cnv=os.path.join(output_somatic_base,"CNV")')
    (259, '')
    (260, 'chroms = ["chr1","chr2","chr3","chr4","chr5","chr6","chr7","chr8","chr9","chr10","chr11","chr12","chr13","chr14","chr15","chr16","chr17","chr18","chr19","chr20","chr21","chr22","chrX","chrY","chrM"]')
    (261, 'intervals_file=os.path.join(BASEDIR,"intervals.list")')
    (262, 'if not os.path.isfile(intervals_file):')
    (263, "    with open(intervals_file, \\'w\\') as f:")
    (264, '        f.write("\\')
    (265, '".join(chroms))')
    (266, '        f.close')
    (267, '')
    (268, '')
    (269, '# Check if user provided at least')
    (270, '# one useable variant caller')
    (271, "caller_list=[caller_name.lower() for caller_name in config[\\'input_params\\'][\\'VARIANT_CALLERS\\']]")
    (272, "caller_list=list(set(caller_list) & set(config[\\'available_somatic_callers\\'][tn_mode]))")
    (273, '')
    (274, 'if not caller_list:')
    (275, '    # Did not provide a variant')
    (276, '    # caller that can be used')
    (277, '    raise TypeError("""\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['if (len(caller_list) >= 1)']
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
    (149, '\\\\tFatal: Either a valid pairs file or sample names must be provided.')
    (150, '            Pairs file path provided: {}')
    (151, '            Sample names provided: {}')
    (152, '            """.format(pairs_filepath, sample_names, sys.argv[0])')
    (153, '            )')
    (154, '        else:')
    (155, '            tumor_ids=sample_names')
    (156, '    ')
    (157, '    ## Overlap with given sample names')
    (158, '    if sample_names:')
    (159, '        overlapped_pairs = {k: paired_ids[k] for k in sample_names if k in paired_ids}')
    (160, '        overlapped_tumors = list(set(tumor_ids) & set(sample_names))')
    (161, '        ')
    (162, '        # print(str(len(overlapped_pairs)) + " of " + str(len(paired_ids)) + " pairs in pairs file matched given sample names")')
    (163, '        # print(str(len(overlapped_tumors)) + " of " + str(len(tumor_ids)) + " tumors in pairs file matched given sample names")')
    (164, '        ')
    (165, '        paired_ids=overlapped_pairs')
    (166, '        tumor_ids=overlapped_tumors')
    (167, '    ')
    (168, '    out_dict={"paired":paired_ids, "tumor_only": dict.fromkeys(set(tumor_ids))}')
    (169, '    ')
    (170, '    if tn_mode=="paired":')
    (171, '        out_dict["tumor_only"]=[]')
    (172, '    elif tn_mode=="tumor_only":')
    (173, '        out_dict["paired"]=[]')
    (174, '    ')
    (175, '    return(out_dict)')
    (176, '')
    (177, 'configfile:"config/config.json"')
    (178, '')
    (179, '')
    (180, '######### PARSE CONFIG PARAMS #########')
    (181, "TN_MODE=config[\\'input_params\\'][\\'TN_MODE\\']")
    (182, "exome_targets_bed=config[\\'input_params\\'][\\'EXOME_TARGETS\\']")
    (183, "BASEDIR=os.path.realpath(config[\\'input_params\\'][\\'BASE_OUTDIR\\'])")
    (184, '')
    (185, 'input_fqdir=os.path.join(BASEDIR,"input_files","fastq")')
    (186, "fq_source=config[\\'input_params\\'][\\'FASTQ_SOURCE\\']")
    (187, "fqs_found=glob.glob(os.path.join(fq_source,\\'*.fastq.gz\\'))")
    (188, '')
    (189, 'input_bamdir=os.path.join(BASEDIR,"input_files","bam")')
    (190, "bam_source=os.path.join(config[\\'input_params\\'][\\'BAM_SOURCE\\'])")
    (191, "bams_found=glob.glob(os.path.join(bam_source,\\'*.bam\\'))")
    (192, '')
    (193, "output_fqdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'FASTQ\\'])")
    (194, "output_bamdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'BAM\\'])")
    (195, 'output_qcdir=os.path.join(BASEDIR,"QC")')
    (196, '')
    (197, "VCF2MAF_WRAPPER=config[\\'scripts\\'][\\'vcf2maf_wrapper\\']")
    (198, 'SOBDetector_out=os.path.join(BASEDIR,"ffpe_filter","sobdetector")')
    (199, 'SOBDetector_JARFILE=os.path.join(SOBDetector_out, "jarfile","SOBDetector_v1.0.2.jar")')
    (200, '')
    (201, 'name_symlinks=[]')
    (202, 'if fqs_found:')
    (203, '    name_suffix=".R[1,2].fastq.gz"')
    (204, '    if not os.path.exists(input_fqdir):')
    (205, '        # print("making"+output_fqdir)')
    (206, '        os.makedirs(input_fqdir) ')
    (207, '        name_symlinks=_sym_safe_(fqs_found, input_fqdir)')
    (208, '    else:')
    (209, "        name_symlinks=glob.glob(os.path.join(input_fqdir,\\'*.fastq.gz\\'))")
    (210, 'elif bams_found:')
    (211, '    name_suffix=".input.bam"')
    (212, '    if not os.path.exists(input_bamdir):')
    (213, '        os.makedirs(input_bamdir) ')
    (214, '    if (len(os.listdir(input_bamdir))==0):')
    (215, '        bam_symlinks=_sym_safe_(bams_found, input_bamdir)')
    (216, "    name_symlinks=glob.glob(os.path.join(input_bamdir,\\'*.input.bam\\'))")
    (217, 'else:')
    (218, '    raise NameError("""\\')
    (219, '\\\\tFatal: No relevant files found in the BAM or FASTQ directory!')
    (220, '        FASTQ source path provided: {}')
    (221, '        BAM source path provided: {}')
    (222, "        Folders should contain files ending with \\'.fastq.gz\\' or \\'.bam\\' respectively.")
    (223, '        """.format(fq_source, bam_source, sys.argv[0])')
    (224, '    )')
    (225, '')
    (226, 'samples = set([re.sub(name_suffix,"",os.path.basename(fname)) for fname in name_symlinks]) ## Only returns paired fqs')
    (227, '')
    (228, "pairs_file = config[\\'input_params\\'][\\'PAIRS_FILE\\']")
    (229, '')
    (230, '# tn_mode="tumor_only"')
    (231, "tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (232, '# print(tn_mode)')
    (233, "# if \\'TN_MODE\\' in config[\\'input_params\\']:")
    (234, "#     tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (235, '#     print(tn_mode)')
    (236, '#     if tn_mode=="auto":')
    (237, '#         if os.path.isfile(pairs_file):')
    (238, '#             tn_mode="paired"')
    (239, '')
    (240, "tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (241, 'sample_info=read_pairsfile(tn_mode=tn_mode, pairs_filepath=pairs_file, sample_names=samples)')
    (242, '# print(sample_info)')
    (243, '')
    (244, 'if tn_mode=="auto":')
    (245, '    if sample_info["paired"]:')
    (246, '        tn_mode="paired"')
    (247, '    else:')
    (248, '        tn_mode="tumor_only"')
    (249, '')
    (250, '# print(tn_mode)')
    (251, '# exit')
    (252, 'pairs_dict=sample_info[tn_mode]')
    (253, 'pairs_ids=list(pairs_dict.keys())')
    (254, '')
    (255, 'output_germline_base=os.path.join(BASEDIR,"germline")')
    (256, 'output_somatic_base=os.path.join(BASEDIR,"somatic_"+tn_mode)')
    (257, 'output_somatic_snpindels=os.path.join(output_somatic_base,"SNP_Indels")')
    (258, 'output_somatic_cnv=os.path.join(output_somatic_base,"CNV")')
    (259, '')
    (260, 'chroms = ["chr1","chr2","chr3","chr4","chr5","chr6","chr7","chr8","chr9","chr10","chr11","chr12","chr13","chr14","chr15","chr16","chr17","chr18","chr19","chr20","chr21","chr22","chrX","chrY","chrM"]')
    (261, 'intervals_file=os.path.join(BASEDIR,"intervals.list")')
    (262, 'if not os.path.isfile(intervals_file):')
    (263, "    with open(intervals_file, \\'w\\') as f:")
    (264, '        f.write("\\')
    (265, '".join(chroms))')
    (266, '        f.close')
    (267, '')
    (268, '')
    (269, '# Check if user provided at least')
    (270, '# one useable variant caller')
    (271, "caller_list=[caller_name.lower() for caller_name in config[\\'input_params\\'][\\'VARIANT_CALLERS\\']]")
    (272, "caller_list=list(set(caller_list) & set(config[\\'available_somatic_callers\\'][tn_mode]))")
    (273, '')
    (274, 'if not caller_list:')
    (275, '    # Did not provide a variant')
    (276, '    # caller that can be used')
    (277, '    raise TypeError("""\\')
    (278, '\\\\tFatal: Must define one or more {} variant caller!\\')
    (279, '{}')
    (280, '    """.format(tn_mode, config[\\\'available_somatic_callers\\\'][tn_mode])')
    (281, '    )')
    (282, '')
    (283, "merge_outdir=config[\\'output_params\\'][\\'MERGED_SOMATIC_OUTDIR\\']")
    (284, 'somatic_callers_dirs = [caller + "_out" for caller in list(caller_list)]')
    (285, '')
    (286, 'samples_for_caller_merge=[]')
    (287, 'merge_callers_args=dict.fromkeys(pairs_ids)')
    (288, 'merge_callers_rodlist=",".join(caller_list)')
    (289, 'if (len(caller_list) >= 1):')
    (290, '    merge_callers_args_list = [["--variant:{} {}/{}/{}.FINAL.vcf".format(re.sub("_out","",vc_out), os.path.join(output_somatic_snpindels, vc_out),"vcf",pair_id) for vc_out in somatic_callers_dirs] for pair_id in pairs_ids]')
    (291, '    merge_callers_args = dict(zip(pairs_ids, [" ".join(arglist) for arglist in merge_callers_args_list]))')
    (292, '    samples_for_caller_merge=pairs_ids')
    (293, '    somatic_callers_dirs=list(somatic_callers_dirs + [merge_outdir])')
    (294, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ["if \\'FFPE_FILTER\\' in config[\\'input_params\\']", "if config[\\'input_params\\'][\\'FFPE_FILTER\\'].lower() in [\\'true\\',\\'t\\',\\'yes\\']"]
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
    (149, '\\\\tFatal: Either a valid pairs file or sample names must be provided.')
    (150, '            Pairs file path provided: {}')
    (151, '            Sample names provided: {}')
    (152, '            """.format(pairs_filepath, sample_names, sys.argv[0])')
    (153, '            )')
    (154, '        else:')
    (155, '            tumor_ids=sample_names')
    (156, '    ')
    (157, '    ## Overlap with given sample names')
    (158, '    if sample_names:')
    (159, '        overlapped_pairs = {k: paired_ids[k] for k in sample_names if k in paired_ids}')
    (160, '        overlapped_tumors = list(set(tumor_ids) & set(sample_names))')
    (161, '        ')
    (162, '        # print(str(len(overlapped_pairs)) + " of " + str(len(paired_ids)) + " pairs in pairs file matched given sample names")')
    (163, '        # print(str(len(overlapped_tumors)) + " of " + str(len(tumor_ids)) + " tumors in pairs file matched given sample names")')
    (164, '        ')
    (165, '        paired_ids=overlapped_pairs')
    (166, '        tumor_ids=overlapped_tumors')
    (167, '    ')
    (168, '    out_dict={"paired":paired_ids, "tumor_only": dict.fromkeys(set(tumor_ids))}')
    (169, '    ')
    (170, '    if tn_mode=="paired":')
    (171, '        out_dict["tumor_only"]=[]')
    (172, '    elif tn_mode=="tumor_only":')
    (173, '        out_dict["paired"]=[]')
    (174, '    ')
    (175, '    return(out_dict)')
    (176, '')
    (177, 'configfile:"config/config.json"')
    (178, '')
    (179, '')
    (180, '######### PARSE CONFIG PARAMS #########')
    (181, "TN_MODE=config[\\'input_params\\'][\\'TN_MODE\\']")
    (182, "exome_targets_bed=config[\\'input_params\\'][\\'EXOME_TARGETS\\']")
    (183, "BASEDIR=os.path.realpath(config[\\'input_params\\'][\\'BASE_OUTDIR\\'])")
    (184, '')
    (185, 'input_fqdir=os.path.join(BASEDIR,"input_files","fastq")')
    (186, "fq_source=config[\\'input_params\\'][\\'FASTQ_SOURCE\\']")
    (187, "fqs_found=glob.glob(os.path.join(fq_source,\\'*.fastq.gz\\'))")
    (188, '')
    (189, 'input_bamdir=os.path.join(BASEDIR,"input_files","bam")')
    (190, "bam_source=os.path.join(config[\\'input_params\\'][\\'BAM_SOURCE\\'])")
    (191, "bams_found=glob.glob(os.path.join(bam_source,\\'*.bam\\'))")
    (192, '')
    (193, "output_fqdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'FASTQ\\'])")
    (194, "output_bamdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'BAM\\'])")
    (195, 'output_qcdir=os.path.join(BASEDIR,"QC")')
    (196, '')
    (197, "VCF2MAF_WRAPPER=config[\\'scripts\\'][\\'vcf2maf_wrapper\\']")
    (198, 'SOBDetector_out=os.path.join(BASEDIR,"ffpe_filter","sobdetector")')
    (199, 'SOBDetector_JARFILE=os.path.join(SOBDetector_out, "jarfile","SOBDetector_v1.0.2.jar")')
    (200, '')
    (201, 'name_symlinks=[]')
    (202, 'if fqs_found:')
    (203, '    name_suffix=".R[1,2].fastq.gz"')
    (204, '    if not os.path.exists(input_fqdir):')
    (205, '        # print("making"+output_fqdir)')
    (206, '        os.makedirs(input_fqdir) ')
    (207, '        name_symlinks=_sym_safe_(fqs_found, input_fqdir)')
    (208, '    else:')
    (209, "        name_symlinks=glob.glob(os.path.join(input_fqdir,\\'*.fastq.gz\\'))")
    (210, 'elif bams_found:')
    (211, '    name_suffix=".input.bam"')
    (212, '    if not os.path.exists(input_bamdir):')
    (213, '        os.makedirs(input_bamdir) ')
    (214, '    if (len(os.listdir(input_bamdir))==0):')
    (215, '        bam_symlinks=_sym_safe_(bams_found, input_bamdir)')
    (216, "    name_symlinks=glob.glob(os.path.join(input_bamdir,\\'*.input.bam\\'))")
    (217, 'else:')
    (218, '    raise NameError("""\\')
    (219, '\\\\tFatal: No relevant files found in the BAM or FASTQ directory!')
    (220, '        FASTQ source path provided: {}')
    (221, '        BAM source path provided: {}')
    (222, "        Folders should contain files ending with \\'.fastq.gz\\' or \\'.bam\\' respectively.")
    (223, '        """.format(fq_source, bam_source, sys.argv[0])')
    (224, '    )')
    (225, '')
    (226, 'samples = set([re.sub(name_suffix,"",os.path.basename(fname)) for fname in name_symlinks]) ## Only returns paired fqs')
    (227, '')
    (228, "pairs_file = config[\\'input_params\\'][\\'PAIRS_FILE\\']")
    (229, '')
    (230, '# tn_mode="tumor_only"')
    (231, "tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (232, '# print(tn_mode)')
    (233, "# if \\'TN_MODE\\' in config[\\'input_params\\']:")
    (234, "#     tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (235, '#     print(tn_mode)')
    (236, '#     if tn_mode=="auto":')
    (237, '#         if os.path.isfile(pairs_file):')
    (238, '#             tn_mode="paired"')
    (239, '')
    (240, "tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (241, 'sample_info=read_pairsfile(tn_mode=tn_mode, pairs_filepath=pairs_file, sample_names=samples)')
    (242, '# print(sample_info)')
    (243, '')
    (244, 'if tn_mode=="auto":')
    (245, '    if sample_info["paired"]:')
    (246, '        tn_mode="paired"')
    (247, '    else:')
    (248, '        tn_mode="tumor_only"')
    (249, '')
    (250, '# print(tn_mode)')
    (251, '# exit')
    (252, 'pairs_dict=sample_info[tn_mode]')
    (253, 'pairs_ids=list(pairs_dict.keys())')
    (254, '')
    (255, 'output_germline_base=os.path.join(BASEDIR,"germline")')
    (256, 'output_somatic_base=os.path.join(BASEDIR,"somatic_"+tn_mode)')
    (257, 'output_somatic_snpindels=os.path.join(output_somatic_base,"SNP_Indels")')
    (258, 'output_somatic_cnv=os.path.join(output_somatic_base,"CNV")')
    (259, '')
    (260, 'chroms = ["chr1","chr2","chr3","chr4","chr5","chr6","chr7","chr8","chr9","chr10","chr11","chr12","chr13","chr14","chr15","chr16","chr17","chr18","chr19","chr20","chr21","chr22","chrX","chrY","chrM"]')
    (261, 'intervals_file=os.path.join(BASEDIR,"intervals.list")')
    (262, 'if not os.path.isfile(intervals_file):')
    (263, "    with open(intervals_file, \\'w\\') as f:")
    (264, '        f.write("\\')
    (265, '".join(chroms))')
    (266, '        f.close')
    (267, '')
    (268, '')
    (269, '# Check if user provided at least')
    (270, '# one useable variant caller')
    (271, "caller_list=[caller_name.lower() for caller_name in config[\\'input_params\\'][\\'VARIANT_CALLERS\\']]")
    (272, "caller_list=list(set(caller_list) & set(config[\\'available_somatic_callers\\'][tn_mode]))")
    (273, '')
    (274, 'if not caller_list:')
    (275, '    # Did not provide a variant')
    (276, '    # caller that can be used')
    (277, '    raise TypeError("""\\')
    (278, '\\\\tFatal: Must define one or more {} variant caller!\\')
    (279, '{}')
    (280, '    """.format(tn_mode, config[\\\'available_somatic_callers\\\'][tn_mode])')
    (281, '    )')
    (282, '')
    (283, "merge_outdir=config[\\'output_params\\'][\\'MERGED_SOMATIC_OUTDIR\\']")
    (284, 'somatic_callers_dirs = [caller + "_out" for caller in list(caller_list)]')
    (285, '')
    (286, 'samples_for_caller_merge=[]')
    (287, 'merge_callers_args=dict.fromkeys(pairs_ids)')
    (288, 'merge_callers_rodlist=",".join(caller_list)')
    (289, 'if (len(caller_list) >= 1):')
    (290, '    merge_callers_args_list = [["--variant:{} {}/{}/{}.FINAL.vcf".format(re.sub("_out","",vc_out), os.path.join(output_somatic_snpindels, vc_out),"vcf",pair_id) for vc_out in somatic_callers_dirs] for pair_id in pairs_ids]')
    (291, '    merge_callers_args = dict(zip(pairs_ids, [" ".join(arglist) for arglist in merge_callers_args_list]))')
    (292, '    samples_for_caller_merge=pairs_ids')
    (293, '    somatic_callers_dirs=list(somatic_callers_dirs + [merge_outdir])')
    (294, '')
    (295, "VCF2MAF_WRAPPER=config[\\'scripts\\'][\\'vcf2maf_wrapper\\']")
    (296, 'SOBDetector_out=os.path.join(output_somatic_base,"ffpe_filter","sobdetector")')
    (297, 'SOBDetector_JARFILE=os.path.join(SOBDetector_out, "jarfile","SOBDetector_v1.0.2.jar")')
    (298, '')
    (299, "exome_targets_bed=config[\\'input_params\\'][\\'EXOME_TARGETS\\']")
    (300, '')
    (301, 'ffpe_caller_list=[]')
    (302, 'ffpe_sample_list=[]')
    (303, "if \\'FFPE_FILTER\\' in config[\\'input_params\\']:")
    (304, "    if config[\\'input_params\\'][\\'FFPE_FILTER\\'].lower() in [\\'true\\',\\'t\\',\\'yes\\']:")
    (305, '        ffpe_caller_list=somatic_callers_dirs')
    (306, '        ffpe_sample_list=pairs_ids')
    (307, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ["if \\'CNV_CALLING\\' in config[\\'input_params\\']", "if config[\\'input_params\\'][\\'CNV_CALLING\\'].lower() in [\\'true\\',\\'t\\',\\'yes\\']", "if \\'TN_MODE\\' in config[\\'input_params\\']"]
    (102, 'def read_pairsfile(tn_mode="auto", pairs_filepath="", sample_names=[]):    ')
    (103, '    ## Make sure tn_mode is valid')
    (104, '    if not tn_mode in ["auto","paired","tumor_only"]:')
    (105, '        raise NameError("""\\')
    (106, "\\\\tFatal: tn_mode must be one of \\'auto\\', \\'paired\\', or \\'tumor_only\\'")
    (107, '        Argument received: {}')
    (108, '        """.format(tn_mode, sys.argv[0])')
    (109, '        )')
    (110, '    ')
    (111, '    ## Initialize some empty variables')
    (112, '    tumor_ids = []')
    (113, '    normal_ids = []')
    (114, '    paired_ids={}')
    (115, '    ')
    (116, '    ## If pairs file exists, try to use it')
    (117, '    if os.path.isfile(pairs_filepath):')
    (118, '        ## Read pairs file as data frame')
    (119, "        df = pd.read_csv(pairs_filepath, header=0, sep=\\'\\\\t\\')")
    (120, '        df.columns = df.columns.str.lower() ## Make column names case-insensitive')
    (121, '        ')
    (122, '        ## Make sure it contains a "tumor" column')
    (123, '        if not "tumor" in df:')
    (124, '            raise NameError("""\\')
    (125, "\\\\tFatal: Pairs file must contain at least a \\'tumor\\' column")
    (126, '            Columns found: {}')
    (127, '            """.format(df.columns.tolist(), sys.argv[0])')
    (128, '            )')
    (129, '        ')
    (130, '        df = df[pd.notna(df["tumor"])] ## Remove rows where tumor id is empty/na')
    (131, '        tumor_ids = df["tumor"]')
    (132, '        ')
    (133, '        if "normal" in df:')
    (134, '            normal_ids = df["normal"]')
    (135, '        ')
    (136, '        ## Make sure normal ids are not empty/na')
    (137, '        if any(pd.notna(normal_ids)):')
    (138, '            t_pair=tumor_ids[pd.notna(normal_ids)]')
    (139, '            n_pair=normal_ids[pd.notna(normal_ids)]')
    (140, '            paired_ids=dict(zip(t_pair.tolist(), n_pair.tolist()))    ')
    (141, '    ')
    (142, '    ## If pairs file not found, try to use provided sample names as tumor-only IDs')
    (143, '    else:')
    (144, '        if tn_mode == "paired":')
    (145, '            print("WARNING: Paired mode selected without a valid pairs file!!!")')
    (146, '            ')
    (147, '        if not sample_names:')
    (148, '            raise NameError("""\\')
    (149, '\\\\tFatal: Either a valid pairs file or sample names must be provided.')
    (150, '            Pairs file path provided: {}')
    (151, '            Sample names provided: {}')
    (152, '            """.format(pairs_filepath, sample_names, sys.argv[0])')
    (153, '            )')
    (154, '        else:')
    (155, '            tumor_ids=sample_names')
    (156, '    ')
    (157, '    ## Overlap with given sample names')
    (158, '    if sample_names:')
    (159, '        overlapped_pairs = {k: paired_ids[k] for k in sample_names if k in paired_ids}')
    (160, '        overlapped_tumors = list(set(tumor_ids) & set(sample_names))')
    (161, '        ')
    (162, '        # print(str(len(overlapped_pairs)) + " of " + str(len(paired_ids)) + " pairs in pairs file matched given sample names")')
    (163, '        # print(str(len(overlapped_tumors)) + " of " + str(len(tumor_ids)) + " tumors in pairs file matched given sample names")')
    (164, '        ')
    (165, '        paired_ids=overlapped_pairs')
    (166, '        tumor_ids=overlapped_tumors')
    (167, '    ')
    (168, '    out_dict={"paired":paired_ids, "tumor_only": dict.fromkeys(set(tumor_ids))}')
    (169, '    ')
    (170, '    if tn_mode=="paired":')
    (171, '        out_dict["tumor_only"]=[]')
    (172, '    elif tn_mode=="tumor_only":')
    (173, '        out_dict["paired"]=[]')
    (174, '    ')
    (175, '    return(out_dict)')
    (176, '')
    (177, 'configfile:"config/config.json"')
    (178, '')
    (179, '')
    (180, '######### PARSE CONFIG PARAMS #########')
    (181, "TN_MODE=config[\\'input_params\\'][\\'TN_MODE\\']")
    (182, "exome_targets_bed=config[\\'input_params\\'][\\'EXOME_TARGETS\\']")
    (183, "BASEDIR=os.path.realpath(config[\\'input_params\\'][\\'BASE_OUTDIR\\'])")
    (184, '')
    (185, 'input_fqdir=os.path.join(BASEDIR,"input_files","fastq")')
    (186, "fq_source=config[\\'input_params\\'][\\'FASTQ_SOURCE\\']")
    (187, "fqs_found=glob.glob(os.path.join(fq_source,\\'*.fastq.gz\\'))")
    (188, '')
    (189, 'input_bamdir=os.path.join(BASEDIR,"input_files","bam")')
    (190, "bam_source=os.path.join(config[\\'input_params\\'][\\'BAM_SOURCE\\'])")
    (191, "bams_found=glob.glob(os.path.join(bam_source,\\'*.bam\\'))")
    (192, '')
    (193, "output_fqdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'FASTQ\\'])")
    (194, "output_bamdir=os.path.join(BASEDIR,config[\\'output_params\\'][\\'BAM\\'])")
    (195, 'output_qcdir=os.path.join(BASEDIR,"QC")')
    (196, '')
    (197, "VCF2MAF_WRAPPER=config[\\'scripts\\'][\\'vcf2maf_wrapper\\']")
    (198, 'SOBDetector_out=os.path.join(BASEDIR,"ffpe_filter","sobdetector")')
    (199, 'SOBDetector_JARFILE=os.path.join(SOBDetector_out, "jarfile","SOBDetector_v1.0.2.jar")')
    (200, '')
    (201, 'name_symlinks=[]')
    (202, 'if fqs_found:')
    (203, '    name_suffix=".R[1,2].fastq.gz"')
    (204, '    if not os.path.exists(input_fqdir):')
    (205, '        # print("making"+output_fqdir)')
    (206, '        os.makedirs(input_fqdir) ')
    (207, '        name_symlinks=_sym_safe_(fqs_found, input_fqdir)')
    (208, '    else:')
    (209, "        name_symlinks=glob.glob(os.path.join(input_fqdir,\\'*.fastq.gz\\'))")
    (210, 'elif bams_found:')
    (211, '    name_suffix=".input.bam"')
    (212, '    if not os.path.exists(input_bamdir):')
    (213, '        os.makedirs(input_bamdir) ')
    (214, '    if (len(os.listdir(input_bamdir))==0):')
    (215, '        bam_symlinks=_sym_safe_(bams_found, input_bamdir)')
    (216, "    name_symlinks=glob.glob(os.path.join(input_bamdir,\\'*.input.bam\\'))")
    (217, 'else:')
    (218, '    raise NameError("""\\')
    (219, '\\\\tFatal: No relevant files found in the BAM or FASTQ directory!')
    (220, '        FASTQ source path provided: {}')
    (221, '        BAM source path provided: {}')
    (222, "        Folders should contain files ending with \\'.fastq.gz\\' or \\'.bam\\' respectively.")
    (223, '        """.format(fq_source, bam_source, sys.argv[0])')
    (224, '    )')
    (225, '')
    (226, 'samples = set([re.sub(name_suffix,"",os.path.basename(fname)) for fname in name_symlinks]) ## Only returns paired fqs')
    (227, '')
    (228, "pairs_file = config[\\'input_params\\'][\\'PAIRS_FILE\\']")
    (229, '')
    (230, '# tn_mode="tumor_only"')
    (231, "tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (232, '# print(tn_mode)')
    (233, "# if \\'TN_MODE\\' in config[\\'input_params\\']:")
    (234, "#     tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (235, '#     print(tn_mode)')
    (236, '#     if tn_mode=="auto":')
    (237, '#         if os.path.isfile(pairs_file):')
    (238, '#             tn_mode="paired"')
    (239, '')
    (240, "tn_mode=config[\\'input_params\\'][\\'TN_MODE\\']")
    (241, 'sample_info=read_pairsfile(tn_mode=tn_mode, pairs_filepath=pairs_file, sample_names=samples)')
    (242, '# print(sample_info)')
    (243, '')
    (244, 'if tn_mode=="auto":')
    (245, '    if sample_info["paired"]:')
    (246, '        tn_mode="paired"')
    (247, '    else:')
    (248, '        tn_mode="tumor_only"')
    (249, '')
    (250, '# print(tn_mode)')
    (251, '# exit')
    (252, 'pairs_dict=sample_info[tn_mode]')
    (253, 'pairs_ids=list(pairs_dict.keys())')
    (254, '')
    (255, 'output_germline_base=os.path.join(BASEDIR,"germline")')
    (256, 'output_somatic_base=os.path.join(BASEDIR,"somatic_"+tn_mode)')
    (257, 'output_somatic_snpindels=os.path.join(output_somatic_base,"SNP_Indels")')
    (258, 'output_somatic_cnv=os.path.join(output_somatic_base,"CNV")')
    (259, '')
    (260, 'chroms = ["chr1","chr2","chr3","chr4","chr5","chr6","chr7","chr8","chr9","chr10","chr11","chr12","chr13","chr14","chr15","chr16","chr17","chr18","chr19","chr20","chr21","chr22","chrX","chrY","chrM"]')
    (261, 'intervals_file=os.path.join(BASEDIR,"intervals.list")')
    (262, 'if not os.path.isfile(intervals_file):')
    (263, "    with open(intervals_file, \\'w\\') as f:")
    (264, '        f.write("\\')
    (265, '".join(chroms))')
    (266, '        f.close')
    (267, '')
    (268, '')
    (269, '# Check if user provided at least')
    (270, '# one useable variant caller')
    (271, "caller_list=[caller_name.lower() for caller_name in config[\\'input_params\\'][\\'VARIANT_CALLERS\\']]")
    (272, "caller_list=list(set(caller_list) & set(config[\\'available_somatic_callers\\'][tn_mode]))")
    (273, '')
    (274, 'if not caller_list:')
    (275, '    # Did not provide a variant')
    (276, '    # caller that can be used')
    (277, '    raise TypeError("""\\')
    (278, '\\\\tFatal: Must define one or more {} variant caller!\\')
    (279, '{}')
    (280, '    """.format(tn_mode, config[\\\'available_somatic_callers\\\'][tn_mode])')
    (281, '    )')
    (282, '')
    (283, "merge_outdir=config[\\'output_params\\'][\\'MERGED_SOMATIC_OUTDIR\\']")
    (284, 'somatic_callers_dirs = [caller + "_out" for caller in list(caller_list)]')
    (285, '')
    (286, 'samples_for_caller_merge=[]')
    (287, 'merge_callers_args=dict.fromkeys(pairs_ids)')
    (288, 'merge_callers_rodlist=",".join(caller_list)')
    (289, 'if (len(caller_list) >= 1):')
    (290, '    merge_callers_args_list = [["--variant:{} {}/{}/{}.FINAL.vcf".format(re.sub("_out","",vc_out), os.path.join(output_somatic_snpindels, vc_out),"vcf",pair_id) for vc_out in somatic_callers_dirs] for pair_id in pairs_ids]')
    (291, '    merge_callers_args = dict(zip(pairs_ids, [" ".join(arglist) for arglist in merge_callers_args_list]))')
    (292, '    samples_for_caller_merge=pairs_ids')
    (293, '    somatic_callers_dirs=list(somatic_callers_dirs + [merge_outdir])')
    (294, '')
    (295, "VCF2MAF_WRAPPER=config[\\'scripts\\'][\\'vcf2maf_wrapper\\']")
    (296, 'SOBDetector_out=os.path.join(output_somatic_base,"ffpe_filter","sobdetector")')
    (297, 'SOBDetector_JARFILE=os.path.join(SOBDetector_out, "jarfile","SOBDetector_v1.0.2.jar")')
    (298, '')
    (299, "exome_targets_bed=config[\\'input_params\\'][\\'EXOME_TARGETS\\']")
    (300, '')
    (301, 'ffpe_caller_list=[]')
    (302, 'ffpe_sample_list=[]')
    (303, "if \\'FFPE_FILTER\\' in config[\\'input_params\\']:")
    (304, "    if config[\\'input_params\\'][\\'FFPE_FILTER\\'].lower() in [\\'true\\',\\'t\\',\\'yes\\']:")
    (305, '        ffpe_caller_list=somatic_callers_dirs')
    (306, '        ffpe_sample_list=pairs_ids')
    (307, '')
    (308, 'cnv_sample_list=[]')
    (309, "if \\'CNV_CALLING\\' in config[\\'input_params\\']:")
    (310, "    if config[\\'input_params\\'][\\'CNV_CALLING\\'].lower() in [\\'true\\',\\'t\\',\\'yes\\']:")
    (311, "        if \\'TN_MODE\\' in config[\\'input_params\\']:")
    (312, "            if (config[\\'input_params\\'][\\'TN_MODE\\'].lower() in [\\'paired\\',\\'tumor_normal\\'] or")
    (313, '            config[\\\'input_params\\\'][\\\'PAIRS_FILE\\\'] != "None"):')
    (314, '                cnv_sample_list=pairs_ids')
    (315, '')
    (316, '#### July 28, 2021')
    (317, '## When I tried to run large data set through this pipeline (224 samples from CCLE),')
    (318, '## I was contacted by Biowulf staff pointing out that I was running too many short jobs.')
    (319, '#')
    (320, '#  > On 7/26/21, 2:24 PM, "wresch@hpc.nih.gov" <wresch@hpc.nih.gov> wrote:')
    (321, '#  >')
    (322, '#  >    Dear Mayank,')
    (323, '#  >')
    (324, '#  >    In the last 12 hours you ran 14734 jobs on biowulf that')
    (325, '#  >    finished in less than 5 minutes. Here are some summary stats of')
    (326, '#  >    their runtimes and walltime limits - both in minutes:')
    (327, '#')
    (328, '## Finding a workaround was not super simple; what seems to work for now is increasing')
    (329, '## the resources on the submission node and run these short jobs locally.')
    (330, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kevinrue/snakemake_cellranger_count, file=workflow/rules/cellranger.smk
context_key: ['if sample_prefix != "."']
    (3, 'def get_sample_option(wildcards):')
    (4, "    \\'\\'\\'")
    (5, '    Get cellranger sample option.')
    (6, "    \\'\\'\\'")
    (7, '    ')
    (8, '    option_str = ""')
    (9, '    ')
    (10, "    sample_prefix = samples[\\'prefix\\'][wildcards.sample]")
    (11, '')
    (12, '    if sample_prefix != ".":')
    (13, '        option_str += f"--sample={sample_prefix}"')
    (14, '        ')
    (15, '    return option_str')
    (16, '')
    (17, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kevinrue/snakemake_cellranger_count, file=workflow/rules/cellranger.smk
context_key: ['if jobmode == "local"']
    (30, 'def get_runtime_options():')
    (31, "    \\'\\'\\'")
    (32, '    Get cellranger runtime options.')
    (33, "    \\'\\'\\'")
    (34, '    option_str = ""')
    (35, '    ')
    (36, "    jobmode = config[\\'cellranger\\'][\\'jobmode\\']")
    (37, "    threads = config[\\'cellranger\\'][\\'threads\\']")
    (38, '    ')
    (39, '    if jobmode == "local":')
    (40, '        local_memory = get_local_memory(),')
    (41, '        option_str += f"--jobmode=local --localcores={threads} --localmem={local_memory}"')
    (42, '    elif jobmode == "sge":')
    (43, "        memory_per_cpu = config[\\'cellranger\\'][\\'memory_per_cpu\\']")
    (44, '        option_str += f"--jobmode=sge --maxjobs={threads} --mempercore={memory_per_cpu}"')
    (45, '    else:')
    (46, '        raise NameError(f"Invalid job mode: {jobmode}")')
    (47, '    ')
    (48, '    return option_str')
    (49, '')
    (50, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kevinrue/snakemake_cellranger_count, file=workflow/rules/cellranger.smk
context_key: ['if jobmode == "local"']
    (51, 'def get_rule_threads():')
    (52, "    \\'\\'\\'")
    (53, '    Get the number of threads given to run the rule.')
    (54, "    \\'\\'\\'")
    (55, '    threads = 1')
    (56, '    ')
    (57, "    jobmode = config[\\'cellranger\\'][\\'jobmode\\']")
    (58, '    if jobmode == "local":')
    (59, "        threads = config[\\'cellranger\\'][\\'threads\\']")
    (60, '    elif jobmode == "sge":')
    (61, '        threads = 1')
    (62, '    else:')
    (63, '        raise NameError(f"Invalid job mode: {jobmode}")')
    (64, '    ')
    (65, '    return threads')
    (66, '')
    (67, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/pipeline-nanopore-denovo-isoforms, file=snakelib/utils.snake
context_key: ['if match']
    (2, 'def generate_help(sfile):')
    (3, '    """Parse out target and help message from file."""')
    (4, '    handler = open(sfile, "r")')
    (5, '    for line in handler:')
    (6, "        match = re.match(r\\'^rule\\\\s+([a-zA-Z_-]+):.*?## (.*)$$\\', line)")
    (7, '        if match:')
    (8, '            target, help = match.groups()')
    (9, '            print("%-20s %s" % (target, help))')
    (10, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/pipeline-nanopore-denovo-isoforms, file=Snakefile
context_key: ['if r is None']
    (43, 'def build_job_tree(batch_dir):')
    (44, '    batches = glob("{}/isONbatch_*.cer".format(batch_dir))')
    (45, "    batch_ids = [int(re.search(\\'/isONbatch_(.*)\\\\.cer$\\', x).group(1)) for x in batches]")
    (46, '    global LEVELS')
    (47, '    global JOB_TREE')
    (48, '    LEVELS = OrderedDict()')
    (49, '    LEVELS[0] = []')
    (50, '    for Id, bf in sorted(zip(batch_ids, batches), key=lambda x: x[0]):')
    (51, '        n = Node(Id, "clusters/isONcluster_{}.cer".format(Id), None, None, None, 0)')
    (52, '        n.Done = True')
    (53, '        JOB_TREE[Id] = n')
    (54, '        LEVELS[0].append(n)')
    (55, '    level = 0')
    (56, '    max_id = LEVELS[0][-1].Id')
    (57, '    while len(LEVELS[level]) != 1:')
    (58, '        next_level = level + 1')
    (59, '        LEVELS[next_level] = []')
    (60, '        for l, r in grouper(2, LEVELS[level]):')
    (61, '            if r is None:')
    (62, '                LEVELS[level].pop()')
    (63, '                l.Level += 1')
    (64, '                LEVELS[next_level].append(l)')
    (65, '                continue')
    (66, '            max_id += 1')
    (67, '            new_batch = "clusters/isONcluster_{}.cer".format(max_id)')
    (68, '            new_node = Node(max_id, new_batch, l, r, None, next_level)')
    (69, '            l.Parent = new_node')
    (70, '            r.Parent =  new_node')
    (71, '            r.RightSide = True')
    (72, '            LEVELS[next_level].append(new_node)')
    (73, '            JOB_TREE[max_id] = new_node')
    (74, '        level = next_level')
    (75, '    global ROOT')
    (76, '    ROOT = JOB_TREE[len(JOB_TREE)-1].Id')
    (77, '    JOB_TREE[ROOT].RightSide = True')
    (78, '    print("Merge clustering job tree nodes:",file=sys.stderr)')
    (79, '    for n in JOB_TREE.values():')
    (80, '        print("\\\\t{}".format(n),file=sys.stderr)')
    (81, '')
    (82, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/pipeline-nanopore-denovo-isoforms, file=Snakefile
context_key: ['if not b']
    (127, 'def count_fastq_bases(fname, size=128000000):')
    (128, '    fh = open(fname, "r")')
    (129, '    count = 0')
    (130, '    while True:')
    (131, '        b = fh.read(size)')
    (132, '        if not b:')
    (133, '            break')
    (134, '        count += b.count("A")')
    (135, '        count += b.count("T")')
    (136, '        count += b.count("G")')
    (137, '        count += b.count("C")')
    (138, '        count += b.count("U")')
    (139, '    fh.close()')
    (140, '    return count')
    (141, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/pipeline-nanopore-denovo-isoforms, file=Snakefile
context_key: ['if os.path.isdir("processed_reads")']
    (142, 'def preprocess_reads(fq):')
    (143, '    pc_opts = config["pychopper_opts"]')
    (144, '    concat = config["concatenate"]')
    (145, '    thr = config["cores"]')
    (146, '')
    (147, '    out_fq = "processed_reads/full_length_reads.fq"')
    (148, '    if os.path.isdir("processed_reads"):')
    (149, '        return out_fq')
    (150, '')
    (151, '    shell("mkdir -p processed_reads")')
    (152, '    if concat:')
    (153, '        print("Concatenating reads under directory: " + fq)')
    (154, '        shell("find %s  -regextype posix-extended -regex \\\'.*\\\\.(fastq|fq)$\\\' -exec cat {{}} \\\\\\\\; > processed_reads/input_reads.fq" % fq)')
    (155, '    else:')
    (156, '        shell("ln -s `realpath %s` processed_reads/input_reads.fq" % fq)')
    (157, '')
    (158, '    if config["run_pychopper"]:')
    (159, '        print("Running pychopper of fastq file: processed_reads/input_reads.fq")')
    (160, '        shell("(cd processed_reads; cdna_classifier.py -t %d %s input_reads.fq full_length_reads.fq)" % (thr, pc_opts))')
    (161, '    else:')
    (162, '        shell("ln -s `realpath processed_reads/input_reads.fq` processed_reads/full_length_reads.fq")')
    (163, '')
    (164, '    return out_fq')
    (165, '')
    (166, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/pipeline-nanopore-denovo-isoforms, file=Snakefile
context_key: ['if ((not os.path.isfile(os.path.join(WORKDIR,"sorted","sorted_reads.fastq"))) or (not os.path.isfile(os.path.join(SNAKEDIR, DYNAMIC_RULES))))', "if config[\\'batch_size\\'] < 0"]
    (142, 'def preprocess_reads(fq):')
    (143, '    pc_opts = config["pychopper_opts"]')
    (144, '    concat = config["concatenate"]')
    (145, '    thr = config["cores"]')
    (146, '')
    (147, '    out_fq = "processed_reads/full_length_reads.fq"')
    (148, '    if os.path.isdir("processed_reads"):')
    (149, '        return out_fq')
    (150, '')
    (151, '    shell("mkdir -p processed_reads")')
    (152, '    if concat:')
    (153, '        print("Concatenating reads under directory: " + fq)')
    (154, '        shell("find %s  -regextype posix-extended -regex \\\'.*\\\\.(fastq|fq)$\\\' -exec cat {{}} \\\\\\\\; > processed_reads/input_reads.fq" % fq)')
    (155, '    else:')
    (156, '        shell("ln -s `realpath %s` processed_reads/input_reads.fq" % fq)')
    (157, '')
    (158, '    if config["run_pychopper"]:')
    (159, '        print("Running pychopper of fastq file: processed_reads/input_reads.fq")')
    (160, '        shell("(cd processed_reads; cdna_classifier.py -t %d %s input_reads.fq full_length_reads.fq)" % (thr, pc_opts))')
    (161, '    else:')
    (162, '        shell("ln -s `realpath processed_reads/input_reads.fq` processed_reads/full_length_reads.fq")')
    (163, '')
    (164, '    return out_fq')
    (165, '')
    (166, '')
    (167, 'ROOT = None')
    (168, 'DYNAMIC_RULES="job_rules.snk"')
    (169, 'if ((not os.path.isfile(os.path.join(WORKDIR,"sorted","sorted_reads.fastq"))) or (not os.path.isfile(os.path.join(SNAKEDIR, DYNAMIC_RULES)))):')
    (170, '    print("Preprocessing read in fastq file:", in_fastq)')
    (171, '    proc_fastq = preprocess_reads(in_fastq)')
    (172, '    print("Counting records in input fastq:", proc_fastq)')
    (173, '    nr_bases = count_fastq_bases(proc_fastq)')
    (174, '    print("Bases in input: {} megabases".format(int(nr_bases/10**6)))')
    (175, "    if config[\\'batch_size\\'] < 0:")
    (176, '        config[\\\'batch_size\\\'] = int(nr_bases/1000/config["cores"])')
    (177, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/pipeline-nanopore-denovo-isoforms, file=Snakefile
context_key: ['if ((not os.path.isfile(os.path.join(WORKDIR,"sorted","sorted_reads.fastq"))) or (not os.path.isfile(os.path.join(SNAKEDIR, DYNAMIC_RULES))))']
    (142, 'def preprocess_reads(fq):')
    (143, '    pc_opts = config["pychopper_opts"]')
    (144, '    concat = config["concatenate"]')
    (145, '    thr = config["cores"]')
    (146, '')
    (147, '    out_fq = "processed_reads/full_length_reads.fq"')
    (148, '    if os.path.isdir("processed_reads"):')
    (149, '        return out_fq')
    (150, '')
    (151, '    shell("mkdir -p processed_reads")')
    (152, '    if concat:')
    (153, '        print("Concatenating reads under directory: " + fq)')
    (154, '        shell("find %s  -regextype posix-extended -regex \\\'.*\\\\.(fastq|fq)$\\\' -exec cat {{}} \\\\\\\\; > processed_reads/input_reads.fq" % fq)')
    (155, '    else:')
    (156, '        shell("ln -s `realpath %s` processed_reads/input_reads.fq" % fq)')
    (157, '')
    (158, '    if config["run_pychopper"]:')
    (159, '        print("Running pychopper of fastq file: processed_reads/input_reads.fq")')
    (160, '        shell("(cd processed_reads; cdna_classifier.py -t %d %s input_reads.fq full_length_reads.fq)" % (thr, pc_opts))')
    (161, '    else:')
    (162, '        shell("ln -s `realpath processed_reads/input_reads.fq` processed_reads/full_length_reads.fq")')
    (163, '')
    (164, '    return out_fq')
    (165, '')
    (166, '')
    (167, 'ROOT = None')
    (168, 'DYNAMIC_RULES="job_rules.snk"')
    (169, 'if ((not os.path.isfile(os.path.join(WORKDIR,"sorted","sorted_reads.fastq"))) or (not os.path.isfile(os.path.join(SNAKEDIR, DYNAMIC_RULES)))):')
    (170, '    print("Preprocessing read in fastq file:", in_fastq)')
    (171, '    proc_fastq = preprocess_reads(in_fastq)')
    (172, '    print("Counting records in input fastq:", proc_fastq)')
    (173, '    nr_bases = count_fastq_bases(proc_fastq)')
    (174, '    print("Bases in input: {} megabases".format(int(nr_bases/10**6)))')
    (175, "    if config[\\'batch_size\\'] < 0:")
    (176, '        config[\\\'batch_size\\\'] = int(nr_bases/1000/config["cores"])')
    (177, '')
    (178, '    print("Batch size is: {}".format(config[\\\'batch_size\\\']))')
    (179, '')
    (180, '    init_cls_options = """ --batch-size {} --kmer-size {} --window-size {} --min-shared {} --min-qual {}\\\\')
    (181, '                         --mapped-threshold {} --aligned-threshold {} --min-fraction {} --min-prob-no-hits {} -M {} -P {} -g {} -c {} -F {} """')
    (182, '    init_cls_options = init_cls_options.format(config["batch_size"], config["kmer_size"], config["window_size"], config["min_shared"], config["min_qual"], \\\\')
    (183, '                        config["mapped_threshold"], config["aligned_threshold"], config["min_fraction"], config["min_prob_no_hits"], config["batch_max_seq"], config["consensus_period"],')
    (184, '    config["consensus_minimum"], config["consensus_maximum"], config["min_left_cls"])')
    (185, '')
    (186, '    shell("""')
    (187, '            rm -fr clusters sorted')
    (188, '            mkdir -p sorted; isONclust2 sort {} -v -o sorted {};')
    (189, '            mkdir -p clusters;')
    (190, '        """.format(init_cls_options, proc_fastq))')
    (191, '    JOB_TREE = OrderedDict()')
    (192, '    LEVELS = None')
    (193, '')
    (194, '    build_job_tree("sorted/batches")')
    (195, '    generate_rules(LEVELS, "{}/job_rules.snk".format(SNAKEDIR))')
    (196, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/prealignment, file=workflow/rules/common.smk
context_key: ['if t == "R']
    (73, 'def compile_output_list(wildcards: snakemake.io.Wildcards):')
    (74, '    output_files = [')
    (75, '        "prealignment/merged/{}_{}_{}.fastq.gz".format(sample, t, read)')
    (76, '        for sample in get_samples(samples)')
    (77, '        for t in get_unit_types(units, sample)')
    (78, '        for read in ["fastq1", "fastq2"]')
    (79, '    ]')
    (80, '    output_files.append(')
    (81, '        [')
    (82, '            "prealignment/sortmerna/{}_R.rrna.fq.gz".format(sample)')
    (83, '            for sample in get_samples(samples)')
    (84, '            for t in get_unit_types(units, sample)')
    (85, '            if t == "R"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=meringlab/og_consistency_pipeline, file=Snakefile
context_key: ['if node in species']
    (27, 'def read_level_hierarchy():')
    (28, '    # returns level hierarchy as dictionary {parent -> children}')
    (29, '    species = set()')
    (30, "    with open(config[\\'species_names\\']) as f:")
    (31, '        for line in f:')
    (32, "            l = line.rstrip().split(\\'\\\\t\\')")
    (33, '            species.add(l[0])')
    (34, '    ')
    (35, '    hierarchy = defaultdict(list)')
    (36, "    with open(config[\\'level_hierarchy\\']) as f:")
    (37, '        for line in f:')
    (38, '            node, parent = line.rstrip().split()')
    (39, '            if node in species:')
    (40, '                continue')
    (41, '            hierarchy[parent].append(node)')
    (42, '    return hierarchy')
    (43, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=meringlab/og_consistency_pipeline, file=Snakefile
context_key: ['if level_hierarchy is None']
    (44, 'def get_children_paths(wildcards):')
    (45, '    # returns location of children levels, using input_dir if leaf (no sub-levels)')
    (46, '    children_paths = []')
    (47, '')
    (48, '    global level_hierarchy')
    (49, '    if level_hierarchy is None:')
    (50, '        level_hierarchy = read_level_hierarchy()')
    (51, "    assert wildcards.level_id in level_hierarchy, \\'level_id %s not found in level hiearchy!\\'%wildcards.level_id")
    (52, '')
    (53, '    # return children location')
    (54, '    for child_id in level_hierarchy[wildcards.level_id]:')
    (55, '        if child_id in level_hierarchy:')
    (56, '            # inner level')
    (57, "            children_paths.append(path.join(config[\\'consistent_ogs\\'],\\'%s.tsv\\'%child_id))")
    (58, '        else:')
    (59, '            # leaf level')
    (60, "            children_paths.append(path.join(\\'preprocessed_data/orthologous_groups\\',\\'%s.tsv\\'%child_id))")
    (61, '')
    (62, '    return children_paths')
    (63, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=huishenlab/multiscale_methylation_plot_pipeline, file=workflow/Snakefile
context_key: ['if step < 1000']
    (32, 'def create_tag(step):')
    (33, '    if step < 1000: # Just bp')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=huishenlab/multiscale_methylation_plot_pipeline, file=workflow/Snakefile
context_key: ['elif step < 1000000']
    (32, 'def create_tag(step):')
    (33, '    if step < 1000: # Just bp')
    (34, "        return f\\'{step}bp\\'")
    (35, '    elif step < 1000000: # kilobases')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imc, file=workflow/Snakefile
context_key: ['if _sample_dict is None']
    (363, 'def get_sample_dict():')
    (364, '    global _sample_dict')
    (365, '    if _sample_dict is None:')
    (366, '        checkpoints.generate_sample_list.get()')
    (367, '        dat_samples = pd.read_csv(file_path_samples)')
    (368, '        _sample_dict = {pathlib.Path(x).stem: x for x in dat_samples[')
    (369, "            \\'mcd_folders\\']}")
    (370, '    return _sample_dict')
    (371, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/VariantCalling-snakemake, file=rules/functions.py
context_key: ['if get_sample_sex(wildcards.sample) == 2']
    (63, 'def get_intervals_by_sex(wildcards):')
    (64, '    if get_sample_sex(wildcards.sample) == 2 :')
    (65, '        chrs=config.get("call_chr").remove("chrY")')
    (66, '    else :')
    (67, '        chrs=config.get("call_chr")')
    (68, '    sex_aware_call_intervals=[ "wgs_calling_regions_%s.GRCh38.p13.interval_list" %(chrom) for chrom in chrs]')
    (69, '    return sex_aware_call_intervals')
    (70, '')
    (71, '#define the input combination for each sample in the all rule for the variant calling, based on sex')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/VariantCalling-snakemake, file=rules/functions.py
context_key: ["if sex == \\'2\\'"]
    (72, 'def call_variants_by_sex(base_path):')
    (73, '    sample_names = list(samples_df.SAMPLE_ID)')
    (74, '    # call_intervals=expand("wgs_calling_regions_{chr}.GRCh38.p13.interval_list", chr=config["call_chr"])')
    (75, '    all_samples_to_call =[]')
    (76, '    for sample in sample_names:')
    (77, '        sex=list(samples_df[samples_df.SAMPLE_ID == (sample).split(sep="_")[0]].sex)[0]')
    (78, "        if sex == \\'2\\' :")
    (79, "            # print(\\'female\\')")
    (80, '            # chrs=[ chrom for chrom in chroms if chrom != "chrY"] ')
    (81, '            chrs=[ chrom for chrom in config.get("call_chr") if chrom != "chrY"]')
    (82, '            # chrs=config.get("call_chr").remove("chrY")')
    (83, '        else :')
    (84, '            chrs=config.get("call_chr")')
    (85, '            # chrs=chroms')
    (86, '        sex_aware_call_intervals=[ "wgs_calling_regions_%s.GRCh38.p13.interval_list" %(chrom) for chrom in chrs]')
    (87, '        # print(len(sex_aware_call_intervals))')
    (88, '        # all_samples_to_call.append(expand("/{sample}/{sample}_{interval_name}_g.vcf.gz",sample=sample,interval_name=sex_aware_call_intervals))')
    (89, '        all_samples_to_call.append(expand(base_path + "/{sample}/{sample}_{interval_name}_g.vcf.gz",sample=sample,interval_name=sex_aware_call_intervals))')
    (90, '    all_samples_intervals = [sample_interval for sample_intervals in all_samples_to_call for sample_interval in sample_intervals]')
    (91, '')
    (92, '    return all_samples_intervals')
    (93, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/VariantCalling-snakemake, file=rules/functions.py
context_key: ["if not(re.match(\\'@\\', line.strip()))"]
    (94, 'def sample_names_from_interval(interval_name):')
    (95, '    interval_filename=interval_name')
    (96, '    interval_file=os.path.join(*references_abs_path(), config.get("callable_intervals"),interval_filename)')
    (97, "    for line in open(interval_file, \\'r\\'):")
    (98, "        if not(re.match(\\'@\\', line.strip())):")
    (99, '            interval_chr=line.strip().split("\\\\t")[0]')
    (100, '            break')
    (101, '    current_chr=interval_chr')
    (102, '    if current_chr == "chrY" :')
    (103, "        samples=list(samples_df[samples_df.sex == \\'1\\'].SAMPLE_ID)")
    (104, '    else :')
    (105, '        samples = list(samples_df.SAMPLE_ID)')
    (106, '')
    (107, '    return samples')
    (108, '')
    (109, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/VariantCalling-snakemake, file=rules/functions.py
context_key: ["if not(re.match(\\'@\\', line.strip()))"]
    (110, 'def get_chr_from_interval_list(wildcards):')
    (111, '    interval_filename=wildcards.interval_name')
    (112, '    interval_file=os.path.join(*references_abs_path(), config.get("callable_intervals"),interval_filename)')
    (113, "    for line in open(interval_file, \\'r\\'):")
    (114, "        if not(re.match(\\'@\\', line.strip())):")
    (115, '            interval_chr=line.strip().split("\\\\t")[0]')
    (116, '            break')
    (117, '')
    (118, '    return interval_chr')
    (119, '')
    (120, '#get the chromosome name from the input vcf file')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/VariantCalling-snakemake, file=rules/functions.py
context_key: ["if not(re.match(\\'#\\', line.strip()))"]
    (121, 'def get_chr_from_vcf(vcf):')
    (122, '    vcf_filename=vcf')
    (123, "    # for line in open(vcf_filename, \\'r\\'):")
    (124, "    for line in gzip.open(vcf_filename, \\'r\\') if vcf_filename.endswith(\\'.gz\\') else open(vcf_filename, \\'r\\'):")
    (125, "        if not(re.match(\\'#\\', line.strip())):")
    (126, '            vcf_chr=line.strip().split("\\\\t")[0]')
    (127, '            break')
    (128, '')
    (129, '    return vcf_chr')
    (130, '')
    (131, '#get path of an existing GenomicsDBImport database in the "update" branch of the joint calling workflow')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/VariantCalling-snakemake, file=rules/functions.py
context_key: ["if not(re.match(\\'#\\', line.strip()))"]
    (132, 'def get_db_path(db_path_file):')
    (133, "    for line in open(db_path_file, \\'r\\'):")
    (134, "        if not(re.match(\\'#\\', line.strip())):")
    (135, '            db_path=line.strip()')
    (136, '            break')
    (137, '    return db_path')
    (138, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/VariantCalling-snakemake, file=rules/functions.py
context_key: ["if not(re.match(\\'@\\', line.strip()))", "if not(re.match(\\'chrY\\',interval_chr))", "if not(re.match(\\'chrX\\',interval_chr))"]
    (159, 'def get_sex_chr_from_interval(wildcards):')
    (160, '    interval_filename=wildcards.interval_name')
    (161, '    interval_file=os.path.join(*references_abs_path(), config.get("callable_intervals"),interval_filename)')
    (162, "    for line in open(interval_file, \\'r\\'):")
    (163, "        if not(re.match(\\'@\\', line.strip())):")
    (164, '            interval_chr=line.strip().split("\\\\t")[0]')
    (165, "            if not(re.match(\\'chrY\\',interval_chr)):")
    (166, "                if not(re.match(\\'chrX\\',interval_chr)):")
    (167, '                    chr_mode="AUTOSOMAL"')
    (168, '                    break')
    (169, '                else:')
    (170, '                    #need to check if we are working with PAR regions')
    (171, '                    # chrX    10001   2781479 PAR1')
    (172, '                    # chrX    155701383       156030895       PAR2')
    (173, '                    start=int(line.strip().split("\\\\t")[1])')
    (174, '                    end=int(line.strip().split("\\\\t")[2])')
    (175, '                    #we are checking if we are outside the par regions')
    (176, '#                   (c[1] <= 10001 || (c[1] >= 2781479 && c[1] <= 155701383) || c[1] >= 156030895) && (c[2] <= 10001 ||( c[2] >= 2781479 && c[2] <= 155701383)|| c[2] >= 156030895) ) print $0}\\\' | wc -l | cut -f 1 -d " ")')
    (177, '                    if (start < 10001 or start > 2781479) and (start < 155701383 or start > 156030895) and (end < 10001 or end > 2781479) and (end < 155701383 or end > 156030895) :')
    (178, '                        chr_mode="SEXUAL"')
    (179, '                        break')
    (180, '                    else:')
    (181, '                        chr_mode="AUTOSOMAL"')
    (182, '                        break')
    (183, '            else:')
    (184, '                chr_mode="SEXUAL"')
    (185, '                break')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/VariantCalling-snakemake, file=rules/functions.py
context_key: ["if not(re.match(\\'@\\', line.strip()))"]
    (159, 'def get_sex_chr_from_interval(wildcards):')
    (160, '    interval_filename=wildcards.interval_name')
    (161, '    interval_file=os.path.join(*references_abs_path(), config.get("callable_intervals"),interval_filename)')
    (162, "    for line in open(interval_file, \\'r\\'):")
    (163, "        if not(re.match(\\'@\\', line.strip())):")
    (164, '            interval_chr=line.strip().split("\\\\t")[0]')
    (165, "            if not(re.match(\\'chrY\\',interval_chr)):")
    (166, "                if not(re.match(\\'chrX\\',interval_chr)):")
    (167, '                    chr_mode="AUTOSOMAL"')
    (168, '                    break')
    (169, '                else:')
    (170, '                    #need to check if we are working with PAR regions')
    (171, '                    # chrX    10001   2781479 PAR1')
    (172, '                    # chrX    155701383       156030895       PAR2')
    (173, '                    start=int(line.strip().split("\\\\t")[1])')
    (174, '                    end=int(line.strip().split("\\\\t")[2])')
    (175, '                    #we are checking if we are outside the par regions')
    (176, '#                   (c[1] <= 10001 || (c[1] >= 2781479 && c[1] <= 155701383) || c[1] >= 156030895) && (c[2] <= 10001 ||( c[2] >= 2781479 && c[2] <= 155701383)|| c[2] >= 156030895) ) print $0}\\\' | wc -l | cut -f 1 -d " ")')
    (177, '                    if (start < 10001 or start > 2781479) and (start < 155701383 or start > 156030895) and (end < 10001 or end > 2781479) and (end < 155701383 or end > 156030895) :')
    (178, '                        chr_mode="SEXUAL"')
    (179, '                        break')
    (180, '                    else:')
    (181, '                        chr_mode="AUTOSOMAL"')
    (182, '                        break')
    (183, '            else:')
    (184, '                chr_mode="SEXUAL"')
    (185, '                break')
    (186, '    return chr_mode')
    (187, '')
    (188, '')
    (189, '#function to get parameters needed to perform VQSR step with GATK')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/VariantCalling-snakemake, file=rules/functions.py
context_key: ['if wildcards.type == "snp"']
    (190, 'def _get_recal_params(wildcards):')
    (191, '    known_variants = resolve_multi_filepath("", config["known_variants"])')
    (192, '    if wildcards.type == "snp":')
    (193, '        base_params=("--resource:hapmap,known=false,training=true,truth=true,prior=15.0 {hapmap} --resource:omni,known=false,training=true,truth=true,prior=12.0 {omni} --resource:1000G,known=false,training=true,truth=false,prior=10.0 {g1k} --resource:dbsnp,known=true,training=false,truth=false,prior=7.0 {dbsnp}").format(**known_variants)')
    (194, '        tranches=config["rules"]["gatk_variant_recalibrator"]["SNP"]["tranches"]')
    (195, '        annotations=config["rules"]["gatk_variant_recalibrator"]["SNP"]["annotations"]')
    (196, '        arguments=config["rules"]["gatk_variant_recalibrator"]["SNP"]["arguments"]')
    (197, '    else:')
    (198, '        base_params=("--resource:mills,known=false,training=true,truth=true,prior=12.0 {mills} --resource:axiomPoly,known=false,training=true,truth=false,prior=10 {axiom} --resource:dbsnp,known=true,training=false,truth=false,prior=2.0 {dbsnp}").format(**known_variants)')
    (199, '        tranches=config["rules"]["gatk_variant_recalibrator"]["INDEL"]["tranches"]')
    (200, '        annotations=config["rules"]["gatk_variant_recalibrator"]["INDEL"]["annotations"]')
    (201, '        arguments=config["rules"]["gatk_variant_recalibrator"]["INDEL"]["arguments"]')
    (202, '    return "%s %s %s %s" %(base_params, tranches,annotations,arguments)')
    (203, '')
    (204, '')
    (205, '# define a function to get annotated chromosomes for a final concatenated file, but excluding chrY')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/rules/functions.smk
context_key: ['if len(asm_sample_dict) >= max_datasets']
    (35, 'def identify_assembly_sample():')
    (36, '    """ Parse the sqlite database to identify the assembly genome names."""')
    (37, '    asm_sample_dict = {}')
    (38, '    sqlite_db_path = os.path.join(results_dir,"sqlite_db",config["sqlite_db"])')
    (39, '    conn = sqlite3.connect(sqlite_db_path)')
    (40, '    cur = conn.cursor()')
    (41, '    max_datasets = config["max_datasets_assembly"]')
    (42, '')
    (43, '    # Assembled Genome URLs')
    (44, '    asm_fna_urls = cur.execute(config["sqlite_select_command_asm"]).fetchall()')
    (45, '    for url_list in asm_fna_urls:')
    (46, '        if len(asm_sample_dict) >= max_datasets:')
    (47, '            break')
    (48, '        for url in url_list[0].split(";"):')
    (49, '            if url:')
    (50, '                asm_name = url.split("/")[9] + "_genomic"')
    (51, '                asm_sample_dict[asm_name] = [asm_name]')
    (52, '    cur.close()')
    (53, '    return asm_sample_dict')
    (54, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/rules/functions.smk
context_key: ['if len(asm_ftp_list) >= max_datasets']
    (55, 'def identify_assembly_ftp():')
    (56, '    """ Parse the sqlite database to identify the assembly genome name."""')
    (57, '    sqlite_db_path = os.path.join(results_dir,"sqlite_db",config["sqlite_db"])')
    (58, '    conn = sqlite3.connect(sqlite_db_path)')
    (59, '    cur = conn.cursor()')
    (60, '    # Assembled Genome URLs')
    (61, '    asm_fna_urls = cur.execute(config["sqlite_select_command_asm"]).fetchall()')
    (62, '    asm_ftp_list = []')
    (63, '    max_datasets = config["max_datasets_assembly"]')
    (64, '    for url_list in asm_fna_urls:')
    (65, '        for url in url_list[0].split(";"):')
    (66, '            if len(asm_ftp_list) >= max_datasets:')
    (67, '                break')
    (68, '            if url:')
    (69, '                #asm_ftp_list.append(url + "/"+ url.split("/")[9] + "_genomic.fna.gz")')
    (70, '                asm_ftp_list.append(url + "/"+ url.split("/")[9] + "_genomic")')
    (71, '    cur.close()')
    (72, '    return asm_ftp_list')
    (73, '')
    (74, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/rules/functions.smk
context_key: ['if len(sra_sample_dict) >= max_datasets']
    (75, 'def identify_sra_sample():')
    (76, '    """')
    (77, '    Parse the sqlite database to identify the SRA accessions.')
    (78, '    Return a dictionary of accessions and layouts.')
    (79, '    """')
    (80, '    sra_sample_dict = {}')
    (81, '    sqlite_db_path = os.path.join(results_dir,"sqlite_db",config["sqlite_db"])')
    (82, '    conn = sqlite3.connect(sqlite_db_path)')
    (83, '    cur = conn.cursor()')
    (84, '    sra_fetch = cur.execute(config["sqlite_select_command_sra"]).fetchall()')
    (85, '    max_datasets = config["max_datasets_sra"]')
    (86, '    for record in sra_fetch:')
    (87, '        if len(sra_sample_dict) >= max_datasets:')
    (88, '            break')
    (89, '        if record:')
    (90, '            file_acc = record[1].split(";")')
    (91, '            # Duplicate the biosample accession to make it equivalent to sra')
    (92, '            biosample = record[0]')
    (93, '            if biosample not in sra_sample_dict:')
    (94, '                sra_sample_dict[biosample] = []')
    (95, '            sra_sample_dict[biosample].extend(file_acc)')
    (96, '    cur.close()')
    (97, '    return sra_sample_dict')
    (98, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/rules/functions.smk
context_key: ['if sample != dir']
    (99, 'def identify_local_sample():')
    (100, '    """')
    (101, '    Parse the sqlite database to identify the local samples.')
    (102, '    Return a dictionary of accessions and layouts.')
    (103, '    """')
    (104, '    data_dir = os.path.join(results_dir, "data", "local")')
    (105, '    local_sample_dict = {}')
    (106, '    sqlite_db_path = os.path.join(results_dir,"sqlite_db",config["sqlite_db"])')
    (107, '    conn = sqlite3.connect(sqlite_db_path)')
    (108, '    cur = conn.cursor()')
    (109, '    sra_fetch = cur.execute(config["sqlite_select_command_local"]).fetchall()')
    (110, '    # Iterate through records in database')
    (111, '    for record in sra_fetch:')
    (112, '        sample = record[0]')
    (113, '        local_sample_dict[sample] = []')
    (114, '        for dir in os.listdir(data_dir):')
    (115, '            if sample != dir: continue')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/rules/functions.smk
context_key: ['if ".fastq.gz" in file', 'if filename not in local_sample_dict[sample]']
    (99, 'def identify_local_sample():')
    (100, '    """')
    (101, '    Parse the sqlite database to identify the local samples.')
    (102, '    Return a dictionary of accessions and layouts.')
    (103, '    """')
    (104, '    data_dir = os.path.join(results_dir, "data", "local")')
    (105, '    local_sample_dict = {}')
    (106, '    sqlite_db_path = os.path.join(results_dir,"sqlite_db",config["sqlite_db"])')
    (107, '    conn = sqlite3.connect(sqlite_db_path)')
    (108, '    cur = conn.cursor()')
    (109, '    sra_fetch = cur.execute(config["sqlite_select_command_local"]).fetchall()')
    (110, '    # Iterate through records in database')
    (111, '    for record in sra_fetch:')
    (112, '        sample = record[0]')
    (113, '        local_sample_dict[sample] = []')
    (114, '        for dir in os.listdir(data_dir):')
    (115, '            if sample != dir: continue')
    (116, '            sample_dir = os.path.join(data_dir, dir)')
    (117, '            for file in os.listdir(sample_dir):')
    (118, '                if ".fastq.gz" in file:')
    (119, '                    filename = file.split("_")[0]')
    (120, '                    if filename not in local_sample_dict[sample]:')
    (121, '                        local_sample_dict[sample].append(filename)')
    (122, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/rules/functions.smk
context_key: ['if ".fastq.gz" in file']
    (99, 'def identify_local_sample():')
    (100, '    """')
    (101, '    Parse the sqlite database to identify the local samples.')
    (102, '    Return a dictionary of accessions and layouts.')
    (103, '    """')
    (104, '    data_dir = os.path.join(results_dir, "data", "local")')
    (105, '    local_sample_dict = {}')
    (106, '    sqlite_db_path = os.path.join(results_dir,"sqlite_db",config["sqlite_db"])')
    (107, '    conn = sqlite3.connect(sqlite_db_path)')
    (108, '    cur = conn.cursor()')
    (109, '    sra_fetch = cur.execute(config["sqlite_select_command_local"]).fetchall()')
    (110, '    # Iterate through records in database')
    (111, '    for record in sra_fetch:')
    (112, '        sample = record[0]')
    (113, '        local_sample_dict[sample] = []')
    (114, '        for dir in os.listdir(data_dir):')
    (115, '            if sample != dir: continue')
    (116, '            sample_dir = os.path.join(data_dir, dir)')
    (117, '            for file in os.listdir(sample_dir):')
    (118, '                if ".fastq.gz" in file:')
    (119, '                    filename = file.split("_")[0]')
    (120, '                    if filename not in local_sample_dict[sample]:')
    (121, '                        local_sample_dict[sample].append(filename)')
    (122, '')
    (123, '    return local_sample_dict')
    (124, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/rules/functions.smk
context_key: ['if reads_origin != "all"']
    (134, 'def identify_paths(outdir=None, reads_origin=None):')
    (135, '    origin_dir = os.path.join(results_dir, outdir, reads_origin)')
    (136, '    sample_dict = globals()["identify_" + reads_origin + "_sample"]()')
    (137, '')
    (138, "    # the dictionary format will be different if reads_origin is \\'all\\'")
    (139, '    if reads_origin != "all":')
    (140, '        sample_dirs = list(itertools.chain.from_iterable(')
    (141, '                       [[key] * len(sample_dict[key]) for key in sample_dict]')
    (142, '                       )')
    (143, '                     )')
    (144, '        samples = list(itertools.chain.from_iterable(sample_dict.values()))')
    (145, '        paths = expand(origin_dir + "/{sample_dir}/{sample}",')
    (146, '                             zip,')
    (147, '                             sample_dir=sample_dirs,')
    (148, '                             sample=samples)')
    (149, '')
    (150, '    else:')
    (151, '        paths = []')
    (152, '        for origin in sample_dict.keys():')
    (153, '            origin_dir = origin_dir = os.path.join(results_dir, outdir, origin)')
    (154, '            origin_dict = sample_dict[origin]')
    (155, '            sample_dirs = list(itertools.chain.from_iterable(')
    (156, '                               [[key] * len(origin_dict[key]) for key in origin_dict]')
    (157, '                              )')
    (158, '                          )')
    (159, '')
    (160, '  \\t    samples = list(itertools.chain.from_iterable(origin_dict.values()))')
    (161, '            paths = paths + expand(origin_dir + "/{sample_dir}/{sample}",')
    (162, '                             zip,')
    (163, '                             sample_dir=sample_dirs,')
    (164, '                             sample=samples)')
    (165, '')
    (166, '    return paths')
    (167, '')
    (168, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/rules/functions.smk
context_key: ['if item not in res_list']
    (183, 'def remove_duplicates(dup_list):')
    (184, "  \\'\\'\\'Remove duplicates from a list.\\'\\'\\'")
    (185, '  res_list = []')
    (186, '  for item in dup_list:')
    (187, '    if item not in res_list:')
    (188, '       res_list.append(item)')
    (189, '  return res_list')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=csoneson/WagnerEMT2020, file=Snakefile
context_key: ['if not os.path.isfile(config["metatxt"])']
    (15, 'def sanitizefile(str):')
    (16, '\\tif str is None:')
    (17, "\\t\\tstr = \\'\\'")
    (18, '\\treturn str')
    (19, '')
    (20, "config[\\'txome\\'] = sanitizefile(config[\\'txome\\'])")
    (21, "config[\\'gtf\\'] = sanitizefile(config[\\'gtf\\'])")
    (22, "config[\\'genome\\'] = sanitizefile(config[\\'genome\\'])")
    (23, "config[\\'STARindex\\'] = sanitizefile(config[\\'STARindex\\'])")
    (24, "config[\\'salmonindex\\'] = sanitizefile(config[\\'salmonindex\\'])")
    (25, "config[\\'chrnames\\'] = sanitizefile(config[\\'chrnames\\'])")
    (26, "config[\\'metatxt\\'] = sanitizefile(config[\\'metatxt\\'])")
    (27, '')
    (28, '## Read metadata')
    (29, 'if not os.path.isfile(config["metatxt"]):')
    (30, '  sys.exit("Metadata file " + config["metatxt"] + " does not exist.")')
    (31, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=csoneson/WagnerEMT2020, file=Snakefile
context_key: ["if not set([\\'names\\',\\'type\\']).issubset(samples.columns)"]
    (15, 'def sanitizefile(str):')
    (16, '\\tif str is None:')
    (17, "\\t\\tstr = \\'\\'")
    (18, '\\treturn str')
    (19, '')
    (20, "config[\\'txome\\'] = sanitizefile(config[\\'txome\\'])")
    (21, "config[\\'gtf\\'] = sanitizefile(config[\\'gtf\\'])")
    (22, "config[\\'genome\\'] = sanitizefile(config[\\'genome\\'])")
    (23, "config[\\'STARindex\\'] = sanitizefile(config[\\'STARindex\\'])")
    (24, "config[\\'salmonindex\\'] = sanitizefile(config[\\'salmonindex\\'])")
    (25, "config[\\'chrnames\\'] = sanitizefile(config[\\'chrnames\\'])")
    (26, "config[\\'metatxt\\'] = sanitizefile(config[\\'metatxt\\'])")
    (27, '')
    (28, '## Read metadata')
    (29, 'if not os.path.isfile(config["metatxt"]):')
    (30, '  sys.exit("Metadata file " + config["metatxt"] + " does not exist.")')
    (31, '')
    (32, 'import pandas as pd')
    (33, 'samples = pd.read_csv(config["metatxt"], sep=\\\'\\\\t\\\')')
    (34, '')
    (35, "if not set([\\'names\\',\\'type\\']).issubset(samples.columns):")
    (36, '  sys.exit("Make sure \\\'names\\\' and \\\'type\\\' are columns in " + config["metatxt"])')
    (37, '')
    (38, '')
    (39, '## Sanitize provided input and output directories')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ibebio/tiger-pipeline, file=workflow/rules/common.smk
context_key: ['if len(fastqs) == 2']
    (28, 'def get_fastq(wildcards):')
    (29, '    """Get fastq files of given sample."""')
    (30, '    fastqs = samples.loc[(wildcards.sample), ["fq1", "fq2"]].dropna()')
    (31, '    if len(fastqs) == 2:')
    (32, '        return {"r1": fastqs.fq1, "r2": fastqs.fq2}')
    (33, '    return {"r1": fastqs.fq1}')
    (34, '')
    (35, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ibebio/tiger-pipeline, file=workflow/rules/common.smk
context_key: ['if f.read().strip() == "0"']
    (102, 'def get_corrected_refined_breaks_files(wildcards):')
    (103, '    """ Get the filenames of all breaks_files for a crossing id """')
    (104, '    f2_sample_names = [c["f2_samples"] for c in config["crossings"] if c["id"] == wildcards.crossing_id]')
    (105, '    assert len(f2_sample_names) == 1')
    (106, '    f2_sample_names = f2_sample_names[0]')
    (107, '    corrected_refined_breaks_files = []')
    (108, '    for f2_sample in f2_sample_names:')
    (109, '        with checkpoints.tiger_beta_mixture_model.get(f2_sample=f2_sample, crossing_id=wildcards.crossing_id).output[0].open() as f:')
    (110, '            # Was the beta mixture model successfully generated?')
    (111, '            if f.read().strip() == "0":')
    (112, '                corrected_refined_breaks_files.append(')
    (113, '                    "results/tiger_analysis/F2.{crossing_id}/rough_co_breaks_refined_corrected/{f2_sample}.corrected.refined.breaks.txt".format(basedir=workflow.basedir, f2_sample=f2_sample, crossing_id=wildcards.crossing_id)')
    (114, '                    )')
    (115, '    return corrected_refined_breaks_files')
    (116, '')
    (117, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ibebio/tiger-pipeline, file=workflow/rules/common.smk
context_key: ['if f.read().strip() == "0"']
    (118, 'def get_plot_files_for_crossing_id(wildcards):')
    (119, '    """ Get the filenames of all plot files for a crossing id """')
    (120, '    f2_sample_names = [c["f2_samples"] for c in config["crossings"] if c["id"] == wildcards.crossing_id]')
    (121, '    assert len(f2_sample_names) == 1')
    (122, '    f2_sample_names = f2_sample_names[0]')
    (123, '')
    (124, '    plot_files = []')
    (125, '    for f2_sample in f2_sample_names:')
    (126, '        with checkpoints.tiger_beta_mixture_model.get(f2_sample=f2_sample, crossing_id=wildcards.crossing_id).output[0].open() as f:')
    (127, '            # Was the beta mixture model successfully generated?')
    (128, '            if f.read().strip() == "0":')
    (129, '                plot_files.append(')
    (130, '                    "results/plots/F2.{crossing_id}/{f2_sample}.pdf".format(crossing_id=wildcards.crossing_id, f2_sample=f2_sample)')
    (131, '                )')
    (132, '    return plot_files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/hemoMIPs, file=Snakefile
context_key: ['if line.startswith("#")']
    (155, 'def loadSamples(wc):')
    (156, '    file = open("input/%s/sample_index.lst" % wc.dataset, "r")')
    (157, '    output=[]')
    (158, '    for line in file:')
    (159, '      if line.startswith("#"):')
    (160, '          continue')
    (161, '      output.append(line.split("\\\\t")[-1].strip())')
    (162, '    return(output)')
    (163, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JamieCFreeman/DGN_compatible, file=Snakefile
context_key: ['wildcard_constraints', 'include', 'if ROUND == 1', 'elif ROUND == 2', 'rule all', 'rule qc', 'rule round2_index', 'rule mem_stats', 'rule test', 'rule bwa_aln_1', 'rule bwa_aln_2', 'rule bwa_sampe', 'rule aln_flagstat', 'rule bwa_mem', 'rule flagstat', 'rule stampy_map', 'rule sam2bam', 'rule stampy_flagstat', 'rule qfilter_bam', 'rule sort_bam', 'rule mark_dups', 'rule add_RG', 'rule dup_bam_bai', 'input']
    (48, 'def RG_from_sample(wildcards):')
    (49, '\\treturn samples_table.loc[wildcards.sample, "RG"]')
    (50, '')
    (51, 'include: ')
    (52, '\\t"rules/genome_indexing.smk",')
    (53, '\\t"rules/qc.smk"')
    (54, '')
    (55, "# Desired output depends on whether we\\'re running round 1 or 2")
    (56, 'index_check = f"{OUTDIR}/round{ROUND}_index.ok"')
    (57, 'test_fq = expand("test/{sample}_1.fq", sample=SAMPLES)')
    (58, 'alt_ref = expand(f"{OUTDIR}/round1/alt_ref_for_chtc/{{sample}}_ref.fasta.tgz", sample=SAMPLES)')
    (59, 'allsites_vcf = expand(f"{OUTDIR}/round2/vcf/{{sample}}_round2_SNPs.vcf.gz", sample=SAMPLES)')
    (60, '')
    (61, 'rule_all_input_list=[index_check]')
    (62, 'round1_input_list=[test_fq, alt_ref]')
    (63, 'round2_input_list=[allsites_vcf]')
    (64, '')
    (65, 'if ROUND == 1:')
    (66, '\\trule_all_input_list.extend(round1_input_list)')
    (67, '\\tprint("Doing round 1 mapping")')
    (68, 'elif ROUND == 2:')
    (69, '\\trule_all_input_list.extend(round2_input_list)')
    (70, '\\tprint("Doing round 2 mapping")')
    (71, '')
    (72, 'rule all:')
    (73, '\\tinput:')
    (74, '\\t\\trule_all_input_list')
    (75, '')
    (76, '')
    (77, "# Rule qc doesn\\'t work")
    (78, 'rule qc:')
    (79, '\\tinput:')
    (80, '#\\t\\texpand(f"{OUTDIR}/round{ROUND}/qc/multiqc/{{pre}}_multiqc_report.html", pre=config["prefix"])\\t')
    (81, '\\t\\texpand(f"{OUTDIR}/round{ROUND}/qc/bamqc/{{sample}}_stats", sample=SAMPLES)')
    (82, '')
    (83, 'rule round2_index:')
    (84, '\\tinput:')
    (85, '\\t\\texpand(f"{OUTDIR}/round2_{{sample}}_index.ok", sample=SAMPLES),')
    (86, '\\t\\tf"{OUTDIR}/round2_index.ok"')
    (87, '')
    (88, 'rule mem_stats:')
    (89, '\\tinput:')
    (90, '\\t\\texpand(f"{OUTDIR}/logs/bwa_mem/{{sample}}.stats", sample=SAMPLES)')
    (91, '')
    (92, 'rule test:')
    (93, '\\tinput:')
    (94, "\\t\\tfq1 = lambda wc: fq_from_sample(wc, \\'FALSE\\', \\'1\\'),")
    (95, "\\t\\tfq2 = lambda wc: fq_from_sample(wc, \\'FALSE\\', \\'2\\'),")
    (96, '\\t\\tindex_ok = f"{OUTDIR}/round{ROUND}_index.ok"')
    (97, '\\toutput:')
    (98, '\\t\\tcut1 = "test/{sample}_1.fq",')
    (99, '\\t\\tcut2 = "test/{sample}_2.fq"')
    (100, '\\tshell:')
    (101, '\\t\\t"zcat {input.fq1} | awk \\\'(NR<=50000)\\\' > {output.cut1}; zcat {input.fq2} | awk \\\'(NR<=50000)\\\' > {output.cut2}"')
    (102, '')
    (103, 'rule bwa_aln_1:')
    (104, '\\tinput:')
    (105, "\\t\\tfq1 = lambda wc: fq_from_sample(wc, ISTESTING, \\'1\\'),")
    (106, '\\t\\tREF = lambda wc: get_ref_fa(wc, ROUND, OUTDIR) #,')
    (107, '#\\t\\tindex = f"{OUTDIR}/round{ROUND}_index.ok"')
    (108, '\\toutput:')
    (109, '\\t\\tsai1 = temp( f"{OUTDIR}/round{ROUND}/bwa_aln/{{sample}}_1.sai")')
    (110, '\\tlog: f"{OUTDIR}/round{ROUND}/logs/bwa_aln/{{sample}}_map1.log"')
    (111, '\\tthreads: 10')
    (112, '\\tshell:')
    (113, '\\t\\t"bwa aln -t {threads} {input.REF} {input.fq1} 2> {log} > {output.sai1}"')
    (114, '')
    (115, 'rule bwa_aln_2:')
    (116, '\\tinput:  ')
    (117, "\\t\\tfq2 = lambda wc: fq_from_sample(wc, ISTESTING, \\'2\\'),")
    (118, '\\t\\tREF = lambda wc: get_ref_fa(wc, ROUND, OUTDIR) #,')
    (119, '#\\t\\tindex = f"{OUTDIR}/round{ROUND}_index.ok"')
    (120, '\\toutput: ')
    (121, '\\t\\tsai2 = temp(f"{OUTDIR}/round{ROUND}/bwa_aln/{{sample}}_2.sai")')
    (122, '\\tlog: f"{OUTDIR}/round{ROUND}/logs/bwa_aln/{{sample}}_map2.log"')
    (123, '\\tthreads: 10')
    (124, '\\tshell:  ')
    (125, '\\t\\t"bwa aln -t {threads} {input.REF} {input.fq2} 2> {log} > {output.sai2}"')
    (126, '')
    (127, 'rule bwa_sampe:')
    (128, '\\tinput:')
    (129, '\\t\\tsai1 = f"{OUTDIR}/round{ROUND}/bwa_aln/{{sample}}_1.sai",')
    (130, '\\t\\tsai2 = f"{OUTDIR}/round{ROUND}/bwa_aln/{{sample}}_2.sai",')
    (131, "\\t\\tfq1 = lambda wc: fq_from_sample(wc, ISTESTING, \\'1\\'),")
    (132, "\\t\\tfq2 = lambda wc: fq_from_sample(wc, ISTESTING, \\'2\\'),")
    (133, '\\t\\tREF = lambda wc: get_ref_fa(wc, ROUND, OUTDIR)')
    (134, '\\toutput: ')
    (135, '\\t\\ttemp(f"{OUTDIR}/round{ROUND}/bwa_aln/{{sample}}.bam")')
    (136, '\\tlog: f"{OUTDIR}/round{ROUND}/logs/bwa_sampe/{{sample}}_map2.log"')
    (137, '\\tshell:')
    (138, '\\t\\t"bwa sampe -P {input.REF} {input.sai1} {input.sai2} {input.fq1} {input.fq2} 2> {log} | samtools view -bS - > {output}"')
    (139, '')
    (140, 'rule aln_flagstat:')
    (141, '\\tinput:')
    (142, '\\t\\tf"{OUTDIR}/round{ROUND}/bwa_aln/{{sample}}.bam"')
    (143, '\\toutput:')
    (144, '\\t\\tf"{OUTDIR}/round{ROUND}/logs/bwa_aln/{{sample}}.stats"')
    (145, '\\tshell:')
    (146, '\\t\\t"samtools flagstat {input} > {output}"')
    (147, 'rule bwa_mem:')
    (148, '\\tinput:')
    (149, "\\t\\tfq1 = lambda wc: fq_from_sample(wc, ISTESTING, \\'1\\'),")
    (150, "\\t\\tfq2 = lambda wc: fq_from_sample(wc, ISTESTING, \\'2\\'),")
    (151, '\\t\\tREF = lambda wc: get_ref_fa(wc, ROUND, OUTDIR)')
    (152, '\\toutput:')
    (153, '\\t\\tf"{OUTDIR}/bwa_mem/{{sample}}.bam" ')
    (154, '\\tparams: REF = config["genome"]')
    (155, '\\tlog:  f"{OUTDIR}/logs/bwa_mem/{{sample}}.log"')
    (156, '\\tthreads: 8')
    (157, '\\tshell:')
    (158, '\\t\\t"bwa mem -M -t {threads} {input.REF} {input.fq1} {input.fq2} 2> {log} | samtools view -bS - > {output}"')
    (159, '')
    (160, 'rule flagstat:')
    (161, '\\tinput:')
    (162, '\\t\\tf"{OUTDIR}/bwa_mem/{{sample}}.bam"')
    (163, '\\toutput:')
    (164, '\\t\\tf"{OUTDIR}/logs/bwa_mem/{{sample}}.stats"')
    (165, '\\tshell:')
    (166, '\\t\\t"samtools flagstat {input} > {output}"')
    (167, '')
    (168, 'rule stampy_map:')
    (169, '\\tinput:')
    (170, '\\t\\tbam = f"{OUTDIR}/round{ROUND}/bwa_aln/{{sample}}.bam",')
    (171, '\\t\\tREF = lambda wc: get_ref_fa(wc, ROUND, OUTDIR)')
    (172, '\\toutput:')
    (173, '\\t\\ttemp(f"{OUTDIR}/round{ROUND}/stampy/{{sample}}.sam")')
    (174, '\\tparams: stampy = "/opt/bioscript/stampy/stampy.py"')
    (175, '\\tlog: f"{OUTDIR}/round{ROUND}/logs/stampy/{{sample}}.log"')
    (176, '\\tconda: "envs/py2.yaml"')
    (177, '\\tshell:')
    (178, '\\t\\t"python {params.stampy} -g {input.REF} -h {input.REF} --bamkeepgoodreads -M {input.bam} -o {output} 2> {log} "')
    (179, '# "python2.6 " . $stampy . "stampy.py -g " . $reference . " -h " . $reference . " --bamkeepgoodreads -M " . $FastqFile[$i] . ".bam -o " . $FastqFile[$i] . "_remapped.sam"; ')
    (180, '# #Stampy MAPPING STEP. THIS IS A RELATIVELY LONG STEP (AS LONG AS 15 HOURS ON SOME OF THE HIGHEST COVERAGE DPGP2 GENOMES)')
    (181, '')
    (182, 'rule sam2bam:')
    (183, '\\tinput:')
    (184, '\\t\\tf"{OUTDIR}/round{ROUND}/stampy/{{sample}}.sam"')
    (185, '\\toutput:')
    (186, '\\t\\ttemp(f"{OUTDIR}/round{ROUND}/stampy/{{sample}}.bam")')
    (187, '\\tshell:')
    (188, '\\t\\t"samtools view -bS {input} > {output}"')
    (189, '')
    (190, 'rule stampy_flagstat:')
    (191, '\\tinput:')
    (192, '\\t\\tf"{OUTDIR}/round{ROUND}/stampy/{{sample}}.bam"')
    (193, '\\toutput:')
    (194, '\\t\\tf"{OUTDIR}/round{ROUND}/logs/stampy/{{sample}}.stats"')
    (195, '\\tshell:')
    (196, '\\t\\t"samtools flagstat {input} > {output}"')
    (197, '\\t\\t')
    (198, 'rule qfilter_bam:')
    (199, '\\tinput:')
    (200, '\\t\\tbam = f"{OUTDIR}/round{ROUND}/stampy/{{sample}}.bam",')
    (201, '\\t\\tstats = f"{OUTDIR}/round{ROUND}/logs/stampy/{{sample}}.stats"')
    (202, '\\toutput:')
    (203, '\\t\\ttemp(f"{OUTDIR}/round{ROUND}/stampy/qfilter_{{sample}}.bam")')
    (204, '\\tshell:')
    (205, '\\t\\t"samtools view -q 20 -h {input.bam} > {output}"')
    (206, '')
    (207, 'rule sort_bam:')
    (208, '\\tinput:')
    (209, '\\t\\tf"{OUTDIR}/round{ROUND}/stampy/qfilter_{{sample}}.bam"')
    (210, '\\toutput:')
    (211, '\\t\\ttemp(f"{OUTDIR}/round{ROUND}/stampy/qfilter_{{sample}}_sort.bam")')
    (212, '\\tshell:')
    (213, '\\t\\t"samtools sort {input} > {output}"')
    (214, '')
    (215, '# Jeremy filters out unmapped reads next with Picard CleanSam.jar')
    (216, '#')
    (217, '#')
    (218, '')
    (219, 'rule mark_dups:')
    (220, '\\tinput:')
    (221, '\\t\\tf"{OUTDIR}/round{ROUND}/stampy/RG_{{sample}}.bam"')
    (222, '\\toutput:')
    (223, '\\t\\tbam = temp(f"{OUTDIR}/round{ROUND}/stampy/mark_dup/{{sample}}.bam"),')
    (224, '\\t\\tmetrics = f"{OUTDIR}/round{ROUND}/logs/stampy/{{sample}}_dups.txt"')
    (225, '\\tconda:  "envs/picard.yaml"')
    (226, '\\tresources:')
    (227, '\\t\\tmem_Gb = 3')
    (228, '\\tbenchmark:')
    (229, '\\t\\tf"{OUTDIR}/benchmarks/round{ROUND}/{{sample}}.markdup.benchmark.txt"')
    (230, '\\tshell:')
    (231, '\\t\\t"picard MarkDuplicates INPUT={input} OUTPUT={output.bam} METRICS_FILE={output.metrics}"\\t')
    (232, '')
    (233, '# Function RG_from_sample provides RG info from sample table')
    (234, 'rule add_RG:')
    (235, '\\tinput:')
    (236, '\\t\\tbam = f"{OUTDIR}/round{ROUND}/stampy/qfilter_{{sample}}_sort.bam"')
    (237, '\\toutput:')
    (238, '\\t\\ttemp(f"{OUTDIR}/round{ROUND}/stampy/RG_{{sample}}.bam")')
    (239, '\\tparams: RG= lambda wildcards: RG_from_sample(wildcards)')
    (240, '\\tshell:')
    (241, '\\t\\t"samtools addreplacerg -r \\\'{params.RG}\\\' -o {output} {input}"')
    (242, '')
    (243, 'rule dup_bam_bai:')
    (244, '        input:')
    (245, '                f"{OUTDIR}/round{ROUND}/stampy/mark_dup/{{sample}}.bam"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CUMoellerLab/sn-mg-pipeline, file=resources/snakefiles/prototype_selection.smk
context_key: ['if num_prototypes < 2']
    (7, 'def _validate_parameters(dm, num_prototypes, seedset=None):')
    (8, "    \\'\\'\\'Validate the paramters for each algorithm.")
    (9, '    Parameters')
    (10, '    ----------')
    (11, '    dm: skbio.stats.distance.DistanceMatrix')
    (12, '        Pairwise distances for all elements in the full set S.')
    (13, '    num_prototypes: int')
    (14, '        Number of prototypes to select for distance matrix.')
    (15, '        Must be >= 2, since a single prototype is useless.')
    (16, '        Must be smaller than the number of elements in the distance matrix,')
    (17, '        otherwise no reduction is necessary.')
    (18, '    seedset: iterable of str')
    (19, '        A set of element IDs that are pre-selected as prototypes. Remaining')
    (20, '        prototypes are then recruited with the prototype selection algorithm.')
    (21, '        Warning: It will most likely violate the global objective function.')
    (22, '    Raises')
    (23, '    ------')
    (24, '    ValueError')
    (25, '        The number of prototypes to be found should be at least 2 and at most')
    (26, '        one element smaller than elements in the distance matrix. Otherwise, a')
    (27, '        ValueError is raised.')
    (28, '        The IDs in the seed set must be unique, and must be present in the')
    (29, '        distance matrix. Otherwise, a ValueError is raised.')
    (30, '        The size of the seed set must be smaller than the number of prototypes')
    (31, '        to be found. Otherwise, a ValueError is raised.')
    (32, "    \\'\\'\\'")
    (33, '    if num_prototypes < 2:')
    (34, '        raise ValueError("\\\'num_prototypes\\\' must be >= 2, since a single "')
    (35, '                         "prototype is useless.")')
    (36, '    if num_prototypes >= dm.shape[0]:')
    (37, '        raise ValueError("\\\'num_prototypes\\\' must be smaller than the number of "')
    (38, '                         "elements in the distance matrix, otherwise no "')
    (39, '                         "reduction is necessary.")')
    (40, '    if seedset is not None:')
    (41, '        seeds = set(seedset)')
    (42, '        if len(seeds) < len(seedset):')
    (43, '            raise ValueError("There are duplicated IDs in \\\'seedset\\\'.")')
    (44, '        if not seeds < set(dm.ids):  # test if set A is a subset of set B')
    (45, '            raise ValueError("\\\'seedset\\\' is not a subset of the element IDs in "')
    (46, '                             "the distance matrix.")')
    (47, '        if len(seeds) >= num_prototypes:')
    (48, '            raise ValueError("Size of \\\'seedset\\\' must be smaller than the "')
    (49, '                             "number of prototypes to select.")')
    (50, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CUMoellerLab/sn-mg-pipeline, file=resources/snakefiles/prototype_selection.smk
context_key: ['if seedset is not None']
    (93, '    def prototype_selection_constructive_maxdist(dm: DistanceMatrix,')
    (94, '    num_prototypes: int, seedset: List[str]) -> List[str]:')
    (95, '')
    (96, '    ...')
    (97, '    LICENSE')
    (98, '    From https://github.com/biocore/wol/tree/master/code/prototypeSelection')
    (99, '')
    (100, '    Copyright (c) 2017--, WoL development team. All rights reserved.')
    (101, '')
    (102, '    Redistribution and use in source and binary forms, with or without')
    (103, '    modification, are permitted provided that the following conditions are met:')
    (104, '')
    (105, '    * Redistributions of source code must retain the above copyright notice, this')
    (106, '      list of conditions and the following disclaimer.')
    (107, '')
    (108, '    * Redistributions in binary form must reproduce the above copyright notice,')
    (109, '      this list of conditions and the following disclaimer in the documentation')
    (110, '      and/or other materials provided with the distribution.')
    (111, '')
    (112, '    * Neither the name of the copyright holder nor the names of its contributors')
    (113, '      may be used to endorse or promote products derived from this software')
    (114, '      without specific prior written permission.')
    (115, '')
    (116, '    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"')
    (117, '    AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE')
    (118, '    IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE')
    (119, '    ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE')
    (120, '    LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR')
    (121, '    CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF')
    (122, '    SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS')
    (123, '    INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN')
    (124, '    CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)')
    (125, '    ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE')
    (126, '    POSSIBILITY OF SUCH DAMAGE.')
    (127, "    \\'\\'\\'")
    (128, '    _validate_parameters(dm, num_prototypes, seedset)')
    (129, '')
    (130, '    # clever bookkeeping allows for significant speed-ups!')
    (131, '')
    (132, '    # track the number of available elements')
    (133, '    numRemain = len(dm.ids)')
    (134, '')
    (135, '    # distances from each element to all others')
    (136, '    currDists = dm.data.sum(axis=1)')
    (137, '')
    (138, '    # a dirty hack to ensure that all elements of the seedset will be selected')
    (139, '    # last and thus make it into the resulting set')
    (140, '    maxVal = currDists.max()')
    (141, '    if seedset is not None:')
    (142, '        for e in seedset:')
    (143, '            currDists[dm.index(e)] = maxVal*2')
    (144, '')
    (145, '    # the element to remove first is the one that has smallest distance to all')
    (146, '    # other. "Removing" works by tagging its distance-sum as infinity. Plus, we')
    (147, '    # decrease the number of available elements by one.')
    (148, '    minElmIdx = currDists.argmin()')
    (149, '    currDists[minElmIdx], numRemain = np.infty, numRemain-1')
    (150, '')
    (151, '    # continue until only num_prototype elements are left')
    (152, '    while (numRemain > num_prototypes):')
    (153, '        # substract the distance to the removed element for all remaining')
    (154, '        # elements')
    (155, '        currDists -= dm.data[minElmIdx]')
    (156, '        # find the next element to be removed, again as the one that is')
    (157, '        # closest to all others')
    (158, '        minElmIdx = currDists.argmin()')
    (159, '        currDists[minElmIdx], numRemain = np.infty, numRemain-1')
    (160, '')
    (161, '    # return a list of IDs of the surviving elements, which are the found')
    (162, '    # prototypes.')
    (163, '    return [dm.ids[idx]')
    (164, '            for idx, dist in enumerate(currDists)')
    (165, '            if dist != np.infty]')
    (166, '')
    (167, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CUMoellerLab/sn-mg-pipeline, file=resources/snakefiles/Snakefile-bin.smk
context_key: ['if not grps']
    (31, 'def parse_groups(group_series):')
    (32, '    groups = {}')
    (33, '    for sample, grps in group_series.iteritems():')
    (34, '        if not grps:')
    (35, '            continue')
    (36, "        grp_list = grps.split(\\',\\')")
    (37, '        for grp in grp_list:')
    (38, '            if grp not in groups:')
    (39, '                groups[grp] = [sample]')
    (40, '            else:')
    (41, '                groups[grp].append(sample)')
    (42, '    return(groups)')
    (43, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CUMoellerLab/sn-mg-pipeline, file=resources/snakefiles/Snakefile-bin.smk
context_key: ['if read_grp.keys() != ctg_grp.keys()']
    (44, 'def make_pairings(read_grp, ctg_grp):')
    (45, '    if read_grp.keys() != ctg_grp.keys():')
    (46, "        raise ValueError(\\'Not all keys in both from and to groups!\\')")
    (47, '')
    (48, '    pairings = []')
    (49, '    contig_pairings = {}')
    (50, '    for grp in read_grp.keys():')
    (51, '        r = read_grp[grp]')
    (52, '        c = ctg_grp[grp]')
    (53, '')
    (54, '        for i in r:')
    (55, '            for  j in c:')
    (56, '                pairings.append((i, j))')
    (57, '                if j not in contig_pairings:')
    (58, '                    contig_pairings[j] = [i]')
    (59, '                else:')
    (60, '                    contig_pairings[j].append(i)')
    (61, '')
    (62, '    return(pairings, contig_pairings)')
    (63, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/compression, file=.tests/compatibility/Snakefile
context_key: ['if result']
    (15, 'def extract_module_version_from_readme(modulename):')
    (16, '    search_string = modulename + ":(.+)\\')
    (17, '$"')
    (18, '    with open("../../README.md", "r") as reader:')
    (19, '        for line in reader:')
    (20, '            result = re.search(search_string, line)')
    (21, '            if result:')
    (22, '                return result[1]')
    (23, '')
    (24, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=Snakefile
context_key: ['if DIFFEXPRESS.lower() == "on"']
    (47, '    def getContrast(CONTRASTS):')
    (48, '        return [("contrasts/"+((element.replace("[", \\\'\\\')).replace("]", \\\'\\\')).replace("\\\'", \\\'\\\')) for element in CONTRASTS]')
    (49, '')
    (50, '    def getContrastXtail(CONTRASTS):')
    (51, '        return [("xtail/" + ((element.replace("[", \\\'\\\')).replace("]", \\\'\\\')).replace("\\\'", \\\'\\\') + "_significant.xlsx") for element in CONTRASTS]')
    (52, '')
    (53, '    def getContrastRiborex(CONTRASTS):')
    (54, '        return [("riborex/" + ((element.replace("[", \\\'\\\')).replace("]", \\\'\\\')).replace("\\\'", \\\'\\\') + "_significant.xlsx") for element in CONTRASTS]')
    (55, '')
    (56, '    def getContrastDeltaTE(CONTRASTS):')
    (57, '        return [("deltate/" + ((element.replace("[", \\\'\\\')).replace("]", \\\'\\\')).replace("\\\'", \\\'\\\') + "_significant.xlsx") for element in CONTRASTS]')
    (58, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=Snakefile
context_key: ['if DIFFEXPRESS.lower() == "on"']
    (59, 'def get_wigfiles(wildcards):')
    (60, '    method=samples["method"]')
    (61, '    condition=samples["condition"]')
    (62, '    replicate=samples["replicate"]')
    (63, '    wilds = zip(method, condition, replicate)')
    (64, '')
    (65, '    bigwigs = [["totalmapped", "uniquemapped", "global", "centered", "fiveprime", "threeprime"], ["raw", "mil", "min"], ["forward", "reverse"], list(wilds)]')
    (66, '    bigwigs = list(iter.product(*bigwigs))')
    (67, '')
    (68, '    wigfiles = []')
    (69, '    for bw in bigwigs:')
    (70, '        wigfiles.append("%stracks/%s/%s-%s-%s.%s.%s.%s.bw" %(bw[0], bw[1], bw[3][0], bw[3][1], bw[3][2], bw[1], bw[2], bw[0]))')
    (71, '')
    (72, '    return wigfiles')
    (73, '')
    (74, '')
    (75, '# Preprocessing')
    (76, 'include: "rules/preprocessing.smk"')
    (77, '# Adaper removal and quality control')
    (78, 'include: "rules/trimming.smk"')
    (79, '# removal of reads mapping to ribosomal rna genes')
    (80, 'include: "rules/rrnafiltering.smk"')
    (81, '# mapping')
    (82, 'include: "rules/mapping.smk"')
    (83, '# Visualization')
    (84, 'include: "rules/visualization.smk"')
    (85, 'include: "rules/merge.smk"')
    (86, '# reparation')
    (87, 'include: "rules/reparation.smk"')
    (88, '# metagene')
    (89, 'include: "rules/metageneprofiling.smk"')
    (90, 'include: "rules/auxiliary.smk"')
    (91, '# multiqc')
    (92, 'include: "rules/qcauxiliary.smk"')
    (93, 'include: "rules/qcsingleend.smk"')
    (94, '#readcounts')
    (95, 'include: "rules/readcounting.smk"')
    (96, 'if DIFFEXPRESS.lower() == "on":')
    (97, '    include: "rules/diffex_contrast.smk"')
    (98, '    include: "rules/diffex_xtail.smk"')
    (99, '    include: "rules/diffex_riborex.smk"')
    (100, '    include: "rules/diffex_deltate.smk"')
    (101, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=Snakefile
context_key: ['if DEEPRIBO.lower() == "on"']
    (59, 'def get_wigfiles(wildcards):')
    (60, '    method=samples["method"]')
    (61, '    condition=samples["condition"]')
    (62, '    replicate=samples["replicate"]')
    (63, '    wilds = zip(method, condition, replicate)')
    (64, '')
    (65, '    bigwigs = [["totalmapped", "uniquemapped", "global", "centered", "fiveprime", "threeprime"], ["raw", "mil", "min"], ["forward", "reverse"], list(wilds)]')
    (66, '    bigwigs = list(iter.product(*bigwigs))')
    (67, '')
    (68, '    wigfiles = []')
    (69, '    for bw in bigwigs:')
    (70, '        wigfiles.append("%stracks/%s/%s-%s-%s.%s.%s.%s.bw" %(bw[0], bw[1], bw[3][0], bw[3][1], bw[3][2], bw[1], bw[2], bw[0]))')
    (71, '')
    (72, '    return wigfiles')
    (73, '')
    (74, '')
    (75, '# Preprocessing')
    (76, 'include: "rules/preprocessing.smk"')
    (77, '# Adaper removal and quality control')
    (78, 'include: "rules/trimming.smk"')
    (79, '# removal of reads mapping to ribosomal rna genes')
    (80, 'include: "rules/rrnafiltering.smk"')
    (81, '# mapping')
    (82, 'include: "rules/mapping.smk"')
    (83, '# Visualization')
    (84, 'include: "rules/visualization.smk"')
    (85, 'include: "rules/merge.smk"')
    (86, '# reparation')
    (87, 'include: "rules/reparation.smk"')
    (88, '# metagene')
    (89, 'include: "rules/metageneprofiling.smk"')
    (90, 'include: "rules/auxiliary.smk"')
    (91, '# multiqc')
    (92, 'include: "rules/qcauxiliary.smk"')
    (93, 'include: "rules/qcsingleend.smk"')
    (94, '#readcounts')
    (95, 'include: "rules/readcounting.smk"')
    (96, 'if DIFFEXPRESS.lower() == "on":')
    (97, '    include: "rules/diffex_contrast.smk"')
    (98, '    include: "rules/diffex_xtail.smk"')
    (99, '    include: "rules/diffex_riborex.smk"')
    (100, '    include: "rules/diffex_deltate.smk"')
    (101, '')
    (102, 'if DEEPRIBO.lower() == "on":')
    (103, '    #deepribo')
    (104, '    include: "rules/deepribo.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=Snakefile
context_key: ['if hasRIBO', 'if DIFFEXPRESS.lower() == "on" and DEEPRIBO.lower() == "on"', 'rule all', 'input']
    (59, 'def get_wigfiles(wildcards):')
    (60, '    method=samples["method"]')
    (61, '    condition=samples["condition"]')
    (62, '    replicate=samples["replicate"]')
    (63, '    wilds = zip(method, condition, replicate)')
    (64, '')
    (65, '    bigwigs = [["totalmapped", "uniquemapped", "global", "centered", "fiveprime", "threeprime"], ["raw", "mil", "min"], ["forward", "reverse"], list(wilds)]')
    (66, '    bigwigs = list(iter.product(*bigwigs))')
    (67, '')
    (68, '    wigfiles = []')
    (69, '    for bw in bigwigs:')
    (70, '        wigfiles.append("%stracks/%s/%s-%s-%s.%s.%s.%s.bw" %(bw[0], bw[1], bw[3][0], bw[3][1], bw[3][2], bw[1], bw[2], bw[0]))')
    (71, '')
    (72, '    return wigfiles')
    (73, '')
    (74, '')
    (75, '# Preprocessing')
    (76, 'include: "rules/preprocessing.smk"')
    (77, '# Adaper removal and quality control')
    (78, 'include: "rules/trimming.smk"')
    (79, '# removal of reads mapping to ribosomal rna genes')
    (80, 'include: "rules/rrnafiltering.smk"')
    (81, '# mapping')
    (82, 'include: "rules/mapping.smk"')
    (83, '# Visualization')
    (84, 'include: "rules/visualization.smk"')
    (85, 'include: "rules/merge.smk"')
    (86, '# reparation')
    (87, 'include: "rules/reparation.smk"')
    (88, '# metagene')
    (89, 'include: "rules/metageneprofiling.smk"')
    (90, 'include: "rules/auxiliary.smk"')
    (91, '# multiqc')
    (92, 'include: "rules/qcauxiliary.smk"')
    (93, 'include: "rules/qcsingleend.smk"')
    (94, '#readcounts')
    (95, 'include: "rules/readcounting.smk"')
    (96, 'if DIFFEXPRESS.lower() == "on":')
    (97, '    include: "rules/diffex_contrast.smk"')
    (98, '    include: "rules/diffex_xtail.smk"')
    (99, '    include: "rules/diffex_riborex.smk"')
    (100, '    include: "rules/diffex_deltate.smk"')
    (101, '')
    (102, 'if DEEPRIBO.lower() == "on":')
    (103, '    #deepribo')
    (104, '    include: "rules/deepribo.smk"')
    (105, 'else:')
    (106, '    include: "rules/conditionals.smk"')
    (107, '')
    (108, 'if hasRIBO:')
    (109, '    if DIFFEXPRESS.lower() == "on" and DEEPRIBO.lower() == "on":')
    (110, '       rule all:')
    (111, '          input:')
    (112, '              expand("metageneprofiling/TIS/raw/{method}-{condition}-{replicate}", zip, method=samples_meta_start["method"], condition=samples_meta_start["condition"], replicate=samples_meta_start["replicate"]),')
    (113, '              expand("metageneprofiling/TIS/norm/{method}-{condition}-{replicate}", zip, method=samples_meta_start["method"], condition=samples_meta_start["condition"], replicate=samples_meta_start["replicate"]),')
    (114, '              expand("metageneprofiling/TTS/raw/{method}-{condition}-{replicate}", zip, method=samples_meta_stop["method"], condition=samples_meta_stop["condition"], replicate=samples_meta_stop["replicate"]),')
    (115, '              expand("metageneprofiling/TTS/norm/{method}-{condition}-{replicate}", zip, method=samples_meta_stop["method"], condition=samples_meta_stop["condition"], replicate=samples_meta_stop["replicate"]),')
    (116, '              get_wigfiles,')
    (117, '              "qc/multi/multiqc_report.html",')
    (118, '              "tracks/potentialStopCodons.gff",')
    (119, '              "tracks/potentialStartCodons.gff",')
    (120, '              "tracks/potentialAlternativeStartCodons.gff",')
    (121, '              "tracks/potentialRibosomeBindingSite.gff",')
    (122, '              "auxiliary/annotation_total.xlsx",')
    (123, '              "auxiliary/annotation_unique.xlsx",')
    (124, '              "auxiliary/total_read_counts.xlsx",')
    (125, '              "auxiliary/unique_read_counts.xlsx",')
    (126, '              "auxiliary/samples.xlsx",')
    (127, '              "auxiliary/predictions_reparation.xlsx",')
    (128, '              "figures/heatmap_SpearmanCorr_readCounts.pdf",')
    (129, '              "auxiliary/predictions_deepribo.xlsx",')
    (130, '              rules.createOverviewTableAll.output,')
    (131, '              unpack(getContrast),')
    (132, '              unpack(getContrastXtail),')
    (133, '              unpack(getContrastRiborex),')
    (134, '              unpack(getContrastDeltaTE),')
    (135, '              "metageneprofiling/merged_offsets.json"')
    (136, '')
    (137, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=emleddin/md-analysis-workflow, file=rules/common.smk
context_key: ['if len(files) == 1']
    (48, 'def gnuplot_rms(outfile, tag, fs, div, start_residue, end_residue, sys, files):')
    (49, '    """Create a gnuplot script for plotting RMSD, RMSF, and number of hbonds')
    (50, '    for all replicates of a system.')
    (51, '')
    (52, '    Parameters')
    (53, '    ----------')
    (54, '    outfile : str')
    (55, '        The name of the output gnuplot script.')
    (56, '    sys_tag : str')
    (57, '        Specifies the inital title information for the input data files.')
    (58, '    file_sep : str')
    (59, '        Determines the separator to use for the file name.')
    (60, '    start_residue : int')
    (61, '        The initial residue for RMSF by-residue plot.')
    (62, '    end_residue : int')
    (63, '        The final residue for RMSF by-reside plot.')
    (64, '    sys : str')
    (65, '        The system to make the plot for.')
    (66, '    files : list')
    (67, '        A list of the replicates for a given `sys`.')
    (68, '')
    (69, '    Returns')
    (70, '    -------')
    (71, '    outfile : gnuplot')
    (72, '        The input script for gnuplot.')
    (73, '')
    (74, '    Notes')
    (75, '    -----')
    (76, "    You cannot use line smoothing with matplotlib, which is why you\\'d go")
    (77, '    through the hassle for gnuplot.')
    (78, '    The gnuplot output are explicitly named and written to using EPS format.')
    (79, '    EPS does a better job of determining where information should go instead')
    (80, '    of directly saving as a PNG.')
    (81, '    """')
    (82, '    f = open(outfile, "w+")')
    (83, "    f.write(\\'set encoding iso_8859_1\\")
    (84, "\\')")
    (85, '    f.write(\\\'set term postscript enhanced color font "Arial,24";\\')
    (86, '\\')
    (87, "\\')")
    (88, '    # Explicitly set the color scheme https://personal.sron.nl/~pault/')
    (89, '    f.write(\\\'set linetype 1 lc rgb "#332288" lw 2 pt 1\\')
    (90, "\\')")
    (91, '    f.write(\\\'set linetype 2 lc rgb "#88CCEE" lw 2 pt 2\\')
    (92, "\\')")
    (93, '    f.write(\\\'set linetype 3 lc rgb "#44AA99" lw 2 pt 3\\')
    (94, "\\')")
    (95, '    f.write(\\\'set linetype 4 lc rgb "#117733" lw 2 pt 4\\')
    (96, "\\')")
    (97, '    f.write(\\\'set linetype 5 lc rgb "#999933" lw 2 pt 5\\')
    (98, "\\')")
    (99, '    f.write(\\\'set linetype 6 lc rgb "#cc6677" lw 2 pt 6\\')
    (100, "\\')")
    (101, '    f.write(\\\'set linetype 7 lc rgb "#882255" lw 2 pt 7\\')
    (102, "\\')")
    (103, '    f.write(\\\'set linetype 8 lc rgb "#aa4499" lw 2 pt 8\\')
    (104, "\\')")
    (105, "    f.write(\\'set linetype cycle 8\\")
    (106, "\\')")
    (107, "    f.write(\\'\\")
    (108, '\\')
    (109, "\\')")
    (110, '    f.write(\\\'set xlabel "Time (ns)"\\')
    (111, "\\')")
    (112, '    f.write(\\\'set ylabel "RMSD ({\\\\\\\\305})"\\')
    (113, "\\')")
    (114, "    f.write(\\'set key left bottom Left reverse\\")
    (115, '\\')
    (116, "\\')")
    (117, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}rmsds.eps\\\\";\\')
    (118, '")')
    (119, '    counted = False')
    (120, '    for rep in range(len(files)):')
    (121, '        # Deal with only one graph')
    (122, '        if len(files) == 1:')
    (123, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=emleddin/md-analysis-workflow, file=rules/common.smk
context_key: ['elif counted == False']
    (48, 'def gnuplot_rms(outfile, tag, fs, div, start_residue, end_residue, sys, files):')
    (49, '    """Create a gnuplot script for plotting RMSD, RMSF, and number of hbonds')
    (50, '    for all replicates of a system.')
    (51, '')
    (52, '    Parameters')
    (53, '    ----------')
    (54, '    outfile : str')
    (55, '        The name of the output gnuplot script.')
    (56, '    sys_tag : str')
    (57, '        Specifies the inital title information for the input data files.')
    (58, '    file_sep : str')
    (59, '        Determines the separator to use for the file name.')
    (60, '    start_residue : int')
    (61, '        The initial residue for RMSF by-residue plot.')
    (62, '    end_residue : int')
    (63, '        The final residue for RMSF by-reside plot.')
    (64, '    sys : str')
    (65, '        The system to make the plot for.')
    (66, '    files : list')
    (67, '        A list of the replicates for a given `sys`.')
    (68, '')
    (69, '    Returns')
    (70, '    -------')
    (71, '    outfile : gnuplot')
    (72, '        The input script for gnuplot.')
    (73, '')
    (74, '    Notes')
    (75, '    -----')
    (76, "    You cannot use line smoothing with matplotlib, which is why you\\'d go")
    (77, '    through the hassle for gnuplot.')
    (78, '    The gnuplot output are explicitly named and written to using EPS format.')
    (79, '    EPS does a better job of determining where information should go instead')
    (80, '    of directly saving as a PNG.')
    (81, '    """')
    (82, '    f = open(outfile, "w+")')
    (83, "    f.write(\\'set encoding iso_8859_1\\")
    (84, "\\')")
    (85, '    f.write(\\\'set term postscript enhanced color font "Arial,24";\\')
    (86, '\\')
    (87, "\\')")
    (88, '    # Explicitly set the color scheme https://personal.sron.nl/~pault/')
    (89, '    f.write(\\\'set linetype 1 lc rgb "#332288" lw 2 pt 1\\')
    (90, "\\')")
    (91, '    f.write(\\\'set linetype 2 lc rgb "#88CCEE" lw 2 pt 2\\')
    (92, "\\')")
    (93, '    f.write(\\\'set linetype 3 lc rgb "#44AA99" lw 2 pt 3\\')
    (94, "\\')")
    (95, '    f.write(\\\'set linetype 4 lc rgb "#117733" lw 2 pt 4\\')
    (96, "\\')")
    (97, '    f.write(\\\'set linetype 5 lc rgb "#999933" lw 2 pt 5\\')
    (98, "\\')")
    (99, '    f.write(\\\'set linetype 6 lc rgb "#cc6677" lw 2 pt 6\\')
    (100, "\\')")
    (101, '    f.write(\\\'set linetype 7 lc rgb "#882255" lw 2 pt 7\\')
    (102, "\\')")
    (103, '    f.write(\\\'set linetype 8 lc rgb "#aa4499" lw 2 pt 8\\')
    (104, "\\')")
    (105, "    f.write(\\'set linetype cycle 8\\")
    (106, "\\')")
    (107, "    f.write(\\'\\")
    (108, '\\')
    (109, "\\')")
    (110, '    f.write(\\\'set xlabel "Time (ns)"\\')
    (111, "\\')")
    (112, '    f.write(\\\'set ylabel "RMSD ({\\\\\\\\305})"\\')
    (113, "\\')")
    (114, "    f.write(\\'set key left bottom Left reverse\\")
    (115, '\\')
    (116, "\\')")
    (117, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}rmsds.eps\\\\";\\')
    (118, '")')
    (119, '    counted = False')
    (120, '    for rep in range(len(files)):')
    (121, '        # Deal with only one graph')
    (122, '        if len(files) == 1:')
    (123, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (124, '")')
    (125, '        # Plot first if multiple')
    (126, '        elif counted == False:')
    (127, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=emleddin/md-analysis-workflow, file=rules/common.smk
context_key: ['elif files[rep] == files[-1]']
    (48, 'def gnuplot_rms(outfile, tag, fs, div, start_residue, end_residue, sys, files):')
    (49, '    """Create a gnuplot script for plotting RMSD, RMSF, and number of hbonds')
    (50, '    for all replicates of a system.')
    (51, '')
    (52, '    Parameters')
    (53, '    ----------')
    (54, '    outfile : str')
    (55, '        The name of the output gnuplot script.')
    (56, '    sys_tag : str')
    (57, '        Specifies the inital title information for the input data files.')
    (58, '    file_sep : str')
    (59, '        Determines the separator to use for the file name.')
    (60, '    start_residue : int')
    (61, '        The initial residue for RMSF by-residue plot.')
    (62, '    end_residue : int')
    (63, '        The final residue for RMSF by-reside plot.')
    (64, '    sys : str')
    (65, '        The system to make the plot for.')
    (66, '    files : list')
    (67, '        A list of the replicates for a given `sys`.')
    (68, '')
    (69, '    Returns')
    (70, '    -------')
    (71, '    outfile : gnuplot')
    (72, '        The input script for gnuplot.')
    (73, '')
    (74, '    Notes')
    (75, '    -----')
    (76, "    You cannot use line smoothing with matplotlib, which is why you\\'d go")
    (77, '    through the hassle for gnuplot.')
    (78, '    The gnuplot output are explicitly named and written to using EPS format.')
    (79, '    EPS does a better job of determining where information should go instead')
    (80, '    of directly saving as a PNG.')
    (81, '    """')
    (82, '    f = open(outfile, "w+")')
    (83, "    f.write(\\'set encoding iso_8859_1\\")
    (84, "\\')")
    (85, '    f.write(\\\'set term postscript enhanced color font "Arial,24";\\')
    (86, '\\')
    (87, "\\')")
    (88, '    # Explicitly set the color scheme https://personal.sron.nl/~pault/')
    (89, '    f.write(\\\'set linetype 1 lc rgb "#332288" lw 2 pt 1\\')
    (90, "\\')")
    (91, '    f.write(\\\'set linetype 2 lc rgb "#88CCEE" lw 2 pt 2\\')
    (92, "\\')")
    (93, '    f.write(\\\'set linetype 3 lc rgb "#44AA99" lw 2 pt 3\\')
    (94, "\\')")
    (95, '    f.write(\\\'set linetype 4 lc rgb "#117733" lw 2 pt 4\\')
    (96, "\\')")
    (97, '    f.write(\\\'set linetype 5 lc rgb "#999933" lw 2 pt 5\\')
    (98, "\\')")
    (99, '    f.write(\\\'set linetype 6 lc rgb "#cc6677" lw 2 pt 6\\')
    (100, "\\')")
    (101, '    f.write(\\\'set linetype 7 lc rgb "#882255" lw 2 pt 7\\')
    (102, "\\')")
    (103, '    f.write(\\\'set linetype 8 lc rgb "#aa4499" lw 2 pt 8\\')
    (104, "\\')")
    (105, "    f.write(\\'set linetype cycle 8\\")
    (106, "\\')")
    (107, "    f.write(\\'\\")
    (108, '\\')
    (109, "\\')")
    (110, '    f.write(\\\'set xlabel "Time (ns)"\\')
    (111, "\\')")
    (112, '    f.write(\\\'set ylabel "RMSD ({\\\\\\\\305})"\\')
    (113, "\\')")
    (114, "    f.write(\\'set key left bottom Left reverse\\")
    (115, '\\')
    (116, "\\')")
    (117, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}rmsds.eps\\\\";\\')
    (118, '")')
    (119, '    counted = False')
    (120, '    for rep in range(len(files)):')
    (121, '        # Deal with only one graph')
    (122, '        if len(files) == 1:')
    (123, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (124, '")')
    (125, '        # Plot first if multiple')
    (126, '        elif counted == False:')
    (127, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (128, '")')
    (129, '            counted = True')
    (130, '        # Deal with final case')
    (131, '        elif files[rep] == files[-1]:')
    (132, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=emleddin/md-analysis-workflow, file=rules/common.smk
context_key: ['if len(files) == 1']
    (48, 'def gnuplot_rms(outfile, tag, fs, div, start_residue, end_residue, sys, files):')
    (49, '    """Create a gnuplot script for plotting RMSD, RMSF, and number of hbonds')
    (50, '    for all replicates of a system.')
    (51, '')
    (52, '    Parameters')
    (53, '    ----------')
    (54, '    outfile : str')
    (55, '        The name of the output gnuplot script.')
    (56, '    sys_tag : str')
    (57, '        Specifies the inital title information for the input data files.')
    (58, '    file_sep : str')
    (59, '        Determines the separator to use for the file name.')
    (60, '    start_residue : int')
    (61, '        The initial residue for RMSF by-residue plot.')
    (62, '    end_residue : int')
    (63, '        The final residue for RMSF by-reside plot.')
    (64, '    sys : str')
    (65, '        The system to make the plot for.')
    (66, '    files : list')
    (67, '        A list of the replicates for a given `sys`.')
    (68, '')
    (69, '    Returns')
    (70, '    -------')
    (71, '    outfile : gnuplot')
    (72, '        The input script for gnuplot.')
    (73, '')
    (74, '    Notes')
    (75, '    -----')
    (76, "    You cannot use line smoothing with matplotlib, which is why you\\'d go")
    (77, '    through the hassle for gnuplot.')
    (78, '    The gnuplot output are explicitly named and written to using EPS format.')
    (79, '    EPS does a better job of determining where information should go instead')
    (80, '    of directly saving as a PNG.')
    (81, '    """')
    (82, '    f = open(outfile, "w+")')
    (83, "    f.write(\\'set encoding iso_8859_1\\")
    (84, "\\')")
    (85, '    f.write(\\\'set term postscript enhanced color font "Arial,24";\\')
    (86, '\\')
    (87, "\\')")
    (88, '    # Explicitly set the color scheme https://personal.sron.nl/~pault/')
    (89, '    f.write(\\\'set linetype 1 lc rgb "#332288" lw 2 pt 1\\')
    (90, "\\')")
    (91, '    f.write(\\\'set linetype 2 lc rgb "#88CCEE" lw 2 pt 2\\')
    (92, "\\')")
    (93, '    f.write(\\\'set linetype 3 lc rgb "#44AA99" lw 2 pt 3\\')
    (94, "\\')")
    (95, '    f.write(\\\'set linetype 4 lc rgb "#117733" lw 2 pt 4\\')
    (96, "\\')")
    (97, '    f.write(\\\'set linetype 5 lc rgb "#999933" lw 2 pt 5\\')
    (98, "\\')")
    (99, '    f.write(\\\'set linetype 6 lc rgb "#cc6677" lw 2 pt 6\\')
    (100, "\\')")
    (101, '    f.write(\\\'set linetype 7 lc rgb "#882255" lw 2 pt 7\\')
    (102, "\\')")
    (103, '    f.write(\\\'set linetype 8 lc rgb "#aa4499" lw 2 pt 8\\')
    (104, "\\')")
    (105, "    f.write(\\'set linetype cycle 8\\")
    (106, "\\')")
    (107, "    f.write(\\'\\")
    (108, '\\')
    (109, "\\')")
    (110, '    f.write(\\\'set xlabel "Time (ns)"\\')
    (111, "\\')")
    (112, '    f.write(\\\'set ylabel "RMSD ({\\\\\\\\305})"\\')
    (113, "\\')")
    (114, "    f.write(\\'set key left bottom Left reverse\\")
    (115, '\\')
    (116, "\\')")
    (117, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}rmsds.eps\\\\";\\')
    (118, '")')
    (119, '    counted = False')
    (120, '    for rep in range(len(files)):')
    (121, '        # Deal with only one graph')
    (122, '        if len(files) == 1:')
    (123, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (124, '")')
    (125, '        # Plot first if multiple')
    (126, '        elif counted == False:')
    (127, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (128, '")')
    (129, '            counted = True')
    (130, '        # Deal with final case')
    (131, '        elif files[rep] == files[-1]:')
    (132, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (133, '")')
    (134, '        # Use line without plot and with continuation if 1) not only,')
    (135, '        # 2) not first, and 3) not last')
    (136, '        else:')
    (137, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (138, '")')
    (139, '    # Start Number of Hydrogen Bonds')
    (140, "    f.write(\\'\\")
    (141, "\\')")
    (142, '    f.write(\\\'set xlabel "Time (ns)"\\')
    (143, "\\')")
    (144, '    f.write(\\\'set ylabel "Number of hydrogen bonds"\\')
    (145, "\\')")
    (146, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}hbonds.eps\\\\";\\')
    (147, '")')
    (148, '    counted = False')
    (149, '    for rep in range(len(files)):')
    (150, '        # Deal with only one graph')
    (151, '        if len(files) == 1:')
    (152, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=emleddin/md-analysis-workflow, file=rules/common.smk
context_key: ['elif counted == False']
    (48, 'def gnuplot_rms(outfile, tag, fs, div, start_residue, end_residue, sys, files):')
    (49, '    """Create a gnuplot script for plotting RMSD, RMSF, and number of hbonds')
    (50, '    for all replicates of a system.')
    (51, '')
    (52, '    Parameters')
    (53, '    ----------')
    (54, '    outfile : str')
    (55, '        The name of the output gnuplot script.')
    (56, '    sys_tag : str')
    (57, '        Specifies the inital title information for the input data files.')
    (58, '    file_sep : str')
    (59, '        Determines the separator to use for the file name.')
    (60, '    start_residue : int')
    (61, '        The initial residue for RMSF by-residue plot.')
    (62, '    end_residue : int')
    (63, '        The final residue for RMSF by-reside plot.')
    (64, '    sys : str')
    (65, '        The system to make the plot for.')
    (66, '    files : list')
    (67, '        A list of the replicates for a given `sys`.')
    (68, '')
    (69, '    Returns')
    (70, '    -------')
    (71, '    outfile : gnuplot')
    (72, '        The input script for gnuplot.')
    (73, '')
    (74, '    Notes')
    (75, '    -----')
    (76, "    You cannot use line smoothing with matplotlib, which is why you\\'d go")
    (77, '    through the hassle for gnuplot.')
    (78, '    The gnuplot output are explicitly named and written to using EPS format.')
    (79, '    EPS does a better job of determining where information should go instead')
    (80, '    of directly saving as a PNG.')
    (81, '    """')
    (82, '    f = open(outfile, "w+")')
    (83, "    f.write(\\'set encoding iso_8859_1\\")
    (84, "\\')")
    (85, '    f.write(\\\'set term postscript enhanced color font "Arial,24";\\')
    (86, '\\')
    (87, "\\')")
    (88, '    # Explicitly set the color scheme https://personal.sron.nl/~pault/')
    (89, '    f.write(\\\'set linetype 1 lc rgb "#332288" lw 2 pt 1\\')
    (90, "\\')")
    (91, '    f.write(\\\'set linetype 2 lc rgb "#88CCEE" lw 2 pt 2\\')
    (92, "\\')")
    (93, '    f.write(\\\'set linetype 3 lc rgb "#44AA99" lw 2 pt 3\\')
    (94, "\\')")
    (95, '    f.write(\\\'set linetype 4 lc rgb "#117733" lw 2 pt 4\\')
    (96, "\\')")
    (97, '    f.write(\\\'set linetype 5 lc rgb "#999933" lw 2 pt 5\\')
    (98, "\\')")
    (99, '    f.write(\\\'set linetype 6 lc rgb "#cc6677" lw 2 pt 6\\')
    (100, "\\')")
    (101, '    f.write(\\\'set linetype 7 lc rgb "#882255" lw 2 pt 7\\')
    (102, "\\')")
    (103, '    f.write(\\\'set linetype 8 lc rgb "#aa4499" lw 2 pt 8\\')
    (104, "\\')")
    (105, "    f.write(\\'set linetype cycle 8\\")
    (106, "\\')")
    (107, "    f.write(\\'\\")
    (108, '\\')
    (109, "\\')")
    (110, '    f.write(\\\'set xlabel "Time (ns)"\\')
    (111, "\\')")
    (112, '    f.write(\\\'set ylabel "RMSD ({\\\\\\\\305})"\\')
    (113, "\\')")
    (114, "    f.write(\\'set key left bottom Left reverse\\")
    (115, '\\')
    (116, "\\')")
    (117, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}rmsds.eps\\\\";\\')
    (118, '")')
    (119, '    counted = False')
    (120, '    for rep in range(len(files)):')
    (121, '        # Deal with only one graph')
    (122, '        if len(files) == 1:')
    (123, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (124, '")')
    (125, '        # Plot first if multiple')
    (126, '        elif counted == False:')
    (127, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (128, '")')
    (129, '            counted = True')
    (130, '        # Deal with final case')
    (131, '        elif files[rep] == files[-1]:')
    (132, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (133, '")')
    (134, '        # Use line without plot and with continuation if 1) not only,')
    (135, '        # 2) not first, and 3) not last')
    (136, '        else:')
    (137, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (138, '")')
    (139, '    # Start Number of Hydrogen Bonds')
    (140, "    f.write(\\'\\")
    (141, "\\')")
    (142, '    f.write(\\\'set xlabel "Time (ns)"\\')
    (143, "\\')")
    (144, '    f.write(\\\'set ylabel "Number of hydrogen bonds"\\')
    (145, "\\')")
    (146, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}hbonds.eps\\\\";\\')
    (147, '")')
    (148, '    counted = False')
    (149, '    for rep in range(len(files)):')
    (150, '        # Deal with only one graph')
    (151, '        if len(files) == 1:')
    (152, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (153, '")')
    (154, '        # Plot first if multiple')
    (155, '        elif counted == False:')
    (156, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=emleddin/md-analysis-workflow, file=rules/common.smk
context_key: ['elif files[rep] == files[-1]']
    (48, 'def gnuplot_rms(outfile, tag, fs, div, start_residue, end_residue, sys, files):')
    (49, '    """Create a gnuplot script for plotting RMSD, RMSF, and number of hbonds')
    (50, '    for all replicates of a system.')
    (51, '')
    (52, '    Parameters')
    (53, '    ----------')
    (54, '    outfile : str')
    (55, '        The name of the output gnuplot script.')
    (56, '    sys_tag : str')
    (57, '        Specifies the inital title information for the input data files.')
    (58, '    file_sep : str')
    (59, '        Determines the separator to use for the file name.')
    (60, '    start_residue : int')
    (61, '        The initial residue for RMSF by-residue plot.')
    (62, '    end_residue : int')
    (63, '        The final residue for RMSF by-reside plot.')
    (64, '    sys : str')
    (65, '        The system to make the plot for.')
    (66, '    files : list')
    (67, '        A list of the replicates for a given `sys`.')
    (68, '')
    (69, '    Returns')
    (70, '    -------')
    (71, '    outfile : gnuplot')
    (72, '        The input script for gnuplot.')
    (73, '')
    (74, '    Notes')
    (75, '    -----')
    (76, "    You cannot use line smoothing with matplotlib, which is why you\\'d go")
    (77, '    through the hassle for gnuplot.')
    (78, '    The gnuplot output are explicitly named and written to using EPS format.')
    (79, '    EPS does a better job of determining where information should go instead')
    (80, '    of directly saving as a PNG.')
    (81, '    """')
    (82, '    f = open(outfile, "w+")')
    (83, "    f.write(\\'set encoding iso_8859_1\\")
    (84, "\\')")
    (85, '    f.write(\\\'set term postscript enhanced color font "Arial,24";\\')
    (86, '\\')
    (87, "\\')")
    (88, '    # Explicitly set the color scheme https://personal.sron.nl/~pault/')
    (89, '    f.write(\\\'set linetype 1 lc rgb "#332288" lw 2 pt 1\\')
    (90, "\\')")
    (91, '    f.write(\\\'set linetype 2 lc rgb "#88CCEE" lw 2 pt 2\\')
    (92, "\\')")
    (93, '    f.write(\\\'set linetype 3 lc rgb "#44AA99" lw 2 pt 3\\')
    (94, "\\')")
    (95, '    f.write(\\\'set linetype 4 lc rgb "#117733" lw 2 pt 4\\')
    (96, "\\')")
    (97, '    f.write(\\\'set linetype 5 lc rgb "#999933" lw 2 pt 5\\')
    (98, "\\')")
    (99, '    f.write(\\\'set linetype 6 lc rgb "#cc6677" lw 2 pt 6\\')
    (100, "\\')")
    (101, '    f.write(\\\'set linetype 7 lc rgb "#882255" lw 2 pt 7\\')
    (102, "\\')")
    (103, '    f.write(\\\'set linetype 8 lc rgb "#aa4499" lw 2 pt 8\\')
    (104, "\\')")
    (105, "    f.write(\\'set linetype cycle 8\\")
    (106, "\\')")
    (107, "    f.write(\\'\\")
    (108, '\\')
    (109, "\\')")
    (110, '    f.write(\\\'set xlabel "Time (ns)"\\')
    (111, "\\')")
    (112, '    f.write(\\\'set ylabel "RMSD ({\\\\\\\\305})"\\')
    (113, "\\')")
    (114, "    f.write(\\'set key left bottom Left reverse\\")
    (115, '\\')
    (116, "\\')")
    (117, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}rmsds.eps\\\\";\\')
    (118, '")')
    (119, '    counted = False')
    (120, '    for rep in range(len(files)):')
    (121, '        # Deal with only one graph')
    (122, '        if len(files) == 1:')
    (123, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (124, '")')
    (125, '        # Plot first if multiple')
    (126, '        elif counted == False:')
    (127, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (128, '")')
    (129, '            counted = True')
    (130, '        # Deal with final case')
    (131, '        elif files[rep] == files[-1]:')
    (132, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (133, '")')
    (134, '        # Use line without plot and with continuation if 1) not only,')
    (135, '        # 2) not first, and 3) not last')
    (136, '        else:')
    (137, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (138, '")')
    (139, '    # Start Number of Hydrogen Bonds')
    (140, "    f.write(\\'\\")
    (141, "\\')")
    (142, '    f.write(\\\'set xlabel "Time (ns)"\\')
    (143, "\\')")
    (144, '    f.write(\\\'set ylabel "Number of hydrogen bonds"\\')
    (145, "\\')")
    (146, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}hbonds.eps\\\\";\\')
    (147, '")')
    (148, '    counted = False')
    (149, '    for rep in range(len(files)):')
    (150, '        # Deal with only one graph')
    (151, '        if len(files) == 1:')
    (152, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (153, '")')
    (154, '        # Plot first if multiple')
    (155, '        elif counted == False:')
    (156, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (157, '")')
    (158, '            counted = True')
    (159, '        # Deal with final case')
    (160, '        elif files[rep] == files[-1]:')
    (161, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=emleddin/md-analysis-workflow, file=rules/common.smk
context_key: ['if len(files) == 1']
    (48, 'def gnuplot_rms(outfile, tag, fs, div, start_residue, end_residue, sys, files):')
    (49, '    """Create a gnuplot script for plotting RMSD, RMSF, and number of hbonds')
    (50, '    for all replicates of a system.')
    (51, '')
    (52, '    Parameters')
    (53, '    ----------')
    (54, '    outfile : str')
    (55, '        The name of the output gnuplot script.')
    (56, '    sys_tag : str')
    (57, '        Specifies the inital title information for the input data files.')
    (58, '    file_sep : str')
    (59, '        Determines the separator to use for the file name.')
    (60, '    start_residue : int')
    (61, '        The initial residue for RMSF by-residue plot.')
    (62, '    end_residue : int')
    (63, '        The final residue for RMSF by-reside plot.')
    (64, '    sys : str')
    (65, '        The system to make the plot for.')
    (66, '    files : list')
    (67, '        A list of the replicates for a given `sys`.')
    (68, '')
    (69, '    Returns')
    (70, '    -------')
    (71, '    outfile : gnuplot')
    (72, '        The input script for gnuplot.')
    (73, '')
    (74, '    Notes')
    (75, '    -----')
    (76, "    You cannot use line smoothing with matplotlib, which is why you\\'d go")
    (77, '    through the hassle for gnuplot.')
    (78, '    The gnuplot output are explicitly named and written to using EPS format.')
    (79, '    EPS does a better job of determining where information should go instead')
    (80, '    of directly saving as a PNG.')
    (81, '    """')
    (82, '    f = open(outfile, "w+")')
    (83, "    f.write(\\'set encoding iso_8859_1\\")
    (84, "\\')")
    (85, '    f.write(\\\'set term postscript enhanced color font "Arial,24";\\')
    (86, '\\')
    (87, "\\')")
    (88, '    # Explicitly set the color scheme https://personal.sron.nl/~pault/')
    (89, '    f.write(\\\'set linetype 1 lc rgb "#332288" lw 2 pt 1\\')
    (90, "\\')")
    (91, '    f.write(\\\'set linetype 2 lc rgb "#88CCEE" lw 2 pt 2\\')
    (92, "\\')")
    (93, '    f.write(\\\'set linetype 3 lc rgb "#44AA99" lw 2 pt 3\\')
    (94, "\\')")
    (95, '    f.write(\\\'set linetype 4 lc rgb "#117733" lw 2 pt 4\\')
    (96, "\\')")
    (97, '    f.write(\\\'set linetype 5 lc rgb "#999933" lw 2 pt 5\\')
    (98, "\\')")
    (99, '    f.write(\\\'set linetype 6 lc rgb "#cc6677" lw 2 pt 6\\')
    (100, "\\')")
    (101, '    f.write(\\\'set linetype 7 lc rgb "#882255" lw 2 pt 7\\')
    (102, "\\')")
    (103, '    f.write(\\\'set linetype 8 lc rgb "#aa4499" lw 2 pt 8\\')
    (104, "\\')")
    (105, "    f.write(\\'set linetype cycle 8\\")
    (106, "\\')")
    (107, "    f.write(\\'\\")
    (108, '\\')
    (109, "\\')")
    (110, '    f.write(\\\'set xlabel "Time (ns)"\\')
    (111, "\\')")
    (112, '    f.write(\\\'set ylabel "RMSD ({\\\\\\\\305})"\\')
    (113, "\\')")
    (114, "    f.write(\\'set key left bottom Left reverse\\")
    (115, '\\')
    (116, "\\')")
    (117, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}rmsds.eps\\\\";\\')
    (118, '")')
    (119, '    counted = False')
    (120, '    for rep in range(len(files)):')
    (121, '        # Deal with only one graph')
    (122, '        if len(files) == 1:')
    (123, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (124, '")')
    (125, '        # Plot first if multiple')
    (126, '        elif counted == False:')
    (127, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (128, '")')
    (129, '            counted = True')
    (130, '        # Deal with final case')
    (131, '        elif files[rep] == files[-1]:')
    (132, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (133, '")')
    (134, '        # Use line without plot and with continuation if 1) not only,')
    (135, '        # 2) not first, and 3) not last')
    (136, '        else:')
    (137, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (138, '")')
    (139, '    # Start Number of Hydrogen Bonds')
    (140, "    f.write(\\'\\")
    (141, "\\')")
    (142, '    f.write(\\\'set xlabel "Time (ns)"\\')
    (143, "\\')")
    (144, '    f.write(\\\'set ylabel "Number of hydrogen bonds"\\')
    (145, "\\')")
    (146, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}hbonds.eps\\\\";\\')
    (147, '")')
    (148, '    counted = False')
    (149, '    for rep in range(len(files)):')
    (150, '        # Deal with only one graph')
    (151, '        if len(files) == 1:')
    (152, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (153, '")')
    (154, '        # Plot first if multiple')
    (155, '        elif counted == False:')
    (156, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (157, '")')
    (158, '            counted = True')
    (159, '        # Deal with final case')
    (160, '        elif files[rep] == files[-1]:')
    (161, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (162, '")')
    (163, '        # Use line without plot and with continuation if 1) not only,')
    (164, '        # 2) not first, and 3) not last')
    (165, '        else:')
    (166, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (167, '")')
    (168, "    f.write(\\'\\")
    (169, "\\')")
    (170, '    # Start RMSF')
    (171, '    f.write(\\\'set xlabel "Residue number"\\')
    (172, "\\')")
    (173, '    f.write(\\\'set ylabel "RMSF ({\\\\\\\\305})"\\')
    (174, "\\')")
    (175, "    f.write(\\'set key top left Left reverse\\")
    (176, "\\')")
    (177, "    f.write(f\\'set xrange [{start_residue}:{end_residue}]\\")
    (178, "\\')")
    (179, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}rmsf.eps\\\\";\\')
    (180, '")')
    (181, '    counted = False')
    (182, '    for rep in range(len(files)):')
    (183, '        # Deal with only one graph')
    (184, '        if len(files) == 1:')
    (185, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}rmsf{fs}byres.dat\\\\" u 1:2 w lines t \\\\"{files[rep]}\\\\" lw 4;\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=emleddin/md-analysis-workflow, file=rules/common.smk
context_key: ['elif counted == False']
    (48, 'def gnuplot_rms(outfile, tag, fs, div, start_residue, end_residue, sys, files):')
    (49, '    """Create a gnuplot script for plotting RMSD, RMSF, and number of hbonds')
    (50, '    for all replicates of a system.')
    (51, '')
    (52, '    Parameters')
    (53, '    ----------')
    (54, '    outfile : str')
    (55, '        The name of the output gnuplot script.')
    (56, '    sys_tag : str')
    (57, '        Specifies the inital title information for the input data files.')
    (58, '    file_sep : str')
    (59, '        Determines the separator to use for the file name.')
    (60, '    start_residue : int')
    (61, '        The initial residue for RMSF by-residue plot.')
    (62, '    end_residue : int')
    (63, '        The final residue for RMSF by-reside plot.')
    (64, '    sys : str')
    (65, '        The system to make the plot for.')
    (66, '    files : list')
    (67, '        A list of the replicates for a given `sys`.')
    (68, '')
    (69, '    Returns')
    (70, '    -------')
    (71, '    outfile : gnuplot')
    (72, '        The input script for gnuplot.')
    (73, '')
    (74, '    Notes')
    (75, '    -----')
    (76, "    You cannot use line smoothing with matplotlib, which is why you\\'d go")
    (77, '    through the hassle for gnuplot.')
    (78, '    The gnuplot output are explicitly named and written to using EPS format.')
    (79, '    EPS does a better job of determining where information should go instead')
    (80, '    of directly saving as a PNG.')
    (81, '    """')
    (82, '    f = open(outfile, "w+")')
    (83, "    f.write(\\'set encoding iso_8859_1\\")
    (84, "\\')")
    (85, '    f.write(\\\'set term postscript enhanced color font "Arial,24";\\')
    (86, '\\')
    (87, "\\')")
    (88, '    # Explicitly set the color scheme https://personal.sron.nl/~pault/')
    (89, '    f.write(\\\'set linetype 1 lc rgb "#332288" lw 2 pt 1\\')
    (90, "\\')")
    (91, '    f.write(\\\'set linetype 2 lc rgb "#88CCEE" lw 2 pt 2\\')
    (92, "\\')")
    (93, '    f.write(\\\'set linetype 3 lc rgb "#44AA99" lw 2 pt 3\\')
    (94, "\\')")
    (95, '    f.write(\\\'set linetype 4 lc rgb "#117733" lw 2 pt 4\\')
    (96, "\\')")
    (97, '    f.write(\\\'set linetype 5 lc rgb "#999933" lw 2 pt 5\\')
    (98, "\\')")
    (99, '    f.write(\\\'set linetype 6 lc rgb "#cc6677" lw 2 pt 6\\')
    (100, "\\')")
    (101, '    f.write(\\\'set linetype 7 lc rgb "#882255" lw 2 pt 7\\')
    (102, "\\')")
    (103, '    f.write(\\\'set linetype 8 lc rgb "#aa4499" lw 2 pt 8\\')
    (104, "\\')")
    (105, "    f.write(\\'set linetype cycle 8\\")
    (106, "\\')")
    (107, "    f.write(\\'\\")
    (108, '\\')
    (109, "\\')")
    (110, '    f.write(\\\'set xlabel "Time (ns)"\\')
    (111, "\\')")
    (112, '    f.write(\\\'set ylabel "RMSD ({\\\\\\\\305})"\\')
    (113, "\\')")
    (114, "    f.write(\\'set key left bottom Left reverse\\")
    (115, '\\')
    (116, "\\')")
    (117, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}rmsds.eps\\\\";\\')
    (118, '")')
    (119, '    counted = False')
    (120, '    for rep in range(len(files)):')
    (121, '        # Deal with only one graph')
    (122, '        if len(files) == 1:')
    (123, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (124, '")')
    (125, '        # Plot first if multiple')
    (126, '        elif counted == False:')
    (127, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (128, '")')
    (129, '            counted = True')
    (130, '        # Deal with final case')
    (131, '        elif files[rep] == files[-1]:')
    (132, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (133, '")')
    (134, '        # Use line without plot and with continuation if 1) not only,')
    (135, '        # 2) not first, and 3) not last')
    (136, '        else:')
    (137, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (138, '")')
    (139, '    # Start Number of Hydrogen Bonds')
    (140, "    f.write(\\'\\")
    (141, "\\')")
    (142, '    f.write(\\\'set xlabel "Time (ns)"\\')
    (143, "\\')")
    (144, '    f.write(\\\'set ylabel "Number of hydrogen bonds"\\')
    (145, "\\')")
    (146, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}hbonds.eps\\\\";\\')
    (147, '")')
    (148, '    counted = False')
    (149, '    for rep in range(len(files)):')
    (150, '        # Deal with only one graph')
    (151, '        if len(files) == 1:')
    (152, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (153, '")')
    (154, '        # Plot first if multiple')
    (155, '        elif counted == False:')
    (156, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (157, '")')
    (158, '            counted = True')
    (159, '        # Deal with final case')
    (160, '        elif files[rep] == files[-1]:')
    (161, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (162, '")')
    (163, '        # Use line without plot and with continuation if 1) not only,')
    (164, '        # 2) not first, and 3) not last')
    (165, '        else:')
    (166, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (167, '")')
    (168, "    f.write(\\'\\")
    (169, "\\')")
    (170, '    # Start RMSF')
    (171, '    f.write(\\\'set xlabel "Residue number"\\')
    (172, "\\')")
    (173, '    f.write(\\\'set ylabel "RMSF ({\\\\\\\\305})"\\')
    (174, "\\')")
    (175, "    f.write(\\'set key top left Left reverse\\")
    (176, "\\')")
    (177, "    f.write(f\\'set xrange [{start_residue}:{end_residue}]\\")
    (178, "\\')")
    (179, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}rmsf.eps\\\\";\\')
    (180, '")')
    (181, '    counted = False')
    (182, '    for rep in range(len(files)):')
    (183, '        # Deal with only one graph')
    (184, '        if len(files) == 1:')
    (185, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}rmsf{fs}byres.dat\\\\" u 1:2 w lines t \\\\"{files[rep]}\\\\" lw 4;\\')
    (186, '")')
    (187, '        # Plot first if multiple')
    (188, '        elif counted == False:')
    (189, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}rmsf{fs}byres.dat\\\\" u 1:2 w lines t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=emleddin/md-analysis-workflow, file=rules/common.smk
context_key: ['elif files[rep] == files[-1]']
    (48, 'def gnuplot_rms(outfile, tag, fs, div, start_residue, end_residue, sys, files):')
    (49, '    """Create a gnuplot script for plotting RMSD, RMSF, and number of hbonds')
    (50, '    for all replicates of a system.')
    (51, '')
    (52, '    Parameters')
    (53, '    ----------')
    (54, '    outfile : str')
    (55, '        The name of the output gnuplot script.')
    (56, '    sys_tag : str')
    (57, '        Specifies the inital title information for the input data files.')
    (58, '    file_sep : str')
    (59, '        Determines the separator to use for the file name.')
    (60, '    start_residue : int')
    (61, '        The initial residue for RMSF by-residue plot.')
    (62, '    end_residue : int')
    (63, '        The final residue for RMSF by-reside plot.')
    (64, '    sys : str')
    (65, '        The system to make the plot for.')
    (66, '    files : list')
    (67, '        A list of the replicates for a given `sys`.')
    (68, '')
    (69, '    Returns')
    (70, '    -------')
    (71, '    outfile : gnuplot')
    (72, '        The input script for gnuplot.')
    (73, '')
    (74, '    Notes')
    (75, '    -----')
    (76, "    You cannot use line smoothing with matplotlib, which is why you\\'d go")
    (77, '    through the hassle for gnuplot.')
    (78, '    The gnuplot output are explicitly named and written to using EPS format.')
    (79, '    EPS does a better job of determining where information should go instead')
    (80, '    of directly saving as a PNG.')
    (81, '    """')
    (82, '    f = open(outfile, "w+")')
    (83, "    f.write(\\'set encoding iso_8859_1\\")
    (84, "\\')")
    (85, '    f.write(\\\'set term postscript enhanced color font "Arial,24";\\')
    (86, '\\')
    (87, "\\')")
    (88, '    # Explicitly set the color scheme https://personal.sron.nl/~pault/')
    (89, '    f.write(\\\'set linetype 1 lc rgb "#332288" lw 2 pt 1\\')
    (90, "\\')")
    (91, '    f.write(\\\'set linetype 2 lc rgb "#88CCEE" lw 2 pt 2\\')
    (92, "\\')")
    (93, '    f.write(\\\'set linetype 3 lc rgb "#44AA99" lw 2 pt 3\\')
    (94, "\\')")
    (95, '    f.write(\\\'set linetype 4 lc rgb "#117733" lw 2 pt 4\\')
    (96, "\\')")
    (97, '    f.write(\\\'set linetype 5 lc rgb "#999933" lw 2 pt 5\\')
    (98, "\\')")
    (99, '    f.write(\\\'set linetype 6 lc rgb "#cc6677" lw 2 pt 6\\')
    (100, "\\')")
    (101, '    f.write(\\\'set linetype 7 lc rgb "#882255" lw 2 pt 7\\')
    (102, "\\')")
    (103, '    f.write(\\\'set linetype 8 lc rgb "#aa4499" lw 2 pt 8\\')
    (104, "\\')")
    (105, "    f.write(\\'set linetype cycle 8\\")
    (106, "\\')")
    (107, "    f.write(\\'\\")
    (108, '\\')
    (109, "\\')")
    (110, '    f.write(\\\'set xlabel "Time (ns)"\\')
    (111, "\\')")
    (112, '    f.write(\\\'set ylabel "RMSD ({\\\\\\\\305})"\\')
    (113, "\\')")
    (114, "    f.write(\\'set key left bottom Left reverse\\")
    (115, '\\')
    (116, "\\')")
    (117, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}rmsds.eps\\\\";\\')
    (118, '")')
    (119, '    counted = False')
    (120, '    for rep in range(len(files)):')
    (121, '        # Deal with only one graph')
    (122, '        if len(files) == 1:')
    (123, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (124, '")')
    (125, '        # Plot first if multiple')
    (126, '        elif counted == False:')
    (127, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (128, '")')
    (129, '            counted = True')
    (130, '        # Deal with final case')
    (131, '        elif files[rep] == files[-1]:')
    (132, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (133, '")')
    (134, '        # Use line without plot and with continuation if 1) not only,')
    (135, '        # 2) not first, and 3) not last')
    (136, '        else:')
    (137, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}total{fs}bb{fs}rms.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (138, '")')
    (139, '    # Start Number of Hydrogen Bonds')
    (140, "    f.write(\\'\\")
    (141, "\\')")
    (142, '    f.write(\\\'set xlabel "Time (ns)"\\')
    (143, "\\')")
    (144, '    f.write(\\\'set ylabel "Number of hydrogen bonds"\\')
    (145, "\\')")
    (146, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}hbonds.eps\\\\";\\')
    (147, '")')
    (148, '    counted = False')
    (149, '    for rep in range(len(files)):')
    (150, '        # Deal with only one graph')
    (151, '        if len(files) == 1:')
    (152, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (153, '")')
    (154, '        # Plot first if multiple')
    (155, '        elif counted == False:')
    (156, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (157, '")')
    (158, '            counted = True')
    (159, '        # Deal with final case')
    (160, '        elif files[rep] == files[-1]:')
    (161, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4;\\')
    (162, '")')
    (163, '        # Use line without plot and with continuation if 1) not only,')
    (164, '        # 2) not first, and 3) not last')
    (165, '        else:')
    (166, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}hbond.dat\\\\" u ($1/{div}):($2) w lines s bezier t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (167, '")')
    (168, "    f.write(\\'\\")
    (169, "\\')")
    (170, '    # Start RMSF')
    (171, '    f.write(\\\'set xlabel "Residue number"\\')
    (172, "\\')")
    (173, '    f.write(\\\'set ylabel "RMSF ({\\\\\\\\305})"\\')
    (174, "\\')")
    (175, "    f.write(\\'set key top left Left reverse\\")
    (176, "\\')")
    (177, "    f.write(f\\'set xrange [{start_residue}:{end_residue}]\\")
    (178, "\\')")
    (179, '    f.write(f"set output \\\\"../../analysis/RMS/{tag}{fs}{sys}{fs}rmsf.eps\\\\";\\')
    (180, '")')
    (181, '    counted = False')
    (182, '    for rep in range(len(files)):')
    (183, '        # Deal with only one graph')
    (184, '        if len(files) == 1:')
    (185, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}rmsf{fs}byres.dat\\\\" u 1:2 w lines t \\\\"{files[rep]}\\\\" lw 4;\\')
    (186, '")')
    (187, '        # Plot first if multiple')
    (188, '        elif counted == False:')
    (189, '            f.write(f"plot \\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}rmsf{fs}byres.dat\\\\" u 1:2 w lines t \\\\"{files[rep]}\\\\" lw 4, \\\\\\\\\\')
    (190, '")')
    (191, '            counted = True')
    (192, '        # Deal with final case')
    (193, '        elif files[rep] == files[-1]:')
    (194, '            f.write(f"\\\\"../../analysis/{sys}/{files[rep]}/{tag}{fs}{sys}{fs}rmsf{fs}byres.dat\\\\" u 1:2 w lines t \\\\"{files[rep]}\\\\" lw 4;\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=emleddin/md-analysis-workflow, file=rules/common.smk
context_key: ['if rep == 0']
    (206, 'def eda_diff_script(outfile, base_script, cwd, tag, fs, ROI, sys, files):')
    (207, '    """Sets up the R script for averaging the EDA data for replicates of a')
    (208, '    system.')
    (209, '')
    (210, '    Parameters')
    (211, '    ----------')
    (212, '    outfile : str')
    (213, '        The name of the output gnuplot script.')
    (214, '    base_script : txt')
    (215, "        The file containing the bulk of the R script that\\'s not")
    (216, '        system-dependent.')
    (217, '    cwd : str')
    (218, '        The path to the current working directory.')
    (219, '    tag : str')
    (220, '        Specifies the inital title information for the input data files.')
    (221, '    fs : str')
    (222, '        Determines the separator to use for the file name.')
    (223, '    ROI : int')
    (224, '        The residue of interest for EDA to get the interactions of each residue')
    (225, '        with.')
    (226, '    sys : str')
    (227, '        The system to create the averages for.')
    (228, '    files : list')
    (229, '        A list of the replicates for a given `sys`.')
    (230, '')
    (231, '    Returns')
    (232, '    -------')
    (233, '    outfile : gnuplot')
    (234, '        The input script for gnuplot.')
    (235, '')
    (236, '    Notes')
    (237, '    -----')
    (238, "    You cannot use line smoothing with matplotlib, which is why you\\'d go")
    (239, '    through the hassle for gnuplot.')
    (240, '    The gnuplot output are explicitly named and written to using EPS format.')
    (241, '    EPS does a better job of determining where information should go instead')
    (242, '    of directly saving as a PNG.')
    (243, '    """')
    (244, '    f = open(outfile, "w+")')
    (245, '    for rep in range(len(files)):')
    (246, '        if rep == 0:')
    (247, '            f.write(f\\\'infile1Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_coulomb_interaction.dat")\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=emleddin/md-analysis-workflow, file=rules/common.smk
context_key: ['elif rep == 1']
    (206, 'def eda_diff_script(outfile, base_script, cwd, tag, fs, ROI, sys, files):')
    (207, '    """Sets up the R script for averaging the EDA data for replicates of a')
    (208, '    system.')
    (209, '')
    (210, '    Parameters')
    (211, '    ----------')
    (212, '    outfile : str')
    (213, '        The name of the output gnuplot script.')
    (214, '    base_script : txt')
    (215, "        The file containing the bulk of the R script that\\'s not")
    (216, '        system-dependent.')
    (217, '    cwd : str')
    (218, '        The path to the current working directory.')
    (219, '    tag : str')
    (220, '        Specifies the inital title information for the input data files.')
    (221, '    fs : str')
    (222, '        Determines the separator to use for the file name.')
    (223, '    ROI : int')
    (224, '        The residue of interest for EDA to get the interactions of each residue')
    (225, '        with.')
    (226, '    sys : str')
    (227, '        The system to create the averages for.')
    (228, '    files : list')
    (229, '        A list of the replicates for a given `sys`.')
    (230, '')
    (231, '    Returns')
    (232, '    -------')
    (233, '    outfile : gnuplot')
    (234, '        The input script for gnuplot.')
    (235, '')
    (236, '    Notes')
    (237, '    -----')
    (238, "    You cannot use line smoothing with matplotlib, which is why you\\'d go")
    (239, '    through the hassle for gnuplot.')
    (240, '    The gnuplot output are explicitly named and written to using EPS format.')
    (241, '    EPS does a better job of determining where information should go instead')
    (242, '    of directly saving as a PNG.')
    (243, '    """')
    (244, '    f = open(outfile, "w+")')
    (245, '    for rep in range(len(files)):')
    (246, '        if rep == 0:')
    (247, '            f.write(f\\\'infile1Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_coulomb_interaction.dat")\\')
    (248, "\\')")
    (249, '            f.write(f\\\'infile1Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_vdw_interaction.dat")\\')
    (250, "\\')")
    (251, '            # f.write(f\\\'infile1Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.803")\\')
    (252, "\\')")
    (253, '            # f.write(f\\\'infile1Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.806")\\')
    (254, "\\')")
    (255, '        elif rep == 1:')
    (256, '            f.write(f\\\'infile2Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_coulomb_interaction.dat")\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=emleddin/md-analysis-workflow, file=rules/common.smk
context_key: ['elif rep == 2']
    (206, 'def eda_diff_script(outfile, base_script, cwd, tag, fs, ROI, sys, files):')
    (207, '    """Sets up the R script for averaging the EDA data for replicates of a')
    (208, '    system.')
    (209, '')
    (210, '    Parameters')
    (211, '    ----------')
    (212, '    outfile : str')
    (213, '        The name of the output gnuplot script.')
    (214, '    base_script : txt')
    (215, "        The file containing the bulk of the R script that\\'s not")
    (216, '        system-dependent.')
    (217, '    cwd : str')
    (218, '        The path to the current working directory.')
    (219, '    tag : str')
    (220, '        Specifies the inital title information for the input data files.')
    (221, '    fs : str')
    (222, '        Determines the separator to use for the file name.')
    (223, '    ROI : int')
    (224, '        The residue of interest for EDA to get the interactions of each residue')
    (225, '        with.')
    (226, '    sys : str')
    (227, '        The system to create the averages for.')
    (228, '    files : list')
    (229, '        A list of the replicates for a given `sys`.')
    (230, '')
    (231, '    Returns')
    (232, '    -------')
    (233, '    outfile : gnuplot')
    (234, '        The input script for gnuplot.')
    (235, '')
    (236, '    Notes')
    (237, '    -----')
    (238, "    You cannot use line smoothing with matplotlib, which is why you\\'d go")
    (239, '    through the hassle for gnuplot.')
    (240, '    The gnuplot output are explicitly named and written to using EPS format.')
    (241, '    EPS does a better job of determining where information should go instead')
    (242, '    of directly saving as a PNG.')
    (243, '    """')
    (244, '    f = open(outfile, "w+")')
    (245, '    for rep in range(len(files)):')
    (246, '        if rep == 0:')
    (247, '            f.write(f\\\'infile1Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_coulomb_interaction.dat")\\')
    (248, "\\')")
    (249, '            f.write(f\\\'infile1Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_vdw_interaction.dat")\\')
    (250, "\\')")
    (251, '            # f.write(f\\\'infile1Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.803")\\')
    (252, "\\')")
    (253, '            # f.write(f\\\'infile1Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.806")\\')
    (254, "\\')")
    (255, '        elif rep == 1:')
    (256, '            f.write(f\\\'infile2Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_coulomb_interaction.dat")\\')
    (257, "\\')")
    (258, '            f.write(f\\\'infile2Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_vdw_interaction.dat")\\')
    (259, "\\')")
    (260, '            # f.write(f\\\'infile2Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.803")\\')
    (261, "\\')")
    (262, '            # f.write(f\\\'infile2Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.806")\\')
    (263, "\\')")
    (264, '        elif rep == 2:')
    (265, '            f.write(f\\\'infile3Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_coulomb_interaction.dat")\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=emleddin/md-analysis-workflow, file=rules/common.smk
context_key: ['elif rep == 3']
    (206, 'def eda_diff_script(outfile, base_script, cwd, tag, fs, ROI, sys, files):')
    (207, '    """Sets up the R script for averaging the EDA data for replicates of a')
    (208, '    system.')
    (209, '')
    (210, '    Parameters')
    (211, '    ----------')
    (212, '    outfile : str')
    (213, '        The name of the output gnuplot script.')
    (214, '    base_script : txt')
    (215, "        The file containing the bulk of the R script that\\'s not")
    (216, '        system-dependent.')
    (217, '    cwd : str')
    (218, '        The path to the current working directory.')
    (219, '    tag : str')
    (220, '        Specifies the inital title information for the input data files.')
    (221, '    fs : str')
    (222, '        Determines the separator to use for the file name.')
    (223, '    ROI : int')
    (224, '        The residue of interest for EDA to get the interactions of each residue')
    (225, '        with.')
    (226, '    sys : str')
    (227, '        The system to create the averages for.')
    (228, '    files : list')
    (229, '        A list of the replicates for a given `sys`.')
    (230, '')
    (231, '    Returns')
    (232, '    -------')
    (233, '    outfile : gnuplot')
    (234, '        The input script for gnuplot.')
    (235, '')
    (236, '    Notes')
    (237, '    -----')
    (238, "    You cannot use line smoothing with matplotlib, which is why you\\'d go")
    (239, '    through the hassle for gnuplot.')
    (240, '    The gnuplot output are explicitly named and written to using EPS format.')
    (241, '    EPS does a better job of determining where information should go instead')
    (242, '    of directly saving as a PNG.')
    (243, '    """')
    (244, '    f = open(outfile, "w+")')
    (245, '    for rep in range(len(files)):')
    (246, '        if rep == 0:')
    (247, '            f.write(f\\\'infile1Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_coulomb_interaction.dat")\\')
    (248, "\\')")
    (249, '            f.write(f\\\'infile1Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_vdw_interaction.dat")\\')
    (250, "\\')")
    (251, '            # f.write(f\\\'infile1Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.803")\\')
    (252, "\\')")
    (253, '            # f.write(f\\\'infile1Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.806")\\')
    (254, "\\')")
    (255, '        elif rep == 1:')
    (256, '            f.write(f\\\'infile2Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_coulomb_interaction.dat")\\')
    (257, "\\')")
    (258, '            f.write(f\\\'infile2Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_vdw_interaction.dat")\\')
    (259, "\\')")
    (260, '            # f.write(f\\\'infile2Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.803")\\')
    (261, "\\')")
    (262, '            # f.write(f\\\'infile2Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.806")\\')
    (263, "\\')")
    (264, '        elif rep == 2:')
    (265, '            f.write(f\\\'infile3Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_coulomb_interaction.dat")\\')
    (266, "\\')")
    (267, '            f.write(f\\\'infile3Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_vdw_interaction.dat")\\')
    (268, "\\')")
    (269, '            # f.write(f\\\'infile3Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.803")\\')
    (270, "\\')")
    (271, '            # f.write(f\\\'infile3Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.806")\\')
    (272, "\\')")
    (273, '        elif rep == 3:')
    (274, '            f.write(f\\\'infile4Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_coulomb_interaction.dat")\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=emleddin/md-analysis-workflow, file=rules/common.smk
context_key: ['elif rep == 4']
    (206, 'def eda_diff_script(outfile, base_script, cwd, tag, fs, ROI, sys, files):')
    (207, '    """Sets up the R script for averaging the EDA data for replicates of a')
    (208, '    system.')
    (209, '')
    (210, '    Parameters')
    (211, '    ----------')
    (212, '    outfile : str')
    (213, '        The name of the output gnuplot script.')
    (214, '    base_script : txt')
    (215, "        The file containing the bulk of the R script that\\'s not")
    (216, '        system-dependent.')
    (217, '    cwd : str')
    (218, '        The path to the current working directory.')
    (219, '    tag : str')
    (220, '        Specifies the inital title information for the input data files.')
    (221, '    fs : str')
    (222, '        Determines the separator to use for the file name.')
    (223, '    ROI : int')
    (224, '        The residue of interest for EDA to get the interactions of each residue')
    (225, '        with.')
    (226, '    sys : str')
    (227, '        The system to create the averages for.')
    (228, '    files : list')
    (229, '        A list of the replicates for a given `sys`.')
    (230, '')
    (231, '    Returns')
    (232, '    -------')
    (233, '    outfile : gnuplot')
    (234, '        The input script for gnuplot.')
    (235, '')
    (236, '    Notes')
    (237, '    -----')
    (238, "    You cannot use line smoothing with matplotlib, which is why you\\'d go")
    (239, '    through the hassle for gnuplot.')
    (240, '    The gnuplot output are explicitly named and written to using EPS format.')
    (241, '    EPS does a better job of determining where information should go instead')
    (242, '    of directly saving as a PNG.')
    (243, '    """')
    (244, '    f = open(outfile, "w+")')
    (245, '    for rep in range(len(files)):')
    (246, '        if rep == 0:')
    (247, '            f.write(f\\\'infile1Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_coulomb_interaction.dat")\\')
    (248, "\\')")
    (249, '            f.write(f\\\'infile1Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_vdw_interaction.dat")\\')
    (250, "\\')")
    (251, '            # f.write(f\\\'infile1Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.803")\\')
    (252, "\\')")
    (253, '            # f.write(f\\\'infile1Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.806")\\')
    (254, "\\')")
    (255, '        elif rep == 1:')
    (256, '            f.write(f\\\'infile2Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_coulomb_interaction.dat")\\')
    (257, "\\')")
    (258, '            f.write(f\\\'infile2Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_vdw_interaction.dat")\\')
    (259, "\\')")
    (260, '            # f.write(f\\\'infile2Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.803")\\')
    (261, "\\')")
    (262, '            # f.write(f\\\'infile2Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.806")\\')
    (263, "\\')")
    (264, '        elif rep == 2:')
    (265, '            f.write(f\\\'infile3Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_coulomb_interaction.dat")\\')
    (266, "\\')")
    (267, '            f.write(f\\\'infile3Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_vdw_interaction.dat")\\')
    (268, "\\')")
    (269, '            # f.write(f\\\'infile3Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.803")\\')
    (270, "\\')")
    (271, '            # f.write(f\\\'infile3Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.806")\\')
    (272, "\\')")
    (273, '        elif rep == 3:')
    (274, '            f.write(f\\\'infile4Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_coulomb_interaction.dat")\\')
    (275, "\\')")
    (276, '            f.write(f\\\'infile4Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_vdw_interaction.dat")\\')
    (277, "\\')")
    (278, '            # f.write(f\\\'infile4Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.803")\\')
    (279, "\\')")
    (280, '            # f.write(f\\\'infile4Av <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort.806")\\')
    (281, "\\')")
    (282, '        elif rep == 4:')
    (283, '            f.write(f\\\'infile5Ac <- Sys.glob("{cwd}/analysis/EDA/{sys}/{files[rep]}/fort_coulomb_interaction.dat")\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nixonlab/HERV_scGCB, file=workflow/rules/stellarscope.smk
context_key: ['if wc.smode == "U"']
    (3, 'def get_strand_cmd(wc):')
    (4, '    # input functions are passed a single parameter, wildcards')
    (5, '    # wildcards is a namespace so refer to the variable with a "."')
    (6, '    if wc.smode == "U": # handle the unstranded case')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/Snakefile
context_key: ['if len(fastq_path) < 1']
    (17, "def find_sequencing_files(fastq_file, sequencing_read_folder=config[\\'reads\\'][\\'path\\']):")
    (18, '    """Searches the given sequencing_read_folder for the samples defined in')
    (19, '    samples created by read_sample_sheet()')
    (20, '    """')
    (21, '    fastq_path = glob.glob(os.path.join(sequencing_read_folder, "**", fastq_file), recursive=True)')
    (22, "    # Couldn\\'t find the fastq file, throw error")
    (23, '    if len(fastq_path) < 1:')
    (24, '        raise FileNotFoundError(f"Error finding sequencing reads for {fastq_path} under the directory {sequencing_read_folder}")')
    (25, '')
    (26, '    # Found too many fastq files, throw error')
    (27, '    if len(fastq_path) > 1:')
    (28, '        raise OSError(f"Found more than one file for {fastq_path} under the directory {sequencing_read_folder}")')
    (29, '        ')
    (30, '    return fastq_path[0]')
    (31, '        ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/Snakefile
context_key: ['if is_human_genome()']
    (72, 'def get_all_target_files():')
    (73, '    target_files = []')
    (74, '    target_files.append("results/pca.svg")')
    (75, '    target_files.append("results/qc/multiqc_report.html")')
    (76, '    ')
    (77, '    target_files.append("results/counts/all.with_gene_name.tsv")')
    (78, '    target_files.append("results/deseq2/normcounts.with_gene_name.tsv")')
    (79, '    target_files.append(expand("results/diffexp/{contrast}.diffexp.top_{regulation_direction}.tsv",')
    (80, '        contrast=config["diffexp"]["contrasts"],')
    (81, '        regulation_direction=["upregulated","downregulated"]))')
    (82, '    target_files.append(expand("results/diffexp/{contrast}.diffexp.with_gene_name.tsv", contrast=config["diffexp"]["contrasts"]))')
    (83, '    target_files.append(expand("results/diffexp/{contrast}.diffexp.top_{regulation_direction}.with_gene_name.tsv",')
    (84, '        contrast=config["diffexp"]["contrasts"],')
    (85, '        regulation_direction=["upregulated","downregulated"]))')
    (86, '')
    (87, '    if is_human_genome():')
    (88, '        target_files.append("results/counts_mane/all.with_gene_name.tsv")')
    (89, '        target_files.append("results/deseq2_mane/normcounts.with_gene_name.tsv")')
    (90, '        target_files.append(expand("results/diffexp_mane/{contrast}.diffexp.top_{regulation_direction}.tsv",')
    (91, '            contrast=config["diffexp"]["contrasts"],')
    (92, '            regulation_direction=["upregulated","downregulated"]))')
    (93, '        target_files.append(expand("results/diffexp_mane/{contrast}.diffexp.with_gene_name.tsv", contrast=config["diffexp"]["contrasts"]))')
    (94, '        target_files.append(expand("results/diffexp_mane/{contrast}.diffexp.top_{regulation_direction}.with_gene_name.tsv",')
    (95, '        contrast=config["diffexp"]["contrasts"],')
    (96, '        regulation_direction=["upregulated","downregulated"]))')
    (97, '')
    (98, '        target_files.append(expand("results/variants/{contrast}.{regulation_direction}_gene_pathogenic_variant_summary.tsv", contrast=config["diffexp"]["contrasts"], regulation_direction=["upregulated","downregulated"]))')
    (99, '')
    (100, '    return target_files')
    (101, '')
    (102, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/common.smk
context_key: ['if "docker" in fields']
    (8, 'def in_docker_container():')
    (9, '    """Checks if we are running inside a Docker conatiner"""')
    (10, '    with open("/proc/self/cgroup", "r") as procfile:')
    (11, '        for line in procfile:')
    (12, '            fields = line.strip().split("/")')
    (13, '            if "docker" in fields:')
    (14, '                return True')
    (15, '    return False')
    (16, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/common.smk
context_key: ['if extra_path != ""']
    (21, 'def get_ensembl_path(extra_path=""):')
    (22, '    if extra_path != "":')
    (23, '        return "resources/{0}/ensembl/{extra_path}".format(config[\\\'ref\\\'][\\\'species\\\'], extra_path=extra_path)')
    (24, '    else:')
    (25, '        return "resources/{0}/ensembl".format(config["ref"]["species"])')
    (26, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/common.smk
context_key: ['if extra_path != ""']
    (27, 'def get_clinvar_path(extra_path=""):')
    (28, '    if extra_path != "":')
    (29, '        return "resources/{0}/clinvar/{extra_path}".format(config[\\\'ref\\\'][\\\'species\\\'], extra_path=extra_path)')
    (30, '    else:')
    (31, '        return "resources/{0}/clinvar".format(config["ref"]["species"])')
    (32, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/common.smk
context_key: ['if "strandedness" in units.columns']
    (53, 'def get_strandness(units):')
    (54, '    if "strandedness" in units.columns:')
    (55, '        return units["strandedness"].tolist()')
    (56, '    else:')
    (57, '        strand_list=["none"]')
    (58, '        return strand_list*units.shape[0]')
    (59, '')
    (60, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/common.smk
context_key: ['if host_path']
    (61, 'def get_fq(wildcards, read_num=1, host_path=True):')
    (62, '    if host_path:')
    (63, '        return units.loc[(wildcards.sample, wildcards.unit), ["fq1_full" if read_num == 1 else "fq2_full"]].dropna()')
    (64, '    if config["trimming"]["skip"]:')
    (65, '        # no trimming, use raw reads')
    (66, '        return f"resources/reads/{wildcards.sample}-{wildcards.unit}.{read_num}.fastq.gz"')
    (67, '    else:')
    (68, '        # yes trimming, use trimmed data')
    (69, '        return expand("results/trimmed/{sample}-{unit}.{read_num}.fastq.gz",')
    (70, '                      read_num=read_num, **wildcards)')
    (71, '')
    (72, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/population-mapping, file=Snakefile
context_key: ['if name.endswith("fq.gz")', 'if name_sub not in TO_MAP_TOGETHER']
    (24, 'def define_to_map_together(given_path):')
    (25, '    TO_MAP_TOGETHER = []')
    (26, '    FULLPATHS=[]')
    (27, '    SAMPLES=[]')
    (28, '    for root, dirs, files in os.walk(given_path):')
    (29, '        for name in files:')
    (30, '            if name.endswith("fq.gz"):')
    (31, '                fullpath = os.path.join(root, name)')
    (32, '')
    (33, '                sample = fullpath.rsplit("/", 2)[1]')
    (34, '                name_sub = name.rsplit("_", 2)[0]')
    (35, '')
    (36, '                if name_sub not in TO_MAP_TOGETHER:')
    (37, '                    TO_MAP_TOGETHER.append(name_sub)')
    (38, '                FULLPATHS.append(fullpath)')
    (39, '                if sample not in SAMPLES:')
    (40, '                    SAMPLES.append(sample)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/population-mapping, file=Snakefile
context_key: ['if name.endswith("fq.gz")']
    (24, 'def define_to_map_together(given_path):')
    (25, '    TO_MAP_TOGETHER = []')
    (26, '    FULLPATHS=[]')
    (27, '    SAMPLES=[]')
    (28, '    for root, dirs, files in os.walk(given_path):')
    (29, '        for name in files:')
    (30, '            if name.endswith("fq.gz"):')
    (31, '                fullpath = os.path.join(root, name)')
    (32, '')
    (33, '                sample = fullpath.rsplit("/", 2)[1]')
    (34, '                name_sub = name.rsplit("_", 2)[0]')
    (35, '')
    (36, '                if name_sub not in TO_MAP_TOGETHER:')
    (37, '                    TO_MAP_TOGETHER.append(name_sub)')
    (38, '                FULLPATHS.append(fullpath)')
    (39, '                if sample not in SAMPLES:')
    (40, '                    SAMPLES.append(sample)')
    (41, '    return(TO_MAP_TOGETHER, FULLPATHS, SAMPLES)')
    (42, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/population-mapping, file=Snakefile
context_key: ['if var in file']
    (44, 'def create_bwa_map_input(given_path, map_dict):')
    (45, '    maptogether, fullpaths, samples=define_to_map_together(given_path)')
    (46, '')
    (47, '    for var in maptogether:')
    (48, '        temp = []')
    (49, '        for file in fullpaths:  ')
    (50, '            if var in file:')
    (51, '                temp.append(file)')
    (52, '        map_dict[var] = temp')
    (53, '    return(map_dict, samples)')
    (54, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/population-mapping, file=Snakefile
context_key: ['if file_nodir.startswith(wildcards.sample_merged)']
    (71, 'def create_names_to_merge(wildcards):')
    (72, '    input_files = []')
    (73, '    mapped_dir = expand("mapped_reads/{sample}.bam", sample=MAP.keys())')
    (74, '    for file in mapped_dir:')
    (75, '        file_nodir = file.rsplit("/")[1]')
    (76, '        if file_nodir.startswith(wildcards.sample_merged):')
    (77, '            input_files.append(file)')
    (78, '    return(input_files)')
    (79, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AnzeLovse/mag, file=workflow/rules/common.smk
context_key: ['if is_paired']
    (88, 'def feature_counts_params(wildcards):')
    (89, '    """Generate a string with featureCounts parameters."""')
    (90, '    params = config["feature_counts"]["additional"]')
    (91, '    if is_paired:')
    (92, '        params += " -p"')
    (93, '    return params')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=TBradley27/FilTar, file=modules/quant_reads/kallisto/Snakefile
context_key: ["elif wildcards.accession in config[\\'single_end\\']"]
    (37, 'def get_input_files (wildcards):')
    (38, "        if wildcards.accession in config[\\'paired_end\\']:")
    (39, '                input_files = [')
    (40, "\\t\\t\\t       \\'results/trimmed_fastq/{}_1_val_1.fq.gz\\'.format(wildcards.accession),")
    (41, "                               \\'results/trimmed_fastq/{}_2_val_2.fq.gz\\'.format(wildcards.accession)")
    (42, '                              ]')
    (43, '                return(input_files)')
    (44, "        elif wildcards.accession in config[\\'single_end\\']::")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=TBradley27/FilTar, file=modules/quant_reads/salmon/Snakefile
context_key: ["if wildcards.run_accession in set(metadata.loc[(metadata[\\'pe_or_se\\'] == \\'paired_end\\')][\\'run_accession\\'])"]
    (42, 'def get_input_files (wildcards):')
    (43, "        if wildcards.run_accession in set(metadata.loc[(metadata[\\'pe_or_se\\'] == \\'paired_end\\')][\\'run_accession\\']):")
    (44, "                input_files = [\\'results/trimmed_fastq/{}_1_val_1.fq.gz\\'.format(wildcards.run_accession),")
    (45, "                                \\'results/trimmed_fastq/{}_2_val_2.fq.gz\\'.format(wildcards.run_accession)")
    (46, '                                ]')
    (47, '                return(input_files)')
    (48, "        elif wildcards.run_accession in set(metadata.loc[(metadata[\\'pe_or_se\\'] == \\'single_end\\')][\\'run_accession\\']):")
    (49, '                input_file = ["results/trimmed_fastq/{}_trimmed.fq.gz".format(wildcards.run_accession)')
    (50, '                             ]')
    (51, '                return(input_file)')
    (52, '        else:')
    (53, '                raise Exception("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Konjkov/QMC-snakemake-workflow, file=Snakefile
context_key: ['if Z <= 2']
    (71, 'def charge_pseudo_atom(Z):')
    (72, '    """Get charge for pseudoatom."""')
    (73, '    if Z <= 2:     # H-He')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Konjkov/QMC-snakemake-workflow, file=Snakefile
context_key: ['elif Z <= 10']
    (71, 'def charge_pseudo_atom(Z):')
    (72, '    """Get charge for pseudoatom."""')
    (73, '    if Z <= 2:     # H-He')
    (74, '        return Z')
    (75, '    elif Z <= 10:  # Li-Ne')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Konjkov/QMC-snakemake-workflow, file=Snakefile
context_key: ['elif Z <= 18']
    (71, 'def charge_pseudo_atom(Z):')
    (72, '    """Get charge for pseudoatom."""')
    (73, '    if Z <= 2:     # H-He')
    (74, '        return Z')
    (75, '    elif Z <= 10:  # Li-Ne')
    (76, '        return Z - 2')
    (77, '    elif Z <= 18:  # Na-Ar')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Konjkov/QMC-snakemake-workflow, file=Snakefile
context_key: ['elif Z <= 30']
    (71, 'def charge_pseudo_atom(Z):')
    (72, '    """Get charge for pseudoatom."""')
    (73, '    if Z <= 2:     # H-He')
    (74, '        return Z')
    (75, '    elif Z <= 10:  # Li-Ne')
    (76, '        return Z - 2')
    (77, '    elif Z <= 18:  # Na-Ar')
    (78, '        return Z - 10')
    (79, '    elif Z <= 30:  # K-Zn')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Konjkov/QMC-snakemake-workflow, file=Snakefile
context_key: ['elif Z <= 36']
    (71, 'def charge_pseudo_atom(Z):')
    (72, '    """Get charge for pseudoatom."""')
    (73, '    if Z <= 2:     # H-He')
    (74, '        return Z')
    (75, '    elif Z <= 10:  # Li-Ne')
    (76, '        return Z - 2')
    (77, '    elif Z <= 18:  # Na-Ar')
    (78, '        return Z - 10')
    (79, '    elif Z <= 30:  # K-Zn')
    (80, '        return Z - 18')
    (81, '    elif Z <= 36:  # Ga-Kr')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Konjkov/QMC-snakemake-workflow, file=Snakefile
context_key: ['elif Z <= 48']
    (71, 'def charge_pseudo_atom(Z):')
    (72, '    """Get charge for pseudoatom."""')
    (73, '    if Z <= 2:     # H-He')
    (74, '        return Z')
    (75, '    elif Z <= 10:  # Li-Ne')
    (76, '        return Z - 2')
    (77, '    elif Z <= 18:  # Na-Ar')
    (78, '        return Z - 10')
    (79, '    elif Z <= 30:  # K-Zn')
    (80, '        return Z - 18')
    (81, '    elif Z <= 36:  # Ga-Kr')
    (82, '        return Z - 28')
    (83, '    elif Z <= 48:  # Rb-Cd')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Konjkov/QMC-snakemake-workflow, file=Snakefile
context_key: ['elif Z <= 54']
    (71, 'def charge_pseudo_atom(Z):')
    (72, '    """Get charge for pseudoatom."""')
    (73, '    if Z <= 2:     # H-He')
    (74, '        return Z')
    (75, '    elif Z <= 10:  # Li-Ne')
    (76, '        return Z - 2')
    (77, '    elif Z <= 18:  # Na-Ar')
    (78, '        return Z - 10')
    (79, '    elif Z <= 30:  # K-Zn')
    (80, '        return Z - 18')
    (81, '    elif Z <= 36:  # Ga-Kr')
    (82, '        return Z - 28')
    (83, '    elif Z <= 48:  # Rb-Cd')
    (84, '        return Z - 36')
    (85, '    elif Z <= 54:  # In-Xe')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Konjkov/QMC-snakemake-workflow, file=Snakefile
context_key: ['elif Z <= 71']
    (71, 'def charge_pseudo_atom(Z):')
    (72, '    """Get charge for pseudoatom."""')
    (73, '    if Z <= 2:     # H-He')
    (74, '        return Z')
    (75, '    elif Z <= 10:  # Li-Ne')
    (76, '        return Z - 2')
    (77, '    elif Z <= 18:  # Na-Ar')
    (78, '        return Z - 10')
    (79, '    elif Z <= 30:  # K-Zn')
    (80, '        return Z - 18')
    (81, '    elif Z <= 36:  # Ga-Kr')
    (82, '        return Z - 28')
    (83, '    elif Z <= 48:  # Rb-Cd')
    (84, '        return Z - 36')
    (85, '    elif Z <= 54:  # In-Xe')
    (86, '        return Z - 46')
    (87, '    elif Z <= 71:  # Cs-Lu')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Konjkov/QMC-snakemake-workflow, file=Snakefile
context_key: ['elif Z <= 80']
    (71, 'def charge_pseudo_atom(Z):')
    (72, '    """Get charge for pseudoatom."""')
    (73, '    if Z <= 2:     # H-He')
    (74, '        return Z')
    (75, '    elif Z <= 10:  # Li-Ne')
    (76, '        return Z - 2')
    (77, '    elif Z <= 18:  # Na-Ar')
    (78, '        return Z - 10')
    (79, '    elif Z <= 30:  # K-Zn')
    (80, '        return Z - 18')
    (81, '    elif Z <= 36:  # Ga-Kr')
    (82, '        return Z - 28')
    (83, '    elif Z <= 48:  # Rb-Cd')
    (84, '        return Z - 36')
    (85, '    elif Z <= 54:  # In-Xe')
    (86, '        return Z - 46')
    (87, '    elif Z <= 71:  # Cs-Lu')
    (88, '        return Z - 54')
    (89, '    elif Z <= 80:  # Hf-Hg')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sorjuela/age_lesions_females, file=Snakefile
context_key: ['if not os.path.isfile(config["metatxt"])']
    (15, 'def sanitizefile(str):')
    (16, '\\tif str is None:')
    (17, "\\t\\tstr = \\'\\'")
    (18, '\\treturn str')
    (19, '')
    (20, "#config[\\'genome\\'] = sanitizefile(config[\\'genome\\'])")
    (21, "config[\\'metatxt\\'] = sanitizefile(config[\\'metatxt\\'])")
    (22, '')
    (23, '## Read metadata')
    (24, 'if not os.path.isfile(config["metatxt"]):')
    (25, '  sys.exit("Metadata file " + config["metatxt"] + " does not exist.")')
    (26, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sorjuela/age_lesions_females, file=Snakefile
context_key: ["if not set([\\'names\\',\\'type\\']).issubset(samples.columns)"]
    (15, 'def sanitizefile(str):')
    (16, '\\tif str is None:')
    (17, "\\t\\tstr = \\'\\'")
    (18, '\\treturn str')
    (19, '')
    (20, "#config[\\'genome\\'] = sanitizefile(config[\\'genome\\'])")
    (21, "config[\\'metatxt\\'] = sanitizefile(config[\\'metatxt\\'])")
    (22, '')
    (23, '## Read metadata')
    (24, 'if not os.path.isfile(config["metatxt"]):')
    (25, '  sys.exit("Metadata file " + config["metatxt"] + " does not exist.")')
    (26, '')
    (27, 'import pandas as pd')
    (28, 'samples = pd.read_csv(config["metatxt"], sep=\\\'\\\\t\\\')')
    (29, '')
    (30, "if not set([\\'names\\',\\'type\\']).issubset(samples.columns):")
    (31, '  sys.exit("Make sure \\\'names\\\' and \\\'type\\\' are columns in " + config["metatxt"])')
    (32, '')
    (33, '')
    (34, '## Sanitize provided input and output directories')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=geparada/rna-seq-kallisto-sleuth, file=workflow/rules/diffexp.smk
context_key: ['if wildcards.model == "all"']
    (21, 'def get_model(wildcards):')
    (22, '    if wildcards.model == "all":')
    (23, '        return {"full": None}')
    (24, '    return config["diffexp"]["models"][wildcards.model]')
    (25, '')
    (26, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=geparada/rna-seq-kallisto-sleuth, file=workflow/rules/quant.smk
context_key: ['if len(input.fq) == 1']
    (13, 'def kallisto_params(wildcards, input):')
    (14, '    extra = config["params"]["kallisto"]')
    (15, '    if len(input.fq) == 1:')
    (16, '        extra += " --single"')
    (17, '        extra += (" --fragment-length {unit.fragment_len_mean} "')
    (18, '                  "--sd {unit.fragment_len_sd}").format(')
    (19, '                    unit=units.loc[')
    (20, '                        (wildcards.sample, wildcards.unit)])')
    (21, '    else:')
    (22, '        extra += " --fusion"')
    (23, '    return extra')
    (24, '')
    (25, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vinay-swamy/ocular_transcriptomes_pipeline, file=Snakefile
context_key: ["if sample_dict[sample][\\'subtissue\\']==subtissue"]
    (14, 'def subtissue_to_gtf(subtissue, sample_dict):')
    (15, '    res=[]')
    (16, '    for sample in sample_dict.keys():')
    (17, "        if sample_dict[sample][\\'subtissue\\']==subtissue :")
    (18, "            res.append(f\\'st_out/{sample}.gtf\\')")
    (19, '    return (res)')
    (20, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vinay-swamy/ocular_transcriptomes_pipeline, file=Snakefile
context_key: ['if paired']
    (21, 'def salmon_input(id,sample_dict,fql):')
    (22, "    paired=sample_dict[id][\\'paired\\']")
    (23, "    id= fql + \\'fastq_files/\\' + id")
    (24, '    if paired:')
    (25, "        return(\\'-1 {s}_1.fastq.gz -2 {s}_2.fastq.gz\\'.format(s=id))")
    (26, '    else:')
    (27, "        return(\\'-r {}.fastq.gz\\'.format(id))")
    (28, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vinay-swamy/ocular_transcriptomes_pipeline, file=Snakefile
context_key: ["if st == \\'all_tissues.combined\\'"]
    (34, 'def make_tx_fasta_input(st):')
    (35, "    if st == \\'all_tissues.combined\\':")
    (36, "        return \\'data/gtfs/all_tissues.combined.gtf\\'")
    (37, "    elif st == \\'pan_eye\\':")
    (38, "        return \\'data/gtfs/pan_eye.gtf\\'")
    (39, "    elif st == \\'gencode\\':")
    (40, "        return \\'ref/gencode_comp_ano.gtf\\'")
    (41, '    else:')
    (42, "        return f\\'data/gtfs/final_tissue_gtfs/{st}.gtf\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vinay-swamy/ocular_transcriptomes_pipeline, file=Snakefile
context_key: ["if sample_dict[key][\\'subtissue\\'] in eye_tissues"]
    (549, 'def all_quant_input(eye_tissues, sample_dict):')
    (550, '    res = []')
    (551, '    eye_tissues=set(eye_tissues)')
    (552, '    for key in sample_dict.keys():')
    (553, "        st = sample_dict[key][\\'subtissue\\']")
    (554, "        res.append(f\\'data/salmon_quant/{st}/{key}/quant.sf\\')")
    (555, "        res.append(f\\'data/salmon_quant/gencode/{key}/quant.sf\\')")
    (556, "        if sample_dict[key][\\'subtissue\\'] in eye_tissues:")
    (557, "            res.append(f\\'data/salmon_quant/pan_eye/{key}/quant.sf\\')")
    (558, '    return res ')
    (559, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bihealth/varfish-wf-validation, file=workflow/Snakefile
context_key: ['if case["name"] == case_name']
    (349, 'def load_case_uuid(path_case_list, case_name):')
    (350, '    with open(path_case_list) as inputf:')
    (351, '        for case in json.load(inputf):')
    (352, '            if case["name"] == case_name:')
    (353, '                return case["sodar_uuid"]  # early exit!')
    (354, '    raise RuntimeError(f"Could not resolve {case_name} to UUID!")')
    (355, '')
    (356, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=robinmeyers/atac-encode-snakemake, file=Snakefile
context_key: ['if condition_paired_end and len(R2_fastqs) == 0']
    (59, 'def get_samples(condition):')
    (60, '    ')
    (61, '    condition_title = ""')
    (62, '    condition_description = ""')
    (63, '    condition_paired_end = True')
    (64, "    condition_samples = samples[samples[\\'Condition\\']==condition]")
    (65, "    condition_dict = {\\'fastqs\\' : {}}")
    (66, '')
    (67, '    biorep = 1')
    (68, '    for sample, row in condition_samples.iterrows():')
    (69, '        R1_fastqs = []')
    (70, '        R2_fastqs = []')
    (71, "        for fastq_dir in config[\\'fastq_dirs\\']:")
    (72, '            R1_fastqs = R1_fastqs + \\\\')
    (73, '                glob.glob(os.path.join(fastq_dir, sample + r1_fastq_suffix)) + \\\\')
    (74, '                glob.glob(os.path.join(fastq_dir, sample, sample + r1_fastq_suffix))')
    (75, '            R2_fastqs = R2_fastqs + \\\\')
    (76, '                glob.glob(os.path.join(fastq_dir, sample + r2_fastq_suffix)) + \\\\')
    (77, '                glob.glob(os.path.join(fastq_dir, sample, sample + r2_fastq_suffix))')
    (78, '')
    (79, '        if condition_paired_end and len(R2_fastqs) == 0:')
    (80, '            condition_paired_end = False')
    (81, '        condition_title = row["Title"] if (condition_title == "" and row["Title"] != "") else condition_title')
    (82, '        condition_description = row["Description"] if condition_description == "" and row["Description"] != "" else condition_description')
    (83, "        condition_dict[\\'fastqs\\'][\\'rep\\' + str(biorep)] = {\\'R1\\' : R1_fastqs, \\'R2\\' : R2_fastqs}")
    (84, '        biorep += 1')
    (85, '    ')
    (86, "    condition_dict[\\'paired_end\\'] = condition_paired_end")
    (87, "    condition_dict[\\'title\\'] = condition_title")
    (88, "    condition_dict[\\'description\\'] = condition_description")
    (89, '')
    (90, '    return(condition_dict)')
    (91, '')
    (92, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=robinmeyers/atac-encode-snakemake, file=Snakefile
context_key: ["if len(conditions_dict) > 1 and config[\\'make_consensus\\']"]
    (59, 'def get_samples(condition):')
    (60, '    ')
    (61, '    condition_title = ""')
    (62, '    condition_description = ""')
    (63, '    condition_paired_end = True')
    (64, "    condition_samples = samples[samples[\\'Condition\\']==condition]")
    (65, "    condition_dict = {\\'fastqs\\' : {}}")
    (66, '')
    (67, '    biorep = 1')
    (68, '    for sample, row in condition_samples.iterrows():')
    (69, '        R1_fastqs = []')
    (70, '        R2_fastqs = []')
    (71, "        for fastq_dir in config[\\'fastq_dirs\\']:")
    (72, '            R1_fastqs = R1_fastqs + \\\\')
    (73, '                glob.glob(os.path.join(fastq_dir, sample + r1_fastq_suffix)) + \\\\')
    (74, '                glob.glob(os.path.join(fastq_dir, sample, sample + r1_fastq_suffix))')
    (75, '            R2_fastqs = R2_fastqs + \\\\')
    (76, '                glob.glob(os.path.join(fastq_dir, sample + r2_fastq_suffix)) + \\\\')
    (77, '                glob.glob(os.path.join(fastq_dir, sample, sample + r2_fastq_suffix))')
    (78, '')
    (79, '        if condition_paired_end and len(R2_fastqs) == 0:')
    (80, '            condition_paired_end = False')
    (81, '        condition_title = row["Title"] if (condition_title == "" and row["Title"] != "") else condition_title')
    (82, '        condition_description = row["Description"] if condition_description == "" and row["Description"] != "" else condition_description')
    (83, "        condition_dict[\\'fastqs\\'][\\'rep\\' + str(biorep)] = {\\'R1\\' : R1_fastqs, \\'R2\\' : R2_fastqs}")
    (84, '        biorep += 1')
    (85, '    ')
    (86, "    condition_dict[\\'paired_end\\'] = condition_paired_end")
    (87, "    condition_dict[\\'title\\'] = condition_title")
    (88, "    condition_dict[\\'description\\'] = condition_description")
    (89, '')
    (90, '    return(condition_dict)')
    (91, '')
    (92, '')
    (93, 'conditions_dict = {}')
    (94, '')
    (95, 'for condition in np.unique(samples["Condition"]).tolist():')
    (96, '    conditions_dict[condition] = get_samples(condition)')
    (97, '')
    (98, "if len(conditions_dict) > 1 and config[\\'make_consensus\\']:")
    (99, '    groupings_dict = {')
    (100, "        \\'consensus\\' : {")
    (101, '            \\\'title\\\' : "Consensus",')
    (102, '            \\\'description\\\' : "All samples combined",')
    (103, "            \\'conditions\\' : list(conditions_dict)")
    (104, '            }')
    (105, '        }')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=robinmeyers/atac-encode-snakemake, file=Snakefile
context_key: ['if "grouping_columns" in config.keys()']
    (59, 'def get_samples(condition):')
    (60, '    ')
    (61, '    condition_title = ""')
    (62, '    condition_description = ""')
    (63, '    condition_paired_end = True')
    (64, "    condition_samples = samples[samples[\\'Condition\\']==condition]")
    (65, "    condition_dict = {\\'fastqs\\' : {}}")
    (66, '')
    (67, '    biorep = 1')
    (68, '    for sample, row in condition_samples.iterrows():')
    (69, '        R1_fastqs = []')
    (70, '        R2_fastqs = []')
    (71, "        for fastq_dir in config[\\'fastq_dirs\\']:")
    (72, '            R1_fastqs = R1_fastqs + \\\\')
    (73, '                glob.glob(os.path.join(fastq_dir, sample + r1_fastq_suffix)) + \\\\')
    (74, '                glob.glob(os.path.join(fastq_dir, sample, sample + r1_fastq_suffix))')
    (75, '            R2_fastqs = R2_fastqs + \\\\')
    (76, '                glob.glob(os.path.join(fastq_dir, sample + r2_fastq_suffix)) + \\\\')
    (77, '                glob.glob(os.path.join(fastq_dir, sample, sample + r2_fastq_suffix))')
    (78, '')
    (79, '        if condition_paired_end and len(R2_fastqs) == 0:')
    (80, '            condition_paired_end = False')
    (81, '        condition_title = row["Title"] if (condition_title == "" and row["Title"] != "") else condition_title')
    (82, '        condition_description = row["Description"] if condition_description == "" and row["Description"] != "" else condition_description')
    (83, "        condition_dict[\\'fastqs\\'][\\'rep\\' + str(biorep)] = {\\'R1\\' : R1_fastqs, \\'R2\\' : R2_fastqs}")
    (84, '        biorep += 1')
    (85, '    ')
    (86, "    condition_dict[\\'paired_end\\'] = condition_paired_end")
    (87, "    condition_dict[\\'title\\'] = condition_title")
    (88, "    condition_dict[\\'description\\'] = condition_description")
    (89, '')
    (90, '    return(condition_dict)')
    (91, '')
    (92, '')
    (93, 'conditions_dict = {}')
    (94, '')
    (95, 'for condition in np.unique(samples["Condition"]).tolist():')
    (96, '    conditions_dict[condition] = get_samples(condition)')
    (97, '')
    (98, "if len(conditions_dict) > 1 and config[\\'make_consensus\\']:")
    (99, '    groupings_dict = {')
    (100, "        \\'consensus\\' : {")
    (101, '            \\\'title\\\' : "Consensus",')
    (102, '            \\\'description\\\' : "All samples combined",')
    (103, "            \\'conditions\\' : list(conditions_dict)")
    (104, '            }')
    (105, '        }')
    (106, 'else:')
    (107, '    groupings_dict = {}')
    (108, '')
    (109, '')
    (110, '')
    (111, 'if "grouping_columns" in config.keys():')
    (112, '    for grouping in config["grouping_columns"]:')
    (113, '        for group in np.unique(samples[grouping]).tolist():')
    (114, '# for grouping, groups in grouping_list.items():')
    (115, '    # for group in groups:')
    (116, '            groupings_dict[group] = {')
    (117, "                \\'title\\' : group,")
    (118, '                \\\'description\\\' : group + " samples grouped by " + grouping,')
    (119, '                \\\'conditions\\\' : np.unique(samples[samples[grouping]==group]["Condition"]).tolist()')
    (120, '                }')
    (121, '')
    (122, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=robinmeyers/atac-encode-snakemake, file=Snakefile
context_key: ['if (wildcards.is_grouped)']
    (185, 'def cromwell_inputs(wildcards):')
    (186, '    inputs = {\\\'json\\\' : os.path.join("jsons", wildcards.is_grouped + wildcards.condition + ".json")}')
    (187, '    if (wildcards.is_grouped):')
    (188, '        inputs[\\\'tagalign\\\'] = os.path.join("results/groups/", wildcards.condition, wildcards.condition + ".grouped.tagAlign.gz")')
    (189, '    return inputs')
    (190, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=robinmeyers/atac-encode-snakemake, file=Snakefile
context_key: ["if condition_dict[\\'paired_end\\']"]
    (233, 'def make_json_from_template(condition, json_out_file):')
    (234, "    json_in = open(config[\\'template_json\\'], \\'r\\')")
    (235, "    json_out = open(json_out_file, \\'w\\')")
    (236, '    sample_json = json.load(json_in)')
    (237, '    condition_dict = conditions_dict[condition]')
    (238, '')
    (239, "    sample_json[\\'atac.title\\'] = condition_dict[\\'title\\']")
    (240, "    sample_json[\\'atac.description\\'] = condition_dict[\\'description\\']")
    (241, "    sample_json[\\'atac.paired_end\\'] = condition_dict[\\'paired_end\\']")
    (242, "    for rep in condition_dict[\\'fastqs\\'].keys():")
    (243, '        sample_json[\\\'atac.fastqs_\\\' + rep + "_R1"] = condition_dict[\\\'fastqs\\\'][rep][\\\'R1\\\']')
    (244, "        if condition_dict[\\'paired_end\\']:")
    (245, '            sample_json[\\\'atac.fastqs_\\\' + rep + "_R2"] = condition_dict[\\\'fastqs\\\'][rep][\\\'R2\\\']')
    (246, '')
    (247, '    json.dump(sample_json, json_out, indent=4)')
    (248, '    json_in.close()')
    (249, '')
    (250, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/snv_indels, file=.tests/compatibility/Snakefile
context_key: ['if result']
    (15, 'def extract_module_version_from_readme(modulename):')
    (16, '    search_string = modulename + ":(.+)\\')
    (17, '$"')
    (18, '    with open("../../README.md", "r") as reader:')
    (19, '        for line in reader:')
    (20, '            result = re.search(search_string, line)')
    (21, '            if result:')
    (22, '                return result[1]')
    (23, '')
    (24, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/snv_indels, file=workflow/rules/common.smk
context_key: ['if "-Xmx" in java_opts']
    (58, 'def get_java_opts(wildcards: snakemake.io.Wildcards):')
    (59, '    java_opts = config.get("haplotypecaller", {}).get("java_opts", "")')
    (60, '    if "-Xmx" in java_opts:')
    (61, '        raise WorkflowError("You are not allowed to use -Xmx in java_opts. Set mem_mb in resources instead.")')
    (62, '    java_opts += "-Xmx{}m".format(config.get("haplotypecaller", {}).get("mem_mb", config["default_resources"]["mem_mb"]))')
    (63, '    return java_opts')
    (64, '')
    (65, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/snv_indels, file=workflow/rules/common.smk
context_key: ['if name == "gatk_mutect2"']
    (66, 'def get_gatk_mutect2_extra(wildcards: snakemake.io.Wildcards, name: str):')
    (67, '    extra = "{} {}".format(')
    (68, '        config.get(name, {}).get("extra", ""),')
    (69, '        "--intervals snv_indels/bed_split/design_bedfile_{}.bed".format(')
    (70, '            wildcards.chr,')
    (71, '        ),')
    (72, '    )')
    (73, '    if name == "gatk_mutect2":')
    (74, '        extra = "{} {}".format(')
    (75, '            extra,')
    (76, '            "--f1r2-tar-gz snv_indels/gatk_mutect2/{}_{}_{}.unfiltered.f1r2.tar.gz".format(')
    (77, '                wildcards.sample,')
    (78, '                wildcards.type,')
    (79, '                wildcards.chr,')
    (80, '            ),')
    (81, '        )')
    (82, '    if name == "gatk_mutect2_gvcf":')
    (83, '        extra = "{} {}".format(extra, "-ERC BP_RESOLUTION")')
    (84, '    return extra')
    (85, '')
    (86, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ["if mat_type == \\'all_visual\\'"]
    (71, 'def get_n_classes(session, mat_type):')
    (72, "    if mat_type == \\'all_visual\\':")
    (73, '        return 1')
    (74, '    else:')
    (75, '        try:')
    (76, '            n = N_CLASSES[session]')
    (77, '        except KeyError:')
    (78, '            # then this is probably the groupaverage session, which is')
    (79, '            # slightly differnetly formatted')
    (80, "            ses = re.findall(\\'(ses-[0-9]+)_v[0-9]+_s[0-9]+\\',session)[0]")
    (81, '            n = N_CLASSES[ses]')
    (82, "        if \\'blanks\\' in mat_type:")
    (83, '            n += 1')
    (84, '        return n')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ["if \\'pilot\\' in wildcards.session"]
    (85, 'def get_stim_files(wildcards):')
    (86, "    if \\'pilot\\' in wildcards.session:")
    (87, '        session_prefix = wildcards.session + "_"')
    (88, '    else:')
    (89, '        session_prefix = ""')
    (90, '    task_prefix = wildcards.task')
    (91, '    file_stem = os.path.join(config[\\\'DATA_DIR\\\'], \\\'stimuli\\\', task_prefix+"_"+session_prefix+"{rest}")')
    (92, "    return {\\'stim\\': file_stem.format(rest=\\'stimuli.npy\\'),")
    (93, "            \\'desc_csv\\': file_stem.format(rest=\\'stim_description.csv\\')}")
    (94, '# the goal of these two dictionaries is to always have a unique integer')
    (95, '# when summed together. to that end the subjects one should increase')
    (96, "# first along the ones digit and then, when you\\'ve run out of digits,")
    (97, '# along the hundreds')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['if seed is not None']
    (160, 'def create_crossval_idx(leave_n_out, session, mat_type, seed=None):')
    (161, '    if seed is not None:')
    (162, '        np.random.seed(seed)')
    (163, '    total_n = get_n_classes(session, mat_type)')
    (164, '    idx = np.arange(total_n)')
    (165, '    np.random.shuffle(idx)')
    (166, "    # if total_n/leave_n_out isn\\'t an integer, then some of these will be longer than the other")
    (167, '    splits = np.array_split(idx, total_n / leave_n_out)')
    (168, '    # this returns a list of strings, each of which looks like e.g., #,#,#,# (if leave_n_out=4)')
    (169, '    return [\\\',\\\'.join(["%02d"%j for j in i]) for i in splits]')
    (170, '')
    (171, '# small tests to make sure snakemake is playing nicely with the job management')
    (172, '# system.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['if crossval_seed is None']
    (202, 'def get_model_subj_outputs(model_type, subject, session, task, batch_size=10, learning_rate=1e-3,')
    (203, "                           crossval_seed=None, bootstrap_num=None, vareas=1, eccen=\\'1-12\\', df_mode=\\'summary\\', gpus=0,")
    (204, "                           mat_type=\\'stim_class\\', atlas_type=\\'bayesian_posterior\\', modeling_goal=\\'initial\\',")
    (205, "                           df_filter=\\'filter-mean\\'):")
    (206, '    output_path = os.path.join(config[\\\'DATA_DIR\\\'], "derivatives", "tuning_2d_model", "{mat_type}",')
    (207, '                               "{atlas_type}", "{df_filter}", "{modeling_goal}", "{subject}", "{session}",')
    (208, '                               "{subject}_{session}_{task}_v{vareas}_e{eccen}_{df_mode}_b{batch}_"')
    (209, '                               "r{lr}_g{gpus}_c{{crossval}}_n{bootstrap_num}_{model_type}_loss.csv")')
    (210, '    output_path = output_path.format(subject=subject, session=session, task=task, batch=batch_size,')
    (211, '                                     lr=learning_rate, model_type=model_type, vareas=vareas,')
    (212, '                                     eccen=eccen, df_mode=df_mode, gpus=gpus, atlas_type=atlas_type,')
    (213, '                                     mat_type=mat_type, modeling_goal=modeling_goal,')
    (214, '                                     bootstrap_num=bootstrap_num, df_filter=df_filter)')
    (215, '    if crossval_seed is None:')
    (216, '        return output_path.format(crossval=None)')
    (217, '    else:')
    (218, '        return [output_path.format(crossval=n) for n in create_crossval_idx(4, session, mat_type, int(crossval_seed))]')
    (219, '')
    (220, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['if crossval_seed is None']
    (221, 'def get_simulated_model_outputs(model_type, sim_model_type, noise_level, num_voxels,')
    (222, '                                batch_size, learning_rate, sigma, sf_ecc_slope, sf_ecc_intercept,')
    (223, '                                rel_mode_cardinals, rel_mode_obliques, rel_amplitude_cardinals,')
    (224, '                                rel_amplitude_obliques, abs_mode_cardinals, abs_mode_obliques,')
    (225, '                                abs_amplitude_cardinals, abs_amplitude_obliques, noise_source,')
    (226, "                                crossval_seed=None, gpus=0, modeling_goal=\\'model_recovery\\'):")
    (227, '    output_path = os.path.join(config[\\\'DATA_DIR\\\'], "derivatives", "tuning_2d_simulated",')
    (228, '                               "{noise_source}", "{modeling_goal}",')
    (229, '                               "n{num_voxels}_{sim_model_type}_s{sigma}_a{sf_ecc_slope}_b{sf_ecc_intercept}_"')
    (230, '                               "rmc{rel_mode_cardinals}_rmo{rel_mode_obliques}_"')
    (231, '                               "rac{rel_amplitude_cardinals}_rao{rel_amplitude_obliques}_"')
    (232, '                               "amc{abs_mode_cardinals}_amo{abs_mode_obliques}_"')
    (233, '                               "aac{abs_amplitude_cardinals}_aao{abs_amplitude_obliques}_"')
    (234, '                               "l{noise_level}_b{batch_size}_r{learning_rate}_g{gpus}_"')
    (235, '                               "c{{crossval}}_{model_type}_loss.csv")')
    (236, '    output_path = output_path.format(batch_size=batch_size, learning_rate=learning_rate,')
    (237, '                                     gpus=gpus, modeling_goal=modeling_goal, num_voxels=num_voxels,')
    (238, '                                     sim_model_type=sim_model_type, noise_level=noise_level, ')
    (239, '                                     sf_ecc_slope=sf_ecc_slope, sf_ecc_intercept=sf_ecc_intercept,')
    (240, '                                     rel_mode_cardinals=rel_mode_cardinals, sigma=sigma,')
    (241, '                                     rel_mode_obliques=rel_mode_obliques, model_type=model_type,')
    (242, '                                     rel_amplitude_cardinals=rel_amplitude_cardinals,')
    (243, '                                     rel_amplitude_obliques=rel_amplitude_obliques,')
    (244, '                                     abs_mode_cardinals=abs_mode_cardinals,')
    (245, '                                     abs_mode_obliques=abs_mode_obliques,')
    (246, '                                     abs_amplitude_cardinals=abs_amplitude_cardinals,')
    (247, '                                     abs_amplitude_obliques=abs_amplitude_obliques,')
    (248, "                                     noise_source=noise_source).replace(\\'0.\\', \\'.\\')")
    (249, '    if crossval_seed is None:')
    (250, '        return output_path.format(crossval=None)')
    (251, '    else:')
    (252, "        ses = re.findall(r\\'(ses-[0-9]+)\\', noise_source)[0]")
    (253, "        return [output_path.format(crossval=n) for n in create_crossval_idx(4, ses, \\'stim_class\\', int(crossval_seed))]")
    (254, '')
    (255, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ["if modeling_goal != \\'initial\\'"]
    (292, "def get_groupaverage_all(tuning_type=\\'2d\\', interp=\\'linear\\', session=\\'ses-04\\', task=\\'task-sfprescaled\\',")
    (293, "                         model_type=\\'full_full_absolute\\', vareas=\\'1\\', eccen=\\'1-12\\', batch_size=10,")
    (294, "                         learning_rate=0.001, gpus=0, df_mode=\\'summary\\', mat_type=\\'stim_class\\',")
    (295, "                         atlas_type=\\'bayesian_posterior\\', modeling_goal=\\'initial\\',")
    (296, "                         df_filter=\\'filter-mean\\'):")
    (297, "    if modeling_goal != \\'initial\\':")
    (298, '        return []')
    (299, "    if tuning_type == \\'2d\\':")
    (300, '        path = os.path.join(config[\\\'DATA_DIR\\\'], "derivatives", "tuning_2d_model", mat_type,')
    (301, '                            atlas_type, df_filter, "initial", f"sub-groupaverage_i-{interp}",')
    (302, '                            f"{session}_v{vareas}_s{{n:02d}}", f"sub-groupaverage_i-{interp}_{session}"')
    (303, '                            f"_v{vareas}_s{{n:02d}}_{task}_v{vareas}_e{eccen}_{df_mode}_b{batch_size}_"')
    (304, '                            f"r{learning_rate}_g{gpus}_cNone_nNone_{model_type}_loss.csv")')
    (305, "    elif tuning_type == \\'curve\\':")
    (306, "        path = os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'tuning_curves\\', mat_type,")
    (307, "                            atlas_type, f\\'sub-groupaverage_i-{interp}\\', f\\'{session}_v{vareas}_s{{n:02d}}\\',")
    (308, "                            f\\'sub-groupaverage_i-{interp}_{session}_v{vareas}_s{{n:02d}}_{task}_\\'")
    (309, "                            f\\'v{vareas}_e{eccen}_eccen_bin_{df_mode}.csv\\')")
    (310, '        ')
    (311, '    seeds = list(range(104))')
    (312, "    # there are 4 seeds that won\\'t work, so we remove them. there are")
    (313, '    # some voxels where some subjects have NaNs after')
    (314, '    # interpolation. when doing our weighted average across subjects, we')
    (315, '    # ignore those NaNs, but for these seeds, they managed to pick')
    (316, "    # subjects that *all* have NaNs in at least one voxel, so there\\'s")
    (317, '    # nothing we can do.')
    (318, '    for i in [17, 31, 51, 65]:')
    (319, '        seeds.remove(i)')
    (320, '    return [path.format(n=n) for n in seeds]')
    (321, '')
    (322, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['if "permuted" in wildcards.mat_type']
    (644, 'def get_permuted(wildcards):')
    (645, '    if "permuted" in wildcards.mat_type:')
    (646, '        return "-p"')
    (647, '    else:')
    (648, '        return ""')
    (649, '')
    (650, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['try', "if wildcards.atlas_type == \\'atlas\\'"]
    (798, "def find_prf_mgz(wildcards, prf_prop=\\'varea\\', subject=None):")
    (799, '    try:')
    (800, '        prf_prop = wildcards.prf_prop')
    (801, '    except AttributeError:')
    (802, '        prf_prop = prf_prop')
    (803, "    if wildcards.atlas_type == \\'atlas\\':")
    (804, "        benson_prefix = \\'benson14_\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['try', "elif wildcards.atlas_type == \\'bayesian_posterior\\'"]
    (798, "def find_prf_mgz(wildcards, prf_prop=\\'varea\\', subject=None):")
    (799, '    try:')
    (800, '        prf_prop = wildcards.prf_prop')
    (801, '    except AttributeError:')
    (802, '        prf_prop = prf_prop')
    (803, "    if wildcards.atlas_type == \\'atlas\\':")
    (804, "        benson_prefix = \\'benson14_\\'")
    (805, "    elif wildcards.atlas_type == \\'bayesian_posterior\\':")
    (806, "        benson_prefix = \\'inferred_\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['try', "elif wildcards.atlas_type == \\'data\\'"]
    (798, "def find_prf_mgz(wildcards, prf_prop=\\'varea\\', subject=None):")
    (799, '    try:')
    (800, '        prf_prop = wildcards.prf_prop')
    (801, '    except AttributeError:')
    (802, '        prf_prop = prf_prop')
    (803, "    if wildcards.atlas_type == \\'atlas\\':")
    (804, "        benson_prefix = \\'benson14_\\'")
    (805, "    elif wildcards.atlas_type == \\'bayesian_posterior\\':")
    (806, "        benson_prefix = \\'inferred_\\'")
    (807, "    elif wildcards.atlas_type == \\'data\\':")
    (808, "        benson_prefix = \\'full-\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ["if wildcards.mat_type.endswith(\\'noise-ceiling-1\\')"]
    (902, 'def GLMdenoise_runs(wildcards):')
    (903, '    """return the runs to use for this mat_type')
    (904, '    """')
    (905, '    total_runs = NRUNS.get((wildcards.subject, wildcards.session), 12)')
    (906, "    # because we\\'re passing this to matlab, we need this to be a list of")
    (907, '    # ints that go from 1 to total_runs (inclusive).')
    (908, "    if wildcards.mat_type.endswith(\\'noise-ceiling-1\\'):")
    (909, '        runs = np.arange(1, total_runs+1, 2)')
    (910, "    elif wildcards.mat_type.endswith(\\'noise-ceiling-2\\'):")
    (911, '        runs = np.arange(2, total_runs+1, 2)')
    (912, '    else:')
    (913, '        runs = []')
    (914, "    runs = \\',\\'.join([str(i) for i in runs])")
    (915, '    runs = f"[{runs}]"')
    (916, '    return runs')
    (917, '')
    (918, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ["if wildcards.subject.startswith(\\'sub-groupaverage\\')"]
    (1086, 'def get_first_level_analysis_input(wildcards):')
    (1087, '    input_dict = {}')
    (1088, '    input_dict[\\\'GLM_results\\\'] = os.path.join(config["DATA_DIR"], "derivatives", "GLMdenoise", "{mat_type}", "{atlas_type}", "{subject}", "{session}", "{subject}_{session}_{task}_results.mat").format(**wildcards)')
    (1089, "    benson_names = [\\'angle\\', \\'eccen\\', \\'varea\\']")
    (1090, "    if wildcards.subject.startswith(\\'sub-groupaverage\\'):")
    (1091, "        benson_prefix = \\'benson14\\'")
    (1092, "        benson_temp = os.path.join(os.path.dirname(ny.__file__), \\'lib\\', \\'data\\', \\'fsaverage\\', \\'surf\\', \\'{hemi}.\\'+benson_prefix+\\'_{filename}.v4_0.mgz\\')")
    (1093, "        benson_names += [\\'sigma\\']")
    (1094, '    else:')
    (1095, "        if wildcards.atlas_type == \\'atlas\\':")
    (1096, "            benson_prefix = \\'benson14\\'")
    (1097, "        elif wildcards.atlas_type == \\'bayesian_posterior\\':")
    (1098, "            benson_prefix = \\'inferred\\'")
    (1099, "        benson_temp = os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'prf_solutions\\', wildcards.subject, wildcards.atlas_type, \\'{hemi}.\\'+benson_prefix+\\'_{filename}.mgz\\')")
    (1100, "        prf_temp = os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'prf_solutions\\', wildcards.subject, \\'data\\', \\'{hemi}.full-{filename}.mgz\\')")
    (1101, "        input_dict[\\'prf_sigma_path\\'] = expand(prf_temp, hemi=[\\'lh\\', \\'rh\\'], filename=[\\'sigma\\', \\'vexpl\\'])")
    (1102, "    input_dict[\\'benson_paths\\'] = expand(benson_temp, hemi=[\\'lh\\', \\'rh\\'], filename=benson_names)")
    (1103, '    return input_dict')
    (1104, '')
    (1105, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ["if \\'pilot\\' in wildcards.session"]
    (1106, 'def get_stim_type(wildcards):')
    (1107, "    if \\'pilot\\' in wildcards.session:")
    (1108, "        return \\'pilot\\'")
    (1109, '    else:')
    (1110, "        if \\'constant\\' in wildcards.task:")
    (1111, "            return \\'constant\\'")
    (1112, '        else:')
    (1113, "            return \\'logpolar\\'")
    (1114, '')
    (1115, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['if isinstance(input.benson_paths, str)']
    (1116, 'def get_benson_template(wildcards, input):')
    (1117, '    # for some reason, sometimes input.benson_paths is a single str,')
    (1118, "    # sometimes it\\'s a list of strings (it always requires multiple")
    (1119, "    # benson_paths as input, just sometimes the way they\\'re stored is")
    (1120, '    # different?). It might be related to snakemake version or something')
    (1121, "    # else I\\'m having trouble controlling across machines. regardless,")
    (1122, '    # this does it')
    (1123, '    if isinstance(input.benson_paths, str):')
    (1124, '        path = input.benson_paths')
    (1125, '    else:')
    (1126, '        path = input.benson_paths[0]')
    (1127, "    return path.replace(\\'lh\\', \\'%s\\').replace(\\'angle\\', \\'%s\\').replace(\\'benson14_\\', \\'\\').replace(\\'inferred_\\', \\'\\').replace(wildcards.atlas_type, \\'%s\\').replace(\\'surf\\', \\'%s\\'),")
    (1128, '')
    (1129, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['if "eccen_bin" in wildcards.binning']
    (1166, 'def get_binning(wildcards):')
    (1167, '    bin_str = ""')
    (1168, '    if "eccen_bin" in wildcards.binning:')
    (1169, '        bin_str += "--eccen "')
    (1170, '    if "angle_bin" in wildcards.binning:')
    (1171, '        bin_str += "--angle"')
    (1172, '    if not bin_str:')
    (1173, '        raise Exception("You must bin by something!")')
    (1174, '    return bin_str')
    (1175, '')
    (1176, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ["if wildcards.groupaverage == \\'individual\\'", "if wildcards.atlas_type == \\'atlas\\'"]
    (1229, 'def get_tuning_curves(wildcards):')
    (1230, "    if wildcards.groupaverage == \\'individual\\':")
    (1231, "        if wildcards.atlas_type == \\'atlas\\':")
    (1232, "            subjects = [\\'sub-wlsubj001\\', \\'sub-wlsubj042\\', \\'sub-wlsubj045\\']")
    (1233, "            sessions = {\\'sub-wlsubj001\\': [\\'ses-pilot01\\'], \\'sub-wlsubj042\\': [\\'ses-pilot01\\'],")
    (1234, "                        \\'sub-wlsubj045\\': [\\'ses-pilot01\\']}")
    (1235, '        else:')
    (1236, '            sessions = {k: [wildcards.session] for k, v in SESSIONS.items() if wildcards.session in v}')
    (1237, '            subjects = [s for s in SUBJECTS if s in sessions.keys()]')
    (1238, "        vareas = wildcards.vareas.split(\\'-\\')")
    (1239, "        return [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'tuning_curves\\', \\'{mat_type}\\',")
    (1240, "                             \\'{atlas_type}\\', \\'{subject}\\', \\'{session}\\', \\'{subject}_{session}_{task}_\\'")
    (1241, "                             \\'v{vareas}_e{eccen}_{binning}_{df_mode}.csv\\').format(mat_type=wildcards.mat_type,")
    (1242, '                                                                                 atlas_type=wildcards.atlas_type,')
    (1243, '                                                                                 subject=sub, session=ses,')
    (1244, '                                                                                 task=TASKS[(sub, ses)],')
    (1245, '                                                                                 vareas=v,')
    (1246, '                                                                                 eccen=wildcards.eccen,')
    (1247, '                                                                                 binning=wildcards.binning,')
    (1248, '                                                                                 df_mode=wildcards.df_mode)')
    (1249, '                for sub in subjects for ses in sessions[sub] for v in vareas]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ["if wildcards.groupaverage == \\'individual\\'", "elif wildcards.groupaverage == \\'sub-groupaverage\\'"]
    (1229, 'def get_tuning_curves(wildcards):')
    (1230, "    if wildcards.groupaverage == \\'individual\\':")
    (1231, "        if wildcards.atlas_type == \\'atlas\\':")
    (1232, "            subjects = [\\'sub-wlsubj001\\', \\'sub-wlsubj042\\', \\'sub-wlsubj045\\']")
    (1233, "            sessions = {\\'sub-wlsubj001\\': [\\'ses-pilot01\\'], \\'sub-wlsubj042\\': [\\'ses-pilot01\\'],")
    (1234, "                        \\'sub-wlsubj045\\': [\\'ses-pilot01\\']}")
    (1235, '        else:')
    (1236, '            sessions = {k: [wildcards.session] for k, v in SESSIONS.items() if wildcards.session in v}')
    (1237, '            subjects = [s for s in SUBJECTS if s in sessions.keys()]')
    (1238, "        vareas = wildcards.vareas.split(\\'-\\')")
    (1239, "        return [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'tuning_curves\\', \\'{mat_type}\\',")
    (1240, "                             \\'{atlas_type}\\', \\'{subject}\\', \\'{session}\\', \\'{subject}_{session}_{task}_\\'")
    (1241, "                             \\'v{vareas}_e{eccen}_{binning}_{df_mode}.csv\\').format(mat_type=wildcards.mat_type,")
    (1242, '                                                                                 atlas_type=wildcards.atlas_type,')
    (1243, '                                                                                 subject=sub, session=ses,')
    (1244, '                                                                                 task=TASKS[(sub, ses)],')
    (1245, '                                                                                 vareas=v,')
    (1246, '                                                                                 eccen=wildcards.eccen,')
    (1247, '                                                                                 binning=wildcards.binning,')
    (1248, '                                                                                 df_mode=wildcards.df_mode)')
    (1249, '                for sub in subjects for ses in sessions[sub] for v in vareas]')
    (1250, "    elif wildcards.groupaverage == \\'sub-groupaverage\\':")
    (1251, "        return get_groupaverage_all(\\'curve\\')")
    (1252, '')
    (1253, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['if ON_CLUSTER']
    (1302, 'def to_log_or_not(wildcards):')
    (1303, '    """we only log directly if we\\\'re not on the cluster, otherwise we trust the cluster to handle it')
    (1304, '    """')
    (1305, '    if ON_CLUSTER:')
    (1306, '        return "; echo"')
    (1307, '    else:')
    (1308, '        return "&> "')
    (1309, '')
    (1310, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['if wildcards.modeling_goal.startswith("visual_field")']
    (1311, 'def visual_field_part(wildcards):')
    (1312, '    """if modeling_goal specifies it, add the string to reduce part of visual field')
    (1313, '    """')
    (1314, '    vis_field = ""')
    (1315, '    if wildcards.modeling_goal.startswith("visual_field"):')
    (1316, '        vis_field += ",restrict_to_part_of_visual_field:" + wildcards.modeling_goal.split(\\\'_\\\')[-1]')
    (1317, '    return vis_field')
    (1318, '')
    (1319, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ["if wildcards.df_filter == \\'filter-any\\'"]
    (1320, 'def get_df_filter_str(wildcards):')
    (1321, "    if wildcards.df_filter == \\'filter-any\\':")
    (1322, "        df_filter_str = \\'drop_voxels_with_any_negative_amplitudes,drop_voxels_near_border\\'")
    (1323, "    elif wildcards.df_filter == \\'filter-mean\\':")
    (1324, "        df_filter_str = \\'drop_voxels_with_mean_negative_amplitudes,drop_voxels_near_border\\'")
    (1325, "    elif wildcards.df_filter == \\'no-filter\\':")
    (1326, '        df_filter_str = None')
    (1327, '    return df_filter_str')
    (1328, '')
    (1329, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ["if wildcards.modeling_goal == \\'bootstrap\\'"]
    (1494, 'def gather_model_results_input(wildcards):')
    (1495, '    inputs = {}')
    (1496, '    # wildcards is not an actual dictionary (it just functions like one')
    (1497, '    # in some regards) and so we need to make one if we want to do the')
    (1498, '    # pop() call below')
    (1499, '    format_kwargs = dict(wildcards)')
    (1500, "    groupaverage = format_kwargs.pop(\\'groupaverage\\', \\'individual\\')")
    (1501, "    if wildcards.modeling_goal == \\'bootstrap\\':")
    (1502, '        loss_files = [get_model_subj_outputs(bootstrap_num=n, **format_kwargs) for n in range(100)]')
    (1503, '    else:')
    (1504, "        if groupaverage == \\'individual\\':")
    (1505, '            loss_files = [get_model_subj_outputs(subject=subj, session=ses, **format_kwargs)')
    (1506, '                          for subj in SUBJECTS for ses in SESSIONS[subj]')
    (1507, '                          if TASKS[(subj, ses)] == wildcards.task]')
    (1508, "        elif groupaverage == \\'sub-groupaverage\\':")
    (1509, '            loss_files = get_groupaverage_all(**format_kwargs)')
    (1510, '    # this will return a list of lists of strings, so we need to flatten it')
    (1511, "    inputs[\\'loss_files\\'] = np.array(loss_files).flatten()")
    (1512, '    return inputs')
    (1513, '')
    (1514, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ["if wildcards.modeling_goal == \\'learning_hyperparams_full\\'"]
    (1683, 'def gather_simulated_model_results_input(wildcards):')
    (1684, '    inputs = {}')
    (1685, "    if wildcards.modeling_goal == \\'learning_hyperparams_full\\':")
    (1686, '        batch = [1, 10, 100]')
    (1687, '        lr = [1e-2, 1e-3, 1e-4]')
    (1688, "        models = [\\'iso_full_iso\\', \\'full_full_full\\']")
    (1689, '        loss_files = []')
    (1690, '        for b, l, m  in itertools.product(batch, lr, models):')
    (1691, '            loss_files.append(get_simulated_model_outputs(')
    (1692, "                m, \\'iso_full_iso\\', 1, 4000, b, l, 1, .75, .25, 0, 0, 0, 0, 0, 0, 0, 0,")
    (1693, '                **wildcards))')
    (1694, '            loss_files.append(get_simulated_model_outputs(')
    (1695, "                m, \\'full_full_full\\', 1, 4000, b, l, 1, .75, .25, .1, .05, .03, .1, .2, .05, .04,")
    (1696, '                .3, **wildcards))')
    (1697, "    elif wildcards.modeling_goal == \\'model_recovery\\':")
    (1698, '        loss_files = []')
    (1699, '        for m  in MODEL_TYPES:')
    (1700, '            loss_files.append(get_simulated_model_outputs(')
    (1701, "                m, \\'iso_full_iso\\', 1, 4000, 10, 1e-3, 1, .75, .25, 0, 0, 0, 0, 0, 0, 0, 0,")
    (1702, '                **wildcards))')
    (1703, '            loss_files.append(get_simulated_model_outputs(')
    (1704, "                m, \\'full_full_full\\', 1, 4000, 10, 1e-3, 1, .75, .25, .1, .05, .03, .1, .2, .05,")
    (1705, '                .04, .3, **wildcards))')
    (1706, '    # this will return a list of lists of strings, so we need to flatten it')
    (1707, "    inputs[\\'loss_files\\'] = np.array(loss_files).flatten()")
    (1708, '    return inputs')
    (1709, '')
    (1710, '')
    (1711, "# this correctly calculates the CV error, in a way we don\\'t get otherwise")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ["if groupaverage == \\'individual\\'", "if wildcards.modeling_goal == \\'initial_cv\\'"]
    (2191, 'def get_loss_files(wildcards):')
    (2192, '    # wildcards is not an actual dictionary (it just functions like one')
    (2193, '    # in some regards) and so we need to make one if we want to do the')
    (2194, '    # pop() call below')
    (2195, '    format_kwargs = dict(wildcards)')
    (2196, "    groupaverage = format_kwargs.pop(\\'groupaverage\\')")
    (2197, "    sub = format_kwargs.pop(\\'subject\\')")
    (2198, '    # this will return a list of lists of strings, so we need to flatten it')
    (2199, "    if groupaverage == \\'individual\\':")
    (2200, "        if wildcards.modeling_goal == \\'initial_cv\\':")
    (2201, '            return np.array([get_model_subj_outputs(m, sub, ses, crossval_seed=0, **format_kwargs)')
    (2202, '                             for m in MODEL_TYPES for ses in SESSIONS[sub]')
    (2203, '                             if TASKS[(sub, ses)] == wildcards.task]).flatten()')
    (2204, "        elif wildcards.modeling_goal == \\'bootstrap\\':")
    (2205, '            return np.array([get_model_subj_outputs(m, sub, ses, bootstrap_num=n, **format_kwargs)')
    (2206, "                             for n in range(100) for m in [\\'full_full_absolute\\']")
    (2207, '                             for ses in SESSIONS[sub]')
    (2208, '                             if TASKS[(sub, ses)] == wildcards.task]).flatten()')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ["if groupaverage == \\'individual\\'", "elif groupaverage == \\'sub-groupaverage\\'", "if wildcards.modeling_goal == \\'initial\\'"]
    (2191, 'def get_loss_files(wildcards):')
    (2192, '    # wildcards is not an actual dictionary (it just functions like one')
    (2193, '    # in some regards) and so we need to make one if we want to do the')
    (2194, '    # pop() call below')
    (2195, '    format_kwargs = dict(wildcards)')
    (2196, "    groupaverage = format_kwargs.pop(\\'groupaverage\\')")
    (2197, "    sub = format_kwargs.pop(\\'subject\\')")
    (2198, '    # this will return a list of lists of strings, so we need to flatten it')
    (2199, "    if groupaverage == \\'individual\\':")
    (2200, "        if wildcards.modeling_goal == \\'initial_cv\\':")
    (2201, '            return np.array([get_model_subj_outputs(m, sub, ses, crossval_seed=0, **format_kwargs)')
    (2202, '                             for m in MODEL_TYPES for ses in SESSIONS[sub]')
    (2203, '                             if TASKS[(sub, ses)] == wildcards.task]).flatten()')
    (2204, "        elif wildcards.modeling_goal == \\'bootstrap\\':")
    (2205, '            return np.array([get_model_subj_outputs(m, sub, ses, bootstrap_num=n, **format_kwargs)')
    (2206, "                             for n in range(100) for m in [\\'full_full_absolute\\']")
    (2207, '                             for ses in SESSIONS[sub]')
    (2208, '                             if TASKS[(sub, ses)] == wildcards.task]).flatten()')
    (2209, "    elif groupaverage == \\'sub-groupaverage\\':")
    (2210, "        if wildcards.modeling_goal == \\'initial\\':")
    (2211, '            return np.array([get_groupaverage_all(model_type=m, **format_kwargs)')
    (2212, "                             for m in [\\'full_full_full\\', \\'full_full_absolute\\']]).flatten()")
    (2213, '')
    (2214, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ["if wildcards.cv_type.endswith(\\'-nc\\')"]
    (2262, 'def get_noise_ceiling_df(wildcards):')
    (2263, "    template = os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'noise_ceiling\\', \\'monte_carlo\\',")
    (2264, "                            \\'stim_class\\', \\'bayesian_posterior\\', \\'{df_filter}\\', \\'monte_carlo_ses-04_{task}_v1_e1-12_{mode}.csv\\')")
    (2265, "    if wildcards.cv_type.endswith(\\'-nc\\'):")
    (2266, "        return template.format(task=wildcards.task, mode=\\'individual\\', df_filter=wildcards.df_filter)")
    (2267, "    elif wildcards.cv_type.endswith(\\'-nc-all\\'):")
    (2268, "        return template.format(task=wildcards.task, mode=\\'all\\', df_filter=wildcards.df_filter)")
    (2269, '    else:')
    (2270, '        return []')
    (2271, '')
    (2272, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['try', "if wildcards.groupaverage == \\'sub-groupaverage\\'", "if wildcards.plot_kind in [\\'dist\\', \\'strip\\', \\'bootstraps\\']"]
    (2488, 'def get_params_csv(wildcards):')
    (2489, "    path_template = os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'tuning_2d_model\\',")
    (2490, '                                 \\\'stim_class\\\', \\\'bayesian_posterior\\\', f"{wildcards.df_filter}", \\\'%s\\\',')
    (2491, "                                 f\\'{wildcards.groupaverage}_{wildcards.task}_v1_e1-12_%s_b10_r0.001_g0_{wildcards.model_type}_all_models.csv\\')")
    (2492, "    precision = os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'first_level_analysis\\',")
    (2493, "                             \\'stim_class\\', \\'bayesian_posterior\\', f\\'{wildcards.task}_v1_e1-12_\\'")
    (2494, "                             f\\'summary_mean_no-filter_precision.csv\\'),")
    (2495, '    paths = {}')
    (2496, '    try:')
    (2497, '        ps = []')
    (2498, "        if wildcards.groupaverage == \\'sub-groupaverage\\':")
    (2499, "            if wildcards.plot_kind in [\\'dist\\', \\'strip\\', \\'bootstraps\\']:")
    (2500, "                ps.append(path_template % (\\'initial\\', \\'summary\\'))")
    (2501, '            else:')
    (2502, '                raise Exception(f"Can\\\'t do sub-groupaverage with plot_kind {wildcards.plot_kind}!"')
    (2503, '                                " Only \\\'dist\\\', \\\'strip\\\', \\\'bootstraps\\\' are allowed")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['try', "if wildcards.groupaverage == \\'sub-groupaverage\\'", 'else']
    (2488, 'def get_params_csv(wildcards):')
    (2489, "    path_template = os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'tuning_2d_model\\',")
    (2490, '                                 \\\'stim_class\\\', \\\'bayesian_posterior\\\', f"{wildcards.df_filter}", \\\'%s\\\',')
    (2491, "                                 f\\'{wildcards.groupaverage}_{wildcards.task}_v1_e1-12_%s_b10_r0.001_g0_{wildcards.model_type}_all_models.csv\\')")
    (2492, "    precision = os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'first_level_analysis\\',")
    (2493, "                             \\'stim_class\\', \\'bayesian_posterior\\', f\\'{wildcards.task}_v1_e1-12_\\'")
    (2494, "                             f\\'summary_mean_no-filter_precision.csv\\'),")
    (2495, '    paths = {}')
    (2496, '    try:')
    (2497, '        ps = []')
    (2498, "        if wildcards.groupaverage == \\'sub-groupaverage\\':")
    (2499, "            if wildcards.plot_kind in [\\'dist\\', \\'strip\\', \\'bootstraps\\']:")
    (2500, "                ps.append(path_template % (\\'initial\\', \\'summary\\'))")
    (2501, '            else:')
    (2502, '                raise Exception(f"Can\\\'t do sub-groupaverage with plot_kind {wildcards.plot_kind}!"')
    (2503, '                                " Only \\\'dist\\\', \\\'strip\\\', \\\'bootstraps\\\' are allowed")')
    (2504, '        else:')
    (2505, "            if wildcards.plot_kind in [\\'dist\\', \\'pair\\', \\'pair-drop\\', \\'compare\\', \\'bootstraps\\',")
    (2506, "                                       \\'dist-overall\\', \\'bootstraps-overall\\']:")
    (2507, "                ps.append(path_template % (\\'bootstrap\\', \\'full\\'))")
    (2508, "            if wildcards.plot_kind in [\\'point\\', \\'strip\\', \\'compare\\', \\'median\\']:")
    (2509, "                if wildcards.vf == \\'vertical\\':")
    (2510, "                    vf = [\\'upper\\', \\'lower\\']")
    (2511, "                elif wildcards.vf == \\'horizontal\\':")
    (2512, "                    vf = [\\'left\\', \\'right\\']")
    (2513, "                elif wildcards.vf == \\'eccen\\':")
    (2514, "                    vf = [\\'inner\\', \\'outer\\']")
    (2515, '                else:')
    (2516, '                    vf = [wildcards.vf]')
    (2517, '                for v in vf:')
    (2518, "                    if v == \\'all\\':")
    (2519, "                        folder = \\'initial\\'")
    (2520, '                    else:')
    (2521, "                        folder = \\'visual_field_%s\\' % v")
    (2522, "                    ps.append(path_template % (folder, \\'summary\\'))")
    (2523, "            if wildcards.plot_kind.endswith(\\'overall\\'):")
    (2524, "                paths[\\'precision\\'] = precision")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['try', "if wildcards.groupaverage == \\'sub-groupaverage\\'"]
    (2488, 'def get_params_csv(wildcards):')
    (2489, "    path_template = os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'tuning_2d_model\\',")
    (2490, '                                 \\\'stim_class\\\', \\\'bayesian_posterior\\\', f"{wildcards.df_filter}", \\\'%s\\\',')
    (2491, "                                 f\\'{wildcards.groupaverage}_{wildcards.task}_v1_e1-12_%s_b10_r0.001_g0_{wildcards.model_type}_all_models.csv\\')")
    (2492, "    precision = os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'first_level_analysis\\',")
    (2493, "                             \\'stim_class\\', \\'bayesian_posterior\\', f\\'{wildcards.task}_v1_e1-12_\\'")
    (2494, "                             f\\'summary_mean_no-filter_precision.csv\\'),")
    (2495, '    paths = {}')
    (2496, '    try:')
    (2497, '        ps = []')
    (2498, "        if wildcards.groupaverage == \\'sub-groupaverage\\':")
    (2499, "            if wildcards.plot_kind in [\\'dist\\', \\'strip\\', \\'bootstraps\\']:")
    (2500, "                ps.append(path_template % (\\'initial\\', \\'summary\\'))")
    (2501, '            else:')
    (2502, '                raise Exception(f"Can\\\'t do sub-groupaverage with plot_kind {wildcards.plot_kind}!"')
    (2503, '                                " Only \\\'dist\\\', \\\'strip\\\', \\\'bootstraps\\\' are allowed")')
    (2504, '        else:')
    (2505, "            if wildcards.plot_kind in [\\'dist\\', \\'pair\\', \\'pair-drop\\', \\'compare\\', \\'bootstraps\\',")
    (2506, "                                       \\'dist-overall\\', \\'bootstraps-overall\\']:")
    (2507, "                ps.append(path_template % (\\'bootstrap\\', \\'full\\'))")
    (2508, "            if wildcards.plot_kind in [\\'point\\', \\'strip\\', \\'compare\\', \\'median\\']:")
    (2509, "                if wildcards.vf == \\'vertical\\':")
    (2510, "                    vf = [\\'upper\\', \\'lower\\']")
    (2511, "                elif wildcards.vf == \\'horizontal\\':")
    (2512, "                    vf = [\\'left\\', \\'right\\']")
    (2513, "                elif wildcards.vf == \\'eccen\\':")
    (2514, "                    vf = [\\'inner\\', \\'outer\\']")
    (2515, '                else:')
    (2516, '                    vf = [wildcards.vf]')
    (2517, '                for v in vf:')
    (2518, "                    if v == \\'all\\':")
    (2519, "                        folder = \\'initial\\'")
    (2520, '                    else:')
    (2521, "                        folder = \\'visual_field_%s\\' % v")
    (2522, "                    ps.append(path_template % (folder, \\'summary\\'))")
    (2523, "            if wildcards.plot_kind.endswith(\\'overall\\'):")
    (2524, "                paths[\\'precision\\'] = precision")
    (2525, '    except AttributeError:')
    (2526, '        # this is the figure_background_with_current or sigma_interpretation')
    (2527, '        # rules (neither of which have wildcards.plot_kind)')
    (2528, "        if wildcards.groupaverage == \\'sub-groupaverage\\':")
    (2529, "            ps = path_template % (\\'initial\\', \\'summary\\')")
    (2530, '        else:')
    (2531, "            ps = path_template % (\\'bootstrap\\', \\'full\\')")
    (2532, "            paths[\\'precision\\'] = precision")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['if "crossvalidation" in wildcards.figure_name']
    (2988, 'def get_compose_input(wildcards):')
    (2989, '    path_template = os.path.join(config[\\\'DATA_DIR\\\'], \\\'derivatives\\\', "figures", wildcards.context,')
    (2990, '                                 "%s.svg")')
    (2991, '    if "crossvalidation" in wildcards.figure_name:')
    (2992, '        seed, df_filter = re.findall("crossvalidation_s-([0-9]+)_(filter-any|filter-mean|no-filter)", wildcards.figure_name)[0]')
    (2993, '        fig_names = ["schematic_models-annot",')
    (2994, "                     f\\'individual_{df_filter}_cv_model_point-remeaned_h_s-{seed}_task-sfprescaled\\']")
    (2995, "        if \\'sort\\' in wildcards.figure_name:")
    (2996, "            fig_names[0] += \\'-sort\\'")
    (2997, "            fig_names[1] = fig_names[1].replace(\\'_h_s\\', \\'_h_sort_s\\')")
    (2998, "        if \\'doubleup\\' in wildcards.figure_name:")
    (2999, "            fig_names[0] += \\'-doubleup\\'")
    (3000, "            fig_names[1] = fig_names[1].replace(\\'_h_s\\', \\'_h_doubleup_s\\')")
    (3001, "        if \\'-nc\\' in wildcards.figure_name:")
    (3002, "            fig_names[1] = fig_names[1].replace(\\'point-remeaned\\', \\'point-remeaned-nc\\')")
    (3003, "        fig_names[0] += \\'_\\' + df_filter")
    (3004, '        paths = [path_template % n for n in fig_names]')
    (3005, '    elif "with_legend" in wildcards.figure_name:')
    (3006, "        paths = [path_template % wildcards.figure_name.replace(\\'_with_legend\\', \\'\\')]")
    (3007, "    elif \\'2d_summary\\' in wildcards.figure_name:")
    (3008, '        groupaverage, df_filter, model, seed = re.findall("([a-z-]+)_([a-z-]+)_([a-z_]+)_2d_summary_s-([0-9]+)", wildcards.figure_name)[0]')
    (3009, "        template_name = (f\\'{groupaverage}_{df_filter}_{model}_feature_visualfield-all_{{}}_bootstraps-overall_\\'")
    (3010, "                         f\\'angles-{{}}_s-{seed}_task-sfprescaled_{{}}\\')")
    (3011, "        angles = {\\'pref-period\\': \\'avg\\', \\'pref-period-contour\\': \\'all\\', \\'max-amp\\': \\'all\\'}")
    (3012, '        paths = [path_template % template_name.format(feature, angles[feature], frame) for')
    (3013, "                 frame, feature in itertools.product([\\'relative\\', \\'absolute\\'],")
    (3014, "                                                     [\\'pref-period\\', \\'pref-period-contour\\', \\'max-amp\\'])]")
    (3015, "    elif \\'1d_summary\\' in wildcards.figure_name:")
    (3016, '        groupaverage, seed = re.findall("([a-z-]+)_1d_summary_s-([0-9]+)", wildcards.figure_name)[0]')
    (3017, "        period_name = f\\'{groupaverage}_1d_pref-period-overall_s-{seed}_task-sfprescaled_with_legend\\'")
    (3018, "        bw_name = f\\'{groupaverage}_1d_bandwidth-overall_s-{seed}_task-sfprescaled\\'")
    (3019, "        paths = [path_template.replace(\\'figures\\', \\'compose_figures\\') % period_name,")
    (3020, '                 path_template % bw_name]')
    (3021, "    elif \\'stimulus\\' in wildcards.figure_name:")
    (3022, '        task = re.findall("_(task-[a-z]+)", wildcards.figure_name)[0]')
    (3023, "        paths = [path_template % f\\'{task}_base_frequencies\\',")
    (3024, "                 path_template % f\\'schematic_stimulus_{task}\\',")
    (3025, "                 path_template % f\\'{task}_presented_frequencies\\']")
    (3026, "    elif \\'background\\' in wildcards.figure_name:")
    (3027, "        paths = [path_template % \\'schematic_background\\']")
    (3028, "    elif \\'example_voxel\\' in wildcards.figure_name:")
    (3029, '        df_filter, model = re.findall("([a-z-]+)_([a-z_]+)_example_voxels", wildcards.figure_name)[0]')
    (3030, '        paths = [path_template % f"peakiness_{df_filter}_{model}_all",')
    (3031, '                 path_template % f"example_voxels_{df_filter}_{model}"]')
    (3032, "    elif \\'parameters\\' in wildcards.figure_name:")
    (3033, '        groupaverage, df_filter, model, seed = re.findall("([a-z-]+)_([a-z-]+)_([a-z_]+)_parameters_s-([0-9]+)", wildcards.figure_name)[0]')
    (3034, '        paths = [path_template % f"{groupaverage}_{df_filter}_{model}_params_visualfield-all_dist_s-None_task-sfprescaled",')
    (3035, '                 path_template % f"{groupaverage}_{df_filter}_{model}_params_visualfield-all_dist-overall_s-{seed}_task-sfprescaled"]')
    (3036, "    elif \\'visual-field-diff\\' in wildcards.figure_name:")
    (3037, '        df_filter, model, seed = re.findall("visual-field-diff_([a-z-]+)_([a-z_]+)_s-([0-9]+)", wildcards.figure_name)[0]')
    (3038, '        paths = [path_template % f"visual-field-diff_comparison_task-sfprescaled_{df_filter}_{model}_s-{seed}",')
    (3039, '                 path_template % f"visual-field-diff_diff_task-sfprescaled_{df_filter}_{model}_s-{seed}"]')
    (3040, "    elif \\'example_ecc_bins_with_stim\\' in wildcards.figure_name:")
    (3041, "        paths = [path_template % \\'example_ecc_bins\\']")
    (3042, "    elif \\'schematic_model_2d\\' in wildcards.figure_name:")
    (3043, "        paths = [path_template % \\'schematic_2d-inputs\\',")
    (3044, "                 path_template.replace(\\'figures\\', \\'compose_figures\\') % \\'schematic_2d_with_legend\\']")
    (3045, '    return paths')
    (3046, '')
    (3047, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['if visual_field_analyses']
    (3413, "def get_figures_all(context=\\'paper\\', visual_field_analyses=False):")
    (3414, "    # now we\\'re only going to use svg")
    (3415, "    ext = \\'svg\\'")
    (3416, '    figs = []')
    (3417, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'individual_1d_{{}}_s-8_{{}}.{ext}\\').format(param, task)")
    (3418, "             for param in [\\'bandwidth\\', \\'pref-period\\', \\'bandwidth-overall\\', \\'pref-period-overall\\'] for task in [\\'task-sfprescaled\\', \\'task-sfpconstant\\']]")
    (3419, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'sub-groupaverage_1d_{{}}_s-7_{{}}.{ext}\\').format(param, task)")
    (3420, "             for param in [\\'bandwidth\\', \\'pref-period\\'] for task in [\\'task-sfprescaled\\']]")
    (3421, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'individual_filter-mean_cv_{{}}_v_s-3_task-sfprescaled.{ext}\\').format(cv)")
    (3422, "             for cv in [\\'raw\\', \\'demeaned\\', \\'model\\', \\'model_point\\', \\'demeaned-remeaned\\',")
    (3423, "                        \\'model-remeaned\\', \\'model_point-remeaned\\', \\'raw-nc\\', \\'model_point-nc\\',")
    (3424, "                        \\'model_point-remeaned-nc\\']]")
    (3425, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'individual_filter-mean_cv_{{}}_h_s-3_task-sfprescaled.{ext}\\').format(cv)")
    (3426, "             for cv in [\\'model_point\\', \\'model_point-remeaned\\']]")
    (3427, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'individual_filter-mean_{{}}_params_visualfield-all_dist-overall_s-5_task-sfprescaled.{ext}\\').format(model)")
    (3428, "             for model in [\\'full_full_full\\', \\'full_full_absolute\\']]")
    (3429, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'individual_filter-mean_{{}}_params_visualfield-all_{{}}_s-None_task-sfprescaled.{ext}\\').format(model, kind)")
    (3430, "             for kind  in [\\'point\\', \\'strip\\', \\'dist\\', \\'compare\\', \\'pair\\', \\'pair-drop\\'] for model in [\\'full_full_full\\', \\'full_full_absolute\\']]")
    (3431, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'sub-groupaverage_filter-mean_{{}}_params_visualfield-all_{{}}_s-7_task-sfprescaled.{ext}\\').format(model, kind)")
    (3432, "             for kind  in [\\'dist\\', \\'strip\\'] for model in [\\'full_full_full\\', \\'full_full_absolute\\']]")
    (3433, '    if visual_field_analyses:')
    (3434, "        figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'individual_filter-mean_{{}}_params_visualfield-{{}}_{{}}_s-None_task-sfprescaled.{ext}\\').format(model, vf, kind)")
    (3435, "                 for vf in [\\'all\\', \\'inner\\', \\'outer\\', \\'left\\', \\'right\\', \\'upper\\', \\'lower\\'] for kind  in [\\'point\\', \\'strip\\'] for model in [\\'full_full_full\\', \\'full_full_absolute\\']]")
    (3436, "        figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'individual_filter-mean_full_full_full_params_visualfield-{{}}_compare_s-None_task-sfprescaled.{ext}\\').format(vf)")
    (3437, "                 for vf in [\\'vertical\\', \\'horizontal\\', \\'eccen\\']]")
    (3438, "        figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'individual_filter-mean_full_full_absolute_params_visualfield-{{}}_compare_s-None_task-sfprescaled.{ext}\\').format(vf)")
    (3439, "                 for vf in [\\'vertical\\', \\'horizontal\\', \\'eccen\\']]")
    (3440, "        figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'individual_filter-mean_{{}}_feature_visualfield-{{}}_pref-period_median_angles-{{}}_s-None_task-sfprescaled_{{}}.{ext}\\').format(model, vf, angles, frame)")
    (3441, "                 for vf in [\\'inner\\', \\'outer\\', \\'left\\', \\'right\\', \\'upper\\', \\'lower\\'] for angles in [\\'all\\', \\'avg\\'] for frame in [\\'relative\\', \\'absolute\\']")
    (3442, "                 for model in [\\'full_full_full\\', \\'full_full_absolute\\']],")
    (3443, "        figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'individual_filter-mean_{{}}_feature_visualfield-{{}}_{{}}_median_angles-all_s-None_task-sfprescaled_{{}}.{ext}\\').format(model, vf, feature, frame)")
    (3444, "                 for vf in [\\'inner\\', \\'outer\\', \\'left\\', \\'right\\', \\'upper\\', \\'lower\\'] for feature in [\\'pref-period-contour\\', \\'iso-pref-period\\', \\'max-amp\\']")
    (3445, "                 for frame in [\\'relative\\', \\'absolute\\'] for model in [\\'full_full_full\\', \\'full_full_absolute\\']],")
    (3446, '        figs += [os.path.join(config[\\\'DATA_DIR\\\'], "derivatives", \\\'figures\\\', "individual_{}_sigma-interp_visualfield-{}_s-5_task-sfprescaled.txt").format(model, vf)')
    (3447, "                 for vf in [\\'inner\\', \\'outer\\', \\'left\\', \\'right\\', \\'upper\\', \\'lower\\'] for model in [\\'full_full_full\\', \\'full_full_absolute\\']]")
    (3448, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'individual_filter-mean_{{}}_feature_visualfield-all_pref-period_{{}}_angles-{{}}_s-{{}}_task-sfprescaled_{{}}.{ext}\\').format(model, kind, angles, seed, frame)")
    (3449, "             for seed, kind in zip([None, None, 5], [\\'median\\', \\'bootstraps\\', \\'bootstraps-overall\\'])")
    (3450, "             for angles in [\\'all\\', \\'avg\\'] for frame in [\\'relative\\', \\'absolute\\']")
    (3451, "             for model in [\\'full_full_full\\', \\'full_full_absolute\\']]")
    (3452, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'sub-groupaverage_filter-mean_{{}}_feature_visualfield-all_pref-period_bootstraps_angles-{{}}_s-5_task-sfprescaled_{{}}.{ext}\\').format(model, angles, frame)")
    (3453, "             for angles in [\\'all\\', \\'avg\\'] for frame in [\\'relative\\', \\'absolute\\'] for model in [\\'full_full_full\\', \\'full_full_absolute\\']]")
    (3454, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'individual_filter-mean_{{}}_feature_visualfield-all_{{}}_{{}}_angles-all_s-{{}}_task-sfprescaled_{{}}.{ext}\\').format(model, feature, kind, seed, frame)")
    (3455, "             for feature in [\\'pref-period-contour\\', \\'iso-pref-period\\', \\'max-amp\\']")
    (3456, "             for seed, kind in zip([None, None, 5], [\\'median\\', \\'bootstraps\\', \\'bootstraps-overall\\'])")
    (3457, "             for frame in [\\'relative\\', \\'absolute\\']")
    (3458, "             for model in [\\'full_full_full\\', \\'full_full_absolute\\']]")
    (3459, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'sub-groupaverage_filter-mean_{{}}_feature_visualfield-all_{{}}_bootstraps_angles-all_s-5_task-sfprescaled_{{}}.{ext}\\').format(model, feature, frame)")
    (3460, "             for feature in [\\'pref-period-contour\\', \\'iso-pref-period\\', \\'max-amp\\']")
    (3461, "             for frame in [\\'relative\\', \\'absolute\\'] for model in [\\'full_full_full\\', \\'full_full_absolute\\']]")
    (3462, "    figs +=[os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'schematic_{{}}.{ext}\\').format(kind)")
    (3463, "            for kind in [\\'2d\\', \\'2d-inputs\\', \\'models-annot-doubleup\\']]")
    (3464, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'background_period.{ext}\\')]")
    (3465, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'{{}}_task-sfprescaled_background_period_{{}}_s-{{}}.{ext}\\').format(group, model, seed)")
    (3466, "             for model in [\\'full_full_full\\', \\'full_full_absolute\\'] for group, seed in zip([\\'sub-groupaverage\\', \\'individual\\'], [7, 5])]")
    (3467, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'individual_filter-mean_{{}}_training-loss-check_task-sfprescaled.{ext}\\').format(t)")
    (3468, "             for t in [\\'initial_cv\\', \\'bootstrap\\']]")
    (3469, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'sub-groupaverage_filter-mean_initial_training-loss-check_task-sfprescaled.{ext}\\')]")
    (3470, "    figs += [os.path.join(config[\\'DATA_DIR\\'], \\'derivatives\\', \\'figures\\', f\\'{context}\\', f\\'mtf.{ext}\\')]")
    (3471, '    figs += [os.path.join(config[\\\'DATA_DIR\\\'], "derivatives", \\\'figures\\\', "{}_{}_sigma-interp_visualfield-all_s-{}_task-sfprescaled.txt").format(group, model, seed)')
    (3472, "             for model in [\\'full_full_full\\', \\'full_full_absolute\\'] for group, seed in zip([\\'sub-groupaverage\\', \\'individual\\'], [7, 5])]")
    (3473, '    return figs')
    (3474, '')
    (3475, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if FILTER_GTF == "y"']
    (106, 'def input_cellranger_mkref():')
    (107, '    """')
    (108, '    Function that returns the file to be used as input for cellranger_mkref.')
    (109, '    If the option to filter the GTF file was set to "y", it will return the filtered GTF. ')
    (110, '    If the option to filter the GTF file was set to "n", it will return the unfiltered GTF.')
    (111, '    """')
    (112, '    if FILTER_GTF == "y":')
    (113, '        return(f"{Path(GTF).stem}.filtered.gtf")')
    (114, '    elif FILTER_GTF == "n" and MKREF == "y":')
    (115, '        return(f"{Path(GTF).stem}.edited.gtf")')
    (116, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if MKREF == "y"']
    (128, 'def get_cellranger_count_input_mkref():')
    (129, '    """')
    (130, '    If the mkref step is used, this function will make sure cellranger_count only')
    (131, '    runs after mkref is finished')
    (132, '    """')
    (133, '    if MKREF == "y":')
    (134, '        return("mkref_done.txt")')
    (135, '    elif MKREF == "n":')
    (136, '        return([])')
    (137, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if not SCRUB_THRESHOLD']
    (138, 'def set_scrub_treshold(wildcards):')
    (139, '    """')
    (140, '    If SCRUB_THRESHOLD is set, return the treshold per sample')
    (141, '    """')
    (142, '    if not SCRUB_THRESHOLD:')
    (143, '        return("")')
    (144, '    else:')
    (145, '        return(SCRUB_THRESHOLD[wildcards.samples])')
    (146, '')
    (147, '')
    (148, '###### TARGETS ######')
    (149, '')
    (150, "# Rules that don\\'t need to run in the cluster")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JetBrains-Research/chipseq-smk-pipeline, file=rules/sicer.smk
context_key: ["if not bool(config[\\'sicer\\'])"]
    (9, 'def sicer_all_peaks_input():')
    (10, '    files = []')
    (11, "    if not bool(config[\\'sicer\\']):")
    (12, '        return files')
    (13, '')
    (14, "    window_size=config[\\'sicer_window\\']")
    (15, "    gap=config[\\'sicer_gap\\']")
    (16, '')
    (17, '    # XXX: change significance only via config, SICER rule takes the value from')
    (18, '    # config, not via wildcards')
    (19, '')
    (20, '    for sample in fastq_aligned_names(config, FASTQ_PATHS):')
    (21, '        if SAMPLE_2_CONTROL_MAP[sample] is None:')
    (22, '            # w/o control')
    (23, "            significance = config[\\'sicer_evalue\\']")
    (24, "            files.append(f\\'sicer/{sample}-W{window_size}-G{gap}-E{significance}.scoreisland\\')")
    (25, '        else:')
    (26, '            # with control')
    (27, "            significance = config[\\'sicer_fdr\\']")
    (28, "            files.append(f\\'sicer/{sample}-W{window_size}-G{gap}-islands-summary-FDR{significance}\\')")
    (29, '')
    (30, '    return files')
    (31, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JetBrains-Research/chipseq-smk-pipeline, file=rules/sicer.smk
context_key: ['if control_sample']
    (57, 'def sicer_input_fun(wildcards):')
    (58, '    sample = wildcards.sample')
    (59, '')
    (60, '    control_args = {}')
    (61, '    control_sample = SAMPLE_2_CONTROL_MAP[sample]')
    (62, '    if control_sample:')
    (63, "        control_args[\\'control_pileup\\'] = f\\'bams/pileup/{control_sample}.bed\\'")
    (64, '')
    (65, '    return dict(')
    (66, "        # pileup_bed=\\'bams/pileup/{sample}.bed\\',")
    (67, "        signal_pileup = f\\'bams/pileup/{sample}.bed\\',")
    (68, '        **control_args,')
    (69, '        chrom_sizes=rules.download_chrom_sizes.output,')
    (70, '        effective_genome_fraction=rules.pileup_bed_effective_genome_fraction.output,')
    (71, '    )')
    (72, '')
    (73, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JetBrains-Research/chipseq-smk-pipeline, file=rules/span.smk
context_key: ['if control_sample']
    (17, 'def span_input_fun(wildcards):')
    (18, '    sample = wildcards.sample')
    (19, '')
    (20, '    control_args = {}')
    (21, '    control_sample = SAMPLE_2_CONTROL_MAP[sample]')
    (22, '    if control_sample:')
    (23, "        control_args[\\'control\\'] = f\\'bams/{control_sample}.bam\\'")
    (24, '')
    (25, '    return dict(')
    (26, "        signal=f\\'bams/{sample}.bam\\',")
    (27, '        **control_args,')
    (28, '        span=rules.download_span.output,')
    (29, '        chrom_sizes=rules.download_chrom_sizes.output,')
    (30, '    )')
    (31, '')
    (32, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JetBrains-Research/chipseq-smk-pipeline, file=rules/macs2.smk
context_key: ['if control_sample']
    (12, 'def macs2_input_fun(wildcards):')
    (13, '    sample = wildcards.sample')
    (14, '')
    (15, '    control_args = {}')
    (16, '    control_sample = SAMPLE_2_CONTROL_MAP[sample]')
    (17, '    if control_sample:')
    (18, "        control_args[\\'control\\'] = f\\'bams/{control_sample}.bam\\'")
    (19, '')
    (20, '    return dict(')
    (21, "        signal=f\\'bams/{sample}.bam\\',")
    (22, '        **control_args')
    (23, '    )')
    (24, '')
    (25, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nioo-knaw/epiGBS2, file=src/rules/denovo.rules
context_key: ['if param_id == "default" or param_id == ""']
    (5, 'def getParam_id(param_id):')
    (6, '    if param_id == "default" or param_id == "":')
    (7, '        id = 0.97')
    (8, '    else:')
    (9, '        id = param_id')
    (10, '    return id')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nioo-knaw/epiGBS2, file=src/rules/denovo.rules
context_key: ['if param_mind=="default" or param_mind == ""']
    (11, 'def getParam_mind(param_mind):')
    (12, '    if param_mind=="default" or param_mind == "":')
    (13, '        mind = 10')
    (14, '    else:')
    (15, '        mind = param_mind')
    (16, '    return mind')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nioo-knaw/epiGBS2, file=src/rules/denovo.rules
context_key: ['if param_maxd=="default" or param_maxd == ""']
    (17, 'def getParam_maxd(param_maxd):')
    (18, '    if param_maxd=="default" or param_maxd == "":')
    (19, '        dep = 10000')
    (20, '    else:')
    (21, '        dep = param_maxd')
    (22, '    return dep')
    (23, '')
    (24, '#Run the denovo creation python script')
    (25, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nioo-knaw/epiGBS2, file=src/rules/legacy.rules
context_key: ['if param_id == "default" or param_id == ""']
    (4, 'def getParam_id(param_id):')
    (5, '    if param_id == "default" or param_id == "":')
    (6, '        id = 0.95')
    (7, '    else:')
    (8, '        id = param_id')
    (9, '    return id')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nioo-knaw/epiGBS2, file=src/rules/legacy.rules
context_key: ['if param_mind=="default" or param_mind == ""']
    (10, 'def getParam_mind(param_mind):')
    (11, '    if param_mind=="default" or param_mind == "":')
    (12, '        mind = 0')
    (13, '    else:')
    (14, '        mind = param_mind')
    (15, '    return mind')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nioo-knaw/epiGBS2, file=src/rules/legacy.rules
context_key: ['if param_maxd=="default" or param_maxd == ""']
    (16, 'def getParam_maxd(param_maxd):')
    (17, '    if param_maxd=="default" or param_maxd == "":')
    (18, '        dep = 0')
    (19, '    else:')
    (20, '        dep = param_maxd')
    (21, '    return dep')
    (22, '')
    (23, '')
    (24, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=metagenome-atlas/genecatalog_atlas, file=rules/clustering.smk
context_key: ["if id >= float(config[\\'minid\\'])"]
    (164, 'def get_subcluster_id(wildcards):')
    (165, '')
    (166, '    id= int(wildcards.id)')
    (167, '')
    (168, '    assert (id>30) & (id<100), f"id should be an integer in [30,100], got {wildcards.id}"')
    (169, '')
    (170, '    id = id/100')
    (171, '')
    (172, "    if id >= float(config[\\'minid\\']):")
    (173, '        logger.error("Id for gene subclustering should be lower than that the gene catalog"')
    (174, '                     f" {id} is not smaller than {config[\\\'minid\\\']}"')
    (175, '                     )')
    (176, '        exit(1)')
    (177, '    return id')
    (178, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', "if \\'size-2048,2600\\' in wildcards.image_name", "if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name", "if \\'V1\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name", 'if float(wildcards.scaling) < .1']
    (529, 'def get_mem_estimate(wildcards, partition=None):')
    (530, '    r"""estimate the amount of memory that this will need, in GB')
    (531, '    """')
    (532, '    try:')
    (533, "        if \\'size-2048,2600\\' in wildcards.image_name:")
    (534, "            if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (535, "                if \\'V1\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (536, '                    if float(wildcards.scaling) < .1:')
    (537, '                        mem = 128')
    (538, '                    else:')
    (539, '                        mem = 64')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', "if \\'size-2048,2600\\' in wildcards.image_name", "if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name", "if \\'V1\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name", "elif \\'RGC\\' in wildcards.model_name"]
    (529, 'def get_mem_estimate(wildcards, partition=None):')
    (530, '    r"""estimate the amount of memory that this will need, in GB')
    (531, '    """')
    (532, '    try:')
    (533, "        if \\'size-2048,2600\\' in wildcards.image_name:")
    (534, "            if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (535, "                if \\'V1\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (536, '                    if float(wildcards.scaling) < .1:')
    (537, '                        mem = 128')
    (538, '                    else:')
    (539, '                        mem = 64')
    (540, "                elif \\'RGC\\' in wildcards.model_name:")
    (541, '                    # this is an approximation of the size of their windows,')
    (542, "                    # and if you have at least 3 times this memory, you\\'re")
    (543, '                    # good. double-check this value -- the 1.36 is for')
    (544, '                    # converting form 2048,3528 (which the numbers came')
    (545, '                    # from) to 2048,2600 (which has 1.36x fewer pixels)')
    (546, '                    window_size = 1.17430726 / (1.36*float(wildcards.scaling))')
    (547, '                    mem = int(5 * window_size)')
    (548, "                    # running out of memory for larger scaling values, so let\\'s")
    (549, '                    # never request less than 32 GB')
    (550, '                    mem = max(mem, 32)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', "if \\'size-2048,2600\\' in wildcards.image_name", "if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name", "elif \\'cosine\\' in wildcards.model_name", "if \\'V1\\' in wildcards.model_name"]
    (529, 'def get_mem_estimate(wildcards, partition=None):')
    (530, '    r"""estimate the amount of memory that this will need, in GB')
    (531, '    """')
    (532, '    try:')
    (533, "        if \\'size-2048,2600\\' in wildcards.image_name:")
    (534, "            if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (535, "                if \\'V1\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (536, '                    if float(wildcards.scaling) < .1:')
    (537, '                        mem = 128')
    (538, '                    else:')
    (539, '                        mem = 64')
    (540, "                elif \\'RGC\\' in wildcards.model_name:")
    (541, '                    # this is an approximation of the size of their windows,')
    (542, "                    # and if you have at least 3 times this memory, you\\'re")
    (543, '                    # good. double-check this value -- the 1.36 is for')
    (544, '                    # converting form 2048,3528 (which the numbers came')
    (545, '                    # from) to 2048,2600 (which has 1.36x fewer pixels)')
    (546, '                    window_size = 1.17430726 / (1.36*float(wildcards.scaling))')
    (547, '                    mem = int(5 * window_size)')
    (548, "                    # running out of memory for larger scaling values, so let\\'s")
    (549, '                    # never request less than 32 GB')
    (550, '                    mem = max(mem, 32)')
    (551, "            elif \\'cosine\\' in wildcards.model_name:")
    (552, "                if \\'V1\\' in wildcards.model_name:")
    (553, '                    # most it will need is 32 GB')
    (554, '                    mem = 32')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', "if \\'size-2048,2600\\' in wildcards.image_name", "if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name", "elif \\'cosine\\' in wildcards.model_name", "elif \\'RGC\\' in wildcards.model_name"]
    (529, 'def get_mem_estimate(wildcards, partition=None):')
    (530, '    r"""estimate the amount of memory that this will need, in GB')
    (531, '    """')
    (532, '    try:')
    (533, "        if \\'size-2048,2600\\' in wildcards.image_name:")
    (534, "            if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (535, "                if \\'V1\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (536, '                    if float(wildcards.scaling) < .1:')
    (537, '                        mem = 128')
    (538, '                    else:')
    (539, '                        mem = 64')
    (540, "                elif \\'RGC\\' in wildcards.model_name:")
    (541, '                    # this is an approximation of the size of their windows,')
    (542, "                    # and if you have at least 3 times this memory, you\\'re")
    (543, '                    # good. double-check this value -- the 1.36 is for')
    (544, '                    # converting form 2048,3528 (which the numbers came')
    (545, '                    # from) to 2048,2600 (which has 1.36x fewer pixels)')
    (546, '                    window_size = 1.17430726 / (1.36*float(wildcards.scaling))')
    (547, '                    mem = int(5 * window_size)')
    (548, "                    # running out of memory for larger scaling values, so let\\'s")
    (549, '                    # never request less than 32 GB')
    (550, '                    mem = max(mem, 32)')
    (551, "            elif \\'cosine\\' in wildcards.model_name:")
    (552, "                if \\'V1\\' in wildcards.model_name:")
    (553, '                    # most it will need is 32 GB')
    (554, '                    mem = 32')
    (555, "                elif \\'RGC\\' in wildcards.model_name:")
    (556, '                    # this is an approximation of the size of their windows,')
    (557, "                    # and if you have at least 3 times this memory, you\\'re")
    (558, '                    # good')
    (559, '                    window_size = 0.49238059 / float(wildcards.scaling)')
    (560, '                    mem = int(5 * window_size)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', "if \\'size-2048,2600\\' in wildcards.image_name", "if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name", 'else']
    (529, 'def get_mem_estimate(wildcards, partition=None):')
    (530, '    r"""estimate the amount of memory that this will need, in GB')
    (531, '    """')
    (532, '    try:')
    (533, "        if \\'size-2048,2600\\' in wildcards.image_name:")
    (534, "            if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (535, "                if \\'V1\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (536, '                    if float(wildcards.scaling) < .1:')
    (537, '                        mem = 128')
    (538, '                    else:')
    (539, '                        mem = 64')
    (540, "                elif \\'RGC\\' in wildcards.model_name:")
    (541, '                    # this is an approximation of the size of their windows,')
    (542, "                    # and if you have at least 3 times this memory, you\\'re")
    (543, '                    # good. double-check this value -- the 1.36 is for')
    (544, '                    # converting form 2048,3528 (which the numbers came')
    (545, '                    # from) to 2048,2600 (which has 1.36x fewer pixels)')
    (546, '                    window_size = 1.17430726 / (1.36*float(wildcards.scaling))')
    (547, '                    mem = int(5 * window_size)')
    (548, "                    # running out of memory for larger scaling values, so let\\'s")
    (549, '                    # never request less than 32 GB')
    (550, '                    mem = max(mem, 32)')
    (551, "            elif \\'cosine\\' in wildcards.model_name:")
    (552, "                if \\'V1\\' in wildcards.model_name:")
    (553, '                    # most it will need is 32 GB')
    (554, '                    mem = 32')
    (555, "                elif \\'RGC\\' in wildcards.model_name:")
    (556, '                    # this is an approximation of the size of their windows,')
    (557, "                    # and if you have at least 3 times this memory, you\\'re")
    (558, '                    # good')
    (559, '                    window_size = 0.49238059 / float(wildcards.scaling)')
    (560, '                    mem = int(5 * window_size)')
    (561, '            else:')
    (562, '                mem = 32')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', "if \\'size-2048,2600\\' in wildcards.image_name", 'else']
    (529, 'def get_mem_estimate(wildcards, partition=None):')
    (530, '    r"""estimate the amount of memory that this will need, in GB')
    (531, '    """')
    (532, '    try:')
    (533, "        if \\'size-2048,2600\\' in wildcards.image_name:")
    (534, "            if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (535, "                if \\'V1\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (536, '                    if float(wildcards.scaling) < .1:')
    (537, '                        mem = 128')
    (538, '                    else:')
    (539, '                        mem = 64')
    (540, "                elif \\'RGC\\' in wildcards.model_name:")
    (541, '                    # this is an approximation of the size of their windows,')
    (542, "                    # and if you have at least 3 times this memory, you\\'re")
    (543, '                    # good. double-check this value -- the 1.36 is for')
    (544, '                    # converting form 2048,3528 (which the numbers came')
    (545, '                    # from) to 2048,2600 (which has 1.36x fewer pixels)')
    (546, '                    window_size = 1.17430726 / (1.36*float(wildcards.scaling))')
    (547, '                    mem = int(5 * window_size)')
    (548, "                    # running out of memory for larger scaling values, so let\\'s")
    (549, '                    # never request less than 32 GB')
    (550, '                    mem = max(mem, 32)')
    (551, "            elif \\'cosine\\' in wildcards.model_name:")
    (552, "                if \\'V1\\' in wildcards.model_name:")
    (553, '                    # most it will need is 32 GB')
    (554, '                    mem = 32')
    (555, "                elif \\'RGC\\' in wildcards.model_name:")
    (556, '                    # this is an approximation of the size of their windows,')
    (557, "                    # and if you have at least 3 times this memory, you\\'re")
    (558, '                    # good')
    (559, '                    window_size = 0.49238059 / float(wildcards.scaling)')
    (560, '                    mem = int(5 * window_size)')
    (561, '            else:')
    (562, '                mem = 32')
    (563, '        else:')
    (564, "            # don\\'t have a good estimate for these")
    (565, '            mem = 16')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', "if wildcards.size == \\'2048,2600\\'", "if wildcards.window_type == \\'gaussian\\'"]
    (529, 'def get_mem_estimate(wildcards, partition=None):')
    (530, '    r"""estimate the amount of memory that this will need, in GB')
    (531, '    """')
    (532, '    try:')
    (533, "        if \\'size-2048,2600\\' in wildcards.image_name:")
    (534, "            if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (535, "                if \\'V1\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (536, '                    if float(wildcards.scaling) < .1:')
    (537, '                        mem = 128')
    (538, '                    else:')
    (539, '                        mem = 64')
    (540, "                elif \\'RGC\\' in wildcards.model_name:")
    (541, '                    # this is an approximation of the size of their windows,')
    (542, "                    # and if you have at least 3 times this memory, you\\'re")
    (543, '                    # good. double-check this value -- the 1.36 is for')
    (544, '                    # converting form 2048,3528 (which the numbers came')
    (545, '                    # from) to 2048,2600 (which has 1.36x fewer pixels)')
    (546, '                    window_size = 1.17430726 / (1.36*float(wildcards.scaling))')
    (547, '                    mem = int(5 * window_size)')
    (548, "                    # running out of memory for larger scaling values, so let\\'s")
    (549, '                    # never request less than 32 GB')
    (550, '                    mem = max(mem, 32)')
    (551, "            elif \\'cosine\\' in wildcards.model_name:")
    (552, "                if \\'V1\\' in wildcards.model_name:")
    (553, '                    # most it will need is 32 GB')
    (554, '                    mem = 32')
    (555, "                elif \\'RGC\\' in wildcards.model_name:")
    (556, '                    # this is an approximation of the size of their windows,')
    (557, "                    # and if you have at least 3 times this memory, you\\'re")
    (558, '                    # good')
    (559, '                    window_size = 0.49238059 / float(wildcards.scaling)')
    (560, '                    mem = int(5 * window_size)')
    (561, '            else:')
    (562, '                mem = 32')
    (563, '        else:')
    (564, "            # don\\'t have a good estimate for these")
    (565, '            mem = 16')
    (566, '    except AttributeError:')
    (567, "        # then we don\\'t have a image_name wildcard (and thus this is")
    (568, '        # being called by cache_windows)')
    (569, "        if wildcards.size == \\'2048,2600\\':")
    (570, "            if wildcards.window_type == \\'gaussian\\':")
    (571, '                # this is an approximation of the size of their windows,')
    (572, "                # and if you have at least 3 times this memory, you\\'re")
    (573, '                # good. double-check this value -- the 1.36 is for')
    (574, '                # converting form 2048,3528 (which the numbers came')
    (575, '                # from) to 2048,2600 (which has 1.36x fewer pixels)')
    (576, '                window_size = 1.17430726 / (1.36*float(wildcards.scaling))')
    (577, '                mem = int(5 * window_size)')
    (578, "            elif wildcards.window_type == \\'cosine\\':")
    (579, '                # this is an approximation of the size of their windows,')
    (580, "                # and if you have at least 3 times this memory, you\\'re")
    (581, '                # good')
    (582, '                window_size = 0.49238059 / float(wildcards.scaling)')
    (583, '                mem = int(5 * window_size)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', "if wildcards.size == \\'2048,2600\\'", 'else']
    (529, 'def get_mem_estimate(wildcards, partition=None):')
    (530, '    r"""estimate the amount of memory that this will need, in GB')
    (531, '    """')
    (532, '    try:')
    (533, "        if \\'size-2048,2600\\' in wildcards.image_name:")
    (534, "            if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (535, "                if \\'V1\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (536, '                    if float(wildcards.scaling) < .1:')
    (537, '                        mem = 128')
    (538, '                    else:')
    (539, '                        mem = 64')
    (540, "                elif \\'RGC\\' in wildcards.model_name:")
    (541, '                    # this is an approximation of the size of their windows,')
    (542, "                    # and if you have at least 3 times this memory, you\\'re")
    (543, '                    # good. double-check this value -- the 1.36 is for')
    (544, '                    # converting form 2048,3528 (which the numbers came')
    (545, '                    # from) to 2048,2600 (which has 1.36x fewer pixels)')
    (546, '                    window_size = 1.17430726 / (1.36*float(wildcards.scaling))')
    (547, '                    mem = int(5 * window_size)')
    (548, "                    # running out of memory for larger scaling values, so let\\'s")
    (549, '                    # never request less than 32 GB')
    (550, '                    mem = max(mem, 32)')
    (551, "            elif \\'cosine\\' in wildcards.model_name:")
    (552, "                if \\'V1\\' in wildcards.model_name:")
    (553, '                    # most it will need is 32 GB')
    (554, '                    mem = 32')
    (555, "                elif \\'RGC\\' in wildcards.model_name:")
    (556, '                    # this is an approximation of the size of their windows,')
    (557, "                    # and if you have at least 3 times this memory, you\\'re")
    (558, '                    # good')
    (559, '                    window_size = 0.49238059 / float(wildcards.scaling)')
    (560, '                    mem = int(5 * window_size)')
    (561, '            else:')
    (562, '                mem = 32')
    (563, '        else:')
    (564, "            # don\\'t have a good estimate for these")
    (565, '            mem = 16')
    (566, '    except AttributeError:')
    (567, "        # then we don\\'t have a image_name wildcard (and thus this is")
    (568, '        # being called by cache_windows)')
    (569, "        if wildcards.size == \\'2048,2600\\':")
    (570, "            if wildcards.window_type == \\'gaussian\\':")
    (571, '                # this is an approximation of the size of their windows,')
    (572, "                # and if you have at least 3 times this memory, you\\'re")
    (573, '                # good. double-check this value -- the 1.36 is for')
    (574, '                # converting form 2048,3528 (which the numbers came')
    (575, '                # from) to 2048,2600 (which has 1.36x fewer pixels)')
    (576, '                window_size = 1.17430726 / (1.36*float(wildcards.scaling))')
    (577, '                mem = int(5 * window_size)')
    (578, "            elif wildcards.window_type == \\'cosine\\':")
    (579, '                # this is an approximation of the size of their windows,')
    (580, "                # and if you have at least 3 times this memory, you\\'re")
    (581, '                # good')
    (582, '                window_size = 0.49238059 / float(wildcards.scaling)')
    (583, '                mem = int(5 * window_size)')
    (584, '        else:')
    (585, "            # don\\'t have a good estimate here")
    (586, '            mem = 16')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', 'try', 'if wildcards.save_all']
    (529, 'def get_mem_estimate(wildcards, partition=None):')
    (530, '    r"""estimate the amount of memory that this will need, in GB')
    (531, '    """')
    (532, '    try:')
    (533, "        if \\'size-2048,2600\\' in wildcards.image_name:")
    (534, "            if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (535, "                if \\'V1\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (536, '                    if float(wildcards.scaling) < .1:')
    (537, '                        mem = 128')
    (538, '                    else:')
    (539, '                        mem = 64')
    (540, "                elif \\'RGC\\' in wildcards.model_name:")
    (541, '                    # this is an approximation of the size of their windows,')
    (542, "                    # and if you have at least 3 times this memory, you\\'re")
    (543, '                    # good. double-check this value -- the 1.36 is for')
    (544, '                    # converting form 2048,3528 (which the numbers came')
    (545, '                    # from) to 2048,2600 (which has 1.36x fewer pixels)')
    (546, '                    window_size = 1.17430726 / (1.36*float(wildcards.scaling))')
    (547, '                    mem = int(5 * window_size)')
    (548, "                    # running out of memory for larger scaling values, so let\\'s")
    (549, '                    # never request less than 32 GB')
    (550, '                    mem = max(mem, 32)')
    (551, "            elif \\'cosine\\' in wildcards.model_name:")
    (552, "                if \\'V1\\' in wildcards.model_name:")
    (553, '                    # most it will need is 32 GB')
    (554, '                    mem = 32')
    (555, "                elif \\'RGC\\' in wildcards.model_name:")
    (556, '                    # this is an approximation of the size of their windows,')
    (557, "                    # and if you have at least 3 times this memory, you\\'re")
    (558, '                    # good')
    (559, '                    window_size = 0.49238059 / float(wildcards.scaling)')
    (560, '                    mem = int(5 * window_size)')
    (561, '            else:')
    (562, '                mem = 32')
    (563, '        else:')
    (564, "            # don\\'t have a good estimate for these")
    (565, '            mem = 16')
    (566, '    except AttributeError:')
    (567, "        # then we don\\'t have a image_name wildcard (and thus this is")
    (568, '        # being called by cache_windows)')
    (569, "        if wildcards.size == \\'2048,2600\\':")
    (570, "            if wildcards.window_type == \\'gaussian\\':")
    (571, '                # this is an approximation of the size of their windows,')
    (572, "                # and if you have at least 3 times this memory, you\\'re")
    (573, '                # good. double-check this value -- the 1.36 is for')
    (574, '                # converting form 2048,3528 (which the numbers came')
    (575, '                # from) to 2048,2600 (which has 1.36x fewer pixels)')
    (576, '                window_size = 1.17430726 / (1.36*float(wildcards.scaling))')
    (577, '                mem = int(5 * window_size)')
    (578, "            elif wildcards.window_type == \\'cosine\\':")
    (579, '                # this is an approximation of the size of their windows,')
    (580, "                # and if you have at least 3 times this memory, you\\'re")
    (581, '                # good')
    (582, '                window_size = 0.49238059 / float(wildcards.scaling)')
    (583, '                mem = int(5 * window_size)')
    (584, '        else:')
    (585, "            # don\\'t have a good estimate here")
    (586, '            mem = 16')
    (587, '    try:')
    (588, '        if wildcards.save_all:')
    (589, '            # for this estimate, RGC with scaling .095 went from 36GB requested')
    (590, '            # to about 54GB used when stored iterations went from 100 to 1000.')
    (591, "            # that\\'s 1.5x higher, and we add a bit of a buffer. also, don\\'t")
    (592, '            # want to reduce memory estimate')
    (593, '            mem_factor = max((int(wildcards.max_iter) / 100) * (1.7/10), 1)')
    (594, '            mem *= mem_factor')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', "if partition == \\'rusty\\'", 'if int(wildcards.gpu) == 0']
    (529, 'def get_mem_estimate(wildcards, partition=None):')
    (530, '    r"""estimate the amount of memory that this will need, in GB')
    (531, '    """')
    (532, '    try:')
    (533, "        if \\'size-2048,2600\\' in wildcards.image_name:")
    (534, "            if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (535, "                if \\'V1\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (536, '                    if float(wildcards.scaling) < .1:')
    (537, '                        mem = 128')
    (538, '                    else:')
    (539, '                        mem = 64')
    (540, "                elif \\'RGC\\' in wildcards.model_name:")
    (541, '                    # this is an approximation of the size of their windows,')
    (542, "                    # and if you have at least 3 times this memory, you\\'re")
    (543, '                    # good. double-check this value -- the 1.36 is for')
    (544, '                    # converting form 2048,3528 (which the numbers came')
    (545, '                    # from) to 2048,2600 (which has 1.36x fewer pixels)')
    (546, '                    window_size = 1.17430726 / (1.36*float(wildcards.scaling))')
    (547, '                    mem = int(5 * window_size)')
    (548, "                    # running out of memory for larger scaling values, so let\\'s")
    (549, '                    # never request less than 32 GB')
    (550, '                    mem = max(mem, 32)')
    (551, "            elif \\'cosine\\' in wildcards.model_name:")
    (552, "                if \\'V1\\' in wildcards.model_name:")
    (553, '                    # most it will need is 32 GB')
    (554, '                    mem = 32')
    (555, "                elif \\'RGC\\' in wildcards.model_name:")
    (556, '                    # this is an approximation of the size of their windows,')
    (557, "                    # and if you have at least 3 times this memory, you\\'re")
    (558, '                    # good')
    (559, '                    window_size = 0.49238059 / float(wildcards.scaling)')
    (560, '                    mem = int(5 * window_size)')
    (561, '            else:')
    (562, '                mem = 32')
    (563, '        else:')
    (564, "            # don\\'t have a good estimate for these")
    (565, '            mem = 16')
    (566, '    except AttributeError:')
    (567, "        # then we don\\'t have a image_name wildcard (and thus this is")
    (568, '        # being called by cache_windows)')
    (569, "        if wildcards.size == \\'2048,2600\\':")
    (570, "            if wildcards.window_type == \\'gaussian\\':")
    (571, '                # this is an approximation of the size of their windows,')
    (572, "                # and if you have at least 3 times this memory, you\\'re")
    (573, '                # good. double-check this value -- the 1.36 is for')
    (574, '                # converting form 2048,3528 (which the numbers came')
    (575, '                # from) to 2048,2600 (which has 1.36x fewer pixels)')
    (576, '                window_size = 1.17430726 / (1.36*float(wildcards.scaling))')
    (577, '                mem = int(5 * window_size)')
    (578, "            elif wildcards.window_type == \\'cosine\\':")
    (579, '                # this is an approximation of the size of their windows,')
    (580, "                # and if you have at least 3 times this memory, you\\'re")
    (581, '                # good')
    (582, '                window_size = 0.49238059 / float(wildcards.scaling)')
    (583, '                mem = int(5 * window_size)')
    (584, '        else:')
    (585, "            # don\\'t have a good estimate here")
    (586, '            mem = 16')
    (587, '    try:')
    (588, '        if wildcards.save_all:')
    (589, '            # for this estimate, RGC with scaling .095 went from 36GB requested')
    (590, '            # to about 54GB used when stored iterations went from 100 to 1000.')
    (591, "            # that\\'s 1.5x higher, and we add a bit of a buffer. also, don\\'t")
    (592, '            # want to reduce memory estimate')
    (593, '            mem_factor = max((int(wildcards.max_iter) / 100) * (1.7/10), 1)')
    (594, '            mem *= mem_factor')
    (595, '    except AttributeError:')
    (596, "        # then we\\'re missing either the save_all or max_iter wildcard, in which")
    (597, "        # case this is probably cache_windows and the above doesn\\'t matter")
    (598, '        pass')
    (599, '    mem = int(np.ceil(mem))')
    (600, "    if partition == \\'rusty\\':")
    (601, '        if int(wildcards.gpu) == 0:')
    (602, "            # in this case, we *do not* want to specify memory (we\\'ll get the")
    (603, '            # whole node allocated but slurm could still kill the job if we go')
    (604, '            # over requested memory). setting mem=0 requests all memory on node')
    (605, "            mem = \\'0\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', "if partition == \\'rusty\\'", 'else']
    (529, 'def get_mem_estimate(wildcards, partition=None):')
    (530, '    r"""estimate the amount of memory that this will need, in GB')
    (531, '    """')
    (532, '    try:')
    (533, "        if \\'size-2048,2600\\' in wildcards.image_name:")
    (534, "            if \\'gaussian\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (535, "                if \\'V1\\' in wildcards.model_name or \\'Obs\\' in wildcards.model_name:")
    (536, '                    if float(wildcards.scaling) < .1:')
    (537, '                        mem = 128')
    (538, '                    else:')
    (539, '                        mem = 64')
    (540, "                elif \\'RGC\\' in wildcards.model_name:")
    (541, '                    # this is an approximation of the size of their windows,')
    (542, "                    # and if you have at least 3 times this memory, you\\'re")
    (543, '                    # good. double-check this value -- the 1.36 is for')
    (544, '                    # converting form 2048,3528 (which the numbers came')
    (545, '                    # from) to 2048,2600 (which has 1.36x fewer pixels)')
    (546, '                    window_size = 1.17430726 / (1.36*float(wildcards.scaling))')
    (547, '                    mem = int(5 * window_size)')
    (548, "                    # running out of memory for larger scaling values, so let\\'s")
    (549, '                    # never request less than 32 GB')
    (550, '                    mem = max(mem, 32)')
    (551, "            elif \\'cosine\\' in wildcards.model_name:")
    (552, "                if \\'V1\\' in wildcards.model_name:")
    (553, '                    # most it will need is 32 GB')
    (554, '                    mem = 32')
    (555, "                elif \\'RGC\\' in wildcards.model_name:")
    (556, '                    # this is an approximation of the size of their windows,')
    (557, "                    # and if you have at least 3 times this memory, you\\'re")
    (558, '                    # good')
    (559, '                    window_size = 0.49238059 / float(wildcards.scaling)')
    (560, '                    mem = int(5 * window_size)')
    (561, '            else:')
    (562, '                mem = 32')
    (563, '        else:')
    (564, "            # don\\'t have a good estimate for these")
    (565, '            mem = 16')
    (566, '    except AttributeError:')
    (567, "        # then we don\\'t have a image_name wildcard (and thus this is")
    (568, '        # being called by cache_windows)')
    (569, "        if wildcards.size == \\'2048,2600\\':")
    (570, "            if wildcards.window_type == \\'gaussian\\':")
    (571, '                # this is an approximation of the size of their windows,')
    (572, "                # and if you have at least 3 times this memory, you\\'re")
    (573, '                # good. double-check this value -- the 1.36 is for')
    (574, '                # converting form 2048,3528 (which the numbers came')
    (575, '                # from) to 2048,2600 (which has 1.36x fewer pixels)')
    (576, '                window_size = 1.17430726 / (1.36*float(wildcards.scaling))')
    (577, '                mem = int(5 * window_size)')
    (578, "            elif wildcards.window_type == \\'cosine\\':")
    (579, '                # this is an approximation of the size of their windows,')
    (580, "                # and if you have at least 3 times this memory, you\\'re")
    (581, '                # good')
    (582, '                window_size = 0.49238059 / float(wildcards.scaling)')
    (583, '                mem = int(5 * window_size)')
    (584, '        else:')
    (585, "            # don\\'t have a good estimate here")
    (586, '            mem = 16')
    (587, '    try:')
    (588, '        if wildcards.save_all:')
    (589, '            # for this estimate, RGC with scaling .095 went from 36GB requested')
    (590, '            # to about 54GB used when stored iterations went from 100 to 1000.')
    (591, "            # that\\'s 1.5x higher, and we add a bit of a buffer. also, don\\'t")
    (592, '            # want to reduce memory estimate')
    (593, '            mem_factor = max((int(wildcards.max_iter) / 100) * (1.7/10), 1)')
    (594, '            mem *= mem_factor')
    (595, '    except AttributeError:')
    (596, "        # then we\\'re missing either the save_all or max_iter wildcard, in which")
    (597, "        # case this is probably cache_windows and the above doesn\\'t matter")
    (598, '        pass')
    (599, '    mem = int(np.ceil(mem))')
    (600, "    if partition == \\'rusty\\':")
    (601, '        if int(wildcards.gpu) == 0:')
    (602, "            # in this case, we *do not* want to specify memory (we\\'ll get the")
    (603, '            # whole node allocated but slurm could still kill the job if we go')
    (604, '            # over requested memory). setting mem=0 requests all memory on node')
    (605, "            mem = \\'0\\'")
    (606, '        else:')
    (607, "            # we\\'ll be plugging this right into the mem request to slurm, so it")
    (608, '            # needs to be exactly correct')
    (609, '            mem = f"{mem}GB"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if \\'norm\\' in wildcards.model_name or wildcards.model_name.startswith(\\'Obs\\')", "if \\'degamma\\' in wildcards.image_name or any([i in wildcards.image_name for i in LINEAR_IMAGES])"]
    (649, 'def get_norm_dict(wildcards):')
    (650, "    if \\'norm\\' in wildcards.model_name or wildcards.model_name.startswith(\\'Obs\\'):")
    (651, "        preproc = \\'\\'")
    (652, "        # lienar images should also use the degamma\\'d textures")
    (653, "        if \\'degamma\\' in wildcards.image_name or any([i in wildcards.image_name for i in LINEAR_IMAGES]):")
    (654, "            preproc += \\'_degamma\\'")
    (655, "        return op.join(config[\\'DATA_DIR\\'], \\'norm_stats\\', f\\'V1_texture{preproc}\\'")
    (656, "                       \\'_norm_stats.pt\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if \\'norm\\' in wildcards.model_name or wildcards.model_name.startswith(\\'Obs\\')", 'else']
    (649, 'def get_norm_dict(wildcards):')
    (650, "    if \\'norm\\' in wildcards.model_name or wildcards.model_name.startswith(\\'Obs\\'):")
    (651, "        preproc = \\'\\'")
    (652, "        # lienar images should also use the degamma\\'d textures")
    (653, "        if \\'degamma\\' in wildcards.image_name or any([i in wildcards.image_name for i in LINEAR_IMAGES]):")
    (654, "            preproc += \\'_degamma\\'")
    (655, "        return op.join(config[\\'DATA_DIR\\'], \\'norm_stats\\', f\\'V1_texture{preproc}\\'")
    (656, "                       \\'_norm_stats.pt\\')")
    (657, '    else:')
    (658, '        return []')
    (659, '')
    (660, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', "if \\'size-\\' in wildcards.image_name"]
    (661, 'def get_windows(wildcards):')
    (662, '    r"""determine the cached window path for the specified model')
    (663, '    """')
    (664, '    window_template = op.join(config["DATA_DIR"], \\\'windows_cache\\\', \\\'scaling-{scaling}_size-{size}\\\'')
    (665, "                              \\'_e0-{min_ecc:.03f}_em-{max_ecc:.01f}_w-{t_width}_{window_type}.pt\\')")
    (666, '    try:')
    (667, "        if \\'size-\\' in wildcards.image_name:")
    (668, "            im_shape = wildcards.image_name[wildcards.image_name.index(\\'size-\\') + len(\\'size-\\'):]")
    (669, "            im_shape = im_shape.replace(\\'.png\\', \\'\\')")
    (670, "            im_shape = [int(i) for i in im_shape.split(\\',\\')]")
    (671, '        else:')
    (672, '            try:')
    (673, '                im = imageio.imread(REF_IMAGE_TEMPLATE_PATH.format(image_name=wildcards.image_name))')
    (674, '                im_shape = im.shape')
    (675, '            except FileNotFoundError:')
    (676, '                raise Exception("Can\\\'t find input image %s or infer its shape, so don\\\'t know what "')
    (677, '                                "windows to cache!" %')
    (678, '                                REF_IMAGE_TEMPLATE_PATH.format(image_name=wildcards.image_name))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', "if \\'cosine\\' in wildcards.model_name"]
    (661, 'def get_windows(wildcards):')
    (662, '    r"""determine the cached window path for the specified model')
    (663, '    """')
    (664, '    window_template = op.join(config["DATA_DIR"], \\\'windows_cache\\\', \\\'scaling-{scaling}_size-{size}\\\'')
    (665, "                              \\'_e0-{min_ecc:.03f}_em-{max_ecc:.01f}_w-{t_width}_{window_type}.pt\\')")
    (666, '    try:')
    (667, "        if \\'size-\\' in wildcards.image_name:")
    (668, "            im_shape = wildcards.image_name[wildcards.image_name.index(\\'size-\\') + len(\\'size-\\'):]")
    (669, "            im_shape = im_shape.replace(\\'.png\\', \\'\\')")
    (670, "            im_shape = [int(i) for i in im_shape.split(\\',\\')]")
    (671, '        else:')
    (672, '            try:')
    (673, '                im = imageio.imread(REF_IMAGE_TEMPLATE_PATH.format(image_name=wildcards.image_name))')
    (674, '                im_shape = im.shape')
    (675, '            except FileNotFoundError:')
    (676, '                raise Exception("Can\\\'t find input image %s or infer its shape, so don\\\'t know what "')
    (677, '                                "windows to cache!" %')
    (678, '                                REF_IMAGE_TEMPLATE_PATH.format(image_name=wildcards.image_name))')
    (679, '    except AttributeError:')
    (680, '        # then there was no wildcards.image_name, so grab the first one from')
    (681, '        # the DEFAULT_METAMERS list')
    (682, '        default_im = IMAGES[0]')
    (683, "        im_shape = default_im[default_im.index(\\'size-\\') + len(\\'size-\\'):]")
    (684, "        im_shape = im_shape.replace(\\'.png\\', \\'\\')")
    (685, "        im_shape = [int(i) for i in im_shape.split(\\',\\')]")
    (686, '    try:')
    (687, '        max_ecc=float(wildcards.max_ecc)')
    (688, '        min_ecc=float(wildcards.min_ecc)')
    (689, '    except AttributeError:')
    (690, '        # then there was no wildcards.max/min_ecc, so grab the default values')
    (691, "        min_ecc = config[\\'DEFAULT_METAMERS\\'][\\'min_ecc\\']")
    (692, "        max_ecc = config[\\'DEFAULT_METAMERS\\'][\\'max_ecc\\']")
    (693, '    t_width = 1.0')
    (694, "    if \\'cosine\\' in wildcards.model_name:")
    (695, "        window_type = \\'cosine\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', "elif \\'gaussian\\' in wildcards.model_name or wildcards.model_name.startswith(\\'Obs\\')"]
    (661, 'def get_windows(wildcards):')
    (662, '    r"""determine the cached window path for the specified model')
    (663, '    """')
    (664, '    window_template = op.join(config["DATA_DIR"], \\\'windows_cache\\\', \\\'scaling-{scaling}_size-{size}\\\'')
    (665, "                              \\'_e0-{min_ecc:.03f}_em-{max_ecc:.01f}_w-{t_width}_{window_type}.pt\\')")
    (666, '    try:')
    (667, "        if \\'size-\\' in wildcards.image_name:")
    (668, "            im_shape = wildcards.image_name[wildcards.image_name.index(\\'size-\\') + len(\\'size-\\'):]")
    (669, "            im_shape = im_shape.replace(\\'.png\\', \\'\\')")
    (670, "            im_shape = [int(i) for i in im_shape.split(\\',\\')]")
    (671, '        else:')
    (672, '            try:')
    (673, '                im = imageio.imread(REF_IMAGE_TEMPLATE_PATH.format(image_name=wildcards.image_name))')
    (674, '                im_shape = im.shape')
    (675, '            except FileNotFoundError:')
    (676, '                raise Exception("Can\\\'t find input image %s or infer its shape, so don\\\'t know what "')
    (677, '                                "windows to cache!" %')
    (678, '                                REF_IMAGE_TEMPLATE_PATH.format(image_name=wildcards.image_name))')
    (679, '    except AttributeError:')
    (680, '        # then there was no wildcards.image_name, so grab the first one from')
    (681, '        # the DEFAULT_METAMERS list')
    (682, '        default_im = IMAGES[0]')
    (683, "        im_shape = default_im[default_im.index(\\'size-\\') + len(\\'size-\\'):]")
    (684, "        im_shape = im_shape.replace(\\'.png\\', \\'\\')")
    (685, "        im_shape = [int(i) for i in im_shape.split(\\',\\')]")
    (686, '    try:')
    (687, '        max_ecc=float(wildcards.max_ecc)')
    (688, '        min_ecc=float(wildcards.min_ecc)')
    (689, '    except AttributeError:')
    (690, '        # then there was no wildcards.max/min_ecc, so grab the default values')
    (691, "        min_ecc = config[\\'DEFAULT_METAMERS\\'][\\'min_ecc\\']")
    (692, "        max_ecc = config[\\'DEFAULT_METAMERS\\'][\\'max_ecc\\']")
    (693, '    t_width = 1.0')
    (694, "    if \\'cosine\\' in wildcards.model_name:")
    (695, "        window_type = \\'cosine\\'")
    (696, "    elif \\'gaussian\\' in wildcards.model_name or wildcards.model_name.startswith(\\'Obs\\'):")
    (697, "        window_type = \\'gaussian\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', 'if wildcards.model_name.startswith("RGC")']
    (661, 'def get_windows(wildcards):')
    (662, '    r"""determine the cached window path for the specified model')
    (663, '    """')
    (664, '    window_template = op.join(config["DATA_DIR"], \\\'windows_cache\\\', \\\'scaling-{scaling}_size-{size}\\\'')
    (665, "                              \\'_e0-{min_ecc:.03f}_em-{max_ecc:.01f}_w-{t_width}_{window_type}.pt\\')")
    (666, '    try:')
    (667, "        if \\'size-\\' in wildcards.image_name:")
    (668, "            im_shape = wildcards.image_name[wildcards.image_name.index(\\'size-\\') + len(\\'size-\\'):]")
    (669, "            im_shape = im_shape.replace(\\'.png\\', \\'\\')")
    (670, "            im_shape = [int(i) for i in im_shape.split(\\',\\')]")
    (671, '        else:')
    (672, '            try:')
    (673, '                im = imageio.imread(REF_IMAGE_TEMPLATE_PATH.format(image_name=wildcards.image_name))')
    (674, '                im_shape = im.shape')
    (675, '            except FileNotFoundError:')
    (676, '                raise Exception("Can\\\'t find input image %s or infer its shape, so don\\\'t know what "')
    (677, '                                "windows to cache!" %')
    (678, '                                REF_IMAGE_TEMPLATE_PATH.format(image_name=wildcards.image_name))')
    (679, '    except AttributeError:')
    (680, '        # then there was no wildcards.image_name, so grab the first one from')
    (681, '        # the DEFAULT_METAMERS list')
    (682, '        default_im = IMAGES[0]')
    (683, "        im_shape = default_im[default_im.index(\\'size-\\') + len(\\'size-\\'):]")
    (684, "        im_shape = im_shape.replace(\\'.png\\', \\'\\')")
    (685, "        im_shape = [int(i) for i in im_shape.split(\\',\\')]")
    (686, '    try:')
    (687, '        max_ecc=float(wildcards.max_ecc)')
    (688, '        min_ecc=float(wildcards.min_ecc)')
    (689, '    except AttributeError:')
    (690, '        # then there was no wildcards.max/min_ecc, so grab the default values')
    (691, "        min_ecc = config[\\'DEFAULT_METAMERS\\'][\\'min_ecc\\']")
    (692, "        max_ecc = config[\\'DEFAULT_METAMERS\\'][\\'max_ecc\\']")
    (693, '    t_width = 1.0')
    (694, "    if \\'cosine\\' in wildcards.model_name:")
    (695, "        window_type = \\'cosine\\'")
    (696, "    elif \\'gaussian\\' in wildcards.model_name or wildcards.model_name.startswith(\\'Obs\\'):")
    (697, "        window_type = \\'gaussian\\'")
    (698, '    if wildcards.model_name.startswith("RGC"):')
    (699, '        # RGC model only needs a single scale of PoolingWindows.')
    (700, "        size = \\',\\'.join([str(i) for i in im_shape])")
    (701, '        return window_template.format(scaling=wildcards.scaling, size=size,')
    (702, '                                      max_ecc=max_ecc, t_width=t_width,')
    (703, '                                      min_ecc=min_ecc, window_type=window_type,)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', "elif wildcards.model_name.startswith(\\'V1\\') or wildcards.model_name.startswith(\\'Obs\\')", 'try']
    (661, 'def get_windows(wildcards):')
    (662, '    r"""determine the cached window path for the specified model')
    (663, '    """')
    (664, '    window_template = op.join(config["DATA_DIR"], \\\'windows_cache\\\', \\\'scaling-{scaling}_size-{size}\\\'')
    (665, "                              \\'_e0-{min_ecc:.03f}_em-{max_ecc:.01f}_w-{t_width}_{window_type}.pt\\')")
    (666, '    try:')
    (667, "        if \\'size-\\' in wildcards.image_name:")
    (668, "            im_shape = wildcards.image_name[wildcards.image_name.index(\\'size-\\') + len(\\'size-\\'):]")
    (669, "            im_shape = im_shape.replace(\\'.png\\', \\'\\')")
    (670, "            im_shape = [int(i) for i in im_shape.split(\\',\\')]")
    (671, '        else:')
    (672, '            try:')
    (673, '                im = imageio.imread(REF_IMAGE_TEMPLATE_PATH.format(image_name=wildcards.image_name))')
    (674, '                im_shape = im.shape')
    (675, '            except FileNotFoundError:')
    (676, '                raise Exception("Can\\\'t find input image %s or infer its shape, so don\\\'t know what "')
    (677, '                                "windows to cache!" %')
    (678, '                                REF_IMAGE_TEMPLATE_PATH.format(image_name=wildcards.image_name))')
    (679, '    except AttributeError:')
    (680, '        # then there was no wildcards.image_name, so grab the first one from')
    (681, '        # the DEFAULT_METAMERS list')
    (682, '        default_im = IMAGES[0]')
    (683, "        im_shape = default_im[default_im.index(\\'size-\\') + len(\\'size-\\'):]")
    (684, "        im_shape = im_shape.replace(\\'.png\\', \\'\\')")
    (685, "        im_shape = [int(i) for i in im_shape.split(\\',\\')]")
    (686, '    try:')
    (687, '        max_ecc=float(wildcards.max_ecc)')
    (688, '        min_ecc=float(wildcards.min_ecc)')
    (689, '    except AttributeError:')
    (690, '        # then there was no wildcards.max/min_ecc, so grab the default values')
    (691, "        min_ecc = config[\\'DEFAULT_METAMERS\\'][\\'min_ecc\\']")
    (692, "        max_ecc = config[\\'DEFAULT_METAMERS\\'][\\'max_ecc\\']")
    (693, '    t_width = 1.0')
    (694, "    if \\'cosine\\' in wildcards.model_name:")
    (695, "        window_type = \\'cosine\\'")
    (696, "    elif \\'gaussian\\' in wildcards.model_name or wildcards.model_name.startswith(\\'Obs\\'):")
    (697, "        window_type = \\'gaussian\\'")
    (698, '    if wildcards.model_name.startswith("RGC"):')
    (699, '        # RGC model only needs a single scale of PoolingWindows.')
    (700, "        size = \\',\\'.join([str(i) for i in im_shape])")
    (701, '        return window_template.format(scaling=wildcards.scaling, size=size,')
    (702, '                                      max_ecc=max_ecc, t_width=t_width,')
    (703, '                                      min_ecc=min_ecc, window_type=window_type,)')
    (704, "    elif wildcards.model_name.startswith(\\'V1\\') or wildcards.model_name.startswith(\\'Obs\\'):")
    (705, '        windows = []')
    (706, '        # need them for every scale')
    (707, '        try:')
    (708, "            num_scales = int(re.findall(\\'s([0-9]+)\\', wildcards.model_name)[0])")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['try', "elif wildcards.model_name.startswith(\\'V1\\') or wildcards.model_name.startswith(\\'Obs\\')"]
    (661, 'def get_windows(wildcards):')
    (662, '    r"""determine the cached window path for the specified model')
    (663, '    """')
    (664, '    window_template = op.join(config["DATA_DIR"], \\\'windows_cache\\\', \\\'scaling-{scaling}_size-{size}\\\'')
    (665, "                              \\'_e0-{min_ecc:.03f}_em-{max_ecc:.01f}_w-{t_width}_{window_type}.pt\\')")
    (666, '    try:')
    (667, "        if \\'size-\\' in wildcards.image_name:")
    (668, "            im_shape = wildcards.image_name[wildcards.image_name.index(\\'size-\\') + len(\\'size-\\'):]")
    (669, "            im_shape = im_shape.replace(\\'.png\\', \\'\\')")
    (670, "            im_shape = [int(i) for i in im_shape.split(\\',\\')]")
    (671, '        else:')
    (672, '            try:')
    (673, '                im = imageio.imread(REF_IMAGE_TEMPLATE_PATH.format(image_name=wildcards.image_name))')
    (674, '                im_shape = im.shape')
    (675, '            except FileNotFoundError:')
    (676, '                raise Exception("Can\\\'t find input image %s or infer its shape, so don\\\'t know what "')
    (677, '                                "windows to cache!" %')
    (678, '                                REF_IMAGE_TEMPLATE_PATH.format(image_name=wildcards.image_name))')
    (679, '    except AttributeError:')
    (680, '        # then there was no wildcards.image_name, so grab the first one from')
    (681, '        # the DEFAULT_METAMERS list')
    (682, '        default_im = IMAGES[0]')
    (683, "        im_shape = default_im[default_im.index(\\'size-\\') + len(\\'size-\\'):]")
    (684, "        im_shape = im_shape.replace(\\'.png\\', \\'\\')")
    (685, "        im_shape = [int(i) for i in im_shape.split(\\',\\')]")
    (686, '    try:')
    (687, '        max_ecc=float(wildcards.max_ecc)')
    (688, '        min_ecc=float(wildcards.min_ecc)')
    (689, '    except AttributeError:')
    (690, '        # then there was no wildcards.max/min_ecc, so grab the default values')
    (691, "        min_ecc = config[\\'DEFAULT_METAMERS\\'][\\'min_ecc\\']")
    (692, "        max_ecc = config[\\'DEFAULT_METAMERS\\'][\\'max_ecc\\']")
    (693, '    t_width = 1.0')
    (694, "    if \\'cosine\\' in wildcards.model_name:")
    (695, "        window_type = \\'cosine\\'")
    (696, "    elif \\'gaussian\\' in wildcards.model_name or wildcards.model_name.startswith(\\'Obs\\'):")
    (697, "        window_type = \\'gaussian\\'")
    (698, '    if wildcards.model_name.startswith("RGC"):')
    (699, '        # RGC model only needs a single scale of PoolingWindows.')
    (700, "        size = \\',\\'.join([str(i) for i in im_shape])")
    (701, '        return window_template.format(scaling=wildcards.scaling, size=size,')
    (702, '                                      max_ecc=max_ecc, t_width=t_width,')
    (703, '                                      min_ecc=min_ecc, window_type=window_type,)')
    (704, "    elif wildcards.model_name.startswith(\\'V1\\') or wildcards.model_name.startswith(\\'Obs\\'):")
    (705, '        windows = []')
    (706, '        # need them for every scale')
    (707, '        try:')
    (708, "            num_scales = int(re.findall(\\'s([0-9]+)\\', wildcards.model_name)[0])")
    (709, '        except (IndexError, ValueError):')
    (710, '            num_scales = 4')
    (711, '        for i in range(num_scales):')
    (712, "            output_size = \\',\\'.join([str(int(np.ceil(j / 2**i))) for j in im_shape])")
    (713, '            windows.append(window_template.format(scaling=wildcards.scaling, size=output_size,')
    (714, '                                                  max_ecc=max_ecc,')
    (715, '                                                  min_ecc=min_ecc,')
    (716, '                                                  t_width=t_width, window_type=window_type))')
    (717, '        return windows')
    (718, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if cluster not in [\\'greene\\', \\'rusty\\']"]
    (719, 'def get_partition(wildcards, cluster):')
    (720, '    # if our V1 scaling value is small enough, we need a V100 and must specify')
    (721, "    # it. otherwise, we can use any GPU, because they\\'ll all have enough")
    (722, '    # memory. The partition name depends on the cluster (greene or rusty), so')
    (723, '    # we have two different params, one for each, and the cluster config grabs')
    (724, "    # the right one. For now, greene doesn\\'t require setting partition.")
    (725, "    if cluster not in [\\'greene\\', \\'rusty\\']:")
    (726, '        raise Exception(f"Don\\\'t know how to handle cluster {cluster}")')
    (727, '    if int(wildcards.gpu) == 0:')
    (728, "        if cluster == \\'rusty\\':")
    (729, "            return \\'ccn\\'")
    (730, "        elif cluster == \\'greene\\':")
    (731, '            return None')
    (732, '    else:')
    (733, "        if cluster == \\'rusty\\':")
    (734, "            return \\'gpu\\'")
    (735, "        elif cluster == \\'greene\\':")
    (736, '            return None')
    (737, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if int(wildcards.gpu) > 0 and cluster == \\'rusty\\'"]
    (738, 'def get_constraint(wildcards, cluster):')
    (739, "    if int(wildcards.gpu) > 0 and cluster == \\'rusty\\':")
    (740, "        return \\'v100-32gb\\'")
    (741, '    else:')
    (742, "        return \\'\\'")
    (743, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['if int(wildcards.gpu) > 0']
    (744, 'def get_cpu_num(wildcards):')
    (745, '    if int(wildcards.gpu) > 0:')
    (746, "        # then we\\'re using the GPU and so don\\'t really need CPUs")
    (747, '        cpus = 1')
    (748, '    else:')
    (749, '        # these are all based on estimates from rusty (which automatically')
    (750, '        # gives each job 28 nodes), and checking seff to see CPU usage')
    (751, '        try:')
    (752, '            if float(wildcards.scaling) > .06:')
    (753, '                cpus = 21')
    (754, '            elif float(wildcards.scaling) > .03:')
    (755, '                cpus = 26')
    (756, '            else:')
    (757, '                cpus = 28')
    (758, '        except AttributeError:')
    (759, "            # then we don\\'t have a scaling attribute")
    (760, '            cpus = 10')
    (761, '    return cpus')
    (762, '')
    (763, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if wildcards.init_type in [\\'white\\', \\'gray\\', \\'pink\\', \\'blue\\']"]
    (764, 'def get_init_image(wildcards):')
    (765, "    if wildcards.init_type in [\\'white\\', \\'gray\\', \\'pink\\', \\'blue\\']:")
    (766, '        return []')
    (767, '    else:')
    (768, '        try:')
    (769, '            # then this is just a noise level, and there is no input required')
    (770, '            float(wildcards.init_type)')
    (771, '            return []')
    (772, '        except ValueError:')
    (773, '            return utils.get_ref_image_full_path(wildcards.init_type)')
    (774, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if wildcards.comp == \\'ref\\'"]
    (1007, 'def get_training_metamers(wildcards):')
    (1008, "    if wildcards.comp == \\'ref\\':")
    (1009, "        scaling = [config[wildcards.model_name.split(\\'_\\')[0]][\\'scaling\\'][0],")
    (1010, "                   config[wildcards.model_name.split(\\'_\\')[0]][\\'scaling\\'][-1]]")
    (1011, '        seed_n = [0]')
    (1012, '    else:')
    (1013, "        all_scaling = (config[wildcards.model_name.split(\\'_\\')[0]][\\'scaling\\'] +")
    (1014, "                       config[wildcards.model_name.split(\\'_\\')[0]][\\'met_v_met_scaling\\'])")
    (1015, '        scaling = [all_scaling[-8], all_scaling[-1]]')
    (1016, '        seed_n = [0, 1]')
    (1017, '    mets = utils.generate_metamer_paths(scaling=scaling, image_name=IMAGES[:2],')
    (1018, '                                        seed_n=seed_n, **wildcards)')
    (1019, "    return [m.replace(\\'metamer.png\\', \\'metamer.npy\\') for m in mets]")
    (1020, '                    ')
    (1021, '')
    (1022, '# for subjects to get a sense for how this is done with metamers')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if (wildcards.model_name == \\'RGC_norm_gaussian\\' and wildcards.comp == \\'met\\')"]
    (1191, 'def get_all_idx(wildcards):')
    (1192, "    if (wildcards.model_name == \\'RGC_norm_gaussian\\' and wildcards.comp == \\'met\\'):")
    (1193, '        sessions = [0]')
    (1194, '    else:')
    (1195, "        sessions = config[\\'PSYCHOPHYSICS\\'][\\'SESSIONS\\']")
    (1196, '    return [op.join(config["DATA_DIR"], \\\'stimuli\\\', \\\'{model_name}\\\', \\\'task-split_comp-{comp}\\\', \\\'{subject}\\\',')
    (1197, "                    \\'{subject}_task-split_comp-{comp}_idx_sess-%02d_run-%02d.npy\\') % (s, r)")
    (1198, "            for s in sessions for r in config[\\'PSYCHOPHYSICS\\'][\\'RUNS\\']]")
    (1199, '')
    (1200, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if wildcards.comp.startswith(\\'met-downsample\\')"]
    (1914, 'def _get_image_for_window_example(wildcards):')
    (1915, "    if wildcards.comp.startswith(\\'met-downsample\\'):")
    (1916, '        # if we set image_name when calling generate_metamer_paths, it will')
    (1917, '        # ignore comp-met-downsample, so we need to manually update it')
    (1918, "        wildcards.image_name = wildcards.image_name.replace(\\'_range\\', \\'_\\' + wildcards.comp.replace(\\'met-\\', \\'\\') + \\'_range\\')")
    (1919, '    return utils.generate_metamer_paths(gamma_corrected=True, **wildcards)')
    (1920, '')
    (1921, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['if isinstance(window, list)']
    (1922, 'def _get_single_window(wildcards):')
    (1923, '    window = get_windows(wildcards)')
    (1924, '    if isinstance(window, list):')
    (1925, "        return window[0].replace(\\'.pt\\', \\'_single.pt\\')")
    (1926, '    else:')
    (1927, "        return window.replace(\\'.pt\\', \\'_single.pt\\')")
    (1928, '')
    (1929, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['if gamma_corrected']
    (2721, 'def get_ref_images(wildcards, gamma_corrected=True):')
    (2722, "    img_sets = config[\\'PSYCHOPHYSICS\\'][\\'IMAGE_SETS\\']")
    (2723, "    images = [op.join(config[\\'DATA_DIR\\'], \\'ref_images_preproc\\', im + \\'.png\\') for im in")
    (2724, "              sorted(img_sets[\\'all\\']) + sorted(img_sets[\\'A\\']) + sorted(img_sets[\\'B\\'])]")
    (2725, '    if gamma_corrected:')
    (2726, "        images = [im.replace(\\'_range-\\', \\'_gamma-corrected_range-\\') for im in images]")
    (2727, '    return images')
    (2728, '')
    (2729, '')
    (2730, '# do this in a different way')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['if not wildcards.synth_model_name.startswith("RGC") or any([wildcards.image_name.startswith(im) for im in met_imgs])']
    (2792, 'def get_all_synth_images(wildcards):')
    (2793, '    synth_imgs = utils.generate_metamer_paths(wildcards.synth_model_name,')
    (2794, '                                               image_name=wildcards.image_name,')
    (2795, "                                               comp=\\'ref\\')")
    (2796, '    # this has a reduced set of metamers that we test')
    (2797, "    met_imgs = [\\'llama\\', \\'highway_symmetric\\', \\'rocks\\', \\'boats\\', \\'gnarled\\']")
    (2798, '    if not wildcards.synth_model_name.startswith("RGC") or any([wildcards.image_name.startswith(im) for im in met_imgs]):')
    (2799, '        synth_imgs += utils.generate_metamer_paths(wildcards.synth_model_name,')
    (2800, '                                                   image_name=wildcards.image_name,')
    (2801, "                                                   comp=\\'met\\')")
    (2802, '    if wildcards.synth_model_name.startswith("V1"):')
    (2803, '        synth_imgs += utils.generate_metamer_paths(wildcards.synth_model_name,')
    (2804, '                                                   image_name=wildcards.image_name,')
    (2805, "                                                   comp=\\'met-natural\\')")
    (2806, '        synth_imgs += utils.generate_metamer_paths(wildcards.synth_model_name,')
    (2807, '                                                   image_name=wildcards.image_name,')
    (2808, "                                                   comp=\\'ref-natural\\')")
    (2809, '    return synth_imgs')
    (2810, '')
    (2811, '')
    (2812, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if wildcards.fill.startswith(\\'random\\') or wildcards.fill == \\'none\\'"]
    (3767, 'def get_window_contour_figure_input(wildcards):')
    (3768, "    if wildcards.fill.startswith(\\'random\\') or wildcards.fill == \\'none\\':")
    (3769, '        return []')
    (3770, '    else:')
    (3771, '        return utils.get_ref_image_full_path(wildcards.fill)')
    (3772, '')
    (3773, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if \\'model_schematic\\' in wildcards.fig_name"]
    (3855, 'def get_compose_figures_input(wildcards):')
    (3856, '    path_template = os.path.join(config[\\\'DATA_DIR\\\'], "figures", wildcards.context, "{}.svg")')
    (3857, "    if \\'model_schematic\\' in wildcards.fig_name:")
    (3858, '        paths = [path_template.format(wildcards.fig_name),')
    (3859, "                 path_template.format(\\'window_contours_fill-random-1_size-2048,2600_scaling-1_linewidth-15_background-white\\'),")
    (3860, "                 path_template.format(\\'window_contours_fill-random-2_size-2048,2600_scaling-2_linewidth-36_background-white\\'),")
    (3861, "                 path_template.format(\\'window_contours_fill-random-3_size-2048,2600_scaling-2_linewidth-36_background-white\\'),")
    (3862, "                 path_template.format(\\'window_contours_fill-random-4_size-2048,2600_scaling-2_linewidth-36_background-white\\'),")
    (3863, "                 path_template.format(\\'window_contours_fill-random-5_size-2048,2600_scaling-2_linewidth-36_background-white\\')]")
    (3864, "    if \\'metamer_comparison\\' in wildcards.fig_name:")
    (3865, "        paths = [path_template.format(wildcards.fig_name.replace(\\'performance_\\', \\'\\').replace(\\'scaling-extended_\\', \\'\\'))]")
    (3866, "        if \\'performance\\' in wildcards.fig_name:")
    (3867, "            if \\'scaling-extended\\' in wildcards.fig_name:")
    (3868, "                paths.append(path_template.format(\\'V1_norm_s6_gaussian/task-split_comp-ref_mcmc_scaling-extended_partially-pooled_performance_focus-outlier\\'))")
    (3869, '            else:')
    (3870, "                paths.append(path_template.format(\\'V1_norm_s6_gaussian/task-split_comp-ref_mcmc_partially-pooled_performance_focus-outlier\\'))")
    (3871, "    if \\'all_comps_summary\\' in wildcards.fig_name:")
    (3872, "        mcmc_model, focus = re.findall(\\'all_comps_summary_([a-z-_]+)_focus-([a-z-_]+)\\', wildcards.fig_name)[0]")
    (3873, '        paths = [')
    (3874, "            path_template.format(\\'{model}/task-split_comp-{comp}_mcmc_{mcmc_model}_performance_focus-{focus}\\').format(")
    (3875, '                mcmc_model=mcmc_model, focus=focus, model=model, comp=comp)')
    (3876, "            for model in [\\'RGC_norm_gaussian\\', \\'V1_norm_s6_gaussian\\'] for comp in [\\'ref\\', \\'met\\']")
    (3877, '        ]')
    (3878, "    if \\'performance_comparison\\' in wildcards.fig_name:")
    (3879, "        mcmc_model, details, comp, extra = re.findall(\\'performance_comparison_([a-z-_]+)_([a-z-]+)_((?:sub-[0-9]+_)?comp-[a-z-]+)([_a-z0-9.-]+)?\\', wildcards.fig_name)[0]")
    (3880, "        paths = [path_template.format(f\\'mcmc_{mcmc_model}_performance_{comp}{extra}\\'),")
    (3881, "                 path_template.format(f\\'mcmc_{mcmc_model}_params-{details}_{comp}\\')]")
    (3882, "    if \\'radial_se\\' in wildcards.fig_name:")
    (3883, "        comp, ecc = re.findall(\\'radial_se_comp-([a-z-]+)_ecc-([A-Za-z0-9,]+)\\', wildcards.fig_name)[0]")
    (3884, "        paths = [path_template.format(f\\'RGC_norm_gaussian/radial_se_comp-{comp}_ecc-{ecc}\\'),")
    (3885, "                 path_template.format(f\\'V1_norm_s6_gaussian/radial_se_comp-{comp}_ecc-{ecc}\\')]")
    (3886, "    if \\'performance-all\\' in wildcards.fig_name:")
    (3887, "        model_name, comp = re.findall(\\'([A-Za-z0-9_]+)_comp-([a-z-]+)_performance-all\\', wildcards.fig_name)[0]")
    (3888, "        paths = [path_template.format(f\\'{model_name}/task-split_comp-{comp}_mcmc_scaling-extended_partially-pooled_performance-all\\'),")
    (3889, "                 path_template.format(f\\'{model_name}/task-split_comp-{comp}_mcmc_scaling-extended_unpooled_performance-all\\')]")
    (3890, '    return paths')
    (3891, '')
    (3892, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if len(image_name) > 1 and \\'small\\' not in wildcards.cutout"]
    (3958, 'def get_metamer_comparison_figure_inputs(wildcards):')
    (3959, "    image_name = wildcards.image_name.split(\\',\\')")
    (3960, "    if len(image_name) > 1 and \\'small\\' not in wildcards.cutout:")
    (3961, '        raise Exception("Only \\\'small\\\' metamer comparison figure can have two images!")')
    (3962, "    scaling = wildcards.scaling.split(\\',\\') * len(image_name)")
    (3963, '    seeds = [0] * len(scaling)')
    (3964, "    # if we\\'re showing two of the same scaling values, for either model, want to")
    (3965, '    # make sure the seeds are different')
    (3966, "    models = [\\'RGC_norm_gaussian\\', \\'RGC_norm_gaussian\\', \\'V1_norm_s6_gaussian\\', \\'V1_norm_s6_gaussian\\']")
    (3967, '    if scaling[0] == scaling[1]:')
    (3968, '        seeds[1] = 1')
    (3969, '    if len(scaling) > 2 and scaling[2] == scaling[3]:')
    (3970, '        seeds[3] = 1')
    (3971, '    if len(scaling) > 5 and scaling[4] == scaling[5]:')
    (3972, '        seeds[5] = 1')
    (3973, '    uniq_imgs = image_name')
    (3974, "    if \\'natural-seed\\' in wildcards.cutout:")
    (3975, '        if len(scaling) != 5:')
    (3976, '            raise Exception(f"When generating {wildcards.cutout} metamer_comparison figure, need 5 scaling values!")')
    (3977, "        if \\'V1\\' in wildcards.cutout:")
    (3978, "            models = [\\'V1_norm_s6_gaussian\\'] * len(scaling)")
    (3979, "        elif \\'RGC\\' in wildcards.cutout:")
    (3980, "            models = [\\'RGC_norm_gaussian\\'] * len(scaling)")
    (3981, '        seeds = [0, 1, 2, 0, 1]')
    (3982, '        image_name = image_name * len(scaling)')
    (3983, "        comps = [\\'ref-natural\\', \\'ref-natural\\', \\'ref-natural\\', \\'ref\\', \\'ref\\']")
    (3984, "    elif \\'small\\' in wildcards.cutout:")
    (3985, '        # we check against 4 but say 2 are required here, because we multiplied')
    (3986, '        # scaling by len(image_name) above. since len(image_name) must be 2')
    (3987, '        # (which we check next), this is equivalent to checking that the input')
    (3988, '        # scaling was 2.')
    (3989, '        if len(scaling) != 4:')
    (3990, '            raise Exception(f"When generating {wildcards.cutout} metamer_comparison figure, need 2 scaling values!")')
    (3991, '        if len(image_name) != 2:')
    (3992, '            raise Exception(f"When generating {wildcards.cutout} metamer_comparison figure, need 2 image_name values!")')
    (3993, "        models = [\\'V1_norm_s6_gaussian\\'] * len(scaling)")
    (3994, '        image_name = image_name * 2')
    (3995, '        # when we increased the length of scaling above, it interleaved the')
    (3996, "        # values. this makes sure they\\'re in the proper order")
    (3997, '        scaling = sorted(scaling)')
    (3998, "        comps = [\\'ref\\'] * len(scaling)")
    (3999, "    elif \\'downsample\\' in wildcards.cutout:")
    (4000, '        if len(scaling) != 4:')
    (4001, '            raise Exception(f"When generating {wildcards.cutout} metamer_comparison figure, need 4 scaling values!")')
    (4002, '        if len(image_name) != 1:')
    (4003, '            raise Exception(f"When generating {wildcards.cutout} metamer_comparison figure, need 1 image_name values!")')
    (4004, "        models = [\\'V1_norm_s6_gaussian\\'] * len(scaling)")
    (4005, '        image_name = image_name * len(scaling)')
    (4006, '        # when we increased the length of scaling above, it interleaved the')
    (4007, "        # values. this makes sure they\\'re in the proper order")
    (4008, '        scaling = sorted(scaling)')
    (4009, "        comps = [\\'met\\'] * 2 +[\\'met-downsample-2\\'] * 2")
    (4010, '    else:')
    (4011, '        if len(scaling) != 4:')
    (4012, '            raise Exception(f"When generating {wildcards.cutout} metamer_comparison figure, need 4 scaling values!")')
    (4013, '        image_name = image_name * len(scaling)')
    (4014, "        comps = [\\'ref\\'] * len(scaling)")
    (4015, '    paths = [')
    (4016, "        op.join(\\'reports\\', \\'figures\\', \\'metamer_comparison_{cutout}.svg\\'),")
    (4017, "        *[op.join(config[\\'DATA_DIR\\'], \\'ref_images_preproc\\', \\'{image_name}_gamma-corrected_range-.05,.95_size-2048,2600.png\\').format(image_name=im)")
    (4018, '          for im in uniq_imgs],')
    (4019, "        *[op.join(config[\\'DATA_DIR\\'], \\'figures\\', \\'{{context}}\\', \\'{model_name}\\',")
    (4020, "                  \\'{image_name}_range-.05,.95_size-2048,2600_scaling-{scaling}_seed-{seed}_comp-{comp}_gpu-{gpu}_linewidth-15_window.png\\').format(")
    (4021, "                      model_name=m, scaling=sc, gpu=0 if float(sc) < config[\\'GPU_SPLIT\\'] else 1, seed=s, image_name=im, comp=comp)")
    (4022, '          for m, im, sc, s, comp in zip(models, image_name, scaling, seeds, comps)]')
    (4023, '    ]')
    (4024, "    if \\'nocutout\\' not in wildcards.cutout:")
    (4025, "        cuts = [\\'with_cutout_cross\\', \\'foveal_cutout_cross\\', \\'peripheral_cutout_cross\\']")
    (4026, "        paths[len(uniq_imgs):] = [p.replace(\\'.png\\', f\\'_{c}.png\\').replace(\\'ref_images_preproc\\', f\\'figures{os.sep}{{context}}\\')")
    (4027, '                                  for p in paths[len(uniq_imgs):] for c in cuts]')
    (4028, '    return paths')
    (4029, '')
    (4030, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if \\'_window\\' in wildcards.image_name"]
    (4087, 'def get_cutout_figures_input(wildcards):')
    (4088, "    if \\'_window\\' in wildcards.image_name:")
    (4089, '        model_name = wildcards.image_name.split(os.sep)[0]')
    (4090, '        image_name = wildcards.image_name.split(os.sep)[1]')
    (4091, "        return op.join(config[\\'DATA_DIR\\'], \\'figures\\', \\'{context}\\', \\'{model_name}\\', \\'{image_name}.png\\').format(")
    (4092, '            model_name=model_name, image_name=image_name, context=wildcards.context)')
    (4093, '    else:')
    (4094, "        return op.join(config[\\'DATA_DIR\\'], \\'ref_images_preproc\\', \\'{image_name}.png\\').format(image_name=wildcards.image_name)")
    (4095, '')
    (4096, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['if isinstance(wdw, list)']
    (4177, 'def get_all_windows(wildcards):')
    (4178, "    wildcards.size = \\'2048,2600\\'")
    (4179, '    windows = []')
    (4180, "    for model in [\\'RGC_norm_gaussian\\', \\'V1_norm_s6_gaussian\\']:")
    (4181, '        wildcards.model_name = model')
    (4182, "        for sc in config[model.split(\\'_\\')[0]][\\'scaling\\']:")
    (4183, '            wildcards.scaling = sc')
    (4184, '            wdw = get_windows(wildcards)')
    (4185, '            if isinstance(wdw, list):')
    (4186, '                windows.extend(get_windows(wildcards))')
    (4187, '            else:')
    (4188, '                windows.append(get_windows(wildcards))')
    (4189, '    return windows')
    (4190, '')
    (4191, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if \\'energy_ref\\' in comp"]
    (4408, "def get_all_metamers(wildcards, comp=[\\'energy_ref\\', \\'energy_met\\', \\'energy_ref-nat\\', \\'energy_met-nat\\',")
    (4409, "                                      \\'luminance_ref\\', \\'luminance_met\\', \\'energy_ref_downsample\\'],")
    (4410, '                     include_gamma=True):')
    (4411, '    from foveated_metamers import stimuli')
    (4412, '    mets = {}')
    (4413, "    if \\'energy_ref\\' in comp:")
    (4414, "        mets[\\'energy_ref\\'] = utils.generate_metamer_paths(\\'V1_norm_s6_gaussian\\')")
    (4415, "    if \\'energy_met\\' in comp:")
    (4416, "        mets[\\'energy_met\\'] = utils.generate_metamer_paths(\\'V1_norm_s6_gaussian\\', comp=\\'met\\')")
    (4417, "    if \\'energy_ref-nat\\' in comp:")
    (4418, "        mets[\\'energy_ref-nat\\'] = utils.generate_metamer_paths(\\'V1_norm_s6_gaussian\\', comp=\\'ref-natural\\')")
    (4419, "    if \\'energy_met-nat\\' in comp:")
    (4420, "        mets[\\'energy_met-nat\\'] = utils.generate_metamer_paths(\\'V1_norm_s6_gaussian\\', comp=\\'met-natural\\')")
    (4421, "    if \\'luminance_ref\\' in comp:")
    (4422, "        mets[\\'luminance_ref\\'] = utils.generate_metamer_paths(\\'RGC_norm_gaussian\\')")
    (4423, "    if \\'luminance_met\\' in comp:")
    (4424, "        mets[\\'luminance_met\\'] = utils.generate_metamer_paths(\\'RGC_norm_gaussian\\', comp=\\'met\\')")
    (4425, '')
    (4426, "    if \\'energy_ref_downsample\\' in comp:")
    (4427, '        imgs = []')
    (4428, '        for i in range(3):')
    (4429, "            imgs.extend(stimuli.get_images_for_session(\\'sub-00\\', i, True))")
    (4430, "        mets[\\'energy_ref_downsample\\'] = utils.generate_metamer_paths(\\'V1_norm_s6_gaussian\\',")
    (4431, "                                                                     comp=\\'met-downsample-2\\',")
    (4432, '                                                                     image_name=imgs)')
    (4433, '    if include_gamma:')
    (4434, '        gamma_mets = {}')
    (4435, '        for k, v in mets.items():')
    (4436, "            gamma_mets[k+\\'_gamma\\'] = [img.replace(\\'metamer.png\\', \\'metamer_gamma-corrected.png\\')")
    (4437, '                                      for img in v]')
    (4438, '        mets.update(gamma_mets)')
    (4439, '    return mets')
    (4440, '')
    (4441, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if to_upload == \\'stimuli\\'"]
    (4523, 'def get_osf_names(wildcards, to_upload):')
    (4524, "    model_name = {\\'energy\\': \\'V1_norm_s6_gaussian\\', \\'luminance\\': \\'RGC_norm_gaussian\\'}[wildcards.osf_model_name]")
    (4525, "    comp = wildcards.osf_comp.replace(\\'-nat\\', \\'-natural\\').replace(\\'_downsample\\', \\'-downsample-2\\')")
    (4526, "    if to_upload == \\'stimuli\\':")
    (4527, "        return [op.join(config[\\'DATA_DIR\\'], \\'stimuli\\', model_name, fn) for fn in [f\\'stimuli_comp-{comp}.npy\\', f\\'stimuli_description_comp-{comp}.csv\\']]")
    (4528, "    elif to_upload == \\'mcmc\\':")
    (4529, '        return op.join(config["DATA_DIR"], \\\'mcmc\\\', model_name, \\\'task-split_comp-{comp}\\\',')
    (4530, "                       \\'task-split_comp-{comp}_mcmc_{mcmc_model}_{hyper}_scaling-extended.nc\\').format(mcmc_model=wildcards.mcmc_model,")
    (4531, '                                                                                                      comp=comp,')
    (4532, "                                                                                                      hyper=utils.get_mcmc_hyperparams({\\'mcmc_model\\': wildcards.mcmc_model,")
    (4533, "                                                                                                                                        \\'model_name\\': model_name,")
    (4534, "                                                                                                                                  \\'comp\\': comp}))")
    (4535, "    elif to_upload == \\'mcmc_compare\\':")
    (4536, '        return op.join(config["DATA_DIR"], \\\'mcmc\\\', model_name, \\\'task-split_comp-{comp}\\\', \\\'task-split_comp-{comp}_mcmc_compare_ic-loo.csv\\\').format(comp=comp)')
    (4537, '')
    (4538, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khalillab/coop-TF-chipseq, file=Snakefile
context_key: ['if passing']
    (16, 'def get_samples(search_dict=CHIPS,')
    (17, '                passing=False,')
    (18, '                spikein=False,')
    (19, '                paired=False,')
    (20, '                groups=None):')
    (21, '    if passing:')
    (22, '        search_dict = {k:v for k,v in search_dict.items() if v["pass-qc"]}')
    (23, '    if spikein:')
    (24, '        search_dict = {k:v for k,v in search_dict.items() if v["spikein"]}')
    (25, '    if paired:')
    (26, '        search_dict = {k:v for k,v in search_dict.items() \\\\')
    (27, '                if v["control"] in get_samples(search_dict=INPUTS,')
    (28, '                                               passing=passing,')
    (29, '                                               spikein=spikein,')
    (30, '                                               paired=False)}')
    (31, '    if groups and "all" not in groups:')
    (32, '        search_dict = {k:v for k,v in search_dict.items() if v["group"] in groups}')
    (33, '    return search_dict')
    (34, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khalillab/coop-TF-chipseq, file=Snakefile
context_key: ['if dict1 == dict2', 'if len(dict1) == 0']
    (35, 'def statuscheck(dict1, dict2):')
    (36, '    if dict1 == dict2:')
    (37, '        if len(dict1) == 0:')
    (38, '            return []')
    (39, '        else:')
    (40, '            return ["passing"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khalillab/coop-TF-chipseq, file=Snakefile
context_key: ['if dict1 == dict2', 'else']
    (35, 'def statuscheck(dict1, dict2):')
    (36, '    if dict1 == dict2:')
    (37, '        if len(dict1) == 0:')
    (38, '            return []')
    (39, '        else:')
    (40, '            return ["passing"]')
    (41, '    else:')
    (42, '        return ["passing", "all"]')
    (43, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khalillab/coop-TF-chipseq, file=Snakefile
context_key: ['if len(conditionlist) == 0']
    (44, 'def conditioncheck(conditionlist):')
    (45, '    if len(conditionlist) == 0:')
    (46, '        return []')
    (47, '    elif len(conditionlist) == 1:')
    (48, '        return conditionlist')
    (49, '    else:')
    (50, '        return conditionlist + ["all"]')
    (51, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khalillab/coop-TF-chipseq, file=Snakefile
context_key: ['if comparisons']
    (44, 'def conditioncheck(conditionlist):')
    (45, '    if len(conditionlist) == 0:')
    (46, '        return []')
    (47, '    elif len(conditionlist) == 1:')
    (48, '        return conditionlist')
    (49, '    else:')
    (50, '        return conditionlist + ["all"]')
    (51, '')
    (52, 'SISAMPLES = get_samples(search_dict=SAMPLES, spikein=True)')
    (53, '')
    (54, 'allgroups = [v["group"] for k,v in get_samples(passing=True, paired=True).items()]')
    (55, 'allgroups_si = [v["group"] for k,v in get_samples(passing=True, spikein=True, paired=True).items()]')
    (56, '#groups with >= 2 passing and paired samples, so that they are valid for peakcalling and diff binding')
    (57, 'validgroups = set(z for z in allgroups if allgroups.count(z)>=2)')
    (58, 'validgroups_si = set(z for z in allgroups_si if allgroups_si.count(z)>=2)')
    (59, '')
    (60, 'comparisons =  config["comparisons"]["libsizenorm"]')
    (61, 'if comparisons:')
    (62, '    controlgroups_all = list(itertools.chain(*[d.values() for d in config["comparisons"]["libsizenorm"] if list(d.keys())[0] and list(d.values())[0] in allgroups]))')
    (63, '    conditiongroups_all = list(itertools.chain(*[d.keys() for d in config["comparisons"]["libsizenorm"] if list(d.keys())[0] and list(d.values())[0] in allgroups]))')
    (64, '    controlgroups = list(itertools.chain(*[d.values() for d in config["comparisons"]["libsizenorm"] if list(d.keys())[0] and list(d.values())[0] in validgroups]))')
    (65, '    conditiongroups = list(itertools.chain(*[d.keys() for d in config["comparisons"]["libsizenorm"] if list(d.keys())[0] and list(d.values())[0] in validgroups]))')
    (66, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khalillab/coop-TF-chipseq, file=Snakefile
context_key: ['if comparisons_si']
    (44, 'def conditioncheck(conditionlist):')
    (45, '    if len(conditionlist) == 0:')
    (46, '        return []')
    (47, '    elif len(conditionlist) == 1:')
    (48, '        return conditionlist')
    (49, '    else:')
    (50, '        return conditionlist + ["all"]')
    (51, '')
    (52, 'SISAMPLES = get_samples(search_dict=SAMPLES, spikein=True)')
    (53, '')
    (54, 'allgroups = [v["group"] for k,v in get_samples(passing=True, paired=True).items()]')
    (55, 'allgroups_si = [v["group"] for k,v in get_samples(passing=True, spikein=True, paired=True).items()]')
    (56, '#groups with >= 2 passing and paired samples, so that they are valid for peakcalling and diff binding')
    (57, 'validgroups = set(z for z in allgroups if allgroups.count(z)>=2)')
    (58, 'validgroups_si = set(z for z in allgroups_si if allgroups_si.count(z)>=2)')
    (59, '')
    (60, 'comparisons =  config["comparisons"]["libsizenorm"]')
    (61, 'if comparisons:')
    (62, '    controlgroups_all = list(itertools.chain(*[d.values() for d in config["comparisons"]["libsizenorm"] if list(d.keys())[0] and list(d.values())[0] in allgroups]))')
    (63, '    conditiongroups_all = list(itertools.chain(*[d.keys() for d in config["comparisons"]["libsizenorm"] if list(d.keys())[0] and list(d.values())[0] in allgroups]))')
    (64, '    controlgroups = list(itertools.chain(*[d.values() for d in config["comparisons"]["libsizenorm"] if list(d.keys())[0] and list(d.values())[0] in validgroups]))')
    (65, '    conditiongroups = list(itertools.chain(*[d.keys() for d in config["comparisons"]["libsizenorm"] if list(d.keys())[0] and list(d.values())[0] in validgroups]))')
    (66, '')
    (67, 'comparisons_si =  config["comparisons"]["spikenorm"]')
    (68, 'if comparisons_si:')
    (69, '    controlgroups_si_all = list(itertools.chain(*[d.values() for d in config["comparisons"]["spikenorm"] if list(d.keys())[0] and list(d.values())[0] in allgroups_si]))')
    (70, '    conditiongroups_si_all = list(itertools.chain(*[d.keys() for d in config["comparisons"]["spikenorm"] if list(d.keys())[0] and list(d.values())[0] in allgroups_si]))')
    (71, '    controlgroups_si = list(itertools.chain(*[d.values() for d in config["comparisons"]["spikenorm"] if list(d.keys())[0] and list(d.values())[0] in validgroups_si]))')
    (72, '    conditiongroups_si = list(itertools.chain(*[d.keys() for d in config["comparisons"]["spikenorm"] if list(d.keys())[0] and list(d.values())[0] in validgroups_si]))')
    (73, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexmsalmeida/virsearch, file=Snakefile
context_key: ["if line.strip() == \\'\\'"]
    (19, 'def checkPreds(wildcards):')
    (20, '    with checkpoints.checkv_input.get(**wildcards).output[0].open() as f:')
    (21, '        line = f.readline()')
    (22, "        if line.strip() == \\'\\':")
    (23, '            return OUTPUT_DIR+"/{id}/checkv/no_viruses.fa", OUTPUT_DIR+"/{id}/checkv/no_viruses_tax.tsv"')
    (24, '        else:')
    (25, '            with checkpoints.checkv_filter.get(**wildcards).output[0].open() as f:')
    (26, '                line = f.readline()')
    (27, "                if line.strip() == \\'\\':")
    (28, '                    return OUTPUT_DIR+"/{id}/checkv/no_viruses.fa", OUTPUT_DIR+"/{id}/checkv/no_viruses_tax.tsv"')
    (29, '                else:')
    (30, '                    return OUTPUT_DIR+"/{id}/cdhit/nr_predictions.fa", OUTPUT_DIR+"/{id}/demovir/DemoVir_assignments.txt"')
    (31, '')
    (32, '# rule that specifies the final expected output files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JTFouquier/snakemake-MERF, file=Snakefile
context_key: ['if dim_method == "pca"']
    (17, 'def dim_reduction():')
    (18, '    dataset_json = json.loads(config["dataset_json"])')
    (19, '    fp_list = []')
    (20, '    for w in dataset_json["datasets"]:')
    (21, '')
    (22, '        ds_name = dataset_json["datasets"][w]["ds_name"]')
    (23, '        file_path = dataset_json["datasets"][w]["file_path"]')
    (24, '        dim_method = dataset_json["datasets"][w]["dim_method"]')
    (25, '        param_dict = dataset_json["datasets"][w]["param_dict"]')
    (26, '')
    (27, '        if dim_method == "pca":')
    (28, '            config["pca_file_path"] = file_path')
    (29, '            config["pca_ds_name"] = ds_name')
    (30, '            f_out = path_dim_pca + ds_name + "/final-pca-dim-reduction.txt"')
    (31, '')
    (32, '        elif dim_method == "scnic":')
    (33, '            config["scnic_file_path"] = file_path')
    (34, '            config["scnic_ds_name"] = ds_name')
    (35, '            f_out = path_dim_scnic + ds_name + "/SCNIC_modules_for_workflow.txt"')
    (36, '')
    (37, '        fp_list.append(f_out)')
    (38, '')
    (39, '    # print(fp_list)')
    (40, '    print(config)')
    (41, '')
    (42, '    return fp_list')
    (43, '')
    (44, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['if os.path.exists(VCF)']
    (90, 'def get_all_inputs(wildcards): #Primary rule whose input values determine which outputs (and corresponding rules) will be run by snakemake')
    (91, '    input_list = ["accessory/.sHWE_pass.txt", f"sHWE/.optimum_popnum.txt", f"{BASE}-rulegraph.png",')
    (92, '                  f"plink/{BASE}_LDp_sHWE.bed", f"{QUERY}.bed", f"plink/{BASE}.eigenvec",')
    (93, '                  f"accessory/samples.txt", f"input/{BASE}_CtlMat.bed", f"plink/{BASE}.genome.gz",')
    (94, '                  f"{BASE}.genesis.txt", f"{BASE}-Mapping-report.pdf"] #, f"{BASE}.genesis.sig.annt.txt"')
    (95, '    #input_list += [f"{BASE}.blink.txt", f"{BASE}.blink.sig.annt.txt"] #Uncomment to enable GWAS with BLINK.  Newer version of BLINK was causing issues with singularity.')
    (96, "    if config[\\'admixMapping\\'][\\'skip\\'] != \\'true\\': #Do not include admixture mapping in desired results, unless requested.")
    (97, '        input_list += [f"{BASE}.admixmap.txt", f"{BASE}.admixmap.txt"] #, f"{BASE}.admixmap.annt.txt"')
    (98, '    if os.path.exists(VCF): input_list += [f"{BASE}.dos.genesis.txt"] #Requests dosage GWAS to be run if a dosage VCF is provided.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['if os.path.exists(VCF)']
    (116, 'def get_report_input(wildcards):')
    (117, '    input_list = [f"{BASE}-rulegraph.png", f"{BASE}.genesis.txt"]')
    (118, '    if config[\\\'admixMapping\\\'][\\\'skip\\\'] != \\\'true\\\': input_list += [f"{BASE}.admixmap.txt", f"{BASE}.globalancestry.txt"]')
    (119, '    if os.path.exists(VCF): input_list += [f"{BASE}.dos.genesis.txt"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ["if os.path.exists(config[\\'conditional_analysis\\'][\\'snp_list\\'])"]
    (122, 'def get_conditional_analysis_input(wildcards):')
    (123, '    inputList = []')
    (124, "    if os.path.exists(config[\\'conditional_analysis\\'][\\'snp_list\\']):")
    (125, "        with open(config[\\'conditional_analysis\\'][\\'snp_list\\'],\\'r\\') as snp_list: # One column with snpIDs")
    (126, '            for line in snp_list:')
    (127, '                marker = line.strip().replace(":", "-")')
    (128, '                inputList.append(f"conditional-analysis/{BASE}-{marker}.genesis.txt")')
    (129, '    else: print("Unable to locate \\\'snp_list\\\' file specified in config file under \\\'conditional_analysis")')
    (130, '    return(inputList)')
    (131, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=steveped/snakemake_h3k27ac, file=workflow/Snakefile
context_key: ['if not all_exist']
    (16, 'def get_here_file():')
    (17, '\\twd = os.getcwd()')
    (18, '\\trproj = os.path.basename(wd) + ".Rproj"')
    (19, '\\trproj_path = os.path.join(wd, rproj)')
    (20, '\\there_file = os.path.join(wd, ".here")')
    (21, '\\tif os.path.isfile(rproj_path):')
    (22, '\\t\\t## Check contents of file for Version in line 1')
    (23, '\\t\\twith open(rproj_path) as f:')
    (24, '\\t\\t\\tln = f.readline().rstrip()')
    (25, "\\t\\tif \\'Version:\\' in ln:")
    (26, '\\t\\t\\there_file = rproj_path')
    (27, '\\treturn(here_file)')
    (28, '')
    (29, '####################################')
    (30, '## Check all external files exist ##')
    (31, '####################################')
    (32, 'all_exist=True')
    (33, "# if config[\\'external\\'][\\'rnaseq\\'] != \\'\\':")
    (34, "# \\tif not os.path.isfile(config[\\'external\\'][\\'rnaseq\\']):")
    (35, '# \\t\\tall_exist=False')
    (36, '# \\t\\tprint(config[\\\'external\\\'][\\\'rnaseq\\\'] + " does not exist")')
    (37, '')
    (38, '')
    (39, 'if not all_exist:')
    (40, '\\tsys.exit(1)')
    (41, '')
    (42, 'def get_ucsc_genome(x):')
    (43, "\\tmap = pd.Series([\\'hg19\\', \\'hg38\\'], index = [\\'GRCh37\\', \\'GRCh38\\'])")
    (44, '\\tif not x in map.keys():')
    (45, '\\t\\tprint("The only currently supported genome builds are:")')
    (46, "\\t\\tprint(*map.keys(), sep = \\' & \\')")
    (47, '\\t\\tsys.exit(1)')
    (48, '\\telse:')
    (49, '\\t\\treturn(map[x])')
    (50, '')
    (51, 'def check_git(x = "."):')
    (52, '\\tis_installed = subprocess.run(')
    (53, "    \\t[\\'which\\', \\'git\\'], universal_newlines=True, check=True,")
    (54, '    \\tstdout=subprocess.PIPE')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=steveped/snakemake_h3k27ac, file=workflow/Snakefile
context_key: ['if pairs', 'rule all', 'input']
    (85, 'def make_pairwise(x):')
    (86, '\\tret_val = []')
    (87, '\\tall_cont = make_contrasts(x)')
    (88, '\\tall_cont.sort()')
    (89, '\\tall_pairs = list(')
    (90, '\\t\\titertools.combinations(all_cont, 2)')
    (91, '\\t)')
    (92, '\\tfor p in all_pairs:')
    (93, '\\t\\tret_val.extend([p[0] + "_" + p[1]])')
    (94, '\\treturn(ret_val)')
    (95, '')
    (96, '')
    (97, '')
    (98, '###############################################################')
    (99, '## Check whether Git is installed & the directory has a repo ##')
    (100, '###############################################################')
    (101, 'git_add = check_git(".")')
    (102, 'git_tries = 100')
    (103, '# git_add = False')
    (104, '')
    (105, '####################')
    (106, '## Define Samples ##')
    (107, '####################')
    (108, "df = pd.read_table(config[\\'samples\\'][\\'file\\'])")
    (109, '')
    (110, '## Now set all values as required')
    (111, "samples = list(set(df[\\'sample\\']))")
    (112, "sources = list(set(df[\\'source\\']))")
    (113, "treats = list(set(df[\\'treat\\']))")
    (114, "pairs=make_pairwise(config[\\'comparisons\\'][\\'contrasts\\'])")
    (115, '')
    (116, '###############')
    (117, '## Key Paths ##')
    (118, '###############')
    (119, 'here_file = get_here_file()')
    (120, "bam_path = config[\\'paths\\'][\\'bam\\']")
    (121, 'rmd_path = "analysis"')
    (122, 'annotation_path = os.path.join("output", "annotations")')
    (123, 'diff_path = os.path.join("output", "differential_binding")')
    (124, 'macs2_path = os.path.join("output", "macs2")')
    (125, 'log_path = os.path.join("workflow", "logs")')
    (126, '')
    (127, '###############################')
    (128, '## External Annotation Files ##')
    (129, '###############################')
    (130, '## These are required input for multiple steps')
    (131, "ucsc_build = get_ucsc_genome(config[\\'genome\\'][\\'build\\'])")
    (132, 'gtf = os.path.join(')
    (133, '\\tannotation_path,')
    (134, '\\t"gencode.v" + config[\\\'genome\\\'][\\\'gencode\\\'] + "lift" +')
    (135, "\\tconfig[\\'genome\\'][\\'build\\'][-2:] +")
    (136, '\\t".annotation.gtf.gz"')
    (137, ')')
    (138, 'blacklist = os.path.join(annotation_path, "blacklist.bed.gz")')
    (139, 'chrom_sizes = os.path.join(annotation_path, "chrom.sizes")')
    (140, '')
    (141, '#####################')
    (142, '## Prepare Outputs ##')
    (143, '#####################')
    (144, 'ALL_OUTPUTS = []')
    (145, '')
    (146, '#####################################')
    (147, '## Annotations Defined in worfklow ##')
    (148, '#####################################')
    (149, '')
    (150, 'ALL_RDS = expand(')
    (151, '\\tos.path.join(annotation_path, "{file}.rds"),')
    (152, "\\tfile = [\\'all_gr\\', \\'colours\\', \\'gene_regions\\', \\'seqinfo\\', \\'trans_models\\',\\'tss\\']")
    (153, ')')
    (154, 'ALL_OUTPUTS.extend(ALL_RDS)')
    (155, '')
    (156, '#######################')
    (157, '## Rmarkdown Outputs ##')
    (158, '#######################')
    (159, 'HTML_OUT = expand(')
    (160, '\\tos.path.join("docs", "{file}.html"),')
    (161, "\\tfile = [\\'annotation_description\\']")
    (162, ')')
    (163, '')
    (164, '')
    (165, '## Macs2 Summaries')
    (166, 'HTML_OUT.extend(')
    (167, '\\texpand(')
    (168, '\\t\\tos.path.join("docs", "{source}_macs2_summary.html"),')
    (169, '\\t\\tsource = sources')
    (170, '\\t)')
    (171, ')')
    (172, '')
    (173, '')
    (174, '## Differential Binding')
    (175, 'HTML_OUT.extend(')
    (176, '\\texpand(')
    (177, '\\t\\tos.path.join("docs", "{cont}_differential_binding.html"),')
    (178, "\\t\\tcont = make_contrasts(config[\\'comparisons\\'][\\'contrasts\\'])")
    (179, '\\t)')
    (180, ')')
    (181, '')
    (182, '## Pairwise Comparisons: Only if required')
    (183, 'if pairs:')
    (184, '\\tHTML_OUT.extend(')
    (185, '\\t\\texpand(')
    (186, '\\t\\t\\tos.path.join("docs", "{comp}_pairwise_comparison.html"),')
    (187, '\\t\\t\\tcomp = pairs')
    (188, '\\t\\t)')
    (189, '\\t)')
    (190, '')
    (191, '')
    (192, 'ALL_OUTPUTS.extend(HTML_OUT)')
    (193, '## Keep the final index separate for easier passing to other rules')
    (194, 'ALL_OUTPUTS.extend([os.path.join("docs", "index.html")])')
    (195, '')
    (196, '## Peaks generated from the Rmd files')
    (197, 'CONS_PEAKS = expand(')
    (198, '\\tos.path.join(macs2_path, "{source}", "{file}"),')
    (199, '\\tsource = sources,')
    (200, "\\tfile = [\\'consensus_peaks.bed\\', \\'oracle_peaks.rds\\']")
    (201, ')')
    (202, 'ALL_OUTPUTS.extend(CONS_PEAKS)')
    (203, '')
    (204, '')
    (205, '###########################')
    (206, '## Peak Files from macs2 ##')
    (207, '###########################')
    (208, "indiv_pre = df[[\\'source\\', \\'sample\\']].apply(")
    (209, "\\tlambda row: \\'/\\'.join(row.values.astype(str)), axis=1")
    (210, ')')
    (211, 'INDIV_PEAKS = expand(')
    (212, '\\tos.path.join(macs2_path, "{path}_peaks.narrowPeak"),')
    (213, '\\tpath = indiv_pre')
    (214, ')')
    (215, 'ALL_OUTPUTS.extend(INDIV_PEAKS)')
    (216, 'merged_pre = set(')
    (217, "\\tdf[[\\'source\\', \\'treat\\']].apply(")
    (218, "\\t\\tlambda row: \\'/\\'.join(row.values.astype(str)), axis=1")
    (219, '\\t)')
    (220, ')')
    (221, 'MERGED_PEAKS = expand(')
    (222, '\\tos.path.join(macs2_path, "{pre}_merged_peaks.narrowPeak"),')
    (223, '\\tpre = merged_pre')
    (224, ')')
    (225, 'ALL_OUTPUTS.extend(MERGED_PEAKS)')
    (226, '')
    (227, '')
    (228, '##################')
    (229, '## BigWig Files ##')
    (230, '##################')
    (231, 'INDIV_BW = expand(')
    (232, '\\tos.path.join(macs2_path, "{path}_treat_pileup.{suffix}"),')
    (233, '\\tpath = indiv_pre,')
    (234, "\\tsuffix = [\\'bw\\']#, \\'summary\\']")
    (235, ')')
    (236, 'ALL_OUTPUTS.extend(INDIV_BW)')
    (237, 'MERGED_BW = expand(')
    (238, '\\tos.path.join(macs2_path, "{path}_merged_treat_pileup.{suffix}"),')
    (239, '\\tpath = merged_pre,')
    (240, "\\tsuffix = [\\'bw\\', \\'summary\\']")
    (241, ')')
    (242, 'ALL_OUTPUTS.extend(MERGED_BW)')
    (243, '')
    (244, 'rule all:')
    (245, '    input:')
    (246, '        ALL_OUTPUTS')
    (247, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zavolanlab/zarp, file=workflow/Snakefile
context_key: ['if search_id', "if search_id == \\'index\\'"]
    (61, 'def get_sample(column_id, search_id=None, search_value=None):')
    (62, '    """ Get relevant per sample information from samples table"""')
    (63, '    if search_id:')
    (64, "        if search_id == \\'index\\':")
    (65, '            return str(samples_table[column_id][samples_table.index == search_value][0])')
    (66, '        else:')
    (67, '            return str(samples_table[column_id][samples_table[search_id] == search_value][0])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zavolanlab/zarp, file=workflow/Snakefile
context_key: ['if search_id', 'else']
    (61, 'def get_sample(column_id, search_id=None, search_value=None):')
    (62, '    """ Get relevant per sample information from samples table"""')
    (63, '    if search_id:')
    (64, "        if search_id == \\'index\\':")
    (65, '            return str(samples_table[column_id][samples_table.index == search_value][0])')
    (66, '        else:')
    (67, '            return str(samples_table[column_id][samples_table[search_id] == search_value][0])')
    (68, '    else:')
    (69, '        return str(samples_table[column_id][0])')
    (70, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zavolanlab/zarp, file=workflow/Snakefile
context_key: ['if key in libtype']
    (71, 'def get_directionality(libtype, tool):')
    (72, '    """ Get directionality value for different tools"""')
    (73, '    directionality =""')
    (74, '')
    (75, '    for key in directionality_dict.keys():')
    (76, "    # Use the first of \\'SF\\' or \\'SR\\' that is found in libtype to look up directionality params for the current tool")
    (77, '        if key in libtype:')
    (78, '            directionality = directionality_dict[key][tool]')
    (79, '            break')
    (80, '')
    (81, "    # If libtype contains neither \\'SF\\', nor \\'SR\\' we don\\'t know what to do")
    (82, '    if not directionality:')
    (83, '        raise ValueError(')
    (84, '            f"Unknown libtype {libtype}.\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=momo54/sage-orderby-experiment, file=Snakefile
context_key: ['if not os.path.isdir(path)']
    (6, 'def list_files(path):')
    (7, '    if not os.path.isdir(path):')
    (8, '        return glob.glob(path)')
    (9, '    files = list()')
    (10, '    for filename in os.listdir(path):')
    (11, '        if filename.endswith(".sparql"):')
    (12, '            files.append(f"{path}/{filename}")')
    (13, '    return files')
    (14, '')
    (15, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=momo54/sage-orderby-experiment, file=Snakefile
context_key: ['if not config["experiments"][xp]["check"]']
    (43, 'def check_files(wcs):')
    (44, '    files = []')
    (45, '    output = "output" if "output" not in config else config["output"]')
    (46, '    for xp in config["experiments"]:')
    (47, '        if not config["experiments"][xp]["check"]:')
    (48, '            continue')
    (49, '        for workload in config["experiments"][xp]["workloads"]:')
    (50, '            for approach in config["experiments"][xp]["approaches"]:')
    (51, '                for filename, query in load_queries(f"workloads/{workload}"):')
    (52, '                    for limit in config["experiments"][xp]["limits"]:')
    (53, '                        for quota in config["experiments"][xp]["quotas"]:')
    (54, '                            files.append((')
    (55, '                                f"{output}/tmp/{xp}/{workload}/"')
    (56, '                                f"{approach}-{quota}ms/{limit}/"')
    (57, '                                f"check/{filename}.csv"))')
    (58, '    return files')
    (59, '')
    (60, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=momo54/sage-orderby-experiment, file=Snakefile
context_key: ['if "autostart" in config and config["autostart"]']
    (61, 'def xp_files(wcs):')
    (62, '    name = config["name"]')
    (63, '    output = config["output"]')
    (64, '    for xp in config["experiments"]:')
    (65, '        if config["experiments"][xp]["check"]:')
    (66, '            return [f"{output}/{name}/run.csv", f"{output}/{name}/check.csv"]')
    (67, '    return [f"{output}/{name}/run.csv"]')
    (68, '')
    (69, '')
    (70, '# def xp_archive(wcs):')
    (71, '#     name = config["name"]')
    (72, '#     output = config["output"]')
    (73, '#     return ancient(f"{output}/{name}/xp.tar.gz")')
    (74, '')
    (75, '')
    (76, 'if "autostart" in config and config["autostart"]:')
    (77, '    onsuccess: shell("bash scripts/server.sh stop all")')
    (78, '    onerror: shell("bash scripts/server.sh stop all")')
    (79, '    onstart: shell("bash scripts/server.sh start all")')
    (80, '')
    (81, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=gustaveroussy/rna-count-kallisto, file=rules/common.smk
context_key: ['if fq.endswith(ext)']
    (58, 'def fq_root() -> Dict[str, str]:\\r')
    (59, '    """\\r')
    (60, '    This function takes the fastq file list and returns the root\\r')
    (61, '    name corresponding to a fastq file\\r')
    (62, '    sample name: sample link path\\r')
    (63, '    """\\r')
    (64, '    # For now, bz2 compression is not taken into account.\\r')
    (65, '    possible_ext = ("fq", "fastq", "fq.gz", "fastq.gz")\\r')
    (66, '\\r')
    (67, '    # Will cause KeyError on single stranded RNA-Seq analysis\\r')
    (68, '    # Better ask forgiveness than permission !\\r')
    (69, '    try:\\r')
    (70, '        # Paired-ended case\\r')
    (71, '        fq_list = chain(design["Upstream_file"], design["Downstream_file"])\\r')
    (72, '    except KeyError:\\r')
    (73, '        # Single ended case\\r')
    (74, '        fq_list = design["Upstream_file"]\\r')
    (75, '\\r')
    (76, '    # Build final result\\r')
    (77, '    result = {}\\r')
    (78, '    for fq in fq_list:\\r')
    (79, '        # I always love writing these crazy for-break-else!\\r')
    (80, '        for ext in possible_ext:\\r')
    (81, '            if fq.endswith(ext):\\r')
    (82, '                # Extension removal\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PNNL-CompBio/perseq, file=Snakefile
context_key: ['if "kaijudb" in config']
    (9, 'def get_kaiju_db_dir(config):')
    (10, '    db_dir = ""')
    (11, '    if "kaijudb" in config:')
    (12, '        db_dir = config["kaijudb"]')
    (13, '    elif os.environ.get("KAIJUDB"):')
    (14, '        db_dir = os.environ.get("KAIJUDB")')
    (15, '    else:')
    (16, '        logger.error("Either set KAIJUDB or pass --config kaijudb=/path")')
    (17, '        sys.exit(1)')
    (18, '    if not os.path.exists(os.path.join(db_dir, "kaiju_db.fmi")):')
    (19, '        logger.error("kaiju_db.fmi does not exist in your database directory")')
    (20, '        sys.exit(1)')
    (21, '    if not os.path.exists(os.path.join(db_dir, "names.dmp")):')
    (22, '        logger.error("names.dmp does not exist in your database directory")')
    (23, '        sys.exit(1)')
    (24, '    if not os.path.exists(os.path.join(db_dir, "nodes.dmp")):')
    (25, '        logger.error("nodes.dmp does not exist in your database directory")')
    (26, '        sys.exit(1)')
    (27, '    return db_dir')
    (28, '')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PNNL-CompBio/perseq, file=Snakefile
context_key: ['if not last', 'if l[0] in ">@"']
    (30, 'def readfx(fp):')
    (31, '    """')
    (32, '    Generator for FAST{A,Q}. See https://github.com/lh3/readfq.')
    (33, '    """')
    (34, '    last = None')
    (35, '    while True:')
    (36, '        if not last:')
    (37, '            for l in fp:')
    (38, '                if l[0] in ">@":')
    (39, '                    last = l[:-1]')
    (40, '                    break')
    (41, '        if not last:')
    (42, '            break')
    (43, '        name, seqs, last = last[1:].partition(" ")[0], [], None')
    (44, '        for l in fp:')
    (45, '            if l[0] in "@+>":')
    (46, '                last = l[:-1]')
    (47, '                break')
    (48, '            seqs.append(l[:-1])')
    (49, '        if not last or last[0] != "+":')
    (50, '            yield name, "".join(seqs), None')
    (51, '            if not last:')
    (52, '                break')
    (53, '        else:')
    (54, '            seq, leng, seqs = "".join(seqs), 0, []')
    (55, '            for l in fp:')
    (56, '                seqs.append(l[:-1])')
    (57, '                leng += len(l) - 1')
    (58, '                if leng >= len(seq):')
    (59, '                    last = None')
    (60, '                    yield name, seq, "".join(seqs)')
    (61, '                    break')
    (62, '            if last:')
    (63, '                yield name, seq, None')
    (64, '                break')
    (65, '')
    (66, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PNNL-CompBio/perseq, file=Snakefile
context_key: ['if not fastq_dir']
    (67, 'def get_samples_from_dir(config):')
    (68, '    # groups = {key: set(value) for key, value in groupby(sorted(mylist, key = lambda e: os.path.splitext(e)[0]), key = lambda e: os.path.splitext(e)[0])}')
    (69, '    fastq_dir = config.get("data")')
    (70, '    if not fastq_dir:')
    (71, '        logger.error("\\\'data\\\' dir with FASTQs has not been set; pass --config data=/path")')
    (72, '        sys.exit(1)')
    (73, '')
    (74, '    raw_fastq_files = list()')
    (75, '    for fq_dir in fastq_dir.split(","):')
    (76, '        if "*" in fastq_dir:')
    (77, '            from glob import glob')
    (78, '            logger.info("Finding samples matching %s" % fq_dir)')
    (79, '            raw_fastq_files.extend(glob(fq_dir))')
    (80, '        else:')
    (81, '            logger.info("Finding samples in %s" % fq_dir)')
    (82, '            raw_fastq_files.extend([os.path.join(fq_dir, i) for i in os.listdir(fq_dir)])')
    (83, '')
    (84, '    samples = dict()')
    (85, '    seen = set()')
    (86, '    for fq_path in raw_fastq_files:')
    (87, '        fname = os.path.basename(fq_path)')
    (88, '        fastq_dir = os.path.dirname(fq_path)')
    (89, '        if not ".fastq" in fname and not ".fq" in fname:')
    (90, '            continue')
    (91, '        if not "_R1" in fname and not "_r1" in fname:')
    (92, '            continue')
    (93, '')
    (94, '        sample_id = fname.partition(".fastq")[0]')
    (95, '        if ".fq" in sample_id:')
    (96, '            sample_id = fname.partition(".fq")[0]')
    (97, '        sample_id = sample_id.replace("_R1", "").replace("_r1", "")')
    (98, '        sample_id = sample_id.replace(".", "_").replace(" ", "_").replace("-", "_")')
    (99, '')
    (100, '        # sample ID after rename')
    (101, '        if sample_id in seen:')
    (102, '            # but FASTQ has yet to be added')
    (103, '            # if one sample has a dash and another an underscore, this')
    (104, '            # is a case where we should warn the user that this file')
    (105, '            # is being skipped')
    (106, '            if not fq_path in seen:')
    (107, '                logger.warning("Duplicate sample %s was found after renaming; skipping..." % sample_id)')
    (108, '            continue')
    (109, '        # simple replace of right-most read index designator')
    (110, '        if fname.find("_R1") > fname.find("_r1"):')
    (111, '            r2 = os.path.join(fastq_dir, "_R2".join(fname.rsplit("_R1", 1)))')
    (112, '        else:')
    (113, '            r2 = os.path.join(fastq_dir, "_r2".join(fname.rsplit("_r1", 1)))')
    (114, '        # not paired-end?')
    (115, '        if not os.path.exists(r2):')
    (116, '            logger.error("File [%s] for %s was not found. Exiting." % (r2, sample_id))')
    (117, '            sys.exit(1)')
    (118, '        seen.add(fq_path)')
    (119, '        seen.add(sample_id)')
    (120, '        samples[sample_id] = {"R1": fq_path, "R2": r2}')
    (121, '')
    (122, '    if len(samples) == 0:')
    (123, '        logger.error("No samples were found for processing.")')
    (124, '        sys.exit(1)')
    (125, '    logger.info("Found %d samples for processing" % len(samples))')
    (126, '    # samples_str = ""')
    (127, '    # for k, v in samples.items():')
    (128, '    #     samples_str += "%s: %s; %s\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PNNL-CompBio/perseq, file=Snakefile
context_key: ['if isinstance(subsample, int)', 'if subsample < 1']
    (148, 'def get_merge_input(wildcards):')
    (149, '    subsample = config.get("subsample", -1)')
    (150, '    if isinstance(subsample, int):')
    (151, '        if subsample < 1:')
    (152, '            # logger.info("No subsampling performed.")')
    (153, '            files = config["samples"][wildcards.sample]')
    (154, '        else:')
    (155, '            files = {')
    (156, '                "R1": "subsampled/{wc.sample}_R1.fastq.gz".format(wc=wildcards),')
    (157, '                "R2": "subsampled/{wc.sample}_R2.fastq.gz".format(wc=wildcards),')
    (158, '            }')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PNNL-CompBio/perseq, file=Snakefile
context_key: ['if isinstance(subsample, int)', 'else']
    (148, 'def get_merge_input(wildcards):')
    (149, '    subsample = config.get("subsample", -1)')
    (150, '    if isinstance(subsample, int):')
    (151, '        if subsample < 1:')
    (152, '            # logger.info("No subsampling performed.")')
    (153, '            files = config["samples"][wildcards.sample]')
    (154, '        else:')
    (155, '            files = {')
    (156, '                "R1": "subsampled/{wc.sample}_R1.fastq.gz".format(wc=wildcards),')
    (157, '                "R2": "subsampled/{wc.sample}_R2.fastq.gz".format(wc=wildcards),')
    (158, '            }')
    (159, '    else:')
    (160, '        logger.error(f"Invalid argument provided to subsample: {subsample}")')
    (161, '        sys.exit(1)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PNNL-CompBio/perseq, file=Snakefile
context_key: ['if isinstance(subsample, int)']
    (148, 'def get_merge_input(wildcards):')
    (149, '    subsample = config.get("subsample", -1)')
    (150, '    if isinstance(subsample, int):')
    (151, '        if subsample < 1:')
    (152, '            # logger.info("No subsampling performed.")')
    (153, '            files = config["samples"][wildcards.sample]')
    (154, '        else:')
    (155, '            files = {')
    (156, '                "R1": "subsampled/{wc.sample}_R1.fastq.gz".format(wc=wildcards),')
    (157, '                "R2": "subsampled/{wc.sample}_R2.fastq.gz".format(wc=wildcards),')
    (158, '            }')
    (159, '    else:')
    (160, '        logger.error(f"Invalid argument provided to subsample: {subsample}")')
    (161, '        sys.exit(1)')
    (162, '    return files')
    (163, '')
    (164, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PNNL-CompBio/perseq, file=Snakefile
context_key: ['if wildcards.hmm == "HAMAP"']
    (165, 'def get_hmm(wildcards):')
    (166, '    if wildcards.hmm == "HAMAP":')
    (167, '        hmm = config["hamap_hmm"]')
    (168, '    elif wildcards.hmm == "dbCAN":')
    (169, '        hmm = config["dbcan_hmm"]')
    (170, '    elif wildcards.hmm == "TIGRFAMs":')
    (171, '        hmm = config["tigrfams_hmm"]')
    (172, '    else:')
    (173, '        logger.error("Unsure which HMM is currently selected.")')
    (174, '        sys.exit(1)')
    (175, '    return dict(')
    (176, '                hmm=hmm,')
    (177, '                h3f="%s.h3f" % hmm,')
    (178, '                h3i="%s.h3i" % hmm,')
    (179, '                h3m="%s.h3m" % hmm,')
    (180, '                h3p="%s.h3p" % hmm')
    (181, '    )')
    (182, '')
    (183, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MerrimanLab/variant_calling_pipeline, file=vcf.snake
context_key: ['if wildcards.build =="b37"']
    (6, 'def get_chr(wildcards):')
    (7, '    if wildcards.build =="b37":')
    (8, '        return ["-L" +  "".join(wildcards.chr)]')
    (9, '    else:')
    (10, '        return ["-L chr" + "".join(wildcards.chr)]')
    (11, '')
    (12, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MerrimanLab/variant_calling_pipeline, file=vcf.snake
context_key: ['if wildcards.build == "b37"']
    (13, 'def get_gvcfs(wildcards) :')
    (14, '    if wildcards.build == "b37":')
    (15, '        return expand("{build}/raw_vcf/{sample}_{build}.raw.snps.indels.g.vcf.gz", sample = sequences.Sample.unique(), build = "b37")')
    (16, '    else:')
    (17, '        return  expand("{build}/raw_vcf/{sample}_{build}.raw.snps.indels.g.vcf.gz", sample = sequences.Sample.unique(), build = "b38")')
    (18, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=taylorreiter/2022-infant-mge, file=Snakefile
context_key: ['if self.samples is None']
    (32, '    def __call__(self, w):')
    (33, "        # get \\'sample\\' from wildcards?")
    (34, '        if self.samples is None:')
    (35, '            return self.do_sample(w)')
    (36, '        else:')
    (37, '            assert not hasattr(w, \\\'sample\\\'), "if \\\'samples\\\' provided to constructor, cannot also be in rule inputs"')
    (38, '')
    (39, '            ret = []')
    (40, '            for sample in self.samples:')
    (41, '                d = dict(sample=sample)')
    (42, '                w = snakemake.io.Wildcards(fromdict=d)')
    (43, '')
    (44, '                x = self.do_sample(w)')
    (45, '                ret.extend(x)')
    (46, '')
    (47, '            return ret')
    (48, '')
    (49, '    def do_sample(self, w):')
    (50, "        # wait for the results of \\'sourmash_gather_mgx\\'; this will trigger exception until that rule has been run.")
    (51, '        checkpoints.sourmash_gather_mgx.get(**w)')
    (52, '')
    (53, '        # parse hitlist_genomes,')
    (54, '        genome_idents = self.get_genome_idents(w.sample)')
    (55, '')
    (56, '        p = expand(self.pattern, acc=genome_idents, **w)')
    (57, '')
    (58, '        return p')
    (59, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jkrapohl/SnakeWRAP, file=Snakefile
context_key: ['if not os.path.isfile(config["metatext"])']
    (15, 'def sanitizefile(str):')
    (16, '\\tif str is None:')
    (17, "\\t\\tstr = \\'\\'")
    (18, '\\treturn str')
    (19, '')
    (20, "config[\\'metatext\\'] = sanitizefile(config[\\'metatext\\'])")
    (21, '')
    (22, '## Read metadata')
    (23, 'if not os.path.isfile(config["metatext"]):')
    (24, '  sys.exit("Metadata file " + config["metatext"] + " does not exist.")')
    (25, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jkrapohl/SnakeWRAP, file=Snakefile
context_key: ["if not set([\\'names\\']).issubset(samples.columns)"]
    (15, 'def sanitizefile(str):')
    (16, '\\tif str is None:')
    (17, "\\t\\tstr = \\'\\'")
    (18, '\\treturn str')
    (19, '')
    (20, "config[\\'metatext\\'] = sanitizefile(config[\\'metatext\\'])")
    (21, '')
    (22, '## Read metadata')
    (23, 'if not os.path.isfile(config["metatext"]):')
    (24, '  sys.exit("Metadata file " + config["metatext"] + " does not exist.")')
    (25, '')
    (26, 'import pandas as pd')
    (27, 'samples = pd.read_csv(config["metatext"], sep=\\\'\\\\t\\\')')
    (28, '')
    (29, "if not set([\\'names\\']).issubset(samples.columns):")
    (30, '  sys.exit("Make sure \\\'names\\\' in column header in " + config["metatext"])')
    (31, '')
    (32, '')
    (33, '## Sanitize provided input and output directories')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=related-sciences/ukb-gwas-pipeline-nealelab, file=Snakefile
context_key: ['if add_protocol']
    (24, 'def bucket_path(path, add_protocol=False):')
    (25, "    path = bucket + \\'/\\' + path")
    (26, '    if add_protocol:')
    (27, "        path = \\'gs://\\' + path")
    (28, '    return path')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=qbicsoftware-archive/exomseq, file=Snakefile
context_key: ["if not os.path.exists(indexed_genome + \\'.fa\\')"]
    (31, 'def etc(path):')
    (32, '    return os.path.join(INI_PATH, path)')
    (33, '')
    (34, 'try:')
    (35, '    with open(etc("params.json")) as f:')
    (36, '        parameters = json.load(f)')
    (37, '')
    (38, '')
    (39, 'except OSError as e:')
    (40, '    print("Could not read parameter file: " + str(e), file=sys.stderr)')
    (41, '    sys.exit(1)')
    (42, 'except ValueError as e:')
    (43, '    print("Invalid parameter file: " + str(e), file=sys.stderr)')
    (44, '    sys.exit(1)')
    (45, '')
    (46, 'default_params = {}')
    (47, 'default_params.update(parameters)')
    (48, 'parameters = default_params')
    (49, '')
    (50, "parameters[\\'indexed_genome\\'] = ref(parameters[\\'indexed_genome\\'])")
    (51, 'indexed_genome = parameters["indexed_genome"]')
    (52, "if not os.path.exists(indexed_genome + \\'.fa\\'):")
    (53, '    raise ValueError("Could not find indexed genome file %s" % indexed_genome)')
    (54, '')
    (55, '')
    (56, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=qbicsoftware-archive/exomseq, file=Snakefile
context_key: ['if len(sample_files) == 2']
    (57, 'def read_design():')
    (58, "    df = pd.read_csv(etc(\\'design.csv\\'), sep=\\'\\\\t\\')")
    (59, '')
    (60, "    test_samples = df[df[\\'SAMPLE TYPE\\'] == \\'Q_TEST_SAMPLE\\']")
    (61, "    #dna_samples = test_samples[test_samples[\\'Q_SAMPLE_TYPE\\'] == \\'DNA\\']")
    (62, '')
    (63, '    pools = {}')
    (64, '    pool = {}')
    (65, "    pools[\\'all\\'] = pool")
    (66, '')
    (67, '    files = sorted(os.listdir(DATA))')
    (68, '')
    (69, '    for barcode in test_samples.Identifier:')
    (70, "        sample_files = [file.split(\\'.\\')[0] for file in files if (file.startswith(barcode) & file.endswith(\\'.fastq\\'))]")
    (71, '        if len(sample_files) == 2:')
    (72, '            pool[barcode] = sample_files')
    (73, '    return pools')
    (74, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=qbicsoftware-archive/exomseq, file=Snakefile
context_key: ['if pool not in pools']
    (79, 'def fastq_per_group(wildcards):')
    (80, "    pool = wildcards[\\'pool\\']")
    (81, "    group = wildcards[\\'group\\']")
    (82, '    ##print(group)')
    (83, '    if pool not in pools:')
    (84, "        return [\\'/i_do_not_exist\\']")
    (85, '    if group not in pools[pool]:')
    (86, "        return [\\'/i_do_not_exist\\']")
    (87, '    names = pools[pool][group]')
    (88, '    return expand(data("{name}.fastq"), name=names)')
    (89, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=qbicsoftware-archive/exomseq, file=Snakefile
context_key: ["if pool not in parameters[\\'pools\\']"]
    (97, '    def inner(wildcards):')
    (98, "        pool = wildcards[\\'pool\\']")
    (99, "        if pool not in parameters[\\'pools\\']:")
    (100, '            return ["/i_do_not_exist"]')
    (101, "        groups = parameters[\\'pools\\'][pool]")
    (102, '        group_names = groups.keys()')
    (103, '        return expand(format_string, pool=[pool], group=group_names)')
    (104, '    return inner')
    (105, '')
    (106, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=qbicsoftware-archive/exomseq, file=Snakefile
context_key: ['if os.path.exists(fname)']
    (115, 'def touch(fname):')
    (116, '    if os.path.exists(fname):')
    (117, '        os.utime(fname, None)')
    (118, '    else:')
    (119, "        open(fname, \\'a\\').close()")
    (120, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/lathe, file=Snakefile
context_key: ['if len(config[\\\'genome_size\\\'].split(",")) == 1']
    (268, 'def choose_merge():')
    (269, '    if len(config[\\\'genome_size\\\'].split(",")) == 1:')
    (270, '        return(rules.no_merge.output)')
    (271, '    else:')
    (272, '        return(rules.merge.output)')
    (273, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/lathe, file=Snakefile
context_key: ["if \\'min_contig_size\\' in config and int(config[\\'min_contig_size\\'] > 0)"]
    (286, 'def choose_contig_cutoff(wildcards):')
    (287, '    #If a contig cutoff is specified in the config, then perform a size cutoff on the assembled contigs by')
    (288, '    #requesting the output of the relevant rule.')
    (289, "    if \\'min_contig_size\\' in config and int(config[\\'min_contig_size\\'] > 0):")
    (290, '        return(rules.contig_size_filter.output)')
    (291, '    else:')
    (292, '        return(choose_merge()[0])')
    (293, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/lathe, file=Snakefile
context_key: ["if \\'skip_polishing\\' in config and (config[\\'skip_polishing\\'] == True or config[\\'skip_polishing\\'] == \\'True\\')"]
    (306, 'def choose_polish(wildcards):')
    (307, '    #Determine which type of polishing should be performed. Options are short read (requires a dataset), long read,')
    (308, '    #both, and none. These are controlled by the skip_polishing and short_read variables in the config. Long read')
    (309, '    #is the default. Short read is performed instead if short reads are provided. No polishing is specified with the')
    (310, '    #skip_polishing config variable.')
    (311, "    if \\'skip_polishing\\' in config and (config[\\'skip_polishing\\'] == True or config[\\'skip_polishing\\'] == \\'True\\'):")
    (312, '        return(rules.assemble_final.output)')
    (313, '        print(wildcards.sample + ": 1")')
    (314, "    elif sr_polish_dict[wildcards.sample] != \\'\\':")
    (315, '        return(rules.pilon_consensus.output)')
    (316, '        print(wildcards.sample + ": 2")')
    (317, '    else:')
    (318, '        return(rules.medaka_aggregate.output)')
    (319, '        print(wildcards.sample + ": 3")')
    (320, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/lathe, file=Snakefile
context_key: ["if \\'polish_both\\' in config and config[\\'polish_both\\'] == True"]
    (321, 'def choose_pilon_input():')
    (322, '    #allows consensus refinement with both long read and short read polishing if the polish_both variable is given the value')
    (323, '    #True in the config file.')
    (324, "    if \\'polish_both\\' in config and config[\\'polish_both\\'] == True:")
    (325, '        return(rules.medaka_aggregate.output)')
    (326, '    else:')
    (327, '        return(rules.assemble_final.output)')
    (328, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/lathe, file=Snakefile
context_key: ['if int(wildcards.iteration) == 1']
    (329, 'def get_racon_input(wildcards):')
    (330, '    #this method will choose a previous iteration of racon or the unpolished assembly, depending on the value of the iteration wildcard')
    (331, '    if int(wildcards.iteration) == 1:')
    (332, "        return(rules.assemble_final.output[0] + \\'.paf\\', rules.assemble_final.output[0])")
    (333, '    else:')
    (334, '        result = "{sample}/2.polish/racon/{sample}_racon_{iter}.fa".format(sample = wildcards.sample, iter = str(int(wildcards.iteration) - 1))')
    (335, "        return(result + \\'.paf\\', result)")
    (336, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=EngreitzLab/crispri-flowfish, file=workflow/rules/make_count_tables.smk
context_key: ['if (os.path.exists(file))']
    (8, 'def make_count_table(samplesheet, group_col, group_id, bins, outfile_raw, outfile_frequencies):')
    (9, '    ## Function to make a count table at various layers of resolution (e.g., by experiment, or by replicate, or by PCR replicate)')
    (10, '    ## To do: Move the python code for these rules into separate python scripts so they can be run independently of the snakemake pipeline (at least, this makes it easier to test and debug the code)')
    (11, '')
    (12, '    currSamples = samplesheet.loc[samplesheet[group_col]==group_id]')
    (13, '')
    (14, '    count_tbls = []')
    (15, '')
    (16, '    for idx, row in currSamples.iterrows():')
    (17, "        s=row[\\'SampleID\\']")
    (18, '        file = "results/counts/{SampleID}.count.txt".format(SampleID=s)')
    (19, '')
    (20, '        if (os.path.exists(file)):')
    (21, "            curr = pd.read_table(file, names=[s,\\'OligoID\\'])")
    (22, '            curr[s] = curr[s].astype(np.int32)')
    (23, '            count_tbls.append(curr)')
    (24, '')
    (25, '    if len(count_tbls) > 0:')
    (26, '        count_tbl = count_tbls.pop()')
    (27, '')
    (28, '        for tbl in count_tbls:')
    (29, "            count_tbl = count_tbl.merge(tbl, on=\\'OligoID\\', how=\\'outer\\')")
    (30, '')
    (31, "        count_tbl = count_tbl.set_index(\\'OligoID\\')")
    (32, '        count_tbl = count_tbl.fillna(0)')
    (33, '        count_tbl = count_tbl.astype(int)')
    (34, '')
    (35, '        ## Now, sum counts per bin')
    (36, "        bin_list = bins + list(set(currSamples[\\'Bin\\'].unique())-set(bins))")
    (37, '        for uniqBin in bin_list:')
    (38, "            samples = currSamples.loc[currSamples[\\'Bin\\'] == uniqBin]")
    (39, '            if len(samples) > 0:')
    (40, "                count_tbl[uniqBin] = count_tbl[samples[\\'SampleID\\']].sum(axis=1).values")
    (41, '            else:')
    (42, '                count_tbl[uniqBin] = 0')
    (43, '        count_tbl = count_tbl[bin_list]')
    (44, '')
    (45, '    else:')
    (46, "        count_tbl = pd.DataFrame({\\'OligoID\\':[]})")
    (47, '        for uniqBin in bins:')
    (48, '            count_tbl[uniqBin] = []')
    (49, "        count_tbl = count_tbl.set_index(\\'OligoID\\')")
    (50, '')
    (51, '    ## Make compatible with multiple sorters')
    (52, '    ## drop bins from bin list that are not used for particular sorters')
    (53, '')
    (54, '    count_tbl = count_tbl.loc[:, (count_tbl.sum(axis=0) != 0)]')
    (55, '')
    (56, '    count_tbl.index.name = "OligoID"')
    (57, "    count_tbl.to_csv(outfile_raw, sep=\\'\\\\t\\')")
    (58, '')
    (59, '    freq_tbl = count_tbl.div(count_tbl.sum(axis=0), axis=1)')
    (60, "    freq_tbl.to_csv(outfile_frequencies, sep=\\'\\\\t\\', float_format=\\'%.6f\\')")
    (61, '')
    (62, '')
    (63, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=EngreitzLab/crispri-flowfish, file=workflow/rules/make_count_tables.smk
context_key: ['if (os.path.exists(file))']
    (64, 'def make_flat_table(samplesheet, outfile):')
    (65, '    count_tbls = []')
    (66, '    for idx, row in samplesheet.iterrows():')
    (67, '        file = "results/counts/{SampleID}.count.txt".format(SampleID=row[\\\'SampleID\\\'])')
    (68, '')
    (69, '        if (os.path.exists(file)):')
    (70, "            curr = pd.read_table(file, names=[\\'count\\',\\'OligoID\\'])")
    (71, "            curr[\\'SampleID\\'] = row[\\'SampleID\\']")
    (72, "            curr[\\'count\\'] = curr[\\'count\\'].astype(np.int32)")
    (73, '            count_tbls.append(curr)')
    (74, '')
    (75, "    flat = pd.concat(count_tbls, axis=\\'index\\', ignore_index=True)")
    (76, "    flat.to_csv(outfile, sep=\\'\\\\t\\', index=False, compression=\\'gzip\\')")
    (77, '')
    (78, '')
    (79, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/Mykrobe_tb_workflow, file=rules/common.smk
context_key: ['if not (len(barcode) == 4 and re.match(regex, barcode))']
    (8, 'def barcode_parser(barcodes_string: str) -> List[str]:')
    (9, '    """Parses the barcodes string and ensures they follow correct format"""')
    (10, '    msg = "Barcode must be of the form BC01. That is, BC followed by 2 digits."')
    (11, '    regex = r"\\\\bBC\\\\d{2}\\\\b"')
    (12, '    barcodes = barcodes_string.split()')
    (13, '    for barcode in barcodes:')
    (14, '        if not (len(barcode) == 4 and re.match(regex, barcode)):')
    (15, '            raise InvalidBarcode(barcode + "\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tjbencomo/cna-pipeline, file=Snakefile
context_key: ['if not check_samples(samples)']
    (4, 'def check_samples(df):')
    (5, "    res = df.groupby(\\'patient\\').size()")
    (6, '    passCountCheck = res.loc[res != 2].shape[0] == 0')
    (7, "    L = [\\'normal\\', \\'tumor\\']")
    (8, '    s = set(L)')
    (9, "    out = df.groupby(\\'patient\\')[\\'sample_type\\'].apply(lambda x: s.issubset(x))")
    (10, '    passSampleCheck = out.sum() == out.shape[0]')
    (11, '    return passCountCheck and passSampleCheck')
    (12, '')
    (13, "CHROMOSOMES = [\\'chr\\' + str(i) for i in range(1, 22)] + [\\'chrX\\', \\'chrY\\']")
    (14, '')
    (15, "configfile: \\'config.yaml\\'")
    (16, '')
    (17, '# Sample Info')
    (18, "samples = pd.read_csv(config[\\'samples\\'])")
    (19, 'if not check_samples(samples):')
    (20, '    raise ValueError("Your samples CSV is not right! Make sure each patient has a normal and tumor bam")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vibaotram/baseDmux, file=baseDmux/data/Snakefile
context_key: ['if not BARCODE_BY_GENOME', 'else', 'if len(READS_FILTERING) == 0', 'else']
    (33, "def by_cond(cond, yes, no, cond_ext = \\'\\', no_ext = \\'\\'): # it\\'s working but needs to be improved ...")
    (34, '\\tif not cond_ext:')
    (35, '\\t\\tcond_ext = not cond')
    (36, '\\tif cond:')
    (37, '\\t\\treturn yes')
    (38, '\\telif cond_ext:')
    (39, '\\t\\treturn no')
    (40, '\\telse:')
    (41, '\\t\\treturn no_ext')
    (42, '')
    (43, '##############################')
    (44, '## guppy_basecaller parameters')
    (45, "RESOURCE = config[\\'RESOURCE\\']")
    (46, '')
    (47, "KIT = config[\\'KIT\\']")
    (48, '')
    (49, "FLOWCELL = config[\\'FLOWCELL\\']")
    (50, '')
    (51, "# QSCORE_FILTERING = config[\\'BASECALLER\\'][\\'QSCORE_FILTERING\\']")
    (52, "MIN_QSCORE = config[\\'RULE_GUPPY_BASECALLING\\'][\\'MIN_QSCORE\\']")
    (53, '')
    (54, "CPU_THREADS_PER_CALLER = config[\\'RULE_GUPPY_BASECALLING\\'][\\'CPU_THREADS_PER_CALLER\\']")
    (55, "NUM_CALLERS = config[\\'RULE_GUPPY_BASECALLING\\'][\\'NUM_CALLERS\\']")
    (56, '')
    (57, "BASECALLER_ADDITION = config[\\'RULE_GUPPY_BASECALLING\\'][\\'ADDITION\\']")
    (58, 'CONFIG = by_cond("--config" in BASECALLER_ADDITION, \\\'\\\', f"--flowcell {FLOWCELL} --kit {KIT}")')
    (59, "# CUDA = config[\\'BASECALLER\\'][\\'CUDA\\']")
    (60, '')
    (61, "GPU_RUNNERS_PER_DEVICE = config[\\'RULE_GUPPY_BASECALLING\\'][\\'GPU_RUNNERS_PER_DEVICE\\']")
    (62, "NUM_GPUS = config[\\'NUM_GPUS\\']")
    (63, '')
    (64, '# adjust guppy_basecaller parameters based on RESOURCE')
    (65, "BASECALLER_OPT = by_cond(cond = RESOURCE == \\'CPU\\',")
    (66, '                         yes = f"{CONFIG} --num_callers {NUM_CALLERS} --cpu_threads_per_caller {CPU_THREADS_PER_CALLER} --min_qscore {MIN_QSCORE} {BASECALLER_ADDITION}",')
    (67, '                         no = f"{CONFIG} --num_callers {NUM_CALLERS} --min_qscore {MIN_QSCORE} --gpu_runners_per_device {GPU_RUNNERS_PER_DEVICE} --device $CUDA {BASECALLER_ADDITION}",')
    (68, "                         cond_ext = RESOURCE == \\'GPU\\')")
    (69, '')
    (70, "BASECALLER_THREADS = by_cond(cond = RESOURCE == \\'CPU\\',")
    (71, '                             yes = NUM_CALLERS*CPU_THREADS_PER_CALLER,')
    (72, '                             no = NUM_CALLERS,')
    (73, "                             cond_ext = RESOURCE == \\'GPU\\')")
    (74, '')
    (75, '')
    (76, '')
    (77, "KEEP_FAIL_READS = config[\\'RULE_GUPPY_BASECALLING\\'][\\'KEEP_FAIL_READS\\']")
    (78, '')
    (79, "FAST5_COMPRESSION = config[\\'RULE_GUPPY_BASECALLING\\'][\\'FAST5_COMPRESSION\\']")
    (80, '')
    (81, "KEEP_LOG_FILES = config[\\'RULE_GUPPY_BASECALLING\\'][\\'KEEP_LOG_FILES\\']")
    (82, '')
    (83, '##############################')
    (84, '## guppy_barcoder parameters')
    (85, "BARCODER_CONFIG = config[\\'RULE_GUPPY_DEMULTIPLEXING\\'][\\'CONFIG\\']")
    (86, "WORKER_THREADS = config[\\'RULE_GUPPY_DEMULTIPLEXING\\'][\\'WORKER_THREADS\\']")
    (87, "BARCODER_ADDITION = config[\\'RULE_GUPPY_DEMULTIPLEXING\\'][\\'ADDITION\\']")
    (88, '')
    (89, '####')
    (90, '')
    (91, "DEVICE = by_cond(RESOURCE == \\'CPU\\', \\'\\', f\\'--device $CUDA\\')")
    (92, '')
    (93, '')
    (94, '')
    (95, '##############################')
    (96, '## MinIONQC parameters')
    (97, '')
    (98, "#QSCORE_CUTOFF = config[\\'RULE_MINIONQC\\'][\\'QSCORE_CUTOFF\\']")
    (99, "MinIONQC_ADDITION = config[\\'RULE_MINIONQC\\'][\\'ADDITION\\']")
    (100, "PROCESSORS = config[\\'RULE_MINIONQC\\'][\\'PROCESSORS\\']")
    (101, 'fig = ["channel_summary", "flowcell_overview", "gb_per_channel_overview", "length_by_hour", "length_histogram", "length_vs_q", "q_by_hour", "q_histogram", "reads_per_hour", "yield_by_length", "yield_over_time"]')
    (102, '')
    (103, '')
    (104, '##############################')
    (105, '## deepbinner classify parameters')
    (106, '')
    (107, "PRESET = config[\\'RULE_DEEPBINNER_CLASSIFICATION\\'][\\'PRESET\\']")
    (108, "OMP_NUM_THREADS = config[\\'RULE_DEEPBINNER_CLASSIFICATION\\'][\\'OMP_NUM_THREADS\\']")
    (109, "DEEPBINNER_ADDITION = config[\\'RULE_DEEPBINNER_CLASSIFICATION\\'][\\'ADDITION\\']")
    (110, "API_THREADS = config[\\'RULE_MULTI_TO_SINGLE_FAST5\\'][\\'THREADS\\']")
    (111, '')
    (112, '')
    (113, '##############################')
    (114, '## get_reads_per_genome')
    (115, '')
    (116, "BARCODE_BY_GENOME = config[\\'RULE_GET_READS_PER_GENOME\\'][\\'BARCODE_BY_GENOME\\']")
    (117, '')
    (118, "## Would need a mecanism that checks that the content of \\'run\\' matches (is included in) the content of BARCODE_BY_GENOME$runID")
    (119, '')
    (120, '')
    (121, 'if not BARCODE_BY_GENOME:')
    (122, '\\tgenome = []')
    (123, '\\tget_demultiplexer = []')
    (124, 'else :')
    (125, '\\tgenome = pd.read_csv(BARCODE_BY_GENOME, sep = "\\\\t", usecols = ["Genome_ID"]).squeeze(\\\'columns\\\').unique()')
    (126, '\\tget_demultiplexer = pd.read_csv(BARCODE_BY_GENOME, sep = "\\\\t", usecols = ["Demultiplexer"]).squeeze(\\\'columns\\\').unique()')
    (127, '')
    (128, 'GET_READS_PER_GENOME_OUTPUT = [directory(expand(os.path.join(outdir, "reads_per_genome/fast5/{genome}"), genome = genome)), expand(os.path.join(outdir, "reads_per_genome/fastq/{genome}.fastq.gz"), genome = genome)]')
    (129, '')
    (130, '')
    (131, '')
    (132, '# READS_FILTERING = sorted(config["READS_FILTERING"], reverse = True)')
    (133, 'READS_FILTERING = config["READS_FILTERING"]')
    (134, '')
    (135, '')
    (136, 'if len(READS_FILTERING) == 0:')
    (137, '\\tfiltered = ""')
    (138, 'else:')
    (139, '\\tfiltered = []')
    (140, '\\tn_filtlong = READS_FILTERING.copy()')
    (141, '\\tif "porechop" in READS_FILTERING:')
    (142, '\\t\\tn_filtlong.remove("porechop")')
    (143, '\\t\\tif len(n_filtlong) > 0:')
    (144, '\\t\\t\\tfor i in n_filtlong:')
    (145, '\\t\\t\\t\\tif re.match("filtlong.+", i):')
    (146, '\\t\\t\\t\\t\\tfiltered.append("_".join(["_porechop", i]))')
    (147, '\\t\\telse:')
    (148, '\\t\\t\\tfiltered.append("_porechop")')
    (149, '\\telse:')
    (150, '\\t\\tfor i in n_filtlong:')
    (151, '\\t\\t\\tif re.match("filtlong.+", i):')
    (152, '\\t\\t\\t\\tpwd\\\\')
    (153, '                    .append("_" + i)')
    (154, '')
    (155, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vibaotram/baseDmux, file=baseDmux/data/Snakefile
context_key: ['if nasID and not os.path.isdir(ssh_dir)', 'rule finish', 'rule guppy_basecalling']
    (33, "def by_cond(cond, yes, no, cond_ext = \\'\\', no_ext = \\'\\'): # it\\'s working but needs to be improved ...")
    (34, '\\tif not cond_ext:')
    (35, '\\t\\tcond_ext = not cond')
    (36, '\\tif cond:')
    (37, '\\t\\treturn yes')
    (38, '\\telif cond_ext:')
    (39, '\\t\\treturn no')
    (40, '\\telse:')
    (41, '\\t\\treturn no_ext')
    (42, '')
    (43, '##############################')
    (44, '## guppy_basecaller parameters')
    (45, "RESOURCE = config[\\'RESOURCE\\']")
    (46, '')
    (47, "KIT = config[\\'KIT\\']")
    (48, '')
    (49, "FLOWCELL = config[\\'FLOWCELL\\']")
    (50, '')
    (51, "# QSCORE_FILTERING = config[\\'BASECALLER\\'][\\'QSCORE_FILTERING\\']")
    (52, "MIN_QSCORE = config[\\'RULE_GUPPY_BASECALLING\\'][\\'MIN_QSCORE\\']")
    (53, '')
    (54, "CPU_THREADS_PER_CALLER = config[\\'RULE_GUPPY_BASECALLING\\'][\\'CPU_THREADS_PER_CALLER\\']")
    (55, "NUM_CALLERS = config[\\'RULE_GUPPY_BASECALLING\\'][\\'NUM_CALLERS\\']")
    (56, '')
    (57, "BASECALLER_ADDITION = config[\\'RULE_GUPPY_BASECALLING\\'][\\'ADDITION\\']")
    (58, 'CONFIG = by_cond("--config" in BASECALLER_ADDITION, \\\'\\\', f"--flowcell {FLOWCELL} --kit {KIT}")')
    (59, "# CUDA = config[\\'BASECALLER\\'][\\'CUDA\\']")
    (60, '')
    (61, "GPU_RUNNERS_PER_DEVICE = config[\\'RULE_GUPPY_BASECALLING\\'][\\'GPU_RUNNERS_PER_DEVICE\\']")
    (62, "NUM_GPUS = config[\\'NUM_GPUS\\']")
    (63, '')
    (64, '# adjust guppy_basecaller parameters based on RESOURCE')
    (65, "BASECALLER_OPT = by_cond(cond = RESOURCE == \\'CPU\\',")
    (66, '                         yes = f"{CONFIG} --num_callers {NUM_CALLERS} --cpu_threads_per_caller {CPU_THREADS_PER_CALLER} --min_qscore {MIN_QSCORE} {BASECALLER_ADDITION}",')
    (67, '                         no = f"{CONFIG} --num_callers {NUM_CALLERS} --min_qscore {MIN_QSCORE} --gpu_runners_per_device {GPU_RUNNERS_PER_DEVICE} --device $CUDA {BASECALLER_ADDITION}",')
    (68, "                         cond_ext = RESOURCE == \\'GPU\\')")
    (69, '')
    (70, "BASECALLER_THREADS = by_cond(cond = RESOURCE == \\'CPU\\',")
    (71, '                             yes = NUM_CALLERS*CPU_THREADS_PER_CALLER,')
    (72, '                             no = NUM_CALLERS,')
    (73, "                             cond_ext = RESOURCE == \\'GPU\\')")
    (74, '')
    (75, '')
    (76, '')
    (77, "KEEP_FAIL_READS = config[\\'RULE_GUPPY_BASECALLING\\'][\\'KEEP_FAIL_READS\\']")
    (78, '')
    (79, "FAST5_COMPRESSION = config[\\'RULE_GUPPY_BASECALLING\\'][\\'FAST5_COMPRESSION\\']")
    (80, '')
    (81, "KEEP_LOG_FILES = config[\\'RULE_GUPPY_BASECALLING\\'][\\'KEEP_LOG_FILES\\']")
    (82, '')
    (83, '##############################')
    (84, '## guppy_barcoder parameters')
    (85, "BARCODER_CONFIG = config[\\'RULE_GUPPY_DEMULTIPLEXING\\'][\\'CONFIG\\']")
    (86, "WORKER_THREADS = config[\\'RULE_GUPPY_DEMULTIPLEXING\\'][\\'WORKER_THREADS\\']")
    (87, "BARCODER_ADDITION = config[\\'RULE_GUPPY_DEMULTIPLEXING\\'][\\'ADDITION\\']")
    (88, '')
    (89, '####')
    (90, '')
    (91, "DEVICE = by_cond(RESOURCE == \\'CPU\\', \\'\\', f\\'--device $CUDA\\')")
    (92, '')
    (93, '')
    (94, '')
    (95, '##############################')
    (96, '## MinIONQC parameters')
    (97, '')
    (98, "#QSCORE_CUTOFF = config[\\'RULE_MINIONQC\\'][\\'QSCORE_CUTOFF\\']")
    (99, "MinIONQC_ADDITION = config[\\'RULE_MINIONQC\\'][\\'ADDITION\\']")
    (100, "PROCESSORS = config[\\'RULE_MINIONQC\\'][\\'PROCESSORS\\']")
    (101, 'fig = ["channel_summary", "flowcell_overview", "gb_per_channel_overview", "length_by_hour", "length_histogram", "length_vs_q", "q_by_hour", "q_histogram", "reads_per_hour", "yield_by_length", "yield_over_time"]')
    (102, '')
    (103, '')
    (104, '##############################')
    (105, '## deepbinner classify parameters')
    (106, '')
    (107, "PRESET = config[\\'RULE_DEEPBINNER_CLASSIFICATION\\'][\\'PRESET\\']")
    (108, "OMP_NUM_THREADS = config[\\'RULE_DEEPBINNER_CLASSIFICATION\\'][\\'OMP_NUM_THREADS\\']")
    (109, "DEEPBINNER_ADDITION = config[\\'RULE_DEEPBINNER_CLASSIFICATION\\'][\\'ADDITION\\']")
    (110, "API_THREADS = config[\\'RULE_MULTI_TO_SINGLE_FAST5\\'][\\'THREADS\\']")
    (111, '')
    (112, '')
    (113, '##############################')
    (114, '## get_reads_per_genome')
    (115, '')
    (116, "BARCODE_BY_GENOME = config[\\'RULE_GET_READS_PER_GENOME\\'][\\'BARCODE_BY_GENOME\\']")
    (117, '')
    (118, "## Would need a mecanism that checks that the content of \\'run\\' matches (is included in) the content of BARCODE_BY_GENOME$runID")
    (119, '')
    (120, '')
    (121, 'if not BARCODE_BY_GENOME:')
    (122, '\\tgenome = []')
    (123, '\\tget_demultiplexer = []')
    (124, 'else :')
    (125, '\\tgenome = pd.read_csv(BARCODE_BY_GENOME, sep = "\\\\t", usecols = ["Genome_ID"]).squeeze(\\\'columns\\\').unique()')
    (126, '\\tget_demultiplexer = pd.read_csv(BARCODE_BY_GENOME, sep = "\\\\t", usecols = ["Demultiplexer"]).squeeze(\\\'columns\\\').unique()')
    (127, '')
    (128, 'GET_READS_PER_GENOME_OUTPUT = [directory(expand(os.path.join(outdir, "reads_per_genome/fast5/{genome}"), genome = genome)), expand(os.path.join(outdir, "reads_per_genome/fastq/{genome}.fastq.gz"), genome = genome)]')
    (129, '')
    (130, '')
    (131, '')
    (132, '# READS_FILTERING = sorted(config["READS_FILTERING"], reverse = True)')
    (133, 'READS_FILTERING = config["READS_FILTERING"]')
    (134, '')
    (135, '')
    (136, 'if len(READS_FILTERING) == 0:')
    (137, '\\tfiltered = ""')
    (138, 'else:')
    (139, '\\tfiltered = []')
    (140, '\\tn_filtlong = READS_FILTERING.copy()')
    (141, '\\tif "porechop" in READS_FILTERING:')
    (142, '\\t\\tn_filtlong.remove("porechop")')
    (143, '\\t\\tif len(n_filtlong) > 0:')
    (144, '\\t\\t\\tfor i in n_filtlong:')
    (145, '\\t\\t\\t\\tif re.match("filtlong.+", i):')
    (146, '\\t\\t\\t\\t\\tfiltered.append("_".join(["_porechop", i]))')
    (147, '\\t\\telse:')
    (148, '\\t\\t\\tfiltered.append("_porechop")')
    (149, '\\telse:')
    (150, '\\t\\tfor i in n_filtlong:')
    (151, '\\t\\t\\tif re.match("filtlong.+", i):')
    (152, '\\t\\t\\t\\tpwd\\\\')
    (153, '                    .append("_" + i)')
    (154, '')
    (155, '')
    (156, 'PORECHOP_PARAMS = config["porechop"]["PARAMS"]')
    (157, '# FILTLONG_PARAMS = config["RULE_FILTLONG"]["PARAMS"]')
    (158, '')
    (159, '##############################')
    (160, '## reports')
    (161, "DEMULTIPLEX_REPORT = config[\\'REPORTS\\'][\\'DEMULTIPLEX_REPORT\\']")
    (162, '')
    (163, '##############################')
    (164, '## use different containers for guppy and deepbinner depending on resources')
    (165, "simg_container = load_configfile(os.path.abspath(\\'data/singularity.yaml\\'))")
    (166, "guppy_cpu_container = simg_container[\\'guppy_conda_api_CPU\\']")
    (167, "guppy_gpu_container = simg_container[\\'guppy_conda_api_GPU\\']")
    (168, '')
    (169, "guppy_container = by_cond(RESOURCE == \\'CPU\\', guppy_cpu_container, guppy_gpu_container, cond_ext = RESOURCE == \\'GPU\\')")
    (170, '')
    (171, "deepbinner_container = simg_container[\\'deepbinner_api\\']")
    (172, '')
    (173, '')
    (174, "FAST5_PER_GENOME = config[\\'RULE_GET_READS_PER_GENOME\\'][\\'GET_FAST5\\']")
    (175, '')
    (176, 'READS_PER_GENOME = [by_cond(FAST5_PER_GENOME, directory(expand(os.path.join(outdir, "reads_per_genome/fast5/{genome}"), genome = genome)), ()), expand(os.path.join(outdir, "reads_per_genome/fastq/{genome}.fastq.gz"), genome = genome)]')
    (177, '')
    (178, '')
    (179, '##############################')
    (180, '## path to scripts')
    (181, '')
    (182, "CHOOSE_AVAIL_GPU = os.path.abspath(\\'data/script/choose_avail_gpu.py\\')")
    (183, "FAST5_SUBSET = os.path.abspath(\\'data/script/fast5_subset.py\\')")
    (184, "GET_FASTQ_PER_BARCODE = os.path.abspath(\\'data/script/get_fastq_per_barcode.py\\')")
    (185, "GET_SUMMARY_PER_BARCODE = os.path.abspath(\\'data/script/get_summary_per_barcode.R\\')")
    (186, "RENAME_FASTQ_GUPPY_BARCODER = os.path.abspath(\\'data/script/rename_fastq_guppy_barcoder.R\\')")
    (187, "DEMULTIPLEX_REPORT_RMD = os.path.abspath(\\'data/report/report_demultiplex.Rmd\\')")
    (188, "GET_READS_PER_GENOME = os.path.abspath(\\'data/script/get_fastq_per_genome.R\\')")
    (189, '')
    (190, '##############################')
    (191, '## cluster variables')
    (192, '')
    (193, 'user = getpass.getuser()')
    (194, "nasID = config[\\'NASID\\']")
    (195, "HOST_PREFIX = by_cond(nasID, user + \\'@\\' + nasID + \\':\\', \\'\\')")
    (196, '')
    (197, '##############################')
    (198, '## make slurm logs directory')
    (199, 'CLUSTER_LOG = os.path.join(outdir, "log/cluster")')
    (200, 'os.makedirs(CLUSTER_LOG, exist_ok=True)')
    (201, '')
    (202, 'SNAKEMAKE_LOG = os.path.join(outdir, "log/snakemake")')
    (203, 'os.makedirs(SNAKEMAKE_LOG, exist_ok=True)')
    (204, '')
    (205, '##############################')
    (206, '## slurm ssh')
    (207, 'ssh_dir = os.path.join(os.getcwd(), ".ssh")')
    (208, 'if nasID and not os.path.isdir(ssh_dir):')
    (209, "\\tuser_ssh = os.path.join(\\'/home\\', user, \\'.ssh\\')")
    (210, "\\tcp_ssh = \\' \\'.join((\\'rsync -avrP\\', user_ssh, os.getcwd()))")
    (211, '\\tos.system(cp_ssh)')
    (212, '')
    (213, '')
    (214, '')
    (215, '# shell.prefix("exec > >(tee "{SNAKEMAKE_LOG}/{log}") 2>&1; ")')
    (216, '')
    (217, '##############################')
    (218, '##############################')
    (219, '')
    (220, 'ruleorder: filtlong > porechop')
    (221, '')
    (222, 'rule finish:')
    (223, '\\tinput:')
    (224, '\\t\\texpand(os.path.join(outdir, "demultiplex/{demultiplexer}/{run}/multiqc/multiqc_report.html"), demultiplexer = demultiplexer, run = run), # DEMULTIPLEXING QC')
    (225, '\\t\\t# expand(os.path.join(outdir, "demultiplex/{demultiplexer}/{run}/fast5_per_barcode.done"), demultiplexer = demultiplexer, run = run),')
    (226, '\\t\\tos.path.join(outdir, "basecall/multiqc/multiqc_report.html"), # BASECALLING QC')
    (227, '\\t\\tby_cond(DEMULTIPLEX_REPORT and len(demultiplexer) > 0, os.path.join(outdir, "report/demultiplex_report.html"), ()),')
    (228, '\\t\\tby_cond(FAST5_PER_GENOME, expand(os.path.join(outdir, "reads_per_genome/fast5/{genome}"), genome = genome), ()),')
    (229, '\\t\\texpand(os.path.join(outdir, "reads_per_genome/fastq{filtered}/{genome}{filtered}.fastq.gz"), genome = genome, filtered = filtered),')
    (230, '')
    (231, '')
    (232, '##############################')
    (233, '################## BASECALLING')
    (234, '##################### BY GUPPY')
    (235, '')
    (236, '')
    (237, 'rule guppy_basecalling:')
    (238, '\\tmessage: "GUPPY basecalling running on {RESOURCE}"')
    (239, '\\tinput: os.path.join(indir, "{run}/fast5")')
    (240, '\\toutput:')
    (241, '\\t\\tsummary = os.path.join(outdir, "basecall/{run}/sequencing_summary.txt"),')
    (242, '\\t\\tpassed_summary = os.path.join(outdir, "basecall/{run}/passed_sequencing_summary.txt"),')
    (243, '\\t\\t# fastq = temp(os.path.join(outdir, "basecall/{run}/{run}.fastq")),')
    (244, '\\t\\tfastq = temp(directory(os.path.join(outdir, "basecall/{run}/pass"))),')
    (245, '\\t\\tcompressed_fastq = os.path.join(outdir, "basecall/{run}/{run}.fastq.gz"),')
    (246, '\\t\\tfast5 = temp(directory(os.path.join(outdir, "basecall/{run}/passed_fast5"))),')
    (247, '\\t\\t# fast5 = directory(os.path.join(outdir, "basecall/{run}/passed_fast5")),')
    (248, '\\t\\tfail = by_cond(cond = KEEP_FAIL_READS, yes = directory(os.path.join(outdir, "basecall/{run}/fail")), no = ())')
    (249, '\\tparams:')
    (250, '\\t\\tsummary = lambda wildcards, output: os.path.basename(output.summary),')
    (251, '\\t\\tpassed_summary = lambda wildcards, output: os.path.basename(output.passed_summary),')
    (252, '\\t\\toutpath = os.path.join(outdir, "basecall/{run}"),')
    (253, '\\t\\tcompressed_fastq = lambda wildcards, output: os.path.basename(output.compressed_fastq),')
    (254, '\\t\\tcompression = FAST5_COMPRESSION,')
    (255, "\\t\\tkeep_log_files = by_cond(KEEP_LOG_FILES, \\'true\\', \\'false\\'),")
    (256, '\\t\\t# fast5_name = "{run}_",')
    (257, '\\t\\tlog = "guppy_basecalling_{run}.log"')
    (258, '\\t# resources:')
    (259, '\\t# \\tngpudevices = NUM_GPUS')
    (260, '\\tthreads: BASECALLER_THREADS')
    (261, '\\tsingularity: guppy_container')
    (262, "\\tconda: \\'conda/conda_minionqc.yaml\\'")
    (263, '\\tshell:')
    (264, '\\t\\t"""')
    (265, '\\t\\texec > >(tee "{SNAKEMAKE_LOG}/{params.log}") 2>&1')
    (266, '\\t\\tif [ -f {params.outpath} ]; then')
    (267, '            rm -R {params.outpath}/*')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MuhammedHasan/gnomad_rocksdb, file=workflow/Snakefile
context_key: ["if version.split(\\'.\\')[0] == \\'2\\'"]
    (35, 'def vcfs(wildcards):')
    (36, "    version = wildcards[\\'version\\']")
    (37, "    chroms = config[\\'chroms\\'].copy()")
    (38, '')
    (39, "    if version.split(\\'.\\')[0] == \\'2\\':")
    (40, "        chroms.remove(\\'Y\\')")
    (41, '')
    (42, "    return expand(config[\\'vcf\\'], version=wildcards[\\'version\\'], chrom=chroms)")
    (43, '')
    (44, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbg-ethz/V-pipe, file=resources/auxiliary_workflows/benchmark/workflow/Snakefile
context_key: ['if not line.startswith(conda_dep_prefix)']
    (64, 'def get_generated_conda_env(wildcards, input):')
    (65, '    # retrieve conda dependencies from script')
    (66, '    conda_dep_prefix = "# CONDA:"')
    (67, '')
    (68, '    conda_dep_list = []')
    (69, '    with open(input.script) as fd:')
    (70, '        for line in fd.readlines():')
    (71, '            if not line.startswith(conda_dep_prefix):')
    (72, '                continue')
    (73, '')
    (74, '            conda_dep_list.append(line[len(conda_dep_prefix) :].strip())')
    (75, '')
    (76, '    # retrieve pip dependencies from script')
    (77, '    pip_dep_prefix = "# PIP:"')
    (78, '')
    (79, '    pip_dep_list = []')
    (80, '    with open(input.script) as fd:')
    (81, '        for line in fd.readlines():')
    (82, '            if not line.startswith(pip_dep_prefix):')
    (83, '                continue')
    (84, '')
    (85, '            pip_dep_list.append(line[len(pip_dep_prefix) :].strip())')
    (86, '')
    (87, '    # format conda env file')
    (88, '    conda_env = """channels:')
    (89, '  - hcc')
    (90, '  - broad-viral')
    (91, '  - conda-forge')
    (92, '  - bioconda')
    (93, '  - defaults')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbg-ethz/V-pipe, file=resources/auxiliary_workflows/benchmark/workflow/Snakefile
context_key: ['if len(conda_dep_list) == 0 and len(pip_dep_list) == 0']
    (64, 'def get_generated_conda_env(wildcards, input):')
    (65, '    # retrieve conda dependencies from script')
    (66, '    conda_dep_prefix = "# CONDA:"')
    (67, '')
    (68, '    conda_dep_list = []')
    (69, '    with open(input.script) as fd:')
    (70, '        for line in fd.readlines():')
    (71, '            if not line.startswith(conda_dep_prefix):')
    (72, '                continue')
    (73, '')
    (74, '            conda_dep_list.append(line[len(conda_dep_prefix) :].strip())')
    (75, '')
    (76, '    # retrieve pip dependencies from script')
    (77, '    pip_dep_prefix = "# PIP:"')
    (78, '')
    (79, '    pip_dep_list = []')
    (80, '    with open(input.script) as fd:')
    (81, '        for line in fd.readlines():')
    (82, '            if not line.startswith(pip_dep_prefix):')
    (83, '                continue')
    (84, '')
    (85, '            pip_dep_list.append(line[len(pip_dep_prefix) :].strip())')
    (86, '')
    (87, '    # format conda env file')
    (88, '    conda_env = """channels:')
    (89, '  - hcc')
    (90, '  - broad-viral')
    (91, '  - conda-forge')
    (92, '  - bioconda')
    (93, '  - defaults')
    (94, 'dependencies:"""')
    (95, '')
    (96, '    if len(conda_dep_list) == 0 and len(pip_dep_list) == 0:')
    (97, '        conda_env += " []"')
    (98, '    else:')
    (99, '        if len(conda_dep_list) > 0:')
    (100, '            conda_env += "\\')
    (101, '  - " + "\\')
    (102, '  - ".join(conda_dep_list)')
    (103, '')
    (104, '        if len(pip_dep_list) > 0:')
    (105, '            conda_env += "\\')
    (106, '  - python=3.9\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlexanderLabWHOI/EUKHeist, file=Snakefile
context_key: ['if FORWARD']
    (40, 'def identify_read_groups(assembly_group_name, STUDY, FORWARD=True):')
    (41, '    outlist=[] ')
    (42, "    ERR_list = METAG_SAMPLELIST.loc[assembly_group_name, \\'ERR_list\\'].split(\\', \\')")
    (43, '    if FORWARD: ')
    (44, '        num = 1')
    (45, '    else: ')
    (46, '        num = 2')
    (47, '    for E in ERR_list: ')
    (48, '        outlist.append(SCRATCHDIR + "/trimmed/{}/{}_{}.trimmed.fastq.gz".format(STUDY,E, num)) ')
    (49, '    return(outlist)')
    (50, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tjbencomo/ngs-pipeline, file=rules/common.smk
context_key: ['if use_pon']
    (125, 'def get_mutect2_input(wildcards):')
    (126, '    files = {}')
    (127, '    files[\\\'tumor\\\'] = f"bams/{wildcards.patient}.tumor.bam"')
    (128, '    if use_pon:')
    (129, "        files[\\'pon\\'] = pon_vcf")
    (130, '    if not tumor_only:')
    (131, '        files[\\\'normal\\\'] = f"bams/{wildcards.patient}.normal.bam"')
    (132, '    return files')
    (133, '    # if use_pon:')
    (134, '    #     return {')
    (135, '    #         \\\'normal\\\' : f"bams/{wildcards.patient}.normal.bam",')
    (136, '    #         \\\'tumor\\\' : f"bams/{wildcards.patient}.tumor.bam",')
    (137, "    #         \\'pon\\' : pon_vcf")
    (138, '    #     }')
    (139, '    # else:')
    (140, '    #     return {')
    (141, '    #             \\\'normal\\\' : f"bams/{wildcards.patient}.normal.bam",')
    (142, '    #             \\\'tumor\\\' : f"bams/{wildcards.patient}.tumor.bam",')
    (143, '    #     }')
    (144, '')
    (145, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tjbencomo/ngs-pipeline, file=rules/common.smk
context_key: ["if alternate_isoforms == \\'\\'"]
    (150, 'def get_vcf2maf_input(wildcards):')
    (151, "    if alternate_isoforms == \\'\\':")
    (152, '        return {')
    (153, '            \\\'vcf\\\' : f"vcfs/{wildcards.patient}.vcf",')
    (154, "            \\'fasta\\' : ref_fasta,")
    (155, "            \\'vep_dir\\' : vep_dir")
    (156, '        }')
    (157, '    else:')
    (158, '        return {')
    (159, '            \\\'vcf\\\' : f"vcfs/{wildcards.patient}.vcf",')
    (160, "            \\'fasta\\' : ref_fasta,")
    (161, "            \\'vep_dir\\' : vep_dir,")
    (162, "            \\'alt_isoforms\\' : alternate_isoforms")
    (163, '        }')
    (164, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tjbencomo/ngs-pipeline, file=rules/common.smk
context_key: ['if not tumor_only']
    (165, 'def get_contamination_input(wildcards):')
    (166, '    out = {}')
    (167, "    out[\\'tumor\\'] = f\\'qc/{wildcards.patient}_tumor_pileupsummaries.table\\'")
    (168, '    if not tumor_only:')
    (169, "        out[\\'normal\\'] = f\\'qc/{wildcards.patient}_normal_pileupsummaries.table\\'")
    (170, '    return out')
    (171, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tjbencomo/ngs-pipeline, file=rules/common.smk
context_key: ['if seqtype == "WES"']
    (172, 'def get_coverage_input(wildcards):')
    (173, '    files = {}')
    (174, '    files[\\\'bam\\\'] = f"bams/{wildcards.patient}.{wildcards.sample_type}.bam"')
    (175, '    if seqtype == "WES":')
    (176, "        files[\\'regions\\'] = regions_bed")
    (177, '    return files')
    (178, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kevinrue/snakemake_alevin_10x, file=workflow/rules/common.smk
context_key: ['if len(fastq1) == 0']
    (18, 'def get_gex_fastq(wildcards):')
    (19, "    \\'\\'\\'")
    (20, '    Identify pairs of FASTQ files from the sample sheet.')
    (21, '    ')
    (22, '    wildcards')
    (23, '    - sample: name of the sample to process.')
    (24, "    \\'\\'\\'")
    (25, '    fastq1_dir = samples["fastqs"][wildcards.sample]')
    (26, '    fastq1_pattern = config["pattern"]["read1"]')
    (27, '    fastq1_glob = f"{fastq1_dir}/*{fastq1_pattern}*"')
    (28, '    fastq1 = glob.glob(fastq1_glob)')
    (29, '    ')
    (30, '    if len(fastq1) == 0:')
    (31, '        raise OSError(f"No file matched pattern: {fastq1_glob}")')
    (32, '    ')
    (33, '    fastq2 = [file.replace(config["pattern"]["read1"], config["pattern"]["read2"]) for file in fastq1]')
    (34, '    for file in fastq2:')
    (35, '        if not os.path.exists(file):')
    (36, '            raise OSError(f"Paired file not found: {file}")')
    (37, '    ')
    (38, "    return {\\'fastq1\\' : fastq1, \\'fastq2\\' : fastq2 }")
    (39, '')
    (40, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kevinrue/snakemake_alevin_10x, file=workflow/rules/common.smk
context_key: ['if expect_cells > 0']
    (41, 'def get_cells_option(wildcards):')
    (42, "    \\'\\'\\'")
    (43, '    Build an string of command line options from the sample sheet.')
    (44, '    ')
    (45, '    wildcards')
    (46, '    - sample: name of the sample to process.')
    (47, '    ')
    (48, "    Note that users should supply only one of \\'expect_cells\\' or \\'force_cells\\'.")
    (49, '    The other one should be set to 0, to be ignored.')
    (50, "    \\'\\'\\'")
    (51, '    option_str = ""')
    (52, '    ')
    (53, "    expect_cells = samples[\\'expect_cells\\'][wildcards.sample]")
    (54, "    force_cells = samples[\\'force_cells\\'][wildcards.sample]")
    (55, '')
    (56, '    if expect_cells > 0:')
    (57, '        option_str += f" --expectCells {expect_cells}"')
    (58, '    ')
    (59, '    if force_cells > 0:')
    (60, '        option_str += f" --forceCells {force_cells}"')
    (61, '    ')
    (62, '    return option_str')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=thomasbtf/OSD, file=workflow/rules/common.smk
context_key: ['if filename.endswith(".fastq.gz"']
    (13, 'def get_merged_metagenomes():')
    (14, '    merged_metagenomes = os.listdir(')
    (15, '        "resources/workable-OSD-2014/workable/metagenomes/merged"')
    (16, '    )')
    (17, '    merged_metagenomes = [')
    (18, '        filename.replace(".fastq.gz", "")')
    (19, '        for filename in merged_metagenomes')
    (20, '        if filename.endswith(".fastq.gz")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=singleron-RD/Dna-seq_Analysis, file=workflow/rules/common.smk
context_key: ['if return_info > 0']
    (50, 'def check_return_info(return_info):')
    (51, '    """')
    (52, '    check fun return')
    (53, '    """')
    (54, '    if return_info > 0:')
    (55, '        return True')
    (56, '    else:')
    (57, '        return False')
    (58, '    ')
    (59, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=singleron-RD/Dna-seq_Analysis, file=workflow/rules/common.smk
context_key: ["if mod == \\'fq1\\'"]
    (107, '    def adapter_list(self,sample_name,fq1,fq2,fq_outdir,mod,adapter_dict_sorted,index_len):')
    (108, '        global I')
    (109, "        if mod == \\'fq1\\':")
    (110, '            fq=fq1')
    (111, "        if mod == \\'fq2\\':")
    (112, '            fq=fq2')
    (113, '')
    (114, '        frist_adapter = list(adapter_dict_sorted.items())[0][0]')
    (115, '        name_list = xopen(f"{fq_outdir}/{sample_name}_{frist_adapter}.lst",\\\'w\\\')')
    (116, '')
    (117, '        fh =  pysam.FastxFile(fq)')
    (118, '        for record in fh:')
    (119, '            header, seq, = record.name, record.sequence')
    (120, '            if hm_dis(seq[:9],adapter_dict_sorted[frist_adapter]) > 1:')
    (121, '                continue')
    (122, "            name_list.write(f\\'{header}\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=singleron-RD/Dna-seq_Analysis, file=workflow/rules/common.smk
context_key: ['if I < index_len']
    (107, '    def adapter_list(self,sample_name,fq1,fq2,fq_outdir,mod,adapter_dict_sorted,index_len):')
    (108, '        global I')
    (109, "        if mod == \\'fq1\\':")
    (110, '            fq=fq1')
    (111, "        if mod == \\'fq2\\':")
    (112, '            fq=fq2')
    (113, '')
    (114, '        frist_adapter = list(adapter_dict_sorted.items())[0][0]')
    (115, '        name_list = xopen(f"{fq_outdir}/{sample_name}_{frist_adapter}.lst",\\\'w\\\')')
    (116, '')
    (117, '        fh =  pysam.FastxFile(fq)')
    (118, '        for record in fh:')
    (119, '            header, seq, = record.name, record.sequence')
    (120, '            if hm_dis(seq[:9],adapter_dict_sorted[frist_adapter]) > 1:')
    (121, '                continue')
    (122, "            name_list.write(f\\'{header}\\")
    (123, "\\')")
    (124, '        name_list.close()')
    (125, '        fh.close()')
    (126, '')
    (127, "        out_fq_R1 = f\\'{fq_outdir}/{sample_name}_{frist_adapter}_R1.fastq\\'")
    (128, "        out_fq_R2 = f\\'{fq_outdir}/{sample_name}_{frist_adapter}_R2.fastq\\'")
    (129, '')
    (130, '        cmd_line = (')
    (131, "                    f\\'seqtk subseq {fq1} {fq_outdir}/{sample_name}_{frist_adapter}.lst > {out_fq_R1};\\'")
    (132, "                    f\\'seqtk subseq {fq2} {fq_outdir}/{sample_name}_{frist_adapter}.lst > {out_fq_R2}\\'")
    (133, '                    )')
    (134, '       ')
    (135, '        subprocess.check_call(cmd_line,shell=True)')
    (136, "        print(f\\'the {I+1} finished\\')")
    (137, '        ')
    (138, '        if I < index_len:')
    (139, '            adapter_dict_sorted.popitem(last=False)')
    (140, '            I += 1')
    (141, '            i = index_len - I ')
    (142, '            return i')
    (143, '')
    (144, '###### Config file and sample sheets #####')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=singleron-RD/Dna-seq_Analysis, file=workflow/rules/common.smk
context_key: ['if len(fastqs) == 2']
    (192, 'def get_fastq(wildcards):')
    (193, '    """Get fastq files of given sample-unit."""')
    (194, '    fastqs = units.loc[(wildcards.sample, wildcards.unit), ["fq1", "fq2"]].dropna()')
    (195, '    if len(fastqs) == 2:')
    (196, '        return {"r1": fastqs.fq1, "r2": fastqs.fq2}')
    (197, '    return {"r1": fastqs.fq1}')
    (198, '')
    (199, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=singleron-RD/Dna-seq_Analysis, file=workflow/rules/common.smk
context_key: ['if not is_single_end(**wildcards)']
    (213, 'def get_trimmed_reads(wildcards):')
    (214, '    """Get trimmed reads of given sample-unit."""')
    (215, '    if not is_single_end(**wildcards):')
    (216, '        # paired-end sample')
    (217, '        return expand(')
    (218, '            "results/trimmed/{sample}-{unit}.{group}.fastq.gz",')
    (219, '            group=[1, 2],')
    (220, '            **wildcards')
    (221, '        )')
    (222, '    # single end sample')
    (223, '    return "results/trimmed/{sample}-{unit}.fastq.gz".format(**wildcards)')
    (224, '')
    (225, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=singleron-RD/Dna-seq_Analysis, file=workflow/rules/common.smk
context_key: ['if regions', 'if padding']
    (235, 'def get_regions_param(regions=config["processing"].get("restrict-regions"), default=""):')
    (236, '    if regions:')
    (237, '        params = "--intervals \\\'{}\\\' ".format(regions)')
    (238, '        padding = config["processing"].get("region-padding")')
    (239, '        if padding:')
    (240, '            params += "--interval-padding {}".format(padding)')
    (241, '        return params')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=singleron-RD/Dna-seq_Analysis, file=workflow/rules/common.smk
context_key: ['if regions']
    (235, 'def get_regions_param(regions=config["processing"].get("restrict-regions"), default=""):')
    (236, '    if regions:')
    (237, '        params = "--intervals \\\'{}\\\' ".format(regions)')
    (238, '        padding = config["processing"].get("region-padding")')
    (239, '        if padding:')
    (240, '            params += "--interval-padding {}".format(padding)')
    (241, '        return params')
    (242, '    return default')
    (243, '')
    (244, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=eriqande/mega-simple-microhap-snakeflow, file=workflow/rules/common.smk
context_key: ['if not config["use_trimmomatic"]']
    (59, 'def fq1_or_trim1_from_sample_and_run(wildcards):')
    (60, '    """Get path to a sample\\\'s read1 fastq file"""')
    (61, '    if not config["use_trimmomatic"]:')
    (62, '        return r"{run_dir}/raw/{fq}".format(')
    (63, '            run_dir=wildcards.run_dir,')
    (64, '            fq=samples.loc[wildcards.sample, "fq1"]')
    (65, '        )')
    (66, '    else:')
    (67, '        return r"{run_dir}/{species_dir}/trimmomatic/{sm}.1.fastq.gz".format(')
    (68, '            run_dir=wildcards.run_dir,')
    (69, '            species_dir=config["species"],')
    (70, '            sm=wildcards.sample)')
    (71, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=eriqande/mega-simple-microhap-snakeflow, file=workflow/rules/common.smk
context_key: ['if not config["use_trimmomatic"]']
    (72, 'def fq2_or_trim2_from_sample_and_run(wildcards):')
    (73, '    """Get path to a sample\\\'s read1 fastq file"""')
    (74, '    if not config["use_trimmomatic"]:')
    (75, '        return r"{run_dir}/raw/{fq}".format(')
    (76, '            run_dir=wildcards.run_dir,')
    (77, '            fq=samples.loc[wildcards.sample, "fq2"]')
    (78, '        )')
    (79, '    else:')
    (80, '        return r"{run_dir}/{species_dir}/trimmomatic/{sm}.2.fastq.gz".format(')
    (81, '            run_dir=wildcards.run_dir,')
    (82, '            species_dir=config["species"],')
    (83, '            sm=wildcards.sample)')
    (84, '')
    (85, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imcsegmentation, file=workflow/Snakefile
context_key: ['if _sample_dict is None']
    (363, 'def get_sample_dict():')
    (364, '    global _sample_dict')
    (365, '    if _sample_dict is None:')
    (366, '        checkpoints.generate_sample_list.get()')
    (367, '        dat_samples = pd.read_csv(file_path_samples)')
    (368, '        _sample_dict = {pathlib.Path(x).stem: x for x in dat_samples[')
    (369, "            \\'mcd_folders\\']}")
    (370, '    return _sample_dict')
    (371, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=labsyspharm/marm2-supplement, file=Snakefile
context_key: ["if \\'EGFR\\' in options"]
    (35, 'def get_instances(wildcards, modifications=None, mutRAS=False):')
    (36, "    options = f\\'{wildcards.dataset}\\'.split(\\'_\\')")
    (37, "    if \\'EGFR\\' in options:")
    (38, "        options.remove(\\'EGFR\\')")
    (39, "    if mutRAS and \\'EGF\\' in options:")
    (40, "        options.remove(\\'EGF\\')")
    (41, '    if mutRAS:')
    (42, "        options.append(\\'NRAS\\')")
    (43, '    instances = []')
    (44, '    for r in range(len(options)+1):')
    (45, '        instances.extend(list(itertools.combinations(options, r)))')
    (46, '    instances = [')
    (47, '        "_".join(sorted(instance)) for instance in instances')
    (48, '    ]')
    (49, '    return [')
    (50, '        get_model_module_file_instance(wildcards.model,')
    (51, '                                       wildcards.variant,')
    (52, '                                       instance, modifications)')
    (53, '        for instance in instances')
    (54, '    ]')
    (55, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=osvaldoreisss/polya-seq_workflow_analysis, file=workflow/rules/common.smk
context_key: ['if len(fastqs) == 2']
    (20, 'def get_fastq(wildcards):')
    (21, '    fastqs = samples.loc[')
    (22, '        (wildcards.sample, int(wildcards.run)), ["fq1", "fq2"]')
    (23, '    ].dropna()')
    (24, '    if len(fastqs) == 2:')
    (25, '        return f"libs/{fastqs.fq1}", f"libs/{fastqs.fq2}"')
    (26, '    return f"libs/{fastqs.fq1}"')
    (27, '')
    (28, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AAFC-BICoE/snakemake-mothur, file=Snakefile
context_key: ['if os.path.isfile(filename)']
    (20, 'def parse_summary(wildcards):\\r')
    (21, '    filename = "{dataset}.summary.txt".format(dataset=dataset)\\r')
    (22, '    if os.path.isfile(filename):\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AAFC-BICoE/snakemake-mothur, file=Snakefile
context_key: ['if line.startswith("Median")']
    (20, 'def parse_summary(wildcards):\\r')
    (21, '    filename = "{dataset}.summary.txt".format(dataset=dataset)\\r')
    (22, '    if os.path.isfile(filename):\\r')
    (23, '        with open(filename) as f:\\r')
    (24, '            for line in f:\\r')
    (25, '                if line.startswith("Median"):\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/RiboReport, file=Snakefile
context_key: ['if hasRIBO', 'rule all', 'input']
    (23, 'def get_wigfiles(wildcards):')
    (24, '  method=samples["method"]')
    (25, '  condition=samples["condition"]')
    (26, '  replicate=samples["replicate"]')
    (27, '  wilds = zip(method, condition, replicate)')
    (28, '')
    (29, '  bigwigs = [["global", "centered", "fiveprime", "threeprime"], ["raw", "mil", "min"], ["forward", "reverse"], list(wilds)]')
    (30, '  bigwigs = list(iter.product(*bigwigs))')
    (31, '')
    (32, '  wigfiles = []')
    (33, '  for bw in bigwigs:')
    (34, '      wigfiles.append("%stracks/%s/%s-%s-%s.%s.%s.%s.bw" %(bw[0], bw[1], bw[3][0], bw[3][1], bw[3][2], bw[1], bw[2], bw[0]))')
    (35, '')
    (36, '  return wigfiles')
    (37, '')
    (38, 'if hasRIBO:')
    (39, '    rule all:')
    (40, '      input:')
    (41, '        expand("maplink/{method}-{condition}-{replicate}.bam.bai", zip, method=samples["method"], condition=samples["condition"], replicate=samples["replicate"]),')
    (42, '        expand("reparation/{condition}-{replicate}/Predicted_ORFs.txt", zip, condition=samples.loc[samples["method"] == "RIBO", "condition"], replicate=samples.loc[samples["method"] == "RIBO", "replicate"]),')
    (43, '        expand("ribotish/{condition}-newORFs.tsv_all.txt", zip, condition=samples.loc[samples["method"] == "RIBO", "condition"]),')
    (44, '        expand("deepribo/{condition}-{replicate}/predictions.csv", zip, condition=samples.loc[samples["method"] == "RIBO", "condition"], replicate=samples.loc[samples["method"] == "RIBO", "replicate"]),')
    (45, '        expand("price/{condition}-{replicate}/results.orfs.filtered.bed", zip, condition=samples.loc[samples["method"] == "RIBO", "condition"], replicate=samples.loc[samples["method"] == "RIBO", "replicate"]),')
    (46, '        "qc/multi/multiqc_report.html",')
    (47, '        get_wigfiles,')
    (48, '        "tracks/potentialStopCodons.gff",')
    (49, '        "tracks/potentialStartCodons.gff",')
    (50, '        "tracks/potentialAlternativeStartCodons.gff",')
    (51, '        "tracks/potentialRibosomeBindingSite.gff",')
    (52, '        "auxiliary/final_annotation.xlsx",')
    (53, '        "auxiliary/final_annotation.gff",')
    (54, '        "auxiliary/final_annotation_complete.gff",')
    (55, '        "tracks/predictions.gtf",')
    (56, '        "price/annotation_ensembl.oml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=gitbackspacer/rna-seq-kallisto-sleuth-test, file=workflow/rules/diffexp.smk
context_key: ['if wildcards.model == "all"']
    (21, 'def get_model(wildcards):')
    (22, '    if wildcards.model == "all":')
    (23, '        return {"full": None}')
    (24, '    return config["diffexp"]["models"][wildcards.model]')
    (25, '')
    (26, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/qc, file=workflow/rules/common.smk
context_key: ['if len(flowcells) > 1']
    (49, 'def get_flowcell(units, wildcards):')
    (50, '    flowcells = set([u.flowcell for u in get_units(units, wildcards)])')
    (51, '    if len(flowcells) > 1:')
    (52, '        raise ValueError("Sample type combination from different sequence flowcells")')
    (53, '    return flowcells.pop()')
    (54, '')
    (55, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/qc, file=workflow/rules/common.smk
context_key: ['if not set(value.get("included_unit_types", [])).isdisjoint(types)']
    (56, 'def compile_output_list(wildcards):')
    (57, '    types = set([u.type for u in units.itertuples()])')
    (58, '    output_files = []')
    (59, '    for qc_type, value in config.get("multiqc", {}).get("reports", {}).items():')
    (60, '        if not set(value.get("included_unit_types", [])).isdisjoint(types):')
    (61, '            output_files.append("qc/multiqc/multiqc_{}.html".format(qc_type))')
    (62, '    output_files += [')
    (63, '        "qc/gatk_calculate_contamination/%s_%s.contamination.table" % (sample, unit_type)')
    (64, '        for sample in get_samples(samples)')
    (65, '        for unit_type in get_unit_types(units, sample)')
    (66, '        if unit_type != "R"')
    (67, '    ]')
    (68, '    output_files += [')
    (69, '        "qc/peddy/peddy.peddy.ped",')
    (70, '        "qc/peddy/peddy.ped_check.csv",')
    (71, '        "qc/peddy/peddy.sex_check.csv",')
    (72, '        "qc/peddy/peddy.het_check.csv",')
    (73, '        "qc/peddy/peddy.html",')
    (74, '        "qc/peddy/peddy.vs.html",')
    (75, '        "qc/peddy/peddy.background_pca.json",')
    (76, '    ]')
    (77, '    return output_files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kevinrue/snakemake_alevin_quant, file=workflow/rules/common.smk
context_key: ['if len(fastq1) == 0']
    (23, 'def get_gex_fastq(wildcards):')
    (24, "    \\'\\'\\'")
    (25, '    Identify pairs of FASTQ files from the sample sheet.')
    (26, '')
    (27, '    wildcards')
    (28, '    - sample: name of the sample to process.')
    (29, "    \\'\\'\\'")
    (30, '    fastq1_dir = samples["fastqs"][wildcards.sample]')
    (31, '    fastq1_pattern = config["pattern"]["read1"]')
    (32, '    fastq1_glob = f"{fastq1_dir}/*{fastq1_pattern}*"')
    (33, '    fastq1 = glob.glob(fastq1_glob)')
    (34, '')
    (35, '    if len(fastq1) == 0:')
    (36, '        raise OSError(f"No file matched pattern: {fastq1_glob}")')
    (37, '')
    (38, '    fastq2 = [file.replace(config["pattern"]["read1"], config["pattern"]["read2"]) for file in fastq1]')
    (39, '')
    (40, '    for file in fastq2:')
    (41, '        if not os.path.exists(file):')
    (42, '            raise OSError(f"Paired file not found: {file}")')
    (43, '')
    (44, "    return {\\'fastq1\\' : fastq1, \\'fastq2\\' : fastq2 }")
    (45, '')
    (46, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kevinrue/snakemake_alevin_quant, file=workflow/rules/common.smk
context_key: ['if expect_cells > 0']
    (47, 'def get_cells_option(wildcards):')
    (48, "    \\'\\'\\'")
    (49, '    Build an string of command line options from the sample sheet.')
    (50, '')
    (51, '    wildcards')
    (52, '    - sample: name of the sample to process.')
    (53, '')
    (54, "    Note that users should supply only one of \\'expect_cells\\' or \\'force_cells\\'.")
    (55, '    The other one should be set to 0, to be ignored.')
    (56, "    \\'\\'\\'")
    (57, '    option_str = ""')
    (58, '')
    (59, "    expect_cells = samples[\\'expect_cells\\'][wildcards.sample]")
    (60, "    force_cells = samples[\\'force_cells\\'][wildcards.sample]")
    (61, '')
    (62, '    if expect_cells > 0:')
    (63, '        option_str += f" --expectCells {expect_cells}"')
    (64, '')
    (65, '    if force_cells > 0:')
    (66, '        option_str += f" --forceCells {force_cells}"')
    (67, '')
    (68, '    return option_str')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vdjonsson/rna-seq-pizzly, file=workflow/rules/sleuth.smk
context_key: ['if wildcards.model == "all"']
    (16, 'def get_model(wildcards):')
    (17, '    if wildcards.model == "all":')
    (18, '        return {"full": None}')
    (19, '    return config["sleuth"]["models"][wildcards.model]')
    (20, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=BodenmillerGroup/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if _sample_dict is None']
    (363, 'def get_sample_dict():')
    (364, '    global _sample_dict')
    (365, '    if _sample_dict is None:')
    (366, '        checkpoints.generate_sample_list.get()')
    (367, '        dat_samples = pd.read_csv(file_path_samples)')
    (368, '        _sample_dict = {pathlib.Path(x).stem: x for x in dat_samples[')
    (369, "            \\'mcd_folders\\']}")
    (370, '    return _sample_dict')
    (371, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bihealth/varfish-db-downloader, file=snakefiles/GRCh3_/extra_annos.smk
context_key: ['if obj == "."']
    (113, '    def object_hook(self, obj):')
    (114, '        if obj == ".":')
    (115, '            return None')
    (116, '        else:')
    (117, '            return super().object_hook(obj)')
    (118, '')
    (119, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bihealth/varfish-db-downloader, file=snakefiles/GRCh3_/extra_annos.smk
context_key: ['if xs[i] is not None', 'if value is None']
    (130, '        def merge_rows(rows):')
    (131, '            result = rows[0][:7] + [[]]')
    (132, '            try:')
    (133, '                values = [')
    (134, '                    json.loads(row[7].replace(",.,", ",null,"), cls=DecodeDotAsNull) for row in rows')
    (135, '                ]')
    (136, '            except:')
    (137, '                print("\\')
    (138, '%s\\')
    (139, '" % rows, file=sys.stderr)')
    (140, '                raise')
    (141, '            for i in range(len(values[0])):')
    (142, '                value = None')
    (143, '                for xs in values:')
    (144, '                    if xs[i] is not None:')
    (145, '                        if value is None:')
    (146, '                            value = xs[i]')
    (147, '                        value = max(value, xs[i])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bihealth/varfish-db-downloader, file=snakefiles/GRCh3_/extra_annos.smk
context_key: ['if xs[i] is not None']
    (130, '        def merge_rows(rows):')
    (131, '            result = rows[0][:7] + [[]]')
    (132, '            try:')
    (133, '                values = [')
    (134, '                    json.loads(row[7].replace(",.,", ",null,"), cls=DecodeDotAsNull) for row in rows')
    (135, '                ]')
    (136, '            except:')
    (137, '                print("\\')
    (138, '%s\\')
    (139, '" % rows, file=sys.stderr)')
    (140, '                raise')
    (141, '            for i in range(len(values[0])):')
    (142, '                value = None')
    (143, '                for xs in values:')
    (144, '                    if xs[i] is not None:')
    (145, '                        if value is None:')
    (146, '                            value = xs[i]')
    (147, '                        value = max(value, xs[i])')
    (148, '                result[-1].append(value)')
    (149, '            result[-1] = json.dumps(result[-1])')
    (150, '            # print("MERGED\\')
    (151, '  %s\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bihealth/varfish-db-downloader, file=snakefiles/GRCh3_/extra_annos.smk
context_key: ['if row[0] == header[0]']
    (130, '        def merge_rows(rows):')
    (131, '            result = rows[0][:7] + [[]]')
    (132, '            try:')
    (133, '                values = [')
    (134, '                    json.loads(row[7].replace(",.,", ",null,"), cls=DecodeDotAsNull) for row in rows')
    (135, '                ]')
    (136, '            except:')
    (137, '                print("\\')
    (138, '%s\\')
    (139, '" % rows, file=sys.stderr)')
    (140, '                raise')
    (141, '            for i in range(len(values[0])):')
    (142, '                value = None')
    (143, '                for xs in values:')
    (144, '                    if xs[i] is not None:')
    (145, '                        if value is None:')
    (146, '                            value = xs[i]')
    (147, '                        value = max(value, xs[i])')
    (148, '                result[-1].append(value)')
    (149, '            result[-1] = json.dumps(result[-1])')
    (150, '            # print("MERGED\\')
    (151, '  %s\\')
    (152, 'TO\\')
    (153, '  %s" % (rows, result), file=sys.stderr)')
    (154, '            return result')
    (155, '')
    (156, '')
    (157, '        header = [')
    (158, '            "release",')
    (159, '            "chromosome",')
    (160, '            "start",')
    (161, '            "end",')
    (162, '            "bin",')
    (163, '            "reference",')
    (164, '            "alternative",')
    (165, '            "anno_data",')
    (166, '        ]')
    (167, '        rows = []')
    (168, '        print("Reading from %s" % input.tsv, file=sys.stderr)')
    (169, '        with open(input.tsv, "rt") as inputf:')
    (170, '            reader = csv.reader(inputf, delimiter="\\\\t")')
    (171, '            with open(output.tsv, "wt") as outputf:')
    (172, '                writer = csv.writer(outputf, delimiter="\\\\t")')
    (173, '                writer.writerow(header)')
    (174, '')
    (175, '                for row in tqdm.tqdm(reader):')
    (176, '                    if row[0] == header[0]:')
    (177, '                        continue  # skip')
    (178, '                    else:')
    (179, '                        if not rows or row[:7] == rows[0][:7]:')
    (180, '                            rows.append(row)')
    (181, '                        else:')
    (182, '                            writer.writerow(merge_rows(rows))')
    (183, '                            rows = [row]')
    (184, '                if rows:')
    (185, '                    writer.writerow(merge_rows(rows))')
    (186, '')
    (187, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dancooke/syntumorsizer, file=workflow/rules/octopus.smk
context_key: ['if "regions" in config']
    (6, 'def _get_calling_bed(wildcards):')
    (7, '    if "regions" in config:')
    (8, '        return config["regions"]')
    (9, '    else:')
    (10, '        return f"data/references/{wildcards.reference}.chromosomes.bed"')
    (11, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=rimjhimroy/SnakeGATK, file=rules/common.smk
context_key: ['if regions', 'if padding']
    (39, 'def get_regions_param(regions=config["processing"].get("regions"), default=""):')
    (40, '    if regions:')
    (41, '        params = "--intervals \\\'{}\\\' ".format(regions)')
    (42, '        padding = config["processing"].get("region-padding")')
    (43, '        if padding:')
    (44, '            params += "--interval-padding {}".format(padding)')
    (45, '        return params')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=rimjhimroy/SnakeGATK, file=rules/common.smk
context_key: ['if regions']
    (39, 'def get_regions_param(regions=config["processing"].get("regions"), default=""):')
    (40, '    if regions:')
    (41, '        params = "--intervals \\\'{}\\\' ".format(regions)')
    (42, '        padding = config["processing"].get("region-padding")')
    (43, '        if padding:')
    (44, '            params += "--interval-padding {}".format(padding)')
    (45, '        return params')
    (46, '    return default')
    (47, '')
    (48, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/inkscape-figure-example, file=Snakefile
context_key: ["if len(create_bm) > 0 and \\'@resolution\\' in create_bm[0]"]
    (83, 'def write_create_bitmap_resolution(path, res=300):')
    (84, '    """Write specified create bitmap resolution to inkscape preferences.xml.')
    (85, '')
    (86, '    Intended to be used once at the beginning and once at the end, like so:')
    (87, '')
    (88, '    ```')
    (89, '    orig_dpi = write_create_bitmap_resolution(path, 300)')
    (90, '    # DO SOME STUFF')
    (91, '    write_create_bitmap_resolution(path, orig_dpi)')
    (92, '    ```')
    (93, '')
    (94, "    Thus we don\\'t end up permanently modifying the resolution.")
    (95, '')
    (96, '    Parameters')
    (97, '    ----------')
    (98, '    path : str')
    (99, '        path to the inkscape preferences file, probably')
    (100, '        ~/.config/inkscape/preferences.xml')
    (101, '    res : int or str, optional')
    (102, '        Target resolution for create bitmap.')
    (103, '')
    (104, '    Returns')
    (105, '    -------')
    (106, '    orig : str')
    (107, "        The original dpi of createbitmap. If none found, returns \\'64\\', the")
    (108, '        default.')
    (109, '')
    (110, '    """')
    (111, '    with open(op.expanduser(path)) as f:')
    (112, '        doc = xmltodict.parse(f.read())')
    (113, "    opts = [i for i in doc[\\'inkscape\\'][\\'group\\'] if \\'options\\' == i[\\'@id\\']][0]")
    (114, "    create_bm =[i for i in opts[\\'group\\'] if \\'createbitmap\\' == i[\\'@id\\']]")
    (115, "    orig = \\'64\\'")
    (116, "    if len(create_bm) > 0 and \\'@resolution\\' in create_bm[0]:")
    (117, "        orig = create_bm[0][\\'@resolution\\']")
    (118, "        create_bm[0][\\'@resolution\\'] = str(res)")
    (119, '    elif len(create_bm) > 0:')
    (120, "        create_bm[0][\\'@resolution\\'] = str(res)")
    (121, '    else:')
    (122, "        create_bm = OrderedDict({\\'@id\\': \\'createbitmap\\', \\'@resolution\\': str(res)})")
    (123, '        opts.append(create_bm)')
    (124, "    with open(path, \\'w\\') as f:")
    (125, '        xmltodict.unparse(doc, output=f)')
    (126, '    return orig')
    (127, '')
    (128, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/inkscape-figure-example, file=Snakefile
context_key: ['if op.exists(v)']
    (129, 'def get_image_ids(path):')
    (130, '    """Get inkscape ids of images that are linked (vs embedded).')
    (131, '')
    (132, '    We only check the images, and we return the ids of all that contain an')
    (133, '    @xlink:href field which points to an existing file.')
    (134, '')
    (135, '    Parameters')
    (136, '    ----------')
    (137, '    path : str')
    (138, '        Path to the svg.')
    (139, '')
    (140, '    Returns')
    (141, '    -------')
    (142, '    ids : list')
    (143, '        List of strings containing the ids of these images. These can then be')
    (144, "        used with the inskcape command line, like so: `f\\'inkscape -g")
    (145, '        --action="select-by-id:{ids[0]};EditDelete;" {path}\\\'`')
    (146, '')
    (147, '    """')
    (148, '    # svgs are just xml, so we can read them like any other xml file.')
    (149, '    with open(path) as f:')
    (150, '        doc = xmltodict.parse(f.read())')
    (151, "    # we can have a strange hierarchy in the svg, depending on how we\\'ve")
    (152, '    # grouped images. this avoids all that by flattening it out...')
    (153, "    flattened_svg = flatten_dict.flatten(doc[\\'svg\\'][\\'g\\'], enumerate_types=(list, ))")
    (154, '    # then we grab the xlink:href field for each image')
    (155, "    images = {k: v for k, v in flattened_svg.items() if \\'image\\' in k")
    (156, "              and \\'@xlink:href\\' in k}")
    (157, '    # and grab only the ids of those images whose xlink:href exists')
    (158, "    ids = [flattened_svg[(*k[:-1], \\'@id\\')] for k, v in images.items()")
    (159, '           if op.exists(v)]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlthafSinghawansaUHN/cfmedipseq_pipeline, file=Snakefile
context_key: ["if \\'settings\\' in config[\\'data\\'][\\'cohorts\\'][cohort]", 'if key in config_data']
    (39, 'def get_cohort_config(cohort):')
    (40, '    """Returns the configuration details for specific cohort.')
    (41, '    Importantly, retrieving a cohort using this function (rather than directly')
    (42, '    accessing it via the config dictionary) will also inject the default settings')
    (43, '    (config > data > defaults) wherever a specific setting is not specified.')
    (44, '    """')
    (45, '    ')
    (46, "    config_data = dict(config[\\'data\\'][\\'defaults\\'])")
    (47, "    if \\'settings\\' in config[\\'data\\'][\\'cohorts\\'][cohort]:")
    (48, "        for key in config[\\'data\\'][\\'cohorts\\'][cohort][\\'settings\\']:")
    (49, '            if key in config_data:')
    (50, "                config_data[key] = config[\\'data\\'][\\'cohorts\\'][cohort][\\'settings\\'][key]")
    (51, '            else:')
    (52, "                raise(Exception(\\'Setting {} does not exist for cohort {}. Available settings: {}\\'.format(")
    (53, "                    key, cohort, \\', \\'.join(config_data.keys())")
    (54, '                )))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlthafSinghawansaUHN/cfmedipseq_pipeline, file=Snakefile
context_key: ["if \\'settings\\' in config[\\'data\\'][\\'cohorts\\'][cohort]"]
    (39, 'def get_cohort_config(cohort):')
    (40, '    """Returns the configuration details for specific cohort.')
    (41, '    Importantly, retrieving a cohort using this function (rather than directly')
    (42, '    accessing it via the config dictionary) will also inject the default settings')
    (43, '    (config > data > defaults) wherever a specific setting is not specified.')
    (44, '    """')
    (45, '    ')
    (46, "    config_data = dict(config[\\'data\\'][\\'defaults\\'])")
    (47, "    if \\'settings\\' in config[\\'data\\'][\\'cohorts\\'][cohort]:")
    (48, "        for key in config[\\'data\\'][\\'cohorts\\'][cohort][\\'settings\\']:")
    (49, '            if key in config_data:')
    (50, "                config_data[key] = config[\\'data\\'][\\'cohorts\\'][cohort][\\'settings\\'][key]")
    (51, '            else:')
    (52, "                raise(Exception(\\'Setting {} does not exist for cohort {}. Available settings: {}\\'.format(")
    (53, "                    key, cohort, \\', \\'.join(config_data.keys())")
    (54, '                )))')
    (55, '    return(config_data)\\t')
    (56, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlthafSinghawansaUHN/cfmedipseq_pipeline, file=Snakefile
context_key: ['if cohort is None']
    (75, 'def get_all_samples(cohort=None):')
    (76, '    """Retrieves all samples to be processed.')
    (77, '    Does so by calling get_cohort_data, and therefore filters out excluded_cases.')
    (78, '    Keyword arguments:')
    (79, '        cohort -- Name of a cohort, OPTIONAL. If not specified, returns all samples')
    (80, '                  across all cohorts.')
    (81, '    """')
    (82, '    ')
    (83, '    all_samples = pd.concat([')
    (84, '        get_cohort_data(cohort_name).assign(cohort_name = cohort_name)')
    (85, '        for cohort_name')
    (86, "        in config[\\'data\\'][\\'cohorts\\']")
    (87, "        if config[\\'data\\'][\\'cohorts\\'][cohort_name][\\'active\\']")
    (88, '        ])')
    (89, '')
    (90, '    if cohort is None:')
    (91, '        return(all_samples)')
    (92, '    else:')
    (93, '        return(all_samples[all_samples.cohort_name == cohort])')
    (94, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlthafSinghawansaUHN/cfmedipseq_pipeline, file=Snakefile
context_key: ['if paired_data']
    (436, 'def get_paired_or_single_end_sorted_merged_bam(wildcards):')
    (437, "    paired_data = get_cohort_config(wildcards.cohort)[\\'paired\\']")
    (438, '    if paired_data:')
    (439, "        return path_to_data + \\'/\\' + wildcards.cohort + \\'/tmp/merge_bam/\\' + wildcards.sample + \\'.paired.aligned.sorted.bam\\'")
    (440, '    else:')
    (441, "        return path_to_data + \\'/\\' + wildcards.cohort + \\'/tmp/merge_bam/\\' + wildcards.sample + \\'.single.aligned.sorted.bam\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlthafSinghawansaUHN/cfmedipseq_pipeline, file=Snakefile
context_key: ['if paired_data']
    (489, 'def get_paired_or_single_end_unsorted_bam(wildcards):')
    (490, "    paired_data = get_cohort_config(wildcards.cohort)[\\'paired\\']")
    (491, '    if paired_data:')
    (492, "        return path_to_data + \\'/\\' + wildcards.cohort + \\'/tmp/bwa_mem/\\' + wildcards.sample + \\'_lib\\' + wildcards.lib + \\'.paired.bam\\'")
    (493, '    else:')
    (494, "        return path_to_data + \\'/\\' + wildcards.cohort + \\'/tmp/bwa_mem/\\' + wildcards.sample + \\'_lib\\' + wildcards.lib + \\'.single.bam\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlthafSinghawansaUHN/cfmedipseq_pipeline, file=Snakefile
context_key: ["if species == \\'human\\'"]
    (550, 'def get_bsgenome_chrom(species, chrom):')
    (551, "    chrom_map = {\\'1\\': \\'Chr1\\', \\'3\\': \\'Chr3\\'}")
    (552, "    if species == \\'human\\':")
    (553, '        return chrom')
    (554, "    elif species == \\'arabidopsis\\':")
    (555, '        return chrom_map[chrom]')
    (556, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jbloomlab/SARSr-CoV_RBD_MAP, file=Snakefile
context_key: ["if any(barcode_runs_expandR1[\\'n_R1\\'] < 1)"]
    (24, 'def nb_markdown(nb):')
    (25, '    """Return path to Markdown results of notebook `nb`."""')
    (26, "    return os.path.join(config[\\'summary_dir\\'],")
    (27, "                        os.path.basename(os.path.splitext(nb)[0]) + \\'.md\\')")
    (28, '')
    (29, '# Information on samples and barcode runs -------------------------------------')
    (30, "barcode_runs = pd.read_csv(config[\\'barcode_runs\\'])")
    (31, '')
    (32, '# combination of the *library* and *sample* columns should be unique.')
    (33, "assert len(barcode_runs.groupby([\\'library\\', \\'sample\\'])) == len(barcode_runs)")
    (34, '')
    (35, '# *sample* should be the hyphen separated concatenation of')
    (36, '# *experiment*, *antibody*, *concentration*, and *selection*.')
    (37, 'sample_vs_expect = (')
    (38, '    barcode_runs')
    (39, "    .assign(expect=lambda x: x[[\\'experiment\\', \\'antibody\\', \\'concentration\\',")
    (40, "                                \\'selection\\']]")
    (41, "                             .apply(lambda r: \\'-\\'.join(r.values.astype(str)),")
    (42, '                                    axis=1),')
    (43, "            equal=lambda x: x[\\'sample\\'] == x[\\'expect\\'],")
    (44, '            )')
    (45, '    )')
    (46, "assert sample_vs_expect[\\'equal\\'].all(), sample_vs_expect.query(\\'equal != True\\')")
    (47, '')
    (48, '# barcode runs with R1 files expanded by glob')
    (49, 'barcode_runs_expandR1 = (')
    (50, '    barcode_runs')
    (51, "    .assign(R1=lambda x: x[\\'R1\\'].str.split(\\'; \\').map(")
    (52, '                    lambda y: list(itertools.chain(*map(glob.glob, y)))),')
    (53, "            n_R1=lambda x: x[\\'R1\\'].map(len),")
    (54, "            sample_lib=lambda x: x[\\'sample\\'] + \\'_\\' + x[\\'library\\'],")
    (55, '            )')
    (56, '    )')
    (57, "assert barcode_runs_expandR1[\\'sample_lib\\'].nunique() == len(barcode_runs_expandR1)")
    (58, "if any(barcode_runs_expandR1[\\'n_R1\\'] < 1):")
    (59, '    raise ValueError(f"no R1 for {barcode_runs_expandR1.query(\\\'n_R1 < 1\\\')}")')
    (60, '')
    (61, '# Rules -----------------------------------------------------------------------')
    (62, '')
    (63, '# this is the target rule (in place of `all`) since it first rule listed')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crs4/fair-crcc-send-data, file=workflow/rules/common.smk
context_key: ['if any(p.is_absolute() for p in source_paths)']
    (88, 'def glob_source_paths() -> Iterable[Path]:')
    (89, '    base_dir = get_repository_path()')
    (90, '    source_paths = [Path(p) for p in config["sources"]["items"]]')
    (91, '    if any(p.is_absolute() for p in source_paths):')
    (92, '        raise ValueError("Source paths must be relative to repository.path (absolute paths found).")')
    (93, '    # glob any directories for files that end with glob_ext')
    (94, '    try:')
    (95, '        cwd = os.getcwd()')
    (96, '        os.chdir(base_dir)')
    (97, '        source_files = [')
    (98, '            slide for p in source_paths if p.is_dir() for slide in p.rglob(f"*{glob_ext}")')
    (99, '        ] + [p for p in source_paths if p.is_file() and p.match("*.c4gh")]')
    (100, '    finally:')
    (101, '        os.chdir(cwd)')
    (102, '    return source_files')
    (103, '')
    (104, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crs4/fair-crcc-send-data, file=workflow/rules/common.smk
context_key: ['if _gRenameIndexCache is None']
    (116, 'def get_original_item_name(new_name: str) -> str:')
    (117, '    """')
    (118, '    Reverse look-up on the rename index.  Given a "new" uuid4 name of a file')
    (119, '    this function retrieves the original name.')
    (120, '')
    (121, '    Can only be used in `input:` sections as it accesses checkpoints.')
    (122, '    """')
    (123, '    global _gRenameIndexCache  # defined in index.smk')
    (124, '    with _gRenameIndexLock:')
    (125, '        if _gRenameIndexCache is None:')
    (126, '            with checkpoints.gen_rename_index.get().output.index.open() as f:')
    (127, '                # each line in the file is a tab-separated tuple (new name, original name)')
    (128, '                _gRenameIndexCache = dict(line.rstrip("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pburnham50/KallistoPipeline, file=workflow/rules/diffexp.smk
context_key: ['if wildcards.model == "all"']
    (21, 'def get_model(wildcards):')
    (22, '    if wildcards.model == "all":')
    (23, '        return {"full": None}')
    (24, '    return config["diffexp"]["models"][wildcards.model]')
    (25, '')
    (26, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pburnham50/KallistoPipeline, file=workflow/rules/quant.smk
context_key: ['if len(input.fq) == 1']
    (13, 'def kallisto_params(wildcards, input):')
    (14, '    extra = config["params"]["kallisto"]')
    (15, '    if len(input.fq) == 1:')
    (16, '        extra += " --single"')
    (17, '        extra += (" --fragment-length {unit.fragment_len_mean} "')
    (18, '                  "--sd {unit.fragment_len_sd}").format(')
    (19, '                    unit=units.loc[')
    (20, '                        (wildcards.sample, wildcards.unit)])')
    (21, '    else:')
    (22, '        extra += " --fusion"')
    (23, '    return extra')
    (24, '')
    (25, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sunjiangming/scRNASeq, file=rules/qc.smk
context_key: ['if constrain_celltypes']
    (53, 'def get_gene_vs_gene_fits(wildcards):')
    (54, '    constrain_celltypes = get_constrain_celltypes(wildcards)')
    (55, '    constrained_markers = markers')
    (56, '    if constrain_celltypes:')
    (57, '        constrained_markers = markers.loc[markers["name"].isin(constrain_celltypes)]')
    (58, '    return expand("analysis/cellassign.{parent}.rds", parent=constrained_markers["parent"].unique())')
    (59, '    ')
    (60, '')
    (61, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=waglecn/mabs, file=stage0.smk
context_key: ["if wildcards.nodeset == \\'complete\\'"]
    (100, 'def aggregate_refseq(wildcards):')
    (101, "    if wildcards.nodeset == \\'complete\\':")
    (102, '        checkpoint_set = checkpoints.refseq_complete_genome_download.get(')
    (103, '            **wildcards')
    (104, '        ).output[0]')
    (105, "    elif wildcards.nodeset == \\'all\\':")
    (106, '        checkpoint_set = checkpoints.refseq_all_genome_download.get(')
    (107, '            **wildcards')
    (108, '        ).output[0]')
    (109, '    checkpoint_outgroup = checkpoints.refseq_outgroup_download.get(')
    (110, '        **wildcards')
    (111, '    ).output[0]')
    (112, '    checkpoint_landmark = checkpoints.refseq_download_landmarks.get(')
    (113, '        **wildcards')
    (114, '    ).output[0]')
    (115, '    assembly_name = glob.glob(checkpoint_set + "/*.gbk.gz")')
    (116, '    outgroup_name = glob.glob(checkpoint_outgroup + "/*.gbk.gz")')
    (117, '    landmark_name = glob.glob(checkpoint_landmark + "/*.gbk.gz")')
    (118, '')
    (119, '    assembly_files = assembly_name + outgroup_name + landmark_name')
    (120, '    return assembly_files')
    (121, '')
    (122, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=waglecn/mabs, file=Snakefile
context_key: ["if ref == \\'mabscessus\\'"]
    (522, 'def snpEff_db(ref):')
    (523, "    if ref == \\'mabscessus\\':")
    (524, '        return "Mycobacterium_abscessus_atcc_19977"')
    (525, "    elif ref == \\'mmassiliense\\':")
    (526, '        return "Mycobacterium_abscessus_subsp_bolletii_ccug_48898_jcm_15300_" \\\\')
    (527, '            "gca_000497265"')
    (528, "    elif ref == \\'mbolettii\\':")
    (529, '        return "Mycobacterium_abscessus_subsp_bolletii_bd"')
    (530, '')
    (531, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/0_0_utilities.smk
context_key: ['try', 'if not os.path.isdir(log_dir)']
    (9, 'def init_log():')
    (10, '    try:')
    (11, "        with open(LOG_CONFIG, \\'rt\\') as f:")
    (12, '            log_config = yaml.safe_load(f.read())')
    (13, "        for handler in log_config[\\'handlers\\']:")
    (14, "            fl = log_config[\\'handlers\\'][handler][\\'filename\\']")
    (15, "            log_config[\\'handlers\\'][handler][\\'filename\\'] = LOG_PATH[fl]")
    (16, "            log_dir = LOG_PATH[fl].split(\\'/\\')[:-1]")
    (17, "            log_dir = \\'/\\'.join(log_dir)")
    (18, '            if not os.path.isdir(log_dir):')
    (19, '                os.makedirs(log_dir)')
    (20, '')
    (21, '        logging.config.dictConfig(log_config)')
    (22, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/0_0_utilities.smk
context_key: ['try', 'except', "if not os.path.exists(\\'dependencies\\')"]
    (28, 'def install_slimfastq():')
    (29, '')
    (30, '    try:')
    (31, "        subprocess.call([\\'dependencies/slimfastq/slimfastq\\', \\'-h\\'], stdout = subprocess.DEVNULL)")
    (32, '    except:')
    (33, "        if not os.path.exists(\\'dependencies\\'):")
    (34, "            os.mkdir(\\'dependencies\\')")
    (35, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/0_0_utilities.smk
context_key: ['try', 'except', 'if p_status == 0']
    (28, 'def install_slimfastq():')
    (29, '')
    (30, '    try:')
    (31, "        subprocess.call([\\'dependencies/slimfastq/slimfastq\\', \\'-h\\'], stdout = subprocess.DEVNULL)")
    (32, '    except:')
    (33, "        if not os.path.exists(\\'dependencies\\'):")
    (34, "            os.mkdir(\\'dependencies\\')")
    (35, '')
    (36, "        wlogger.info(f\\'slimfastq was not found in dependencies. Pulling.]\\')")
    (37, "        p = subprocess.Popen(\\'cd dependencies; \\' +")
    (38, "                              \\'git clone https://github.com/Infinidat/slimfastq.git; \\' +")
    (39, "                              \\'cd slimfastq \\' +")
    (40, "                              \\'git checkout v1.032 && make; \\' +")
    (41, "                              \\'cd ../../snakefiles\\',")
    (42, '                              stdout=subprocess.PIPE,')
    (43, '                              shell=True')
    (44, '                              )')
    (45, '        p_status = p.wait()')
    (46, '        (p_out, p_err) = p.communicate()')
    (47, "        wlogger.info(f\\'slimfastq was not found in dependencies. Pulling. DONE]\\')")
    (48, '        if p_status == 0:')
    (49, "            logger.info(f\\'slimfastq installed to dependencies: {p_out}\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/0_0_utilities.smk
context_key: ["if i.endswith(\\'.sfq\\') and sfq"]
    (56, 'def shstring(l, sfq):')
    (57, "    slimfq = \\'dependencies/slimfastq/slimfastq \\'")
    (58, "    out = \\'\\'")
    (59, '    for i in l:')
    (60, "        if i.endswith(\\'.sfq\\') and sfq:")
    (61, "            out = out + slimfq + i + \\' | gzip -f; \\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/0_0_utilities.smk
context_key: ['if ((f.endswith(".fastq.gz") or f.endswith(".fq.gz") or f.endswith(".sfq")']
    (69, 'def get_sample_files_named_pipe_script(sample, debug=True):')
    (70, '')
    (71, "    input_dir = CONFIG[\\'INPUT-DIR\\']")
    (72, '    r1_files = []')
    (73, '    r1_stems = []')
    (74, '    r2_files = []')
    (75, '    r2_stems = []')
    (76, '    r1_testlist = []')
    (77, '    r2_testlist = []')
    (78, '    sfq = False')
    (79, '')
    (80, '')
    (81, '    for root, dirnames, filenames in os.walk(input_dir):')
    (82, '        for f in filenames:')
    (83, '            if ((f.endswith(".fastq.gz") or f.endswith(".fq.gz") or f.endswith(".sfq"))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/0_0_utilities.smk
context_key: ['if f.find(".sfq") > -1']
    (69, 'def get_sample_files_named_pipe_script(sample, debug=True):')
    (70, '')
    (71, "    input_dir = CONFIG[\\'INPUT-DIR\\']")
    (72, '    r1_files = []')
    (73, '    r1_stems = []')
    (74, '    r2_files = []')
    (75, '    r2_stems = []')
    (76, '    r1_testlist = []')
    (77, '    r2_testlist = []')
    (78, '    sfq = False')
    (79, '')
    (80, '')
    (81, '    for root, dirnames, filenames in os.walk(input_dir):')
    (82, '        for f in filenames:')
    (83, '            if ((f.endswith(".fastq.gz") or f.endswith(".fq.gz") or f.endswith(".sfq"))')
    (84, '                    and f.find(sample) > -1):')
    (85, '                if f.find(".sfq") > -1:')
    (86, '                    sfq = True')
    (87, '')
    (88, '                lane_splits = re.split(CONFIG["LANE-REGEX"], f)')
    (89, "                wlogger.info(f\\'File name split by LANE-REGEX to: [{lane_splits}]\\')")
    (90, '')
    (91, '                if (f.find("_R1_") > -1 or f.endswith("_1.fq.gz")')
    (92, '                        or f.endswith("_1.fastq.gz") or f.endswith("_1.sfq")):')
    (93, '')
    (94, '                    if debug:')
    (95, "                        wlogger.info(f\\'Found R1 file: [{root}, {f}]\\')")
    (96, '                    r1_files.append(root+os.sep+f)')
    (97, '')
    (98, '                    if lane_splits[0].find(sample) > -1:')
    (99, '                        stem = lane_splits[0]')
    (100, '                    else:')
    (101, "                        wlogger.error(f\\'SAMPLE not found in split filename: [{sample}, {lane_splits}]\\')")
    (102, "                        wlogger.error(f\\'Check LANE-REGEX and SAMPLES in CONFIG!\\')")
    (103, '')
    (104, '                        raise Exception(')
    (105, '                            "Substring " + sample + " is not found in split filename string "')
    (106, '                            "by LANE-REGEX: " + CONFIG["LANE-REGEX"] + "\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/0_0_utilities.smk
context_key: ['elif (f.find("_R2_") > -1 or f.endswith("_2.fq.gz"']
    (69, 'def get_sample_files_named_pipe_script(sample, debug=True):')
    (70, '')
    (71, "    input_dir = CONFIG[\\'INPUT-DIR\\']")
    (72, '    r1_files = []')
    (73, '    r1_stems = []')
    (74, '    r2_files = []')
    (75, '    r2_stems = []')
    (76, '    r1_testlist = []')
    (77, '    r2_testlist = []')
    (78, '    sfq = False')
    (79, '')
    (80, '')
    (81, '    for root, dirnames, filenames in os.walk(input_dir):')
    (82, '        for f in filenames:')
    (83, '            if ((f.endswith(".fastq.gz") or f.endswith(".fq.gz") or f.endswith(".sfq"))')
    (84, '                    and f.find(sample) > -1):')
    (85, '                if f.find(".sfq") > -1:')
    (86, '                    sfq = True')
    (87, '')
    (88, '                lane_splits = re.split(CONFIG["LANE-REGEX"], f)')
    (89, "                wlogger.info(f\\'File name split by LANE-REGEX to: [{lane_splits}]\\')")
    (90, '')
    (91, '                if (f.find("_R1_") > -1 or f.endswith("_1.fq.gz")')
    (92, '                        or f.endswith("_1.fastq.gz") or f.endswith("_1.sfq")):')
    (93, '')
    (94, '                    if debug:')
    (95, "                        wlogger.info(f\\'Found R1 file: [{root}, {f}]\\')")
    (96, '                    r1_files.append(root+os.sep+f)')
    (97, '')
    (98, '                    if lane_splits[0].find(sample) > -1:')
    (99, '                        stem = lane_splits[0]')
    (100, '                    else:')
    (101, "                        wlogger.error(f\\'SAMPLE not found in split filename: [{sample}, {lane_splits}]\\')")
    (102, "                        wlogger.error(f\\'Check LANE-REGEX and SAMPLES in CONFIG!\\')")
    (103, '')
    (104, '                        raise Exception(')
    (105, '                            "Substring " + sample + " is not found in split filename string "')
    (106, '                            "by LANE-REGEX: " + CONFIG["LANE-REGEX"] + "\\')
    (107, '"')
    (108, '                            "Filename: " + f + "\\')
    (109, '"')
    (110, '                            "SAMPLE: " + sample')
    (111, '                            )')
    (112, '')
    (113, '                    r1_testlist.append(root + os.sep + stem)')
    (114, '                    r1_stems.append(stem)')
    (115, '')
    (116, '                elif (f.find("_R2_") > -1 or f.endswith("_2.fq.gz")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/0_0_utilities.smk
context_key: ['if debug']
    (69, 'def get_sample_files_named_pipe_script(sample, debug=True):')
    (70, '')
    (71, "    input_dir = CONFIG[\\'INPUT-DIR\\']")
    (72, '    r1_files = []')
    (73, '    r1_stems = []')
    (74, '    r2_files = []')
    (75, '    r2_stems = []')
    (76, '    r1_testlist = []')
    (77, '    r2_testlist = []')
    (78, '    sfq = False')
    (79, '')
    (80, '')
    (81, '    for root, dirnames, filenames in os.walk(input_dir):')
    (82, '        for f in filenames:')
    (83, '            if ((f.endswith(".fastq.gz") or f.endswith(".fq.gz") or f.endswith(".sfq"))')
    (84, '                    and f.find(sample) > -1):')
    (85, '                if f.find(".sfq") > -1:')
    (86, '                    sfq = True')
    (87, '')
    (88, '                lane_splits = re.split(CONFIG["LANE-REGEX"], f)')
    (89, "                wlogger.info(f\\'File name split by LANE-REGEX to: [{lane_splits}]\\')")
    (90, '')
    (91, '                if (f.find("_R1_") > -1 or f.endswith("_1.fq.gz")')
    (92, '                        or f.endswith("_1.fastq.gz") or f.endswith("_1.sfq")):')
    (93, '')
    (94, '                    if debug:')
    (95, "                        wlogger.info(f\\'Found R1 file: [{root}, {f}]\\')")
    (96, '                    r1_files.append(root+os.sep+f)')
    (97, '')
    (98, '                    if lane_splits[0].find(sample) > -1:')
    (99, '                        stem = lane_splits[0]')
    (100, '                    else:')
    (101, "                        wlogger.error(f\\'SAMPLE not found in split filename: [{sample}, {lane_splits}]\\')")
    (102, "                        wlogger.error(f\\'Check LANE-REGEX and SAMPLES in CONFIG!\\')")
    (103, '')
    (104, '                        raise Exception(')
    (105, '                            "Substring " + sample + " is not found in split filename string "')
    (106, '                            "by LANE-REGEX: " + CONFIG["LANE-REGEX"] + "\\')
    (107, '"')
    (108, '                            "Filename: " + f + "\\')
    (109, '"')
    (110, '                            "SAMPLE: " + sample')
    (111, '                            )')
    (112, '')
    (113, '                    r1_testlist.append(root + os.sep + stem)')
    (114, '                    r1_stems.append(stem)')
    (115, '')
    (116, '                elif (f.find("_R2_") > -1 or f.endswith("_2.fq.gz")')
    (117, '                        or f.endswith("_2.fastq.gz") or f.endswith("_2.sfq")):')
    (118, '')
    (119, '                    if debug:')
    (120, "                        wlogger.info(f\\'Found R2 file: [{root}, {f}]\\')")
    (121, '                    r2_files.append(root + os.sep + f)')
    (122, '')
    (123, '                    if lane_splits[0].find(sample) > -1:')
    (124, '                        stem = lane_splits[0]')
    (125, '                    else:')
    (126, "                        wlogger.error(f\\'SAMPLE not found in split filename: [{sample}, {lane_splits}]\\')")
    (127, "                        wlogger.error(f\\'Check LANE-REGEX and SAMPLES in CONFIG!\\')")
    (128, '')
    (129, '                        raise Exception(')
    (130, '                            "Substring " + sample + " is not found in split filename string "')
    (131, '                            "by LANE-REGEX: " + CONFIG["LANE-REGEX"] + "\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/0_0_utilities.smk
context_key: ['if len(list(set(all_stems))) > 1']
    (69, 'def get_sample_files_named_pipe_script(sample, debug=True):')
    (70, '')
    (71, "    input_dir = CONFIG[\\'INPUT-DIR\\']")
    (72, '    r1_files = []')
    (73, '    r1_stems = []')
    (74, '    r2_files = []')
    (75, '    r2_stems = []')
    (76, '    r1_testlist = []')
    (77, '    r2_testlist = []')
    (78, '    sfq = False')
    (79, '')
    (80, '')
    (81, '    for root, dirnames, filenames in os.walk(input_dir):')
    (82, '        for f in filenames:')
    (83, '            if ((f.endswith(".fastq.gz") or f.endswith(".fq.gz") or f.endswith(".sfq"))')
    (84, '                    and f.find(sample) > -1):')
    (85, '                if f.find(".sfq") > -1:')
    (86, '                    sfq = True')
    (87, '')
    (88, '                lane_splits = re.split(CONFIG["LANE-REGEX"], f)')
    (89, "                wlogger.info(f\\'File name split by LANE-REGEX to: [{lane_splits}]\\')")
    (90, '')
    (91, '                if (f.find("_R1_") > -1 or f.endswith("_1.fq.gz")')
    (92, '                        or f.endswith("_1.fastq.gz") or f.endswith("_1.sfq")):')
    (93, '')
    (94, '                    if debug:')
    (95, "                        wlogger.info(f\\'Found R1 file: [{root}, {f}]\\')")
    (96, '                    r1_files.append(root+os.sep+f)')
    (97, '')
    (98, '                    if lane_splits[0].find(sample) > -1:')
    (99, '                        stem = lane_splits[0]')
    (100, '                    else:')
    (101, "                        wlogger.error(f\\'SAMPLE not found in split filename: [{sample}, {lane_splits}]\\')")
    (102, "                        wlogger.error(f\\'Check LANE-REGEX and SAMPLES in CONFIG!\\')")
    (103, '')
    (104, '                        raise Exception(')
    (105, '                            "Substring " + sample + " is not found in split filename string "')
    (106, '                            "by LANE-REGEX: " + CONFIG["LANE-REGEX"] + "\\')
    (107, '"')
    (108, '                            "Filename: " + f + "\\')
    (109, '"')
    (110, '                            "SAMPLE: " + sample')
    (111, '                            )')
    (112, '')
    (113, '                    r1_testlist.append(root + os.sep + stem)')
    (114, '                    r1_stems.append(stem)')
    (115, '')
    (116, '                elif (f.find("_R2_") > -1 or f.endswith("_2.fq.gz")')
    (117, '                        or f.endswith("_2.fastq.gz") or f.endswith("_2.sfq")):')
    (118, '')
    (119, '                    if debug:')
    (120, "                        wlogger.info(f\\'Found R2 file: [{root}, {f}]\\')")
    (121, '                    r2_files.append(root + os.sep + f)')
    (122, '')
    (123, '                    if lane_splits[0].find(sample) > -1:')
    (124, '                        stem = lane_splits[0]')
    (125, '                    else:')
    (126, "                        wlogger.error(f\\'SAMPLE not found in split filename: [{sample}, {lane_splits}]\\')")
    (127, "                        wlogger.error(f\\'Check LANE-REGEX and SAMPLES in CONFIG!\\')")
    (128, '')
    (129, '                        raise Exception(')
    (130, '                            "Substring " + sample + " is not found in split filename string "')
    (131, '                            "by LANE-REGEX: " + CONFIG["LANE-REGEX"] + "\\')
    (132, '"')
    (133, '                            "Filename: " + f + "\\')
    (134, '"')
    (135, '                            "SAMPLE: " + sample')
    (136, '                            )')
    (137, '')
    (138, '                    r2_testlist.append(root + os.sep + stem)')
    (139, '                    r2_stems.append(stem)')
    (140, '')
    (141, '    all_stems = []')
    (142, '    all_stems.extend(r1_stems)')
    (143, '    all_stems.extend(r2_stems)')
    (144, '')
    (145, '    if len(list(set(all_stems))) > 1:')
    (146, "        wlogger.warning(f\\'Split filename by LANE-REGEX is matching to more than one SAMPLES: [{sample}, {all_stems}]\\')")
    (147, '')
    (148, '    if str(r1_testlist) != str(r2_testlist):')
    (149, "        wlogger.error(f\\'R1 and R2 found lists of files does not mathch, pairs are invalid!\\')")
    (150, '        r1_testlist.sort()')
    (151, "        wlogger.error(f\\'R1 file begins: [{r1_testlist}]\\')")
    (152, '')
    (153, '        r2_testlist.sort()')
    (154, "        wlogger.error(f\\'R2 file begins: [{r2_testlist}]\\')")
    (155, '')
    (156, '        raise Exception(')
    (157, '            "Key " + sample + " is not properly differentiating, "')
    (158, '            "pairs are invalid \\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/0_0_utilities.smk
context_key: ['if len(r1_files) == 0 or len(r2_files) == 0']
    (69, 'def get_sample_files_named_pipe_script(sample, debug=True):')
    (70, '')
    (71, "    input_dir = CONFIG[\\'INPUT-DIR\\']")
    (72, '    r1_files = []')
    (73, '    r1_stems = []')
    (74, '    r2_files = []')
    (75, '    r2_stems = []')
    (76, '    r1_testlist = []')
    (77, '    r2_testlist = []')
    (78, '    sfq = False')
    (79, '')
    (80, '')
    (81, '    for root, dirnames, filenames in os.walk(input_dir):')
    (82, '        for f in filenames:')
    (83, '            if ((f.endswith(".fastq.gz") or f.endswith(".fq.gz") or f.endswith(".sfq"))')
    (84, '                    and f.find(sample) > -1):')
    (85, '                if f.find(".sfq") > -1:')
    (86, '                    sfq = True')
    (87, '')
    (88, '                lane_splits = re.split(CONFIG["LANE-REGEX"], f)')
    (89, "                wlogger.info(f\\'File name split by LANE-REGEX to: [{lane_splits}]\\')")
    (90, '')
    (91, '                if (f.find("_R1_") > -1 or f.endswith("_1.fq.gz")')
    (92, '                        or f.endswith("_1.fastq.gz") or f.endswith("_1.sfq")):')
    (93, '')
    (94, '                    if debug:')
    (95, "                        wlogger.info(f\\'Found R1 file: [{root}, {f}]\\')")
    (96, '                    r1_files.append(root+os.sep+f)')
    (97, '')
    (98, '                    if lane_splits[0].find(sample) > -1:')
    (99, '                        stem = lane_splits[0]')
    (100, '                    else:')
    (101, "                        wlogger.error(f\\'SAMPLE not found in split filename: [{sample}, {lane_splits}]\\')")
    (102, "                        wlogger.error(f\\'Check LANE-REGEX and SAMPLES in CONFIG!\\')")
    (103, '')
    (104, '                        raise Exception(')
    (105, '                            "Substring " + sample + " is not found in split filename string "')
    (106, '                            "by LANE-REGEX: " + CONFIG["LANE-REGEX"] + "\\')
    (107, '"')
    (108, '                            "Filename: " + f + "\\')
    (109, '"')
    (110, '                            "SAMPLE: " + sample')
    (111, '                            )')
    (112, '')
    (113, '                    r1_testlist.append(root + os.sep + stem)')
    (114, '                    r1_stems.append(stem)')
    (115, '')
    (116, '                elif (f.find("_R2_") > -1 or f.endswith("_2.fq.gz")')
    (117, '                        or f.endswith("_2.fastq.gz") or f.endswith("_2.sfq")):')
    (118, '')
    (119, '                    if debug:')
    (120, "                        wlogger.info(f\\'Found R2 file: [{root}, {f}]\\')")
    (121, '                    r2_files.append(root + os.sep + f)')
    (122, '')
    (123, '                    if lane_splits[0].find(sample) > -1:')
    (124, '                        stem = lane_splits[0]')
    (125, '                    else:')
    (126, "                        wlogger.error(f\\'SAMPLE not found in split filename: [{sample}, {lane_splits}]\\')")
    (127, "                        wlogger.error(f\\'Check LANE-REGEX and SAMPLES in CONFIG!\\')")
    (128, '')
    (129, '                        raise Exception(')
    (130, '                            "Substring " + sample + " is not found in split filename string "')
    (131, '                            "by LANE-REGEX: " + CONFIG["LANE-REGEX"] + "\\')
    (132, '"')
    (133, '                            "Filename: " + f + "\\')
    (134, '"')
    (135, '                            "SAMPLE: " + sample')
    (136, '                            )')
    (137, '')
    (138, '                    r2_testlist.append(root + os.sep + stem)')
    (139, '                    r2_stems.append(stem)')
    (140, '')
    (141, '    all_stems = []')
    (142, '    all_stems.extend(r1_stems)')
    (143, '    all_stems.extend(r2_stems)')
    (144, '')
    (145, '    if len(list(set(all_stems))) > 1:')
    (146, "        wlogger.warning(f\\'Split filename by LANE-REGEX is matching to more than one SAMPLES: [{sample}, {all_stems}]\\')")
    (147, '')
    (148, '    if str(r1_testlist) != str(r2_testlist):')
    (149, "        wlogger.error(f\\'R1 and R2 found lists of files does not mathch, pairs are invalid!\\')")
    (150, '        r1_testlist.sort()')
    (151, "        wlogger.error(f\\'R1 file begins: [{r1_testlist}]\\')")
    (152, '')
    (153, '        r2_testlist.sort()')
    (154, "        wlogger.error(f\\'R2 file begins: [{r2_testlist}]\\')")
    (155, '')
    (156, '        raise Exception(')
    (157, '            "Key " + sample + " is not properly differentiating, "')
    (158, '            "pairs are invalid \\')
    (159, '"')
    (160, '            "Check lists of R1: " + str(r1_testlist) + "\\')
    (161, '"')
    (162, '            "and R2: " + str(r2_testlist)')
    (163, '            )')
    (164, '')
    (165, '    if len(r1_files) == 0 or len(r2_files) == 0:')
    (166, '        if not os.path.exists(input_dir):')
    (167, "            wlogger.error(f\\'INPUT-DIR does not exists: [{input_dir}]\\')")
    (168, '            print ("{0} directory do not exit!".format(input_dir))')
    (169, '        else:')
    (170, "            wlogger.error(f\\'No suitable files found: [INPUT-DIR:{input_dir}, SAMPLE:{sample}]\\')")
    (171, '            wlogger.error(os.system("ls {0}".format(input_dir)))')
    (172, '')
    (173, '        raise Exception(')
    (174, '            "No matching R1 files for stem {0} in dir: {1}".format(sample, input_dir))')
    (175, '')
    (176, '    r1_files.sort()')
    (177, '    r2_files.sort()')
    (178, "    r1ft = os.path.join(tmp+\\'/{0}R1_input_list.sh\\'.format(sample))")
    (179, "    r2ft = os.path.join(tmp+\\'/{0}R2_input_list.sh\\'.format(sample))")
    (180, '')
    (181, "    f1 = open(r1ft, \\'w\\')")
    (182, '')
    (183, "    wlogger.info(f\\'Writting {sample} R1 list of found files for `cat`: [{r1_files}]\\')")
    (184, '    f1.write(shstring(r1_files, sfq))')
    (185, '    f1.close()')
    (186, "    os.system(\\'chmod a+x %s\\' % (r1ft))")
    (187, '')
    (188, "    f2 = open(r2ft, \\'w\\')")
    (189, '    out = shstring(r2_files, sfq)')
    (190, "    wlogger.info(f\\'Writting {sample} R2 list of found files for `cat`: [{r2_files}]\\')")
    (191, '    f2.write(out)')
    (192, '    f2.close()')
    (193, "    os.system(\\'chmod a+x %s\\' % (r2ft))")
    (194, '')
    (195, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/0_0_utilities.smk
context_key: ['if CONFIG["SUBSAMPLING"]["run"]']
    (196, 'def PolyAAnalysis_trimm_input(wildcards):')
    (197, '')
    (198, '    if CONFIG["SUBSAMPLING"]["run"]:')
    (199, '        input = [')
    (200, '            tmp + "/" + wildcards.stem + "_R1_001subs.fastq.gz",')
    (201, '            tmp + "/" + wildcards.stem + "_R2_001subs.fastq.gz"')
    (202, '        ]')
    (203, '')
    (204, '    else:')
    (205, '        input = [')
    (206, '            tmp + "/" + wildcards.stem + "_R1_001Trimmed.fastq.gz",')
    (207, '            tmp + "/" + wildcards.stem + "_R2_001Trimmed.fastq.gz"')
    (208, '        ]')
    (209, '')
    (210, '    return input')
    (211, '')
    (212, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/0_0_utilities.smk
context_key: ['if i == wildcards.stem']
    (213, 'def get_annotate_ts_strandedness(wildcards):')
    (214, '')
    (215, '    for i in CONFIG["ANNOTATE-TS"]["strandedness"].keys():')
    (216, '        if i == wildcards.stem:')
    (217, '            wlogger.info(f\\\'Set strandedness: {wildcards.stem}: {CONFIG["ANNOTATE-TS"]["strandedness"][i]}\\\')')
    (218, '            return CONFIG["ANNOTATE-TS"]["strandedness"][i]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/common.smk
context_key: ['if os.stat(config["contigs"]).st_size != 0']
    (8, 'def get_contigs():')
    (9, '    with checkpoints.genome_faidx.get().output[0].open() as fai:')
    (10, '        if os.stat(config["contigs"]).st_size != 0:')
    (11, '            contigs = pd.read_csv(config["contigs"],sep="\\\\t",header=None,usecols=[0],dtype=str).squeeze("columns")')
    (12, '        else:')
    (13, '            contigs = pd.read_table(fai, header=None, usecols=[0], dtype=str).squeeze("columns")')
    (14, '        return [re.sub("\\\\*","___",x) for x in contigs]')
    (15, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/common.smk
context_key: ['if len(fastqs) == 2']
    (22, 'def get_fastq(wildcards):')
    (23, '    """Get fastq files of given sample-unit."""')
    (24, '    fastqs = units.loc[(wildcards.sample, wildcards.unit), ["fq1", "fq2"]].dropna()')
    (25, '    if len(fastqs) == 2:')
    (26, '        return {"r1": fastqs.fq1, "r2": fastqs.fq2}')
    (27, '    return {"r1": fastqs.fq1}')
    (28, '')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/common.smk
context_key: ['if not is_single_end(**wildcards)']
    (42, 'def get_trimmed_reads(wildcards):')
    (43, '    """Get trimmed reads of given sample-unit."""')
    (44, '    if not is_single_end(**wildcards):')
    (45, '        # paired-end sample')
    (46, '        return expand("{OUTDIR}/trimmed/{sample}-{unit}.{group}.fastq.gz",')
    (47, '                      OUTDIR=OUTDIR, group=[1, 2], **wildcards)')
    (48, '    # single end sample')
    (49, '    return f"{OUTDIR}/trimmed/{{sample}}-{{unit}}.fastq.gz".format(**wildcards)')
    (50, '')
    (51, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/common.smk
context_key: ['if len(bams) > 1']
    (58, 'def get_merged_bam(sample):')
    (59, '    """Merge aligned reads if there are multiple units."""')
    (60, '    bams = get_sample_bams(sample)')
    (61, '    if len(bams) > 1:')
    (62, '        return f"{OUTDIR}/merged_bams/{sample}.bam",f"{OUTDIR}/merged_bams/{sample}.bai"')
    (63, '    else:')
    (64, '        return bams[0],bams[0].replace(".bam",".bai")')
    (65, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/common.smk
context_key: ['if regions', 'if padding']
    (66, 'def get_regions_param(regions=config["processing"].get("restrict_regions"), default=""):')
    (67, '    if regions:')
    (68, '        params = "--intervals \\\'{}\\\' ".format(regions)')
    (69, '        padding = config["processing"].get("region_padding")')
    (70, '        if padding:')
    (71, '            params += "--interval-padding {}".format(padding)')
    (72, '        return params')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/common.smk
context_key: ['if regions']
    (66, 'def get_regions_param(regions=config["processing"].get("restrict_regions"), default=""):')
    (67, '    if regions:')
    (68, '        params = "--intervals \\\'{}\\\' ".format(regions)')
    (69, '        padding = config["processing"].get("region_padding")')
    (70, '        if padding:')
    (71, '            params += "--interval-padding {}".format(padding)')
    (72, '        return params')
    (73, '    return default')
    (74, '')
    (75, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/common.smk
context_key: ["if str(getattr(row, \\'group\\')) == str(wc.group)"]
    (120, 'def get_vcf_in_group(wc):')
    (121, '    vcfs = []')
    (122, '    for row in samples.itertuples():')
    (123, "        if str(getattr(row, \\'group\\')) == str(wc.group):")
    (124, '            vcfs = vcfs + ["{OUTDIR}/called/{sample}.{contig}.g.vcf.gz".format(OUTDIR=OUTDIR, sample=getattr(row, \\\'sample\\\'), contig=wc.contig)]')
    (125, '    return vcfs')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/trimming_se.smk
context_key: ['if units.loc[wildcards.unit,["fq2"]].isna().all()']
    (36, 'def get_trimmed_reads(wildcards,units):')
    (37, '    print(wildcards.unit)')
    (38, '    if units.loc[wildcards.unit,["fq2"]].isna().all():')
    (39, '        # SE')
    (40, '        return None')
    (41, '    # PE')
    (42, '    else:')
    (43, '        return rules.post_rename_fastq_pe.output.r1,rules.post_rename_fastq_pe.output.r2')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/vsqr.smk
context_key: ['if wildcards.type == "snp"']
    (1, 'def _get_recal_params(wildcards):')
    (2, '    known_variants = resolve_multi_filepath(*references_abs_path(),config["known_variants"])')
    (3, '    if wildcards.type == "snp":')
    (4, '        return (')
    (5, '            "-mode SNP "')
    (6, '            "--max-gaussians 3 "')
    (7, '            "-an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR "')
    (8, '            "-resource:hapmap,known=false,training=true,truth=true,prior=15.0 {hapmap} "')
    (9, '            "-resource:omni,known=false,training=true,truth=true,prior=12.0 {omni} "')
    (10, '            "-resource:1000G,known=false,training=true,truth=false,prior=10.0 {g1k} "')
    (11, '            "-resource:dbsnp,known=true,training=false,truth=false,prior=2.0 {dbsnp}"')
    (12, '        ).format(**known_variants)')
    (13, '    else:')
    (14, '        return (')
    (15, '            "-mode INDEL "')
    (16, '            "-an QD -an FS -an SOR -an MQRankSum -an ReadPosRankSum "')
    (17, '            "--max-gaussians 2 "')
    (18, '            "-resource:mills,known=false,training=true,truth=true,prior=12.0 {mills} "')
    (19, '            "-resource:dbsnp,known=true,training=false,truth=false,prior=2.0 {dbsnp}"')
    (20, '        ).format(**known_variants)')
    (21, '')
    (22, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/functions.py
context_key: ['if arguments']
    (10, 'def _multi_flag(arguments):')
    (11, '    if arguments:')
    (12, '        return " ".join(flag + " " + arg for flag, arg in arguments)')
    (13, "    return \\'\\'")
    (14, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/functions.py
context_key: ['if set_arg and set_arg in samples_files']
    (15, "def _get_samples_set(samples_files, flag=\\'-sf\\'):")
    (16, '    arguments = []')
    (17, '    set_arg=config.get("samples_set", None)')
    (18, '    if set_arg and set_arg in samples_files:')
    (19, '        return "".join(flag + " " + samples_files[set_arg])')
    (20, '    for samples_set in samples_files.keys():')
    (21, '        arguments.append("".join(flag + " " + samples_files[samples_set]))')
    (22, "    return \\' \\'.join(arguments)")
    (23, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/functions.py
context_key: ['if not os.path.isabs(filepath)']
    (32, 'def expand_filepath(filepath):')
    (33, '    filepath = os.path.expandvars(os.path.expanduser(filepath))')
    (34, '    if not os.path.isabs(filepath):')
    (35, '        raise FileNotFoundError(')
    (36, '            errno.ENOENT, os.strerror(errno.ENOENT)+" (path must be absolute)", filepath)')
    (37, '    return filepath')
    (38, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/functions.py
context_key: ['if does not exists, create path and return it. If any errors, retur']
    (43, "def tmp_path(path=\\'\\'):")
    (44, '    """')
    (45, '    if does not exists, create path and return it. If any errors, return')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/functions.py
context_key: ['if path', 'try']
    (43, "def tmp_path(path=\\'\\'):")
    (44, '    """')
    (45, '    if does not exists, create path and return it. If any errors, return')
    (46, '    default path')
    (47, '    :param path: path')
    (48, '    :return: path')
    (49, '    """')
    (50, "    default_path = os.getenv(\\'TMPDIR\\', \\'/tmp\\')")
    (51, '    if path:')
    (52, '        try:')
    (53, '            os.makedirs(path)')
    (54, '        except OSError as e:')
    (55, '            if e.errno != errno.EEXIST:')
    (56, '                return default_path')
    (57, '        return path')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/functions.py
context_key: ['if path']
    (43, "def tmp_path(path=\\'\\'):")
    (44, '    """')
    (45, '    if does not exists, create path and return it. If any errors, return')
    (46, '    default path')
    (47, '    :param path: path')
    (48, '    :return: path')
    (49, '    """')
    (50, "    default_path = os.getenv(\\'TMPDIR\\', \\'/tmp\\')")
    (51, '    if path:')
    (52, '        try:')
    (53, '            os.makedirs(path)')
    (54, '        except OSError as e:')
    (55, '            if e.errno != errno.EEXIST:')
    (56, '                return default_path')
    (57, '        return path')
    (58, '    return default_path')
    (59, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/functions.py
context_key: ['if n >= prefix[s]']
    (72, '    def bytes2human(n):')
    (73, '        # http://code.activestate.com/recipes/578019')
    (74, '        # >>> bytes2human(10000)')
    (75, "        # \\'9.8K\\'")
    (76, '        # >>> bytes2human(100001221)')
    (77, "        # \\'95.4M\\'")
    (78, "        symbols = (\\'K\\', \\'M\\', \\'G\\', \\'T\\', \\'P\\', \\'E\\', \\'Z\\', \\'Y\\')")
    (79, '        prefix = {}')
    (80, '        for i, s in enumerate(symbols):')
    (81, '            prefix[s] = 1 << (i + 1) * 10')
    (82, '        for s in reversed(symbols):')
    (83, '            if n >= prefix[s]:')
    (84, '                value = float(n) / prefix[s]')
    (85, "                return \\'%.0f%s\\' % (value, s)")
    (86, '        return "%sB" % n')
    (87, '')
    (88, '    def preserve(resource, percentage, stock):')
    (89, '        preserved = resource - max(resource * percentage // 100, stock)')
    (90, '        return preserved if preserved != 0 else stock')
    (91, '')
    (92, '    # def preserve(resource, percentage, stock):')
    (93, '    #     return resource - max(resource * percentage // 100, stock)')
    (94, '')
    (95, '    params_template = "\\\'-Xms{} -Xmx{} -XX:ParallelGCThreads={} " \\\\')
    (96, '                      "-Djava.io.tmpdir={}\\\'"')
    (97, '')
    (98, '    mem_min = 1024 ** 3 * 2  # 2GB')
    (99, '')
    (100, '    mem_size = preserve(total_physical_mem_size(), percentage_to_preserve,')
    (101, '                        stock_mem)')
    (102, '    cpu_nums = preserve(cpu_count(), percentage_to_preserve, stock_cpu)')
    (103, '    tmpdir = tmp_path(tmp_dir)')
    (104, '')
    (105, '    return params_template.format(bytes2human(mem_min).lower(),')
    (106, '                                  bytes2human(max(mem_size//cpu_nums*multiply_by,')
    (107, '                                                  mem_min)).lower(),')
    (108, '                                  min(cpu_nums, multiply_by),')
    (109, '                                  tmpdir)')
    (110, '')
    (111, '')
    (112, "# def references_abs_path(ref=\\'references\\'):")
    (113, '#     references = config.get(ref)')
    (114, "#     basepath = references[\\'basepath\\']")
    (115, "#     provider = references[\\'provider\\']")
    (116, "#     release = references[\\'release\\']")
    (117, '')
    (118, '    # return [os.path.join(basepath, provider, release)]')
    (119, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/functions.py
context_key: ['if samples.loc[famid.split(\\\',\\\')[0],"condition"]=="C" and samples.loc[famid.split(\\\',\\\')[1],"condition"]=="T"']
    (157, "def get_sample_by_famid(wildcards, patients, label=\\'sample\\'):")
    (158, '    for famid in patients.loc[wildcards.patient,[label]]:')
    (159, '        if samples.loc[famid.split(\\\',\\\')[0],"condition"]=="C" and samples.loc[famid.split(\\\',\\\')[1],"condition"]=="T":')
    (160, '#            print("all_ok")')
    (161, '            tbam = "reads/recalibrated/" + famid.split(\\\',\\\')[1] + ".dedup.recal.bam"')
    (162, '            cbam = "reads/recalibrated/" + famid.split(\\\',\\\')[0] + ".dedup.recal.bam"')
    (163, '        else:')
    (164, '            tbam = "reads/recalibrated/" + famid.split(\\\',\\\')[0] + ".dedup.recal.bam"')
    (165, '            cbam = "reads/recalibrated/" + famid.split(\\\',\\\')[1] + ".dedup.recal.bam"')
    (166, '    return (tbam,cbam)')
    (167, '')
    (168, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/functions.py
context_key: ['try', 'if force and os.path.exists(path)']
    (169, 'def ensure_dir(path, force=False):')
    (170, '    try:')
    (171, '        if force and os.path.exists(path):')
    (172, '            shutil.rmtree(path)')
    (173, '        os.makedirs(path)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/functions.py
context_key: ['try', 'if not os.path.isdir(path)']
    (169, 'def ensure_dir(path, force=False):')
    (170, '    try:')
    (171, '        if force and os.path.exists(path):')
    (172, '            shutil.rmtree(path)')
    (173, '        os.makedirs(path)')
    (174, '    except OSError:')
    (175, '        if not os.path.isdir(path):')
    (176, '            raise')
    (177, '')
    (178, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/functions.py
context_key: ['try', 'if delete and os.path.exists(path)']
    (179, 'def exist_dir(path, delete=False):')
    (180, '    try:')
    (181, '        if delete and os.path.exists(path):')
    (182, '            shutil.rmtree(path)')
    (183, '        # os.makedirs(path)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/functions.py
context_key: ['try', 'if not os.path.isdir(path)']
    (179, 'def exist_dir(path, delete=False):')
    (180, '    try:')
    (181, '        if delete and os.path.exists(path):')
    (182, '            shutil.rmtree(path)')
    (183, '        # os.makedirs(path)')
    (184, '    except OSError:')
    (185, '        if not os.path.isdir(path):')
    (186, '            raise')
    (187, '')
    (188, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/functions.py
context_key: ['if arguments']
    (189, 'def _multi_flag_dbi(flag, arguments):')
    (190, '    if arguments:')
    (191, '        return " ".join(flag + " " + arg for arg in arguments)')
    (192, "    return \\'\\'")
    (193, '')
    (194, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pmenzel/ont-assembly-snake, file=Snakefile
context_key: ['if not references and [string for string in sample_assemblies if "homopolish" in string]', 'if not references_protein and [string for string in sample_assemblies if "proovframe" in string]', 'rule all', 'rule filtlong', 'rule filtlongMB', 'rule filtlongPC', 'rule filtlongMBql', 'rule filtlongMBqln', 'rule miniasm']
    (43, 'def get_R2_fq(wildcards):')
    (44, '\\treturn glob(\\\'fastq-illumina/\\\' + wildcards.sample.split("+")[0] + \\\'_R2.fastq*\\\')')
    (45, '')
    (46, 'references, = glob_wildcards("references/{ref,[^/\\\\\\\\\\\\\\\\]+}.fa")')
    (47, '')
    (48, 'references_protein, = glob_wildcards("references-protein/{ref,[^/\\\\\\\\\\\\\\\\]+}.faa")')
    (49, '')
    (50, 'sample_assemblies, = glob_wildcards("assemblies/{sample_assembly,[^/]+}/")')
    (51, '#ignore symlinks in assemblies/folder, e.g. sample_flye.fa -> assemblies/sample_flye/output.fa')
    (52, "sample_assemblies = [a for a in sample_assemblies if not re.search(\\'\\\\.fa\\', a)]")
    (53, '')
    (54, '# if any desired assembly requires homopolish then at least one reference genome should be provided')
    (55, 'if not references and [string for string in sample_assemblies if "homopolish" in string]:')
    (56, '\\tquit("Error: must provide at least one reference genome sequence when using homopolish")')
    (57, '')
    (58, '# if any desired assembly requires proovframe then at least one reference proteome should be provided')
    (59, 'if not references_protein and [string for string in sample_assemblies if "proovframe" in string]:')
    (60, '\\tquit("Error: must provide at least one reference protein file when using proovframe")')
    (61, '')
    (62, 'list_outputs = expand("assemblies/{sample_assembly}/output.fa", sample_assembly = sample_assemblies)')
    (63, 'list_outputs_links = expand("assemblies/{sample_assembly}.fa", sample_assembly = sample_assemblies)')
    (64, '# remove homopolish and proovframe assemblies from default list')
    (65, "list_outputs = [i for i in list_outputs if not re.search(\\'homopolish|proovframe\\', i, re.IGNORECASE)]")
    (66, "list_outputs_links = [i for i in list_outputs_links if not re.search(\\'homopolish|proovframe\\', i, re.IGNORECASE)]")
    (67, '')
    (68, '# make lists for homopolish, one entry for each reference genome')
    (69, 'list_outputs_homopolish = expand("assemblies/{sample_assembly}/output_{ref}.fa", ref = references, sample_assembly = [i for i in sample_assemblies if re.search(\\\'homopolish$\\\', i, re.IGNORECASE)])')
    (70, 'list_outputs_links_homopolish = expand("assemblies/{sample_assembly}{ref}.fa", ref = references, sample_assembly =  [i for i in sample_assemblies if re.search(\\\'homopolish$\\\', i, re.IGNORECASE)])')
    (71, '')
    (72, '# make lists for proovframe, one entry for each reference protein file')
    (73, 'list_outputs_proovframe = expand("assemblies/{sample_assembly}/output_{ref}.fa", ref = references_protein, sample_assembly = [i for i in sample_assemblies if re.search(\\\'proovframe$\\\', i, re.IGNORECASE)])')
    (74, 'list_outputs_links_proovframe = expand("assemblies/{sample_assembly}{ref}.fa", ref = references_protein, sample_assembly =  [i for i in sample_assemblies if re.search(\\\'proovframe$\\\', i, re.IGNORECASE)])')
    (75, '')
    (76, 'rule all:')
    (77, '\\tinput:')
    (78, '\\t\\tlist_outputs,')
    (79, '\\t\\tlist_outputs_links,')
    (80, '\\t\\tlist_outputs_homopolish,')
    (81, '\\t\\tlist_outputs_links_homopolish,')
    (82, '\\t\\tlist_outputs_proovframe,')
    (83, '\\t\\tlist_outputs_links_proovframe')
    (84, '')
    (85, 'rule filtlong:')
    (86, '\\tthreads: 1')
    (87, '\\tinput:')
    (88, '\\t\\tfq = get_ont_fq')
    (89, '\\toutput:')
    (90, '\\t\\t"fastq-ont/{sample}+filtlong.fastq"')
    (91, '\\tlog: "fastq-ont/{sample}_filtlong_log.txt"')
    (92, '\\tshell:')
    (93, '\\t\\t"""')
    (94, '\\t\\tfiltlong --min_length {filtlong_min_read_length} {input} > {output} 2>{log}')
    (95, '\\t\\t"""')
    (96, '')
    (97, 'rule filtlongMB:')
    (98, '\\tthreads: 1')
    (99, '\\tinput:')
    (100, '\\t\\tfq = get_ont_fq')
    (101, '\\toutput:')
    (102, '\\t\\t"fastq-ont/{sample}+filtlongMB{num}.fastq"')
    (103, '\\tlog: "fastq-ont/{sample}_filtlongMB{num}_log.txt"')
    (104, '\\tshell:')
    (105, '\\t\\t"""')
    (106, '\\t\\tfiltlong --min_length {filtlong_min_read_length} -t {wildcards.num}000000 {input} > {output} 2>{log}')
    (107, '\\t\\t"""')
    (108, '')
    (109, '# for keeping num PerCent of the bases')
    (110, 'rule filtlongPC:')
    (111, '\\tthreads: 1')
    (112, '\\tinput:')
    (113, '\\t\\tfq = get_ont_fq')
    (114, '\\toutput:')
    (115, '\\t\\t"fastq-ont/{sample}+filtlongPC{num}.fastq"')
    (116, '\\tlog: "fastq-ont/{sample}_filtlongPC{num}_log.txt"')
    (117, '\\tshell:')
    (118, '\\t\\t"""')
    (119, '\\t\\tfiltlong --min_length {filtlong_min_read_length} --keep_percent {wildcards.num} {input} > {output} 2>{log}')
    (120, '\\t\\t"""')
    (121, '')
    (122, 'rule filtlongMBql:')
    (123, '\\tthreads: 1')
    (124, '\\twildcard_constraints:')
    (125, '\\t\\tmb = "[0-9]+",')
    (126, '\\t\\tqweight = "[0-9]+",')
    (127, '\\t\\tlweight = "[0-9]+"')
    (128, '\\tinput:')
    (129, '\\t\\tfq = get_ont_fq')
    (130, '\\toutput:')
    (131, '\\t\\t"fastq-ont/{sample}+filtlongMB{mb},{qweight},{lweight}.fastq"')
    (132, '\\tlog: "fastq-ont/{sample}_filtlongMB{mb},{qweight},{lweight}_log.txt"')
    (133, '\\tshell:')
    (134, '\\t\\t"""')
    (135, '\\t\\tfiltlong --min_length {filtlong_min_read_length} --mean_q_weight {wildcards.qweight} --length_weight {wildcards.lweight}  -t {wildcards.mb}000000 {input} > {output} 2>{log}')
    (136, '\\t\\t"""')
    (137, '')
    (138, 'rule filtlongMBqln:')
    (139, '\\tthreads: 1')
    (140, '\\twildcard_constraints:')
    (141, '\\t\\tmb = "[0-9]+",')
    (142, '\\t\\treadlen = "[0-9]+",')
    (143, '\\t\\tqweight = "[0-9]+",')
    (144, '\\t\\tlweight = "[0-9]+"')
    (145, '\\tinput:')
    (146, '\\t\\tfq = get_ont_fq')
    (147, '\\toutput:')
    (148, '\\t\\t"fastq-ont/{sample}+filtlongMB{mb},{qweight},{lweight},{readlen}.fastq"')
    (149, '\\tlog: "fastq-ont/{sample}_filtlongMB{mb},{qweight},{lweight},{readlen}_log.txt"')
    (150, '\\tshell:')
    (151, '\\t\\t"""')
    (152, '\\t\\tfiltlong --min_length {wildcards.readlen} --mean_q_weight {wildcards.qweight} --length_weight {wildcards.lweight}  -t {wildcards.mb}000000 {input} > {output} 2>{log}')
    (153, '\\t\\t"""')
    (154, '')
    (155, 'rule miniasm:')
    (156, '  conda: "env/conda-miniasm.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pmenzel/ont-assembly-snake, file=Snakefile
context_key: ['if medaka_model is not None']
    (340, 'def get_model_for_sample(wildcards):')
    (341, '  if medaka_model is not None:')
    (342, '    return "-m " + medaka_model')
    (343, '  else:')
    (344, '    if map_medaka_model is not None:')
    (345, '      sample_base = wildcards.sample.split("+",1)[0]')
    (346, '      if map_medaka_model.get(sample_base,False):')
    (347, '        return "-m " + map_medaka_model.get(sample_base,False)')
    (348, '      else:')
    (349, '        return ""')
    (350, '    else:')
    (351, '      return ""')
    (352, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=qferre/ologram-modl_supp_mat, file=Snakefile
context_key: ['if res != []']
    (301, 'def get_peaks_artificial_calibrate_as_list(wildcards): ')
    (302, "    res = sorted(glob.glob(\\'output/artificial_data_calibrate/data/*bed\\'))")
    (303, '    if res != []: return res')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=qferre/ologram-modl_supp_mat, file=Snakefile
context_key: ['if "mcf7" in run']
    (1036, 'def get_queryname(wildcards):')
    (1037, '    run = wildcards.cell_line')
    (1038, '')
    (1039, '    if "mcf7" in run:')
    (1040, '        if "full" in run: return "DHS"')
    (1041, '        else: return "foxa1"')
    (1042, '    if "artificial" in run: return "Query"')
    (1043, '')
    (1044, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/rules/common.smk
context_key: ['if Path(bam).is_file()']
    (26, 'def get_sm_tag(bam): ')
    (27, '    if Path(bam).is_file():')
    (28, '        command = \\\'samtools view -H \\\' + bam + \\\' | grep "^@RG" | grep -Eo "SM[^[:space:]]+" | cut -d":" -f2 | uniq\\\'')
    (29, "        sm = subprocess.check_output(command, shell=True).decode(\\'ascii\\').rstrip()")
    (30, '        return sm')
    (31, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/rules/common.smk
context_key: ["if path.startswith( \\'gs"]
    (40, 'def path_sanitize(path):')
    (41, "    if path.startswith( \\'gs://\\' ):")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/rules/common.smk
context_key: ['if os.path.exists(path)']
    (50, 'def get_DBImport_path2(wildcards):')
    (51, "    path = \\'\\'.join(glob.glob(\\'HaplotypeCaller/DBImport/\\' + wildcards.chrom + \\'/*/__*/\\'))")
    (52, '    myList = []')
    (53, '    if os.path.exists(path):')
    (54, "        myList = [\\'AD.tdb\\', \\'AD_var.tdb\\', \\'ALT.tdb\\', \\'ALT_var.tdb\\', \\'BaseQRankSum.tdb\\', \\'__book_keeping.tdb.gz\\', \\'__coords.tdb\\', \\'DP_FORMAT.tdb\\', \\'DP.tdb\\', \\'DS.tdb\\', \\'END.tdb\\', \\'ExcessHet.tdb\\', \\'FILTER.tdb\\', \\'FILTER_var.tdb\\', \\'GQ.tdb\\', \\'GT.tdb\\', \\'GT_var.tdb\\', \\'ID.tdb\\', \\'ID_var.tdb\\', \\'InbreedingCoeff.tdb\\', \\'MIN_DP.tdb\\', \\'MLEAC.tdb\\', \\'MLEAC_var.tdb\\', \\'MLEAF.tdb\\', \\'MLEAF_var.tdb\\', \\'MQRankSum.tdb\\', \\'PGT.tdb\\', \\'PGT_var.tdb\\', \\'PID.tdb\\', \\'PID_var.tdb\\', \\'PL.tdb\\', \\'PL_var.tdb\\', \\'QUAL.tdb\\', \\'RAW_MQandDP.tdb\\', \\'ReadPosRankSum.tdb\\', \\'REF.tdb\\', \\'REF_var.tdb\\', \\'SB.tdb\\', \\'__tiledb_fragment.tdb\\']")
    (55, '        myList = [path + file for file in myList]')
    (56, '    return(myList)')
    (57, '')
    (58, '')
    (59, '# read in chromosome list from reference dict file (assumes the dict is already created)')
    (60, '# this circumvents the issue of whether to use hg19-style or b37-style chromosome annotation')
    (61, '# as it just pulls the chromosome names directly from the dict index.    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/rules/common.smk
context_key: ["if not subprocess.call([\\'grep\\', \\'-q\\', \\'^\\' + f2 + \\'\\\\\\\\s\\', bedFile])"]
    (62, 'def get_chrom_names(dictionaryFile, bedFile):')
    (63, '    chromList = []')
    (64, '    with open(dictionaryFile) as f:')
    (65, '        next(f)')
    (66, '        for line in f:')
    (67, '            f1 = line.split("\\\\t")[1]')
    (68, '            f2 = f1.split(":")[1]')
    (69, "            # if hc_mode and not subprocess.call([\\'grep\\', \\'-q\\', \\'^\\' + f2 + \\':\\', intervalFile]): # exclude chroms not in the regions of interest, otherwise creates an empty interval file which GATK doesn\\'t like")
    (70, '            # chromList.append(f2)')
    (71, "            if not subprocess.call([\\'grep\\', \\'-q\\', \\'^\\' + f2 + \\'\\\\\\\\s\\', bedFile]):")
    (72, '                chromList.append(f2)')
    (73, '    return(chromList)   ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=osvaldoreisss/transcriptome_assembly_workflow, file=workflow/rules/common.smk
context_key: ['if len(fastqs) == 2']
    (20, 'def get_fastq(wildcards):')
    (21, '    fastqs = samples.loc[(wildcards.sample, int(wildcards.run)), ["fq1", "fq2"]].dropna()')
    (22, '    if len(fastqs) == 2:')
    (23, '        return f"{fastqs.fq1}", f"{fastqs.fq2}"')
    (24, '    return f"{fastqs.fq1}"\'')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pughlab/PLBR_MedRemixBEDPE, file=workflow/rules/cfmedip_nbglm.smk
context_key: ["if bam_or_bedpe == \\'bam\\'"]
    (6, 'def get_cfmedip_nbglm_input(bam_or_bedpe):')
    (7, "    if bam_or_bedpe == \\'bam\\':")
    (8, "        return(path_to_data + \\'/{cohort}/results/bam_merged_bin_stats/bam_bin_stats_{sample}.feather\\')")
    (9, "    elif bam_or_bedpe == \\'bedpe\\': ")
    (10, "        return(path_to_data + \\'/{cohort}/results/bedpe_merged_bin_stats/bedpe_bin_stats_{sample}.feather\\')")
    (11, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pughlab/PLBR_MedRemixBEDPE, file=workflow/rules/cfmedip_nbglm.smk
context_key: ["if bam_or_bedpe == \\'bam\\'"]
    (12, 'def get_cfmedip_nbglm_output(bam_or_bedpe):')
    (13, "    if bam_or_bedpe == \\'bam\\':")
    (14, "        return([path_to_data + \\'/{cohort}/results/bam_cfmedip_nbglm/bam_{sample}_fit_nbglm.tsv\\',")
    (15, "                path_to_data + \\'/{cohort}/results/bam_cfmedip_nbglm/bam_{sample}_fit_nbglm_model.Rds\\',")
    (16, "                path_to_data + \\'/{cohort}/results/bam_cfmedip_nbglm/bam_{sample}_fit_nbglm.bedgraph\\'])")
    (17, "    elif bam_or_bedpe == \\'bedpe\\':")
    (18, "        return([path_to_data + \\'/{cohort}/results/bedpe_cfmedip_nbglm/bedpe_{sample}_fit_nbglm.tsv\\',")
    (19, "                path_to_data + \\'/{cohort}/results/bedpe_cfmedip_nbglm/bedpe_{sample}_fit_nbglm_model.Rds\\',")
    (20, "                path_to_data + \\'/{cohort}/results/bedpe_cfmedip_nbglm/bedpe_{sample}_fit_nbglm.bedgraph\\'])")
    (21, '')
    (22, '# This is the currently used negative binomial GLM approach to fitting')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if "assignments" in config']
    (55, 'def getAssignments():')
    (56, '    if "assignments" in config:')
    (57, '        return list(config["assignments"].keys())')
    (58, '    else:')
    (59, '        return []')
    (60, '')
    (61, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if "experiments" in config']
    (81, 'def getProjects():')
    (82, '    if "experiments" in config:')
    (83, '        return list(config["experiments"].keys())')
    (84, '    else:')
    (85, '        return []')
    (86, '')
    (87, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if "variants" in config["experiments"][project]']
    (108, 'def getVariants(project):')
    (109, '    if "variants" in config["experiments"][project]:')
    (110, '        return config["experiments"][project]["variants"]')
    (111, '    else:')
    (112, '        raise MissingVariantInConfigException(project)')
    (113, '')
    (114, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if condition == None', 'if len(getReplicatesOfCondition(project, condition)) <= 1']
    (183, 'def hasReplicates(project, condition=None):')
    (184, '    if condition == None:')
    (185, '        conditions = getConditions(project)')
    (186, '        for condition in conditions:')
    (187, '            if len(getReplicatesOfCondition(project, condition)) <= 1:')
    (188, '                return False')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if condition == None', 'else']
    (183, 'def hasReplicates(project, condition=None):')
    (184, '    if condition == None:')
    (185, '        conditions = getConditions(project)')
    (186, '        for condition in conditions:')
    (187, '            if len(getReplicatesOfCondition(project, condition)) <= 1:')
    (188, '                return False')
    (189, '    else:')
    (190, '        return len(getReplicatesOfCondition(project, condition)) > 1')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if condition == None']
    (183, 'def hasReplicates(project, condition=None):')
    (184, '    if condition == None:')
    (185, '        conditions = getConditions(project)')
    (186, '        for condition in conditions:')
    (187, '            if len(getReplicatesOfCondition(project, condition)) <= 1:')
    (188, '                return False')
    (189, '    else:')
    (190, '        return len(getReplicatesOfCondition(project, condition)) > 1')
    (191, '    return True')
    (192, '')
    (193, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if not betweenReplicates or hasReplicates(project)', 'try']
    (285, 'def getOutputProjectAssignmentConfig_helper(file, betweenReplicates=False):')
    (286, '    """')
    (287, '    Inserts {project}, {assignment} and {config} (from configs of project) from config into given file.')
    (288, '    When betweenReplicates is True skips projects without replicates in one condition.')
    (289, '    """')
    (290, '    output = []')
    (291, '    projects = getProjects()')
    (292, '    for project in projects:')
    (293, '        if not betweenReplicates or hasReplicates(project):')
    (294, '            try:')
    (295, '                output += expand(')
    (296, '                    file,')
    (297, '                    project=project,')
    (298, '                    assignment=getProjectAssignments(project),')
    (299, '                    config=getConfigs(project),')
    (300, '                )')
    (301, '            except MissingAssignmentInConfigException:')
    (302, '                continue')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if not betweenReplicates or hasReplicates(project)']
    (285, 'def getOutputProjectAssignmentConfig_helper(file, betweenReplicates=False):')
    (286, '    """')
    (287, '    Inserts {project}, {assignment} and {config} (from configs of project) from config into given file.')
    (288, '    When betweenReplicates is True skips projects without replicates in one condition.')
    (289, '    """')
    (290, '    output = []')
    (291, '    projects = getProjects()')
    (292, '    for project in projects:')
    (293, '        if not betweenReplicates or hasReplicates(project):')
    (294, '            try:')
    (295, '                output += expand(')
    (296, '                    file,')
    (297, '                    project=project,')
    (298, '                    assignment=getProjectAssignments(project),')
    (299, '                    config=getConfigs(project),')
    (300, '                )')
    (301, '            except MissingAssignmentInConfigException:')
    (302, '                continue')
    (303, '    return output')
    (304, '')
    (305, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if not betweenReplicates or hasReplicates(project)']
    (306, 'def getOutputProjectConfig_helper(file, betweenReplicates=False):')
    (307, '    """')
    (308, '    Inserts {project}, {config} from config into given file.')
    (309, '    When betweenReplicates is True skips projects without replicates in one condition.')
    (310, '    """')
    (311, '    output = []')
    (312, '    projects = getProjects()')
    (313, '    for project in projects:')
    (314, '        if not betweenReplicates or hasReplicates(project):')
    (315, '            output += expand(')
    (316, '                file,')
    (317, '                project=project,')
    (318, '                config=getConfigs(project),')
    (319, '            )')
    (320, '    return output')
    (321, '')
    (322, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if not betweenReplicates or hasReplicates(project)', 'try']
    (323, 'def getOutputProjectAssignmentConfig_helper(file, betweenReplicates=False):')
    (324, '    """')
    (325, '    Inserts {project}, {assignment}, {config} from config into given file.')
    (326, '    When betweenReplicates is True skips projects without replicates in one condition.')
    (327, '    """')
    (328, '    output = []')
    (329, '    projects = getProjects()')
    (330, '    for project in projects:')
    (331, '        if not betweenReplicates or hasReplicates(project):')
    (332, '            try:')
    (333, '                output += expand(')
    (334, '                    file,')
    (335, '                    project=project,')
    (336, '                    assignment=getProjectAssignments(project),')
    (337, '                    config=getConfigs(project),')
    (338, '                )')
    (339, '            except MissingAssignmentInConfigException:')
    (340, '                continue')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if not betweenReplicates or hasReplicates(project)']
    (323, 'def getOutputProjectAssignmentConfig_helper(file, betweenReplicates=False):')
    (324, '    """')
    (325, '    Inserts {project}, {assignment}, {config} from config into given file.')
    (326, '    When betweenReplicates is True skips projects without replicates in one condition.')
    (327, '    """')
    (328, '    output = []')
    (329, '    projects = getProjects()')
    (330, '    for project in projects:')
    (331, '        if not betweenReplicates or hasReplicates(project):')
    (332, '            try:')
    (333, '                output += expand(')
    (334, '                    file,')
    (335, '                    project=project,')
    (336, '                    assignment=getProjectAssignments(project),')
    (337, '                    config=getConfigs(project),')
    (338, '                )')
    (339, '            except MissingAssignmentInConfigException:')
    (340, '                continue')
    (341, '    return output')
    (342, '')
    (343, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if "variants" in config["experiments"][project]', 'if hasReplicates(project, condition)']
    (344, 'def getOutputVariants_helper(file, betweenReplicates=False):')
    (345, '    """')
    (346, '    Only when variants are set in config file')
    (347, '    Inserts {project}, {condition}, {assignment} and {config} (from configs of project) from config into given file.')
    (348, '    When betweenReplicates is True skips project/condition without replicates in a condition.')
    (349, '    """')
    (350, '    output = []')
    (351, '    projects = getProjects()')
    (352, '    for project in projects:')
    (353, '        conditions = getConditions(project)')
    (354, '        for condition in conditions:')
    (355, '            if "variants" in config["experiments"][project]:')
    (356, '                if hasReplicates(project, condition):')
    (357, '                    output += expand(')
    (358, '                        file,')
    (359, '                        project=project,')
    (360, '                        condition=condition,')
    (361, '                        assignment=getProjectAssignments(project),')
    (362, '                        config=list(config["experiments"][project]["configs"].keys()),')
    (363, '                    )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if "variants" in config["experiments"][project]']
    (344, 'def getOutputVariants_helper(file, betweenReplicates=False):')
    (345, '    """')
    (346, '    Only when variants are set in config file')
    (347, '    Inserts {project}, {condition}, {assignment} and {config} (from configs of project) from config into given file.')
    (348, '    When betweenReplicates is True skips project/condition without replicates in a condition.')
    (349, '    """')
    (350, '    output = []')
    (351, '    projects = getProjects()')
    (352, '    for project in projects:')
    (353, '        conditions = getConditions(project)')
    (354, '        for condition in conditions:')
    (355, '            if "variants" in config["experiments"][project]:')
    (356, '                if hasReplicates(project, condition):')
    (357, '                    output += expand(')
    (358, '                        file,')
    (359, '                        project=project,')
    (360, '                        condition=condition,')
    (361, '                        assignment=getProjectAssignments(project),')
    (362, '                        config=list(config["experiments"][project]["configs"].keys()),')
    (363, '                    )')
    (364, '    return output')
    (365, '')
    (366, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if "global" in config', 'if "assignments" in config["global"]', 'if "split_number" in config["global"]["assignments"]']
    (409, 'def getSplitNumber():')
    (410, '    split = SPLIT_FILES_NUMBER')
    (411, '')
    (412, '    if "global" in config:')
    (413, '        if "assignments" in config["global"]:')
    (414, '            if "split_number" in config["global"]["assignments"]:')
    (415, '                split = config["global"]["assignments"]["split_number"]')
    (416, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if "global" in config']
    (409, 'def getSplitNumber():')
    (410, '    split = SPLIT_FILES_NUMBER')
    (411, '')
    (412, '    if "global" in config:')
    (413, '        if "assignments" in config["global"]:')
    (414, '            if "split_number" in config["global"]["assignments"]:')
    (415, '                split = config["global"]["assignments"]["split_number"]')
    (416, '')
    (417, '    return split')
    (418, '')
    (419, '')
    (420, '# count.smk specific functions')
    (421, '')
    (422, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if isinstance(value, int)']
    (462, 'def counts_getFilterConfig(project, conf, dna_or_rna, command):')
    (463, '    value = config["experiments"][project]["configs"][conf]["filter"][dna_or_rna][')
    (464, '        command')
    (465, '    ]')
    (466, '    if isinstance(value, int):')
    (467, '        return "--%s %d" % (command, value)')
    (468, '    else:')
    (469, '        return "--%s %f" % (command, value)')
    (470, '')
    (471, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if useSampling(project, conf, dna_or_rna)', 'if dna_or_rna in config["experiments"][project]["configs"][conf]["sampling"]']
    (472, 'def counts_getSamplingConfig(project, conf, dna_or_rna, command):')
    (473, '    if useSampling(project, conf, dna_or_rna):')
    (474, '        if dna_or_rna in config["experiments"][project]["configs"][conf]["sampling"]:')
    (475, '            if (')
    (476, '                command')
    (477, '                in config["experiments"][project]["configs"][conf]["sampling"][')
    (478, '                    dna_or_rna')
    (479, '                ]')
    (480, '            ):')
    (481, '                value = config["experiments"][project]["configs"][conf]["sampling"][')
    (482, '                    dna_or_rna')
    (483, '                ][command]')
    (484, '                if isinstance(value, int):')
    (485, '                    return "--%s %d" % (command, value)')
    (486, '                else:')
    (487, '                    return "--%s %f" % (command, value)')
    (488, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if useSampling(project, conf, dna_or_rna)']
    (472, 'def counts_getSamplingConfig(project, conf, dna_or_rna, command):')
    (473, '    if useSampling(project, conf, dna_or_rna):')
    (474, '        if dna_or_rna in config["experiments"][project]["configs"][conf]["sampling"]:')
    (475, '            if (')
    (476, '                command')
    (477, '                in config["experiments"][project]["configs"][conf]["sampling"][')
    (478, '                    dna_or_rna')
    (479, '                ]')
    (480, '            ):')
    (481, '                value = config["experiments"][project]["configs"][conf]["sampling"][')
    (482, '                    dna_or_rna')
    (483, '                ][command]')
    (484, '                if isinstance(value, int):')
    (485, '                    return "--%s %d" % (command, value)')
    (486, '                else:')
    (487, '                    return "--%s %f" % (command, value)')
    (488, '')
    (489, '    return ""')
    (490, '')
    (491, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if raw_or_assigned == "counts"', 'if useSampling(project, conf, rna_or_dna)']
    (492, 'def getFinalCounts(project, conf, rna_or_dna, raw_or_assigned):')
    (493, '    output = ""')
    (494, '    if raw_or_assigned == "counts":')
    (495, '        if useSampling(project, conf, rna_or_dna):')
    (496, '            output = (')
    (497, '                "results/experiments/{project}/%s/{condition}_{replicate}_%s_final_counts.sampling.{config}.tsv.gz"')
    (498, '                % (raw_or_assigned, rna_or_dna)')
    (499, '            )')
    (500, '')
    (501, '        else:')
    (502, '            output = (')
    (503, '                "results/experiments/{project}/%s/{condition}_{replicate}_%s_final_counts.tsv.gz"')
    (504, '                % (raw_or_assigned, rna_or_dna)')
    (505, '            )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if raw_or_assigned == "counts"', 'else']
    (492, 'def getFinalCounts(project, conf, rna_or_dna, raw_or_assigned):')
    (493, '    output = ""')
    (494, '    if raw_or_assigned == "counts":')
    (495, '        if useSampling(project, conf, rna_or_dna):')
    (496, '            output = (')
    (497, '                "results/experiments/{project}/%s/{condition}_{replicate}_%s_final_counts.sampling.{config}.tsv.gz"')
    (498, '                % (raw_or_assigned, rna_or_dna)')
    (499, '            )')
    (500, '')
    (501, '        else:')
    (502, '            output = (')
    (503, '                "results/experiments/{project}/%s/{condition}_{replicate}_%s_final_counts.tsv.gz"')
    (504, '                % (raw_or_assigned, rna_or_dna)')
    (505, '            )')
    (506, '    else:')
    (507, '        output = (')
    (508, '            "results/experiments/{project}/%s/{condition}_{replicate}_%s_final_counts.config.{config}.tsv.gz"')
    (509, '            % (raw_or_assigned, rna_or_dna)')
    (510, '        )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if raw_or_assigned == "counts"']
    (492, 'def getFinalCounts(project, conf, rna_or_dna, raw_or_assigned):')
    (493, '    output = ""')
    (494, '    if raw_or_assigned == "counts":')
    (495, '        if useSampling(project, conf, rna_or_dna):')
    (496, '            output = (')
    (497, '                "results/experiments/{project}/%s/{condition}_{replicate}_%s_final_counts.sampling.{config}.tsv.gz"')
    (498, '                % (raw_or_assigned, rna_or_dna)')
    (499, '            )')
    (500, '')
    (501, '        else:')
    (502, '            output = (')
    (503, '                "results/experiments/{project}/%s/{condition}_{replicate}_%s_final_counts.tsv.gz"')
    (504, '                % (raw_or_assigned, rna_or_dna)')
    (505, '            )')
    (506, '    else:')
    (507, '        output = (')
    (508, '            "results/experiments/{project}/%s/{condition}_{replicate}_%s_final_counts.config.{config}.tsv.gz"')
    (509, '            % (raw_or_assigned, rna_or_dna)')
    (510, '        )')
    (511, '    return output')
    (512, '')
    (513, '')
    (514, '# assigned_counts.smk specific functions')
    (515, '')
    (516, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if "sampling" in config["experiments"][project]["assignments"][assignment]']
    (517, 'def assignedCounts_getAssignmentSamplingConfig(project, assignment, command):')
    (518, '    if "sampling" in config["experiments"][project]["assignments"][assignment]:')
    (519, '        if (')
    (520, '            command')
    (521, '            in config["experiments"][project]["assignments"][assignment]["sampling"]')
    (522, '        ):')
    (523, '            value = config["experiments"][project]["assignments"][assignment][')
    (524, '                "sampling"')
    (525, '            ][command]')
    (526, '            if isinstance(value, int):')
    (527, '                return "--%s %d" % (command, value)')
    (528, '            else:')
    (529, '                return "--%s %f" % (command, value)')
    (530, '')
    (531, '    return ""')
    (532, '')
    (533, '')
    (534, '# statistic.smk specific functions')
    (535, '')
    (536, '')
    (537, '# get all counts of experiment (rule statistic_counts)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=rx32940/BactPrep, file=workflow/Snakefile
context_key: ['if gff_dir_self']
    (37, 'def get_gff_dir(gff_dir_self):')
    (38, '    if gff_dir_self:')
    (39, '        return gff_dir_self')
    (40, '    else:')
    (41, '        return os.path.join(out_dir , "gff/")')
    (42, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=rx32940/BactPrep, file=workflow/Snakefile
context_key: ['if gff_dir_self']
    (45, 'def get_sample_dir(gff_dir_self):')
    (46, '    if gff_dir_self:')
    (47, '        return gff_dir')
    (48, '    else:')
    (49, '        return asm_dir')
    (50, '')
    (51, '')
    (52, '# get all sample ID from either assembly file names or gff file names (if provided)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=davidries84/bismark_snakemake, file=Snakefile_backup2
context_key: ['if len(fastqs) == 2']
    (39, 'def get_fastq(wildcards):')
    (40, '    """Get fastq files of given sample-unit."""')
    (41, '    fastqs = UNITS.loc[(wildcards.sample, wildcards.unit), ["fq1", "fq2"]].dropna()')
    (42, '    print (fastqs)')
    (43, '    if len(fastqs) == 2:')
    (44, '        return {"r1": fastqs.fq1, "r2": fastqs.fq2}')
    (45, '    return {"r1": fastqs.fq1}')
    (46, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=davidries84/bismark_snakemake, file=Snakefile
context_key: ['if len(fastqs) == 2']
    (45, 'def get_fastq(wildcards):')
    (46, '    """Get fastq files of given sample-unit."""')
    (47, '    fastqs = UNITS.loc[(wildcards.sample, wildcards.unit), ["fq1", "fq2"]].dropna()')
    (48, '')
    (49, '    if len(fastqs) == 2:')
    (50, '        return {"r1": fastqs.fq1, "r2": fastqs.fq2}')
    (51, '    return {"r1": fastqs.fq1}')
    (52, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=davidries84/bismark_snakemake, file=Snakefile_backup
context_key: ['if os.path.exists(f) and os.path.exists(f.replace("_R1_001.fastq.gz","_R2_001.fastq.gz"))']
    (86, 'def existFile(wildcards):')
    (87, '    l = []')
    (88, '    all = expand("0_raw_reads/{id}_{sample}_{run}_{barcode}_{bla}_{lane}_R1_001.fastq.gz", sample = wildcards.sample, id = IDS, lane = LANES, run = RUNS, barcode = BARCODES, bla = BLAS)')
    (89, '    #print (all)')
    (90, '    for f in all:')
    (91, '')
    (92, '        if os.path.exists(f) and os.path.exists(f.replace("_R1_001.fastq.gz","_R2_001.fastq.gz")):')
    (93, '            print (f)')
    (94, '            l.append(f)')
    (95, '#        else:')
    (96, '#            print ("Does not exist")')
    (97, '    return l')
    (98, '')
    (99, '')
    (100, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=davidries84/bismark_snakemake, file=Snakefile_backup
context_key: ['if os.path.exists(f)']
    (119, 'def mergeInputR1(wildcards):')
    (120, '    l = []')
    (121, '    all = expand("0_raw_reads/{id}_{sample}_{run}_{barcode}_{bla}_{lane}_R1_001.fastq.gz", sample = wildcards.sample, id = IDS, lane = LANES, run = RUNS, barcode = BARCODES, bla = BLAS)')
    (122, '    #print (all)')
    (123, '    for f in all:')
    (124, '')
    (125, '        if os.path.exists(f):')
    (126, '#            print (f, f.replace("_001.fastq.gz", "_001_val_1.fq.gz").replace("0_raw_reads/", "1_trimmed_reads/"))')
    (127, '            l.append(f.replace("_001.fastq.gz", "_001_val_1.fq.gz").replace("0_raw_reads/", "1_trimmed_reads/"))')
    (128, '#        else:')
    (129, '#            print ("Does not exist")')
    (130, '    return l')
    (131, '')
    (132, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=davidries84/bismark_snakemake, file=Snakefile_backup
context_key: ['if os.path.exists(f)']
    (133, 'def mergeInputR2(wildcards):')
    (134, '    l = []')
    (135, '    all = expand("0_raw_reads/{id}_{sample}_{run}_{barcode}_{bla}_{lane}_R2_001.fastq.gz", sample = wildcards.sample, id = IDS, lane = LANES, run = RUNS, barcode = BARCODES, bla = BLAS)')
    (136, '    for f in all:')
    (137, '        if os.path.exists(f):')
    (138, '#            print (f, f.replace("_001.fastq.gz", "_001_val_2.fq.gz").replace("0_raw_reads/", "1_trimmed_reads/"))')
    (139, '            l.append(f.replace("_001.fastq.gz", "_001_val_2.fq.gz").replace("0_raw_reads/", "1_trimmed_reads/"))')
    (140, '    return l')
    (141, '')
    (142, '')
    (143, '')
    (144, '#1_trimmed_reads/2289_C_run381_CAAAAGAT_S3_L001_R2_001_val_1.fq.gz')
    (145, '')
    (146, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=percyfal/assemblyeval-smk, file=workflow/rules/common.smk
context_key: ['if infile is None']
    (28, 'def _read_tsv(infile, index, schema):')
    (29, '    if infile is None:')
    (30, '        return None')
    (31, '    df = pd.read_csv(infile, sep="\\\\t").set_index(index, drop=False)')
    (32, '    df = df.replace({np.nan: None})')
    (33, '    df.index.names = index')
    (34, '    validate(df, schema=schema)')
    (35, '    return df')
    (36, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if len(file) != 1']
    (118, 'def get_orig_r1_fq(wildcards):')
    (119, '    """Return original R1 fastq with path based on filename')
    (120, '')
    (121, '    Note there are some assumptions here (files always end with')
    (122, '    R1_001.fastq.gz; only one R1 fq per directory).  Same for')
    (123, '    following function.  This assumption should hold true even')
    (124, '    for historic projects, which had seq or extraction duplicates')
    (125, '    run in new folders.')
    (126, '')
    (127, '    Note that assembling the absolute path to a fastq is a bit')
    (128, '    complex; however, this pattern is automatically generated')
    (129, '    and not expected to change in the forseeable future.')
    (130, '    """')
    (131, '    (runID, projID) = sampleDict[wildcards.sample]')
    (132, "    p = fastq_abs_path + runID + \\'/CASAVA/L1/Project_\\' + projID + \\'/Sample_\\' + wildcards.sample + \\'/\\'")
    (133, "    file = [f for f in os.listdir(p) if f.endswith(\\'R1_001.fastq.gz\\')]")
    (134, '    if len(file) != 1:')
    (135, "        sys.exit(\\'ERROR: More than one R1 fastq detected in \\' + p)")
    (136, '    return p + file[0]')
    (137, '')
    (138, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if len(file) != 1']
    (139, 'def get_orig_r2_fq(wildcards):')
    (140, '    """Return original R2 fastq with path based on filename')
    (141, '    See above function for more detail.')
    (142, '    """')
    (143, '    (runID, projID) = sampleDict[wildcards.sample]')
    (144, "    p = fastq_abs_path + runID + \\'/CASAVA/L1/Project_\\' + projID + \\'/Sample_\\' + wildcards.sample + \\'/\\'")
    (145, "    file = [f for f in os.listdir(p) if f.endswith(\\'R2_001.fastq.gz\\')]")
    (146, '    if len(file) != 1:')
    (147, "        sys.exit(\\'ERROR: More than one R2 fastq detected in \\' + p)")
    (148, '    return p + file[0]')
    (149, '')
    (150, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ["if not fq1.endswith(\\'.gz\\')"]
    (151, 'def get_external_r1_fq(wildcards):')
    (152, '    """')
    (153, '    """')
    (154, '    (runID, projID, fq1, fq2) = sampleDict[wildcards.sample]')
    (155, "    if not fq1.endswith(\\'.gz\\'):")
    (156, "        sys.exit(\\'ERROR: Please use gzipped fastqs for this pipeline\\')")
    (157, '    return fq1')
    (158, '')
    (159, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ["if not fq2.endswith(\\'.gz\\')"]
    (160, 'def get_external_r2_fq(wildcards):')
    (161, '    """')
    (162, '    """')
    (163, '    (runID, projID, fq1, fq2) = sampleDict[wildcards.sample]')
    (164, "    if not fq2.endswith(\\'.gz\\'):")
    (165, "        sys.exit(\\'ERROR: Please use gzipped fastqs for this pipeline\\')")
    (166, '    return fq2')
    (167, '')
    (168, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule all', 'input']
    (189, 'def get_ref_full_path(wildcards):')
    (190, '    """')
    (191, '    """')
    (192, '    (refFullPath) = refDict[wildcards.ref]')
    (193, '    return refFullPath')
    (194, '')
    (195, '')
    (196, 'if not Q2_2017:')
    (197, '    rule all:')
    (198, '        input:')
    (199, "            expand(out_dir + \\'fastqs/\\' + \\'{sample}_R1.fastq.gz\\', sample=sampleDict.keys()),")
    (200, "            expand(out_dir + \\'fastqs/\\' + \\'{sample}_R2.fastq.gz\\', sample=sampleDict.keys()),")
    (201, "            expand(out_dir + \\'import_and_demultiplex/{runID}.qzv\\',runID=RUN_IDS),")
    (202, "            out_dir + \\'denoising/feature_tables/merged.qzv\\',")
    (203, "            out_dir + \\'denoising/sequence_tables/merged.qzv\\',")
    (204, "            expand(out_dir + \\'diversity_core_metrics/{ref}/alpha_diversity_metadata.qzv\\', ref=refDict.keys()),")
    (205, "            expand(out_dir + \\'diversity_core_metrics/{ref}/rarefaction.qzv\\', ref=refDict.keys()),")
    (206, "            expand(out_dir + \\'taxonomic_classification/{ref}/taxa.qzv\\', ref=refDict.keys()),")
    (207, "            # expand(out_dir + \\'taxonomic_classification/{ref}/barplots.qzv\\', ref=refDict.keys()),")
    (208, "            expand(out_dir + \\'denoising/stats/{runID}.qzv\\', runID=RUN_IDS),")
    (209, "            out_dir + \\'denoising/feature_tables/feature-table.from_biom.txt\\',")
    (210, "            expand(out_dir + \\'bacteria_only/feature_tables/{ref}/feature-table.from_biom.txt\\', ref=refDict.keys()),")
    (211, "            out_dir + \\'read_feature_and_sample_filtering/feature_tables/1_remove_samples_with_low_read_count.qzv\\',")
    (212, "            out_dir + \\'read_feature_and_sample_filtering/sequence_tables/1_remove_samples_with_low_read_count.qzv\\',")
    (213, "            expand(out_dir + \\'taxonomic_classification/{ref}/barplots_data_files/level-7.csv\\', ref=refDict.keys()),")
    (214, "            expand(out_dir + \\'taxonomic_classification_bacteria_only/{ref}/barplots_data_files/level-7.csv\\', ref=refDict.keys()),")
    (215, "            # expand(out_dir + \\'taxonomic_classification_bacteria_only/{ref}/barplots.qzv\\', ref=refDict.keys()),")
    (216, "            expand(out_dir + \\'bacteria_only/feature_tables/{ref}/merged.qzv\\', ref=refDict.keys())#,")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sravel/RattleSNP, file=rattleSNP/snakefiles/Snakefile
context_key: ['if rattlesnp.cleaning_activated']
    (76, 'def get_fastq_file():')
    (77, '    """return if file provide from cleaning or direct sample"""')
    (78, '')
    (79, '    dico_mapping = {')
    (80, '            "fasta": reference_file,')
    (81, '             "index": rules.bwa_index.output.index')
    (82, '            }')
    (83, '    if rattlesnp.cleaning_activated:')
    (84, '        dico_mapping.update({')
    (85, '            "R1" : rules.run_atropos.output.R1,')
    (86, '            "R2" : rules.run_atropos.output.R2')
    (87, '        })')
    (88, '    else:')
    (89, '        dico_mapping.update({')
    (90, '            "R1" :f"{fastq_dir}{{samples}}_R1.fastq.gz",')
    (91, '            "R2" : f"{fastq_dir}{{samples}}_R2.fastq.gz"')
    (92, '        })')
    (93, '    # print(dico_mapping)')
    (94, '    return dico_mapping')
    (95, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sravel/RattleSNP, file=rattleSNP/snakefiles/Snakefile
context_key: ['if rattlesnp.cleaning_activated and not fastqc_activated']
    (96, 'def output_final(wildcards):')
    (97, '    """FINAL RULE"""')
    (98, '    dico_final = {}')
    (99, '    if rattlesnp.cleaning_activated and not fastqc_activated:')
    (100, '        dico_final.update({')
    (101, '            "atropos_files_R1" : expand(rules.run_atropos.output.R1, samples = rattlesnp.samples),')
    (102, '            "atropos_files_R2" : expand(rules.run_atropos.output.R2, samples = rattlesnp.samples),')
    (103, '        })')
    (104, '    if fastqc_activated:')
    (105, '        dico_final.update({')
    (106, '            "fastQC_files_R1": expand(rules.run_fastqc.output.R1,samples=rattlesnp.samples),')
    (107, '            "fastQC_files_R2": expand(rules.run_fastqc.output.R2,samples=rattlesnp.samples),')
    (108, '        })')
    (109, '    if rattlesnp.mapping_activated and not rattlesnp.mapping_stats_activated:')
    (110, '        dico_final.update({')
    (111, '            "bam": expand( f"{out_dir}1_mapping/{rattlesnp.mapping_tool_activated}/{{samples}}_sorted.bam",samples=rattlesnp.samples),')
    (112, '        })')
    (113, '    if rattlesnp.mapping_stats_activated:')
    (114, '        dico_final.update({')
    (115, '            "bam": expand(f"{out_dir}1_mapping/{rattlesnp.mapping_tool_activated}/{{samples}}_sorted.bam",samples=rattlesnp.samples),')
    (116, '            "report": f"{out_dir}1_mapping/STATS/report.html"')
    (117, '        })')
    (118, '    if rattlesnp.calling_activated and not rattlesnp.vcf_filter_activated:')
    (119, '        dico_final.update({')
    (120, '            "vcf_file": f\\\'{out_dir}2_snp_calling/All_samples_GenotypeGVCFs_WITHOUT_MITO_raw.vcf.gz\\\',')
    (121, '            "report_vcf_raw": expand(f"{out_dir}3_all_snp_calling_stats/report_vcf{{vcf_suffix}}.html",vcf_suffix="_raw")')
    (122, '        })')
    (123, '    if rattlesnp.vcf_filter_activated:')
    (124, '        dico_final.update({')
    (125, '            "report_vcf_filter": expand(f"{out_dir}3_all_snp_calling_stats/report_vcf{{vcf_suffix}}.html", vcf_suffix=config[\\\'PARAMS\\\'][\\\'FILTER_SUFFIX\\\']),')
    (126, '            #"fasta": expand(f\\\'{out_dir}5_fasta_file/All_samples_GenotypeGVCFs_filter{{vcf_suffix}}.fasta\\\', vcf_suffix=config[\\\'PARAMS\\\'][\\\'FILTER_SUFFIX\\\'])')
    (127, '            "geno" :  expand(f\\\'{out_dir}7_geno_file/All_samples_GenotypeGVCFs_filter{{vcf_suffix}}.geno\\\', vcf_suffix=config[\\\'PARAMS\\\'][\\\'FILTER_SUFFIX\\\'])')
    (128, '        })')
    (129, '    if rattlesnp.vcf_path:')
    (130, '        dico_final.update({')
    (131, '            "report_vcf_user": expand(f"{out_dir}3_all_snp_calling_stats/report_vcf{{vcf_suffix}}.html", vcf_suffix="_user")')
    (132, '        })')
    (133, '    if rattlesnp.run_RAXML and rattlesnp.vcf_filter_activated:')
    (134, '        dico_final.update({')
    (135, '            "RAXML": expand(f\\\'{out_dir}6_raxml/filter{{vcf_suffix}}/RAxML_bestTree.All_samples_GenotypeGVCFs_filter{{vcf_suffix}}\\\', vcf_suffix=config[\\\'PARAMS\\\'][\\\'FILTER_SUFFIX\\\'])')
    (136, '        })')
    (137, '    if rattlesnp.run_RAXML_NG and rattlesnp.vcf_filter_activated:')
    (138, '        dico_final.update({')
    (139, '            "RAXML_NG": expand(f\\\'{out_dir}6_raxml_ng/filter{{vcf_suffix}}/All_samples_GenotypeGVCFs_filter{{vcf_suffix}}.raxml.bestTree\\\', vcf_suffix=config[\\\'PARAMS\\\'][\\\'FILTER_SUFFIX\\\'])')
    (140, '        })')
    (141, '    # pp.pprint(dico_final)')
    (142, '    return dico_final')
    (143, '')
    (144, '')
    (145, '# --- Main Build Rules --- #')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sravel/RattleSNP, file=rattleSNP/snakefiles/Snakefile
context_key: ['if rattlesnp.vcf_path and wildcards.vcf_suffix == "_user"']
    (855, 'def vcf_to_stats(wildcards):')
    (856, '    if rattlesnp.vcf_path and wildcards.vcf_suffix == "_user":')
    (857, '        return rattlesnp.vcf_path')
    (858, '    elif wildcards.vcf_suffix == "_raw":')
    (859, '        return rules.bcftools_concat.output.vcf_file')
    (860, "    elif wildcards.vcf_suffix in config[\\'PARAMS\\'][\\'FILTER_SUFFIX\\']:")
    (861, '        return rules.vcftools_filter.output.vcf_file')
    (862, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=marcoralab/imputePipeline, file=workflow/Snakefile
context_key: ['if len(chrs) == 2']
    (13, 'def parse_chrom(chrs):')
    (14, '    clist = [x.split(":") for x in chrs.split(",")]')
    (15, '    parsed = []')
    (16, '    for chrs in clist:')
    (17, '        if len(chrs) == 2:')
    (18, '            chrs = [str(c) for c in range(int(chrs[0]), int(chrs[1]) + 1)]')
    (19, '        elif len(chrs) != 1:')
    (20, '            raise ValueError("Invalid chromosome list.")')
    (21, '        parsed += chrs')
    (22, '    return parsed')
    (23, '')
    (24, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=marcoralab/imputePipeline, file=workflow/Snakefile
context_key: ["if \\'chr\\' in config"]
    (13, 'def parse_chrom(chrs):')
    (14, '    clist = [x.split(":") for x in chrs.split(",")]')
    (15, '    parsed = []')
    (16, '    for chrs in clist:')
    (17, '        if len(chrs) == 2:')
    (18, '            chrs = [str(c) for c in range(int(chrs[0]), int(chrs[1]) + 1)]')
    (19, '        elif len(chrs) != 1:')
    (20, '            raise ValueError("Invalid chromosome list.")')
    (21, '        parsed += chrs')
    (22, '    return parsed')
    (23, '')
    (24, '')
    (25, "if \\'chr\\' in config:")
    (26, '    raise ValueError(')
    (27, '        "Outdated CHR sepec in config. Please use chroms: \\\'from:to,other\\\'")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=marcoralab/imputePipeline, file=workflow/Snakefile
context_key: ["elif \\'chroms\\' not in config"]
    (13, 'def parse_chrom(chrs):')
    (14, '    clist = [x.split(":") for x in chrs.split(",")]')
    (15, '    parsed = []')
    (16, '    for chrs in clist:')
    (17, '        if len(chrs) == 2:')
    (18, '            chrs = [str(c) for c in range(int(chrs[0]), int(chrs[1]) + 1)]')
    (19, '        elif len(chrs) != 1:')
    (20, '            raise ValueError("Invalid chromosome list.")')
    (21, '        parsed += chrs')
    (22, '    return parsed')
    (23, '')
    (24, '')
    (25, "if \\'chr\\' in config:")
    (26, '    raise ValueError(')
    (27, '        "Outdated CHR sepec in config. Please use chroms: \\\'from:to,other\\\'")')
    (28, "elif \\'chroms\\' not in config:")
    (29, "    CHROM = parse_chrom(\\'1:22\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=marcoralab/imputePipeline, file=workflow/Snakefile
context_key: ["if \\'COHORT\\' in config and config[\\'COHORT\\']"]
    (13, 'def parse_chrom(chrs):')
    (14, '    clist = [x.split(":") for x in chrs.split(",")]')
    (15, '    parsed = []')
    (16, '    for chrs in clist:')
    (17, '        if len(chrs) == 2:')
    (18, '            chrs = [str(c) for c in range(int(chrs[0]), int(chrs[1]) + 1)]')
    (19, '        elif len(chrs) != 1:')
    (20, '            raise ValueError("Invalid chromosome list.")')
    (21, '        parsed += chrs')
    (22, '    return parsed')
    (23, '')
    (24, '')
    (25, "if \\'chr\\' in config:")
    (26, '    raise ValueError(')
    (27, '        "Outdated CHR sepec in config. Please use chroms: \\\'from:to,other\\\'")')
    (28, "elif \\'chroms\\' not in config:")
    (29, "    CHROM = parse_chrom(\\'1:22\\')")
    (30, 'else:')
    (31, "    CHROM = parse_chrom(config[\\'chroms\\'])")
    (32, '')
    (33, '# --- Process input files from config ---')
    (34, '')
    (35, "INPATH = os.path.normpath(config[\\'directory\\'])")
    (36, '')
    (37, "if \\'COHORT\\' in config and config[\\'COHORT\\']:")
    (38, '    COHORT = config["COHORT"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=marcoralab/imputePipeline, file=workflow/Snakefile
context_key: ["elif \\'SAMPLES\\' in config and config[\\'SAMPLES\\']"]
    (13, 'def parse_chrom(chrs):')
    (14, '    clist = [x.split(":") for x in chrs.split(",")]')
    (15, '    parsed = []')
    (16, '    for chrs in clist:')
    (17, '        if len(chrs) == 2:')
    (18, '            chrs = [str(c) for c in range(int(chrs[0]), int(chrs[1]) + 1)]')
    (19, '        elif len(chrs) != 1:')
    (20, '            raise ValueError("Invalid chromosome list.")')
    (21, '        parsed += chrs')
    (22, '    return parsed')
    (23, '')
    (24, '')
    (25, "if \\'chr\\' in config:")
    (26, '    raise ValueError(')
    (27, '        "Outdated CHR sepec in config. Please use chroms: \\\'from:to,other\\\'")')
    (28, "elif \\'chroms\\' not in config:")
    (29, "    CHROM = parse_chrom(\\'1:22\\')")
    (30, 'else:')
    (31, "    CHROM = parse_chrom(config[\\'chroms\\'])")
    (32, '')
    (33, '# --- Process input files from config ---')
    (34, '')
    (35, "INPATH = os.path.normpath(config[\\'directory\\'])")
    (36, '')
    (37, "if \\'COHORT\\' in config and config[\\'COHORT\\']:")
    (38, '    COHORT = config["COHORT"]')
    (39, "elif \\'SAMPLES\\' in config and config[\\'SAMPLES\\']:")
    (40, '    COHORT = config["SAMPLES"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=marcoralab/imputePipeline, file=workflow/Snakefile
context_key: ['if not (x in config and config[x] and config[x] is not None)']
    (51, '    def filtstr(x):')
    (52, '        if not (x in config and config[x] and config[x] is not None):')
    (53, '            return None')
    (54, '        elif os.path.isfile(config[x]):')
    (55, "            paramstr = \\'keep\\' if x == \\'include_samp\\' else \\'remove\\'")
    (56, "            return \\'--{} {}\\'.format(paramstr, os.path.normpath(config[x]))")
    (57, '        else:')
    (58, "            filt = \\'inclusion\\' if x == \\'include_samp\\' else \\'exclusion\\'")
    (59, '            raise Exception("Invalid {} list: {}.".format(filt, config[x]))')
    (60, "    x = [filtstr(x) for x in [\\'include_samp\\', \\'exclude_samp\\']]")
    (61, '    x = [i for i in x if i is not None]')
    (62, "    return \\' \\'.join(x) if x else \\'\\'")
    (63, '')
    (64, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=marcoralab/imputePipeline, file=workflow/Snakefile
context_key: ["if \\'imputation\\' in config and \\'default\\' in config[\\'imputation\\']"]
    (51, '    def filtstr(x):')
    (52, '        if not (x in config and config[x] and config[x] is not None):')
    (53, '            return None')
    (54, '        elif os.path.isfile(config[x]):')
    (55, "            paramstr = \\'keep\\' if x == \\'include_samp\\' else \\'remove\\'")
    (56, "            return \\'--{} {}\\'.format(paramstr, os.path.normpath(config[x]))")
    (57, '        else:')
    (58, "            filt = \\'inclusion\\' if x == \\'include_samp\\' else \\'exclusion\\'")
    (59, '            raise Exception("Invalid {} list: {}.".format(filt, config[x]))')
    (60, "    x = [filtstr(x) for x in [\\'include_samp\\', \\'exclude_samp\\']]")
    (61, '    x = [i for i in x if i is not None]')
    (62, "    return \\' \\'.join(x) if x else \\'\\'")
    (63, '')
    (64, '')
    (65, 'keep_remove_command = sampfilt(config)')
    (66, '')
    (67, '# --- Process imputation settings ---')
    (68, '')
    (69, 'imputation_defaults = {')
    (70, "    \\'nih\\': {")
    (71, "        \\'server\\': \\'NIH\\',")
    (72, "        \\'refpanel\\': \\'topmed-r2\\',")
    (73, "        \\'population\\': \\'all\\'},")
    (74, "    \\'michigan\\': {")
    (75, "        \\'server\\': \\'Michigan\\',")
    (76, "        \\'refpanel\\': \\'hrc-r1.1\\',")
    (77, "        \\'population\\': \\'mixed\\'}}")
    (78, '')
    (79, "if \\'imputation\\' in config and \\'default\\' in config[\\'imputation\\']:")
    (80, "    default_cfg = config[\\'imputation\\'][\\'default\\']")
    (81, "    if ( \\'server\\' in default_cfg and default_cfg[\\'server\\'].lower() == \\'michigan\\'")
    (82, "         or (\\'refpanel\\' in default_cfg")
    (83, "             and default_cfg[\\'refpanel\\'].lower() == \\'hrc-r1.1\\')):")
    (84, "        default_imp = imputation_defaults[\\'michigan\\']")
    (85, "    elif (\\'refpanel\\' in default_cfg and")
    (86, "          default_cfg[\\'refpanel\\'].lower() == \\'topmed-r2\\'):")
    (87, "        default_imp = imputation_defaults[\\'nih\\']")
    (88, "    elif (\\'refpanel\\' in default_cfg and default_cfg[\\'refpanel\\']")
    (89, "          and \\'server\\' in default_cfg and default_cfg[\\'server\\']")
    (90, "          and \\'population\\' in default_cfg and default_cfg[\\'population\\']):")
    (91, '        default_imp = default_cfg')
    (92, '    else:')
    (93, "        raise ValueError(\\'Must specify at least server or panel\\')")
    (94, '    default_imp.update(default_cfg)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=marcoralab/imputePipeline, file=workflow/Snakefile
context_key: ["if \\'imputation\\' in config and cht in config[\\'imputation\\']"]
    (51, '    def filtstr(x):')
    (52, '        if not (x in config and config[x] and config[x] is not None):')
    (53, '            return None')
    (54, '        elif os.path.isfile(config[x]):')
    (55, "            paramstr = \\'keep\\' if x == \\'include_samp\\' else \\'remove\\'")
    (56, "            return \\'--{} {}\\'.format(paramstr, os.path.normpath(config[x]))")
    (57, '        else:')
    (58, "            filt = \\'inclusion\\' if x == \\'include_samp\\' else \\'exclusion\\'")
    (59, '            raise Exception("Invalid {} list: {}.".format(filt, config[x]))')
    (60, "    x = [filtstr(x) for x in [\\'include_samp\\', \\'exclude_samp\\']]")
    (61, '    x = [i for i in x if i is not None]')
    (62, "    return \\' \\'.join(x) if x else \\'\\'")
    (63, '')
    (64, '')
    (65, 'keep_remove_command = sampfilt(config)')
    (66, '')
    (67, '# --- Process imputation settings ---')
    (68, '')
    (69, 'imputation_defaults = {')
    (70, "    \\'nih\\': {")
    (71, "        \\'server\\': \\'NIH\\',")
    (72, "        \\'refpanel\\': \\'topmed-r2\\',")
    (73, "        \\'population\\': \\'all\\'},")
    (74, "    \\'michigan\\': {")
    (75, "        \\'server\\': \\'Michigan\\',")
    (76, "        \\'refpanel\\': \\'hrc-r1.1\\',")
    (77, "        \\'population\\': \\'mixed\\'}}")
    (78, '')
    (79, "if \\'imputation\\' in config and \\'default\\' in config[\\'imputation\\']:")
    (80, "    default_cfg = config[\\'imputation\\'][\\'default\\']")
    (81, "    if ( \\'server\\' in default_cfg and default_cfg[\\'server\\'].lower() == \\'michigan\\'")
    (82, "         or (\\'refpanel\\' in default_cfg")
    (83, "             and default_cfg[\\'refpanel\\'].lower() == \\'hrc-r1.1\\')):")
    (84, "        default_imp = imputation_defaults[\\'michigan\\']")
    (85, "    elif (\\'refpanel\\' in default_cfg and")
    (86, "          default_cfg[\\'refpanel\\'].lower() == \\'topmed-r2\\'):")
    (87, "        default_imp = imputation_defaults[\\'nih\\']")
    (88, "    elif (\\'refpanel\\' in default_cfg and default_cfg[\\'refpanel\\']")
    (89, "          and \\'server\\' in default_cfg and default_cfg[\\'server\\']")
    (90, "          and \\'population\\' in default_cfg and default_cfg[\\'population\\']):")
    (91, '        default_imp = default_cfg')
    (92, '    else:')
    (93, "        raise ValueError(\\'Must specify at least server or panel\\')")
    (94, '    default_imp.update(default_cfg)')
    (95, 'else:')
    (96, "    default_imp = imputation_defaults[\\'nih\\']")
    (97, '')
    (98, 'imp_settings = dict()')
    (99, 'server = dict()')
    (100, 'token = dict()')
    (101, '')
    (102, 'for cht in COHORT:')
    (103, '    imp_settings[cht] = default_imp.copy()')
    (104, "    if \\'imputation\\' in config and cht in config[\\'imputation\\']:")
    (105, "        imp_settings[cht].update(config[\\'imputation\\'][cht])")
    (106, "    if \\'token\\' in imp_settings[cht]:")
    (107, "        token[cht] = imp_settings[cht].pop(\\'token\\')")
    (108, '    else:')
    (109, '        raise ValueError("Must provide either cohort or default API token.")')
    (110, "    server[cht] = imp_settings[cht].pop(\\'server\\')")
    (111, '')
    (112, '# --- Done processing ---')
    (113, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=marcoralab/imputePipeline, file=workflow/Snakefile
context_key: ['if not isinstance(el, list)']
    (118, 'def flatten(nested):')
    (119, '    flat = []')
    (120, '    for el in nested:')
    (121, '        if not isinstance(el, list):')
    (122, '            flat.append(el)')
    (123, '        else:')
    (124, '            flat += flatten(el)')
    (125, '    return flat')
    (126, '')
    (127, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tjbencomo/bulk-rnaseq, file=Snakefile
context_key: ['if isPE(wildcards)']
    (52, 'def get_fqs(wildcards):')
    (53, '    if isPE(wildcards):')
    (54, '        return {')
    (55, "            \\'fq1\\' : samples.loc[(wildcards.sample_id), \\'fq1\\'],")
    (56, "            \\'fq2\\' : samples.loc[(wildcards.sample_id), \\'fq2\\']")
    (57, '            }')
    (58, '    else:')
    (59, "        return {\\'fq\\' : samples.loc[(wildcards.sample_id), \\'fq1\\']}")
    (60, '')
    (61, '# def getStrand(wildcards):')
    (62, "#     if quant_program == \\'salmon\\':")
    (63, "#         return \\'Not using kallisto\\'")
    (64, '#     else:')
    (65, "#         if \\'strandedness\\' not in samples.columns:")
    (66, '#             raise ValueError("Set to use kallisto for quantification but not stranding specified!")')
    (67, "#     s = samples.loc[wildcards.sample_id, \\'strandedness\\'].lower()")
    (68, "#     if s == \\'forward\\' or s == \\'stranded\\':")
    (69, "#         return \\'--fr-stranded\\'")
    (70, "#     elif s == \\'reverse\\':")
    (71, "#         return \\'--rf-stranded\\'")
    (72, "#     elif s == \\'none\\' or s == \\'unstranded\\':")
    (73, "#         return \\'\\'")
    (74, '#     else:')
    (75, '#         raise ValueError(f"Unrecognized strand type for {wildcards.sample_id}")')
    (76, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tjbencomo/bulk-rnaseq, file=Snakefile
context_key: ["if pd.isna(samples.loc[wildcards.sample_id, \\'fq2\\'])"]
    (77, 'def isPE(wildcards):')
    (78, "    if pd.isna(samples.loc[wildcards.sample_id, \\'fq2\\']):")
    (79, '        return False')
    (80, '    else:')
    (81, '        return True')
    (82, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tjbencomo/bulk-rnaseq, file=Snakefile
context_key: ["if quant_program == \\'salmon\\'"]
    (83, 'def get_quants(wildcards):')
    (84, "    # if quant_program == \\'kallisto\\':")
    (85, '        # quants = [f"kallisto/{sid}" for sid in samples[\\\'id\\\']]')
    (86, "    if quant_program == \\'salmon\\':")
    (87, '        quants = [f"salmon/{sid}" for sid in samples[\\\'id\\\']]')
    (88, '    else:')
    (89, '        raise ValueError("quant_program must be \\\'salmon\\\'")')
    (90, "    return {\\'cts\\' : quants}")
    (91, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

