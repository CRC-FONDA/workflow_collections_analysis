repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if "all" in config.get("variant-calls", dict())']
    (4, 'if "all" in config.get("variant-calls", dict()):')
    (5, '    raise ValueError(')
    (6, '        "A callset given in the variant-calls section of the config may not be called \\\'all\\\'. "')
    (7, '        "Please choose a different name."')
    (8, '    )')
    (9, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if any']
    (23, 'if any(')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/Snakefile
context_key: ['if "variant-calls" in config', 'rule eval', 'input']
    (13, 'if "variant-calls" in config:')
    (14, '')
    (15, '    include: "rules/eval.smk"')
    (16, '')
    (17, '    rule eval:')
    (18, '        input:')
    (19, '            expand(')
    (20, '                "results/report/precision-recall/{benchmark}/{vartype}",')
    (21, '                benchmark=used_benchmarks,')
    (22, '                vartype=["snvs", "indels"],')
    (23, '            ),')
    (24, '            get_fp_fn_reports,')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=epigen/unsupervised_analysis, file=workflow/Snakefile
context_key: ['if len(config["features_to_plot"]) > 0']
    (32, 'if len(config["features_to_plot"]) > 0:')
    (33, "    umap_content.append(\\'features\\')")
    (34, "    densmap_content.append(\\'features\\')")
    (35, "    pca_content.append(\\'features\\')")
    (36, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/sd-divergence, file=workflow/Snakefile
context_key: ['if "gene_conversion" in config']
    (19, 'if "gene_conversion" in config:')
    (20, '    gc_df = pd.read_csv(config["gene_conversion"], sep="\\\\t")')
    (21, '    gc_df["hap"] = gc_df["hap"].astype(str)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/sd-divergence, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'if os.path.exists("temp")']
    (30, 'if TEMP_DIR != "temp":')
    (31, '    if os.path.exists("temp"):')
    (32, '        if os.path.islink("temp") and os.path.realpath("temp") == os.path.realpath(')
    (33, '            TEMP_DIR')
    (34, '        ):')
    (35, '            print("The temp dir has already been linked.")')
    (36, '        else:')
    (37, '            sys.exit("temp/ already in use, please move it before running.")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/sd-divergence, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'else']
    (38, '    else:')
    (39, '        shell("ln -s {TEMP_DIR} temp")')
    (40, '')
    (41, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=wharvey31/megalodon-smk, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'if os.path.exists("temp")']
    (18, 'if TEMP_DIR != "temp":')
    (19, '    if os.path.exists("temp"):')
    (20, '        if os.path.islink("temp") and os.path.realpath("temp") == os.path.realpath(')
    (21, '            TEMP_DIR')
    (22, '        ):')
    (23, '            print("The temp dir has already been linked.")')
    (24, '        else:')
    (25, '            sys.exit("temp/ already in use, please move it before running.")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=wharvey31/megalodon-smk, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'else']
    (26, '    else:')
    (27, '        shell("ln -s {TEMP_DIR} temp")')
    (28, '')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Xavster838/assembly_validation, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'if os.path.exists("temp")']
    (18, 'if TEMP_DIR != "temp":')
    (19, '    if os.path.exists("temp"):')
    (20, '        if os.path.islink("temp") and os.path.realpath("temp") == os.path.realpath(')
    (21, '            TEMP_DIR')
    (22, '        ):')
    (23, '            print("The temp dir has already been linked.")')
    (24, '        else:')
    (25, '            sys.exit("temp/ already in use, please move it before running.")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Xavster838/assembly_validation, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'else']
    (26, '    else:')
    (27, '        shell("ln -s {TEMP_DIR} temp")')
    (28, '')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ovewh/Climaso, file=workflow/Snakefile
context_key: ["if \\'betzy\\' in hostname"]
    (17, "if \\'betzy\\' in hostname:")
    (18, "    ROOT_PATH = \\'/cluster/shared/ESGF\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/SmkTemplate, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'if os.path.exists("temp")']
    (18, 'if TEMP_DIR != "temp":')
    (19, '    if os.path.exists("temp"):')
    (20, '        if os.path.islink("temp") and os.path.realpath("temp") == os.path.realpath(')
    (21, '            TEMP_DIR')
    (22, '        ):')
    (23, '            print("The temp dir has already been linked.")')
    (24, '        else:')
    (25, '            sys.exit("temp/ already in use, please move it before running.")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/SmkTemplate, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'else']
    (26, '    else:')
    (27, '        shell("ln -s {TEMP_DIR} temp")')
    (28, '')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/count_kmers.smk
context_key: ['if ends_with_gz']
    (8, 'if ends_with_gz:')
    (9, '    R1_OUT= "results/reads/{sample}/{library}_1.fastq.gz"')
    (10, '    R2_OUT= "results/reads/{sample}/{library}_2.fastq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/count_kmers.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule create_symlink', 'input']
    (16, 'if not config["settings"]["trimming"]["activate"]:')
    (17, '    rule create_symlink:')
    (18, '        input:')
    (19, '            fastqs= get_fastqs')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/count_kmers.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule create_symlink', 'output']
    (20, '        output:')
    (21, '            r1 = R1_OUT,')
    (22, '            r2 = R2_OUT')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/count_kmers.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule create_symlink', 'message']
    (23, '        message:')
    (24, '            "Creating symbolic links for fastq files..."')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/count_kmers.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule create_symlink', 'threads']
    (25, '        threads: 1')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/count_kmers.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule create_symlink', 'shell']
    (26, '        shell:')
    (27, '            """')
    (28, '            echo Working on fastq files: {input.fastqs}')
    (29, '            echo Symlink -fastq1: {input.fastqs[0]} to {output.r1}')
    (30, '            ln -rs {input.fastqs[0]} {output.r1}')
    (31, '            echo Symlink -fastq2: {input.fastqs[1]} to {output.r2}')
    (32, '            ln -rs {input.fastqs[1]} {output.r2}')
    (33, '            """')
    (34, '')
    (35, '# =================================================================================================')
    (36, '#     Generate Lists Of Inputs for KMC')
    (37, '# =================================================================================================')
    (38, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/count_kmers.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule generate_input_lists', 'input']
    (39, 'if not config["settings"]["trimming"]["activate"]:')
    (40, '    rule generate_input_lists:')
    (41, '        input:')
    (42, '            r1 = expand(R1_OUT, zip,')
    (43, '                        sample=sample_names,')
    (44, '                        library=library_names),')
    (45, '            r2 = expand(R2_OUT, zip,')
    (46, '                        sample=sample_names,')
    (47, '                        library=library_names),')
    (48, '            qc= "results/qc/multiqc.html"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/count_kmers.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule generate_input_lists', 'output']
    (49, '        output:')
    (50, '            "results/reads/{sample}/input_files.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/count_kmers.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule generate_input_lists', 'params']
    (51, '        params:')
    (52, '            prefix = get_input_path_for_generate_input_lists()')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/count_kmers.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule generate_input_lists', 'log']
    (53, '        log:')
    (54, '            "logs/generate_input_lists/{sample}/{sample}_generate_input_lists.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/count_kmers.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule generate_input_lists', 'message']
    (55, '        message:')
    (56, '            "Generating input list files..."')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/count_kmers.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule generate_input_lists', 'script']
    (57, '        script:')
    (58, '            "../scripts/generate_input_lists.py"')
    (59, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/fetch_reads.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule fetch_source_reads', 'input']
    (31, 'if not config["settings"]["trimming"]["activate"]:')
    (32, '    rule fetch_source_reads:')
    (33, '        input:')
    (34, '            kmers_tab = "results/filter_kmers/{phenos_filt}_kmers_table.txt",')
    (35, '            kmers_list = "results/fetch_kmers/{phenos_filt}_kmers_list.fa",')
    (36, '            fetch_reads = "scripts/external/fetch_reads_with_kmers/fetch_reads",')
    (37, '            filter_kmers = "results/filter_kmers/filter_kmers.done",')
    (38, '            r1 = expand(rules.create_symlink.output.r1, zip,')
    (39, '                            sample=sample_names,')
    (40, '                            library=library_names),')
    (41, '            r2 = expand(rules.create_symlink.output.r2, zip,')
    (42, '                            sample=sample_names,')
    (43, '                            library=library_names),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/fetch_reads.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule fetch_source_reads', 'output']
    (44, '        output:')
    (45, '            dir = directory("results/fetch_reads_with_kmers/{phenos_filt}"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/fetch_reads.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule fetch_source_reads', 'params']
    (46, '        params:')
    (47, '            kmers_list_prefix = lambda w, input: os.path.dirname(input.kmers_list),')
    (48, '            out_prefix = lambda w, output: os.path.dirname(output[0]),')
    (49, '            samples= sample_names,')
    (50, '            library= library_names,')
    (51, '            pheno = "{phenos_filt}",')
    (52, '            kmer_len = config["params"]["kmc"]["kmer_len"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/fetch_reads.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule fetch_source_reads', 'log']
    (53, '        log:')
    (54, '            "logs/fetch_reads_with_kmers/{phenos_filt}/fetch_source_reads_of_kmers.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/fetch_reads.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule fetch_source_reads', 'threads']
    (55, '        threads: ')
    (56, '            config["params"]["fetch_reads"]["threads"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/fetch_reads.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule fetch_source_reads', 'message']
    (57, '        message:')
    (58, '            "Fetching reads that contain significant k-mers find in {input.kmers_list}..."    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/fetch_reads.smk
context_key: ['if not config["settings"]["trimming"]["activate"]', 'rule fetch_source_reads', 'script']
    (59, '        script:')
    (60, '            "../scripts/fetch_source_reads.py"')
    (61, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/common.smk
context_key: ['if align_kmers']
    (279, '    if align_kmers:')
    (280, '        target_output.extend(')
    (281, '            expand(')
    (282, '                "results/align_kmers/align_kmers.done"')
    (283, '            )')
    (284, '        ),')
    (285, '    ')
    (286, '    if align_reads_with_kmers:')
    (287, '        target_output.extend(')
    (288, '            expand(')
    (289, '                "results/align_reads_with_kmers/align_reads_with_kmers.done"')
    (290, '            )')
    (291, '        ),')
    (292, '')
    (293, '    if assemble_reads_with_kmers:')
    (294, '        target_output.extend(')
    (295, '            expand(')
    (296, '                "results/align_contigs/align_contigs.done"')
    (297, '            )')
    (298, '        ),')
    (299, '')
    (300, '    if blast_assembled_reads:')
    (301, '        target_output.extend(')
    (302, '            expand(')
    (303, '                "results/blast_contigs/blast_contigs.done"')
    (304, '            )')
    (305, '        ),')
    (306, '')
    (307, '    return target_output')
    (308, '')
    (309, "# ================================================================================================='")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/Snakefile
context_key: ['onstart', 'try', 'if not os.path.exists(filename)']
    (53, '            if not os.path.exists(filename):')
    (54, '                raise FileNotFoundError(filename)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Xavster838/run_NucFreq, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'if os.path.exists("temp")']
    (18, 'if TEMP_DIR != "temp":')
    (19, '    if os.path.exists("temp"):')
    (20, '        if os.path.islink("temp") and os.path.realpath("temp") == os.path.realpath(')
    (21, '            TEMP_DIR')
    (22, '        ):')
    (23, '            print("The temp dir has already been linked.")')
    (24, '        else:')
    (25, '            sys.exit("temp/ already in use, please move it before running.")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Xavster838/run_NucFreq, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'else']
    (26, '    else:')
    (27, '        shell("ln -s {TEMP_DIR} temp")')
    (28, '')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vembrane/vembrane-benchmark, file=workflow/rules/common.smk
context_key: ['if scenario not in SCENARIOS']
    (29, '            if scenario not in SCENARIOS:')
    (30, '                SCENARIOS.append(scenario)')
    (31, '            for sample in SAMPLES:')
    (32, '                for filetype in data["filetypes"]:')
    (33, '                    VALID_WILDCARD_COMBINATIONS.append(')
    (34, '                        {')
    (35, '                            "tool": tool,')
    (36, '                            "annotation": annotation,')
    (37, '                            "mode": "direct",')
    (38, '                            "scenario": scenario,')
    (39, '                            "sample": sample,')
    (40, '                            "filetype": (')
    (41, '                                filetype,')
    (42, '                                filetype[:3] + (".gz" if filetype == "vcf_z" else ""),')
    (43, '                            ),')
    (44, '                            "filecode": filetype,')
    (45, '                            "fileextension": filetype[:3]')
    (46, '                            + (".gz" if filetype == "vcf_z" else ""),')
    (47, '                        }')
    (48, '                    )')
    (49, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/Rhodonite, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'if os.path.exists("temp")']
    (13, 'if TEMP_DIR != "temp":')
    (14, '    if os.path.exists("temp"):')
    (15, '        if os.path.islink("temp") and os.path.realpath("temp") == os.path.realpath(')
    (16, '            TEMP_DIR')
    (17, '        ):')
    (18, '            print("The temp dir has already been linked.")')
    (19, '        else:')
    (20, '            sys.exit("temp/ already in use, please move it before running.")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/Rhodonite, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'else']
    (21, '    else:')
    (22, '        shell("ln -s {TEMP_DIR} temp")')
    (23, '')
    (24, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/Rhodonite, file=workflow/Snakefile
context_key: ['if df.shape[0] > 0 and df.shape[0] < n_records']
    (51, '    if df.shape[0] > 0 and df.shape[0] < n_records:')
    (52, '        n_records = df.shape[0]')
    (53, '')
    (54, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=epigen/enrichment_analysis, file=workflow/Snakefile
context_key: ['if not os.path.exists(LOLA_path)']
    (24, 'if not os.path.exists(LOLA_path):')
    (25, '    print("start downloading and unpacking LOLA resources")')
    (26, '    os.makedirs(LOLA_path, exist_ok=True)')
    (27, '    ')
    (28, '    LOLA_path = os.path.abspath(LOLA_path)')
    (29, '    ')
    (30, "    URL_Core=\\'http://big.databio.org/regiondb/LOLACoreCaches_180412.tgz\\'")
    (31, "    URL_Ext=\\'http://big.databio.org/regiondb/LOLAExtCaches_170206.tgz\\'")
    (32, '    ')
    (33, '    # download')
    (34, "    getCore_str = \\'wget --directory-prefix={} {}\\'.format(LOLA_path, URL_Core)")
    (35, "    getExt_str = \\'wget --directory-prefix={} {}\\'.format(LOLA_path, URL_Ext)")
    (36, '    ')
    (37, '    subprocess.run(getCore_str, shell=True)')
    (38, '    subprocess.run(getExt_str, shell=True)')
    (39, '    ')
    (40, '    # unpack')
    (41, "    unpackCore_str = \\'tar zxvf {} -C {}\\'.format(os.path.join(LOLA_path, \\'LOLACoreCaches_180412.tgz\\'), LOLA_path)")
    (42, "    unpackExt_str = \\'tar zxvf {} -C {}\\'.format(os.path.join(LOLA_path, \\'LOLAExtCaches_170206.tgz\\'), LOLA_path)")
    (43, '    ')
    (44, '    subprocess.run(unpackCore_str, shell=True)')
    (45, '    subprocess.run(unpackExt_str, shell=True)')
    (46, '    ')
    (47, '    # remove')
    (48, "    removeCore_str = \\'rm -f {}\\'.format(os.path.join(LOLA_path, \\'LOLACoreCaches_180412.tgz\\'))")
    (49, "    removeExt_str = \\'rm -f {}\\'.format(os.path.join(LOLA_path, \\'LOLAExtCaches_170206.tgz\\'))")
    (50, '    ')
    (51, '    subprocess.run(removeCore_str, shell=True)')
    (52, '    subprocess.run(removeExt_str, shell=True)')
    (53, '    ')
    (54, '    print("finished downloading and unpacking LOLA resources")')
    (55, '')
    (56, '')
    (57, '### get annotations ')
    (58, '### for three data types: query genes, query regions and background regions (to be converted to genes)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Xavster838/get_and_annotate_locus, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'if os.path.exists("temp")']
    (18, 'if TEMP_DIR != "temp":')
    (19, '    if os.path.exists("temp"):')
    (20, '        if os.path.islink("temp") and os.path.realpath("temp") == os.path.realpath(')
    (21, '            TEMP_DIR')
    (22, '        ):')
    (23, '            print("The temp dir has already been linked.")')
    (24, '        else:')
    (25, '            sys.exit("temp/ already in use, please move it before running.")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Xavster838/get_and_annotate_locus, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'else']
    (26, '    else:')
    (27, '        shell("ln -s {TEMP_DIR} temp")')
    (28, '')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=shengwang/TransposonSnakemake, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'if os.path.exists("temp")']
    (18, 'if TEMP_DIR != "temp":')
    (19, '    if os.path.exists("temp"):')
    (20, '        if os.path.islink("temp") and os.path.realpath("temp") == os.path.realpath(')
    (21, '            TEMP_DIR')
    (22, '        ):')
    (23, '            print("The temp dir has already been linked.")')
    (24, '        else:')
    (25, '            sys.exit("temp/ already in use, please move it before running.")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=shengwang/TransposonSnakemake, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'else']
    (26, '    else:')
    (27, '        shell("ln -s {TEMP_DIR} temp")')
    (28, '')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nismod/open-gira, file=workflow/rules/download/hazards.smk
context_key: ['output', 'run', 'if re.match("https?']
    (17, '            if re.match("https?://", input_file):')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nismod/open-gira, file=workflow/rules/download/hazards.smk
context_key: ['output', 'run', 'if re.match("^https?']
    (29, '                    if re.match("^https?://", line):')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nismod/open-gira, file=workflow/rules/download/hazards.smk
context_key: ['else', 'if len(remote_files)']
    (43, '            if len(remote_files):')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nismod/open-gira, file=workflow/Snakefile
context_key: ['if any(["/" in h for h in config[\\\'hazard_datasets\\\'].keys()])']
    (22, 'if any(["/" in h for h in config[\\\'hazard_datasets\\\'].keys()]):')
    (23, '    raise ValueError("""Error in config: Hazard dataset names cannot contain / or _""")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nismod/open-gira, file=workflow/Snakefile
context_key: ['if any(["/" in h for h in config[\\\'network_filters\\\'].keys()])']
    (24, 'if any(["/" in h for h in config[\\\'network_filters\\\'].keys()]):')
    (25, '    raise ValueError("""Error in config: Network filter names cannot contain / or _""")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nismod/open-gira, file=workflow/Snakefile
context_key: ['if any(["/" in h for h in config[\\\'infrastructure_datasets\\\'].keys()])']
    (26, 'if any(["/" in h for h in config[\\\'infrastructure_datasets\\\'].keys()]):')
    (27, '    raise ValueError("""Error in config: Infrastructure dataset names cannot contain / or _""")')
    (28, '')
    (29, '# Number of slices to cut dataset into -- must be a square number')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nismod/open-gira, file=workflow/Snakefile
context_key: ["if not isinstance(config[\\'slice_count\\'], int) or \\"]
    (30, "if not isinstance(config[\\'slice_count\\'], int) or \\\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nismod/open-gira, file=workflow/Snakefile
context_key: ['if not os.path.exists(file_path)']
    (35, '    if not os.path.exists(file_path):')
    (36, '        raise FileNotFoundError((')
    (37, '            "Error in config: could not locate network_filter at "')
    (38, '            f"{os.path.join(os.getcwd(), file_path)}"')
    (39, '        ))')
    (40, '')
    (41, '# Constrain wildcards to NOT use _ or /')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if not "mutational_burden_events" in samples.columns']
    (22, 'if not "mutational_burden_events" in samples.columns:')
    (23, '    samples["mutational_burden_events"] = pd.NA')
    (24, '')
    (25, '# construct genome name')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/Snakefile
context_key: ['if is_activated("report/stratify")']
    (40, 'if is_activated("report/stratify"):')
    (41, '    batches = samples[config["report"]["stratify"]["by-column"]].unique()')
    (42, '')
    (43, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-mtb, file=workflow/Snakefile
context_key: ['if "normal" not in dna_seq_varlociraptor.samples["alias"].unique()']
    (21, 'if "normal" not in dna_seq_varlociraptor.samples["alias"].unique():')
    (22, '    list_keys = list(')
    (23, '        dna_seq_varlociraptor.config["calling"]["fdr-control"]["events"].keys()')
    (24, '    )')
    (25, '    for k in list_keys:')
    (26, '        if k.startswith("loh"):')
    (27, '            dna_seq_varlociraptor.config["calling"]["fdr-control"]["events"].pop(k)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-mtb, file=workflow/Snakefile
context_key: ['else', 'if tissue_counts["normal"] != tissue_counts["tumor"]']
    (30, '    if tissue_counts["normal"] != tissue_counts["tumor"]:')
    (31, '        raise WorkflowError(')
    (32, '            "Invalid definition of tumor and normal samples in sample sheet. Only tumor/normal or tumor only groups are allowed at once."')
    (33, '        )')
    (34, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-mtb, file=workflow/Snakefile
context_key: ['if not "ffpe" in dna_seq_varlociraptor.samples.columns']
    (35, 'if not "ffpe" in dna_seq_varlociraptor.samples.columns:')
    (36, '    dna_seq_varlociraptor.samples["ffpe"] = 0')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/imputation_glimpse.smk
context_key: ['if str2bool(recursive_get_and_test(["imputation", genome, "run"], ["False", "True"]))', 'if not chromosomes']
    (13, '    if str2bool(recursive_get_and_test(["imputation", genome, "run"], ["False", "True"])):')
    (14, '        num_imputations = int(recursive_get(["imputation", genome, "num_imputations"], 1))')
    (15, '')
    (16, '        # Imputation will be run by default on all chromosomes. The paramter below allow to select a subset of chromosomes.')
    (17, '        chromosomes = to_str(to_list(recursive_get(["imputation", genome, "chromosomes"], [])))')
    (18, '        if not chromosomes:')
    (19, '            chromosomes = get_chromosome_names(genome)')
    (20, '        else:')
    (21, '            if valid_chromosome_names(genome, chromosomes):')
    (22, '                LOGGER.error(')
    (23, '                    f"ERROR: In config[imputation][{genome}][chromosomes], the following chromosome names are not recognized: {valid_chromosome_names(genome, chromosomes)}!"')
    (24, '                )')
    (25, '                os._exit(1)')
    (26, '')
    (27, '        # This string contains a wildcard where we will place the name of the chromosome')
    (28, '        # something like "path/to/my/panel_chr{chr}.vcf.gz"')
    (29, '        path_panel = recursive_get(["imputation", genome, "path_panel"], "")')
    (30, '        for chr in chromosomes:')
    (31, '            file = path_panel.format(chr=chr)')
    (32, '            if not os.path.isfile(file):')
    (33, '                LOGGER.error(')
    (34, '                    f"ERROR: Panel file config[imputation][{genome}][path_panel] ({path_panel}) does not exist for \\\'chr={chr}\\\'!"')
    (35, '                )')
    (36, '                sys.exit(1)')
    (37, '')
    (38, '        # This string contains a wildcard where we will place the name of the chromosome')
    (39, '        # something like "path/to/my/panel_chr{chr}.vcf.gz"')
    (40, '        path_map = recursive_get(["imputation", genome, "path_map"], "")')
    (41, '        for chr in chromosomes:')
    (42, '            file = path_map.format(chr=chr)')
    (43, '            if not os.path.isfile(file):')
    (44, '                LOGGER.error(')
    (45, '                    f"ERROR: Map file config[imputation][{genome}][path_map] ({path_panel}) does not exist for \\\'chr={chr}\\\'!"')
    (46, '                )')
    (47, '                sys.exit(1)')
    (48, '')
    (49, '')
    (50, '')
    (51, '# -----------------------------------------------------------------------------#')
    (52, '# Some useful functions')
    (53, '')
    (54, '# This function will be useful later to know how many chunks will be merged per chromosome')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/fastq.smk
context_key: ['if PAIRED_END', 'if COLLAPSE']
    (59, 'if PAIRED_END:')
    (60, '    if COLLAPSE:')
    (61, '')
    (62, '        ruleorder: adapter_removal_collapse > adapter_removal_pe > adapter_removal_se')
    (63, '')
    (64, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/fastq.smk
context_key: ['if PAIRED_END', 'else']
    (65, '    else:')
    (66, '')
    (67, '        ruleorder: adapter_removal_pe > adapter_removal_collapse > adapter_removal_se')
    (68, '')
    (69, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/fastq.smk
context_key: ['if save_low_qual', 'rule samtools_filter', 'input']
    (523, 'if save_low_qual:')
    (524, '')
    (525, '    rule samtools_filter:')
    (526, '        """')
    (527, '        Filter mappings following quality and keeping the low quality mappings')
    (528, '        """')
    (529, '        input:')
    (530, '            "{folder}/01_fastq/02_mapped/03_bam_sort/{sm}/{lb}/{id}.{genome}.bam",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/fastq.smk
context_key: ['if save_low_qual', 'rule samtools_filter', 'output']
    (531, '        output:')
    (532, '            mapped=temp(')
    (533, '                "{folder}/01_fastq/03_filtered/01_bam_filter/{sm}/{lb}/{id}.{genome}.bam"')
    (534, '            ),')
    (535, '            low_qual=temp(')
    (536, '                "{folder}/01_fastq/03_filtered/01_bam_filter_low_qual/{sm}/{lb}/{id}.{genome}.bam"')
    (537, '            ),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/fastq.smk
context_key: ['if save_low_qual', 'rule samtools_filter', 'params']
    (538, '        params:')
    (539, '            q=lambda wildcards: SAMPLES[wildcards.sm][wildcards.lb][wildcards.id][')
    (540, '                "MAPQ"')
    (541, '            ],')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/fastq.smk
context_key: ['if save_low_qual', 'rule samtools_filter', 'resources']
    (542, '        resources:')
    (543, '            memory=lambda wildcards, attempt: get_memory_alloc("filtering", attempt, 4),')
    (544, '            runtime=lambda wildcards, attempt: get_runtime_alloc(')
    (545, '                "filtering", attempt, 24')
    (546, '            ),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/fastq.smk
context_key: ['if save_low_qual', 'rule samtools_filter', 'log']
    (547, '        log:')
    (548, '            "{folder}/01_fastq/03_filtered/01_bam_filter/{sm}/{lb}/{id}.{genome}.log",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/fastq.smk
context_key: ['if save_low_qual', 'rule samtools_filter', 'threads']
    (549, '        threads: get_threads("filtering", 4)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/fastq.smk
context_key: ['if save_low_qual', 'rule samtools_filter', 'conda']
    (550, '        conda:')
    (551, '            "../envs/samtools.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/fastq.smk
context_key: ['if save_low_qual', 'rule samtools_filter', 'envmodules']
    (552, '        envmodules:')
    (553, '            module_samtools,')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/fastq.smk
context_key: ['if save_low_qual', 'rule samtools_filter', 'message']
    (554, '        message:')
    (555, '            "--- SAMTOOLS FILTER {input}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/rules/fastq.smk
context_key: ['if save_low_qual', 'rule samtools_filter', 'shell']
    (556, '        shell:')
    (557, '            """')
    (558, '            samtools view -b --threads {threads} -F 4 -q {params.q} \\\\')
    (559, '            -U {output.low_qual} {input} > {output.mapped} 2> {log}')
    (560, '            """')
    (561, '')
    (562, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/Snakefile
context_key: ['if not fileVerbose.is_file()']
    (28, 'if not fileVerbose.is_file():')
    (29, '    fileVerbose.parent.mkdir(parents=True, exist_ok=True)')
    (30, '    fileVerbose.touch()')
    (31, '    VERBOSE = recursive_get(["VERBOSE"], True)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/Snakefile
context_key: ['if len(GENOMES) == 0']
    (64, 'if len(GENOMES) == 0:')
    (65, '    LOGGER.error("ERROR: Reference genome is not specified (parameter config[genome])!")')
    (66, '    os._exit(1)')
    (67, '')
    (68, '## get all chromosome names and store them in the dict config[chromosomes][genome][all] for later use')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/Snakefile
context_key: ['if len(SAMPLES) + len(EXTERNAL_SAMPLES) == 0']
    (82, 'if len(SAMPLES) + len(EXTERNAL_SAMPLES) == 0:')
    (83, '    LOGGER.error(f"ERROR: No samples are specified)!")')
    (84, '    sys.exit(1)')
    (85, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/Snakefile
context_key: ['if subsampling_number == 0']
    (99, 'if subsampling_number == 0:')
    (100, '    run_subsampling = False')
    (101, '')
    (102, '## --------------------------------------------------------------------------------------------------')
    (103, '## adapterremoval')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/Snakefile
context_key: ['if run_filtering']
    (120, 'if run_filtering:')
    (121, '    save_low_qual = str2bool(')
    (122, '        recursive_get_and_test(["filtering", "save_low_qual"], ["True", "False"])')
    (123, '    )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/Snakefile
context_key: ['if remove_duplicates == "dedup" and not PAIRED_END']
    (133, 'if remove_duplicates == "dedup" and not PAIRED_END:')
    (134, '    LOGGER.warning(')
    (135, '        "WARNING: \\\'DeDup\\\' is not recommended for single-end reads (parameter config[remove_duplicates][run])!"')
    (136, '    )')
    (137, '')
    (138, '')
    (139, '## --------------------------------------------------------------------------------------------------')
    (140, '## rescaling damage')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/Snakefile
context_key: ['if run_damage_rescale and recursive_get(["damage", "run"], "False") != "mapDamage"']
    (144, 'if run_damage_rescale and recursive_get(["damage", "run"], "False") != "mapDamage":')
    (145, '    LOGGER.error(')
    (146, '        "ERROR: To use config[damage_rescale][run] the parameter config[damage][run] has to be set to \\\'mapDamage\\\'!"')
    (147, '    )')
    (148, '    sys.exit(1)')
    (149, '')
    (150, '')
    (151, '## --------------------------------------------------------------------------------------------------')
    (152, '## realigning')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/Snakefile
context_key: ['if str2bool']
    (174, '    if str2bool(')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/Snakefile
context_key: ['if str2bool(recursive_get_and_test(["depth", genome, "run"], ["False", "True"]))']
    (186, '    if str2bool(recursive_get_and_test(["depth", genome, "run"], ["False", "True"])):')
    (187, '        run_depth = run_depth + [genome]')
    (188, '        read_depth(genome)')
    (189, '')
    (190, '')
    (191, '## imputation (is assumed to be run ONLY on the first specified reference GENOMES!!!)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/Snakefile
context_key: ['if str2bool(recursive_get_and_test(["imputation", "run"], ["False", "True"]))']
    (194, '    if str2bool(recursive_get_and_test(["imputation", "run"], ["False", "True"])):')
    (195, '        run_imputation = run_imputation + [genome]')
    (196, '')
    (197, '')
    (198, '')
    (199, '##########################################################################################')
    (200, '##  STATISTICS')
    (201, '##########################################################################################')
    (202, '## By default he stats are computed on the mapped bam files. However, one can pass pre-computed bam files to use the stats feature of')
    (203, "## mapache using the paramter \\'bam_list\\'. In this later case only on the specified bam fiels the stats are comouted.")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/Snakefile
context_key: ['if VERBOSE']
    (278, 'if VERBOSE:')
    (279, '    write_log()')
    (280, '')
    (281, '')
    (282, '')
    (283, '##########################################################################################')
    (284, '## STARTING RULES')
    (285, '##########################################################################################')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/Snakefile
context_key: ['onsuccess', 'if os.path.exists(f"{RESULT_DIR}/verbose.touch")']
    (368, '    if os.path.exists(f"{RESULT_DIR}/verbose.touch"):')
    (369, '        os.remove(f"{RESULT_DIR}/verbose.touch")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/Snakefile
context_key: ['onsuccess', 'if email != ""']
    (370, '    if email != "":')
    (371, '        Logger.info(f"Sending email to {email}")')
    (372, '        shell("mail -s \\\\"\\\'Mapache\\\' finished successfully\\\\" {email} < {log}")')
    (373, '')
    (374, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/Snakefile
context_key: ['onerror', 'if os.path.exists(f"{RESULT_DIR}/verbose.touch")']
    (379, '    if os.path.exists(f"{RESULT_DIR}/verbose.touch"):')
    (380, '        os.remove(f"{RESULT_DIR}/verbose.touch")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sneuensc/mapache, file=workflow/Snakefile
context_key: ['onerror', 'if email != ""']
    (381, '    if email != "":')
    (382, '        LOGGER.info(f"Sending email to {email}")')
    (383, '        shell("mail -s \\\\"\\\'Mapache\\\' finished with error(s)\\\\" {email} < {log}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=EichlerLab/ONT_pipelines, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'if os.path.exists("temp")']
    (30, 'if TEMP_DIR != "temp":')
    (31, '    if os.path.exists("temp"):')
    (32, '        if os.path.islink("temp") and os.path.realpath("temp") == os.path.realpath(')
    (33, '            TEMP_DIR')
    (34, '        ):')
    (35, '            print("The temp dir has already been linked.")')
    (36, '        else:')
    (37, '            sys.exit("temp/ already in use, please move it before running.")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=EichlerLab/ONT_pipelines, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'else']
    (38, '    else:')
    (39, '        shell("ln -s {TEMP_DIR} temp")')
    (40, '')
    (41, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/StainedGlass, file=workflow/Snakefile
context_key: ['if W < 500']
    (46, 'if W < 500:')
    (47, '    # MAP_PARAMS = " -a --no-pairing -k21 --sr -A2 -B8 -O12,32 -E2,1 -r100 -g100 --heap-sort=yes -X -w 11  "')
    (48, '    MAP_PARAMS = " -a --no-pairing -X -k21 -w11 -A2 -B8 -O12,32 -E2,1 -r50 -p.5 -N20 -f1000,5000 -n2 -m20 -s40 -g200 -2K50m --heap-sort=yes "')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/StainedGlass, file=workflow/Snakefile
context_key: ['elif SLIDE > 0']
    (49, 'elif SLIDE > 0:')
    (50, '    S = config.pop("mm_s", 100)')
    (51, '    MAP_PARAMS = "-ax map-ont --secondary=no"')
    (52, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/StainedGlass, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'if os.path.exists("temp")', 'if os.path.realpath("temp") == os.path.realpath(TEMP_DIR)']
    (53, 'if TEMP_DIR != "temp":')
    (54, '    if os.path.exists("temp"):')
    (55, '        if os.path.realpath("temp") == os.path.realpath(TEMP_DIR):')
    (56, '            print("The temp dir has already been linked.")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/StainedGlass, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'if os.path.exists("temp")', 'else']
    (57, '        else:')
    (58, '            sys.exit("temp/ already in use, please move it before running.")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/StainedGlass, file=workflow/Snakefile
context_key: ['if TEMP_DIR != "temp"', 'else']
    (59, '    else:')
    (60, '        shell("ln -s {TEMP_DIR} temp")')
    (61, '')
    (62, '#')
    (63, '# required arguments')
    (64, '#')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/StainedGlass, file=workflow/Snakefile
context_key: ['if not os.path.exists(FAI)']
    (68, 'if not os.path.exists(FAI):')
    (69, '    sys.exit(f"Input fasta must be indexed, try:\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=rules/common.smk
context_key: ['if "humann" in CORETOOLS and "megan" in CORETOOLS']
    (6, '    if "humann" in CORETOOLS and "megan" in CORETOOLS:')
    (7, '        print(f"{bcolors.OKBLUE}INFO: Running pipeline with core tools MEGAN6 and HUMAnN 3.0 to classify input reads.{bcolors.ENDC}")')
    (8, '        return humann + megan + compare')
    (9, '')
    (10, '    elif "humann" in CORETOOLS:')
    (11, '        print(f"{bcolors.OKBLUE}INFO: Running pipeline with core tool HUMAnN 3.0.")')
    (12, '        return humann')
    (13, '')
    (14, '    elif "megan" in CORETOOLS:')
    (15, '        print(f"{bcolors.OKBLUE}INFO: Running pipeline with core tool MEGAN6 to classify input reads.{bcolors.ENDC}")')
    (16, '        return megan')
    (17, '')
    (18, '    else:')
    (19, '        print(f"{bcolors.FAIL}WARNING: No core tool was chosen to classify the reads. Running all core tools now..{bcolors.ENDC}")')
    (20, '        return humann + megan + compare')
    (21, '')
    (22, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=Snakefile
context_key: ['if ".csv" in READS']
    (13, 'if ".csv" in READS:')
    (14, '    # reads= is a csv file containing read info')
    (15, '    SAMPLESHEET = pd.read_csv(READS)')
    (16, '    print(f"{bcolors.OKBLUE}INFO: Loading samples from file \\\'{READS}\\\'.{bcolors.ENDC}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=Snakefile
context_key: ['elif READS == ""']
    (17, 'elif READS == "":')
    (18, '    # reads is empty, exit')
    (19, '    print(f"{bcolors.FAIL}CRITICAL: No samplesheet or directory containing reads were provided to parameter \\\'reads=\\\'! Exiting..{bcolors.ENDC}")')
    (20, '    exit()')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=Snakefile
context_key: ['if "fasta" in EXT or "fna" in EXT or "fa." in EXT']
    (37, 'if "fasta" in EXT or "fna" in EXT or "fa." in EXT:')
    (38, '    FORMAT = "-f" #"-q" fasta:fastq')
    (39, '    FASTQC = False')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=Snakefile
context_key: ['elif "fastq" in EXT or "fq." in EXT']
    (40, 'elif "fastq" in EXT or "fq." in EXT:')
    (41, '    FORMAT = "-q"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=Snakefile
context_key: ['if RESULTDIR != "results"']
    (76, '    if RESULTDIR != "results":')
    (77, '        shell(f"if [ ! -d results ]; then ln -s {RESULTDIR} results; fi")')
    (78, '    if CACHEDIR != "cache":')
    (79, '        shell(f"if [ ! -d cache ]; then ln -s {CACHEDIR} cache; fi")')
    (80, '    if TEMPDIR != "temp" and CLEAN != "true":')
    (81, '        shell(f"if [ ! -d temp ]; then ln -s {TEMPDIR} temp; fi")')
    (82, '    if CLEAN == "true":')
    (83, '        try:')
    (84, '            os.remove(TEMPDIR)')
    (85, '            shutil.move(os.path.join(RESULTDIR, "04-DifferentialGeneAbundance", "humann","dga_humann.html"), os.path.join(RESULTDIR, "05-Summary", "dga_report_humann.html"))')
    (86, '            shutil.move(os.path.join(RESULTDIR, "04-DifferentialGeneAbundance", "megan","dga_megan.html"), os.path.join(RESULTDIR, "05-Summary", "dga_report_megan.html"))')
    (87, '        except OSError:')
    (88, '            pass')
    (89, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=Snakefile
context_key: ['onerror', 'if RESULTDIR != "results"']
    (92, '    if RESULTDIR != "results":')
    (93, '        shell(f"if [ ! -d results ]; then ln -s {RESULTDIR} results; fi")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=Snakefile
context_key: ['onerror', 'if CACHEDIR != "cache"']
    (94, '    if CACHEDIR != "cache":')
    (95, '        shell(f"if [ ! -d cache ]; then ln -s {CACHEDIR} cache; fi")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=fischer-hub/metagenomics, file=Snakefile
context_key: ['onerror', 'if TEMPDIR != "temp"']
    (96, '    if TEMPDIR != "temp":')
    (97, '        shell(f"if [ ! -d temp ]; then ln -s {TEMPDIR} temp; fi")')
    (98, '')
    (99, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zavolanlab/polyAsite_workflow, file=Snakefile
context_key: ['output', 'if [[ "${{#SRR[@]}}" > "1" ]];the']
    (488, '        if [[ "${{#SRR[@]}}" > "1" ]];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zavolanlab/polyAsite_workflow, file=Snakefile
context_key: ['if sites != 0']
    (1764, '            if sites != 0:')
    (1765, '                out.write("sites.noBG.withPAS.percent\\\\t%i\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Finn-Lab/MAG_Snakemake_wf, file=Snakefile
context_key: ['if not os.path.exists("logs")']
    (41, 'if not os.path.exists("logs"):')
    (42, '    os.makedirs("logs")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/probe, file=workflow/Snakefile
context_key: ['if cloud']
    (18, 'if cloud:')
    (19, '    import malariagen_data')
    (20, '    ag3 = malariagen_data.Ag3()')
    (21, '    metadata = ag3.sample_metadata(sample_sets=ag3_sample_sets)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBISweden/fungal-trans, file=source/rules/filter.smk
context_key: ['else', 'if remaining != 0']
    (557, '        if remaining != 0:')
    (558, '            sys.exit("WARNING: Could not get all reads for {output.R2}\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/ASCENT, file=workflow/rules/isa.smk
context_key: ['if [ ! -d {params.parentdir} ];the']
    (23, 'if [ ! -d {params.parentdir} ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/ASCENT, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(RESULTSDIR)']
    (155, 'if not os.path.exists(RESULTSDIR):')
    (156, '    os.mkdir(RESULTSDIR)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/ASCENT, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(STARINDEXDIR)']
    (177, 'if not os.path.exists(STARINDEXDIR):')
    (178, '    os.mkdir(STARINDEXDIR)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/ASCENT, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(RSEMINDEXDIR)']
    (181, 'if not os.path.exists(RSEMINDEXDIR):')
    (182, '    os.mkdir(RSEMINDEXDIR)')
    (183, '')
    (184, '#########################################################')
    (185, '# get maxrl')
    (186, '#########################################################')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/ASCENT, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(BAMDIR)']
    (208, 'if not os.path.exists(BAMDIR):')
    (209, '    os.mkdir(BAMDIR)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/ASCENT, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(RMATSTXTDIR)']
    (210, 'if not os.path.exists(RMATSTXTDIR):')
    (211, '    os.mkdir(RMATSTXTDIR)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/ASCENT, file=workflow/rules/init.smk
context_key: ['if not os.path.exists(outfilename)']
    (220, '    if not os.path.exists(outfilename):')
    (221, "        outfile=open(outfilename,\\'w\\')")
    (222, '        outfile.write(group_txt)')
    (223, '        outfile.close()')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexmsalmeida/protean, file=Snakefile
context_key: ['if not os.path.exists(direct)']
    (14, '    if not os.path.exists(direct):')
    (15, '        os.makedirs(direct)')
    (16, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule all', 'input']
    (14, 'if LEVEL == "GENOME": ')
    (15, '     rule all:')
    (16, '         input:')
    (17, '            expand("{sample}.sorted.bam", sample = TREAT),')
    (18, '            expand("{sample}.sorted.bam", sample = CONTROL),')
    (19, '            expand("{sample}.{group}.txt", sample = TREAT, group = config[\\\'TREAT_NAME\\\']),')
    (20, '            expand("{sample}.{group}.txt", sample = CONTROL, group = config[\\\'CONTROL_NAME\\\']),')
    (21, '            expand("{treat}_{control}_cpm.csv", treat =config[\\\'TREAT_NAME\\\'],control =config[\\\'CONTROL_NAME\\\']),')
    (22, '            expand("{treat}_{control}_dge.csv", treat =config[\\\'TREAT_NAME\\\'],control =config[\\\'CONTROL_NAME\\\'])')
    (23, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if PAIRED', 'rule trim', 'input']
    (31, 'if PAIRED: ')
    (32, '    rule trim: ')
    (33, '       input: ')
    (34, '           r1 = "{sample}.r_1.fq.gz",')
    (35, '           r2 = "{sample}.r_2.fq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if PAIRED', 'rule trim', 'output']
    (36, '       output: ')
    (37, '           "galore/{sample}.r_1_val_1.fq.gz",')
    (38, '           "galore/{sample}.r_2_val_2.fq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if PAIRED', 'rule trim', 'conda']
    (39, "       conda: \\'env/env-trim.yaml\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if PAIRED', 'rule trim', 'shell']
    (40, '       shell: ')
    (41, '           """')
    (42, '           mkdir -p galore ')
    (43, '           mkdir -p fastqc ')
    (44, '           trim_galore --gzip --retain_unpaired --trim1 --fastqc --fastqc_args "--outdir fastqc" -o galore --paired {input.r1} {input.r2}')
    (45, '           """ ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if PAIRED', 'if LEVEL =="GENOME"', 'rule tobam', 'input']
    (61, 'if PAIRED: ')
    (62, '    if LEVEL =="GENOME":')
    (63, '         rule tobam:')
    (64, '              input:')
    (65, '                  r1 = "galore/{sample}.r_1_val_1.fq.gz",')
    (66, '                  r2 = "galore/{sample}.r_2_val_2.fq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if PAIRED', 'if LEVEL =="GENOME"', 'rule tobam', 'params']
    (67, '              params:')
    (68, "                  genome=config[\\'GENOME\\'],")
    (69, "                  mem = config[\\'MEMORY\\'],")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if PAIRED', 'if LEVEL =="GENOME"', 'rule tobam', 'output']
    (70, '              output:')
    (71, '                  "{sample}.bam"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if PAIRED', 'if LEVEL =="GENOME"', 'rule tobam', 'conda']
    (72, "              conda: \\'env/env-align.yaml\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if PAIRED', 'if LEVEL =="GENOME"', 'rule tobam', 'shell']
    (73, '              shell:')
    (74, '                  """')
    (75, '                  bbmap.sh {params.mem} in={input[0]} in2={input[1]} out={output} ref={params.genome}')
    (76, '                  """ ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if PAIRED', 'else', 'rule quant', 'input']
    (77, '    else: ')
    (78, '          rule quant:  ')
    (79, '             input: ')
    (80, '                  r1 = "galore/{sample}.r_1_val_1.fq.gz",')
    (81, '                  r2 = "galore/{sample}.r_2_val_2.fq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if PAIRED', 'else', 'rule quant', 'params']
    (82, '             params: ')
    (83, "                index = config[\\'SALMON_INDEX\\'],")
    (84, "                lib = config[\\'SALMON_LIBRARY\\'],")
    (85, '                outdir = "SALMON_{sample}.{group}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if PAIRED', 'else', 'rule quant', 'conda']
    (86, "             conda: \\'env/env-quant.yaml\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if PAIRED', 'else', 'rule quant', 'output']
    (87, '             output:')
    (88, '                  "SALMON_{sample}.{group}/quant.sf",')
    (89, '                  "{sample}.{group}.quant.sf" ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if PAIRED', 'else', 'rule quant', 'shell']
    (90, '             shell: ')
    (91, '                 """  ')
    (92, '                 salmon quant -i {params.index} --libType {params.lib} -1 {input.r1} -2 {input.r2} -o {params.outdir} -p 3 --writeUnmappedNames --seqBias --gcBias --validateMappings')
    (93, '                 ln -fs {output[0]} {output[1]} ')
    (94, '                 """')
    (95, '')
    (96, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['else', 'if LEVEL =="GENOME"', 'rule tobam', 'input']
    (98, '      if LEVEL =="GENOME": ')
    (99, '         rule tobam:')
    (100, '              input:')
    (101, '                   "galore/{sample}_trimmed.fq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['else', 'if LEVEL =="GENOME"', 'rule tobam', 'params']
    (102, '              params:')
    (103, "                   genome=config[\\'GENOME\\'],")
    (104, "                   mem = config[\\'MEMORY\\'],")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['else', 'if LEVEL =="GENOME"', 'rule tobam', 'output']
    (105, '              output:')
    (106, '                   "{sample}.bam"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['else', 'if LEVEL =="GENOME"', 'rule tobam', 'conda']
    (107, "              conda: \\'env/env-align.yaml\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['else', 'if LEVEL =="GENOME"', 'rule tobam', 'shell']
    (108, '              shell:')
    (109, '                   """')
    (110, '                   bbmap.sh {params.mem} in={input} out={output} ref={params.genome}')
    (111, '                   """')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule sort', 'input']
    (130, 'if LEVEL == "GENOME":')
    (131, '    rule sort:')
    (132, '       input:')
    (133, '            "{sample}.bam"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule sort', 'output']
    (134, '       output:')
    (135, '            "{sample}.sorted.bam"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule sort', 'params']
    (136, '       params:')
    (137, '            "{sample}.tmp.sorted"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule sort', 'log']
    (138, '       log:')
    (139, '            "{sample}.sorted.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule sort', 'conda']
    (140, "       conda: \\'env/env-align.yaml\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule sort', 'shell']
    (141, '       shell:')
    (142, '            """samtools sort -T {params} -n -o {output} {input}""" ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule feature_count', 'input']
    (143, '    rule feature_count:')
    (144, '         input: ')
    (145, '             "{sample}.sorted.bam"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule feature_count', 'params']
    (146, '         params:')
    (147, "             config[\\'STRAND\\'],")
    (148, "             config[\\'GTF\\'], ")
    (149, '             file = "tmp.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule feature_count', 'output']
    (150, '         output:')
    (151, '            "{sample}.{group}.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule feature_count', 'conda']
    (152, '         conda: "env/env-feature.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule feature_count', 'shell']
    (153, '         shell:')
    (154, '              """')
    (155, '              featureCounts -p -t exon -g gene_id -a {params[1]} -o {params[2]} {input[0]} -s {params[0]} ')
    (156, '              tail -n +3 {params[2]} | cut -f1,7 > {output[0]}')
    (157, '              """')
    (158, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule DGE_genome', 'input']
    (159, '    rule DGE_genome:')
    (160, '        input:')
    (161, '           expand("{sample}.{group}.txt", sample = TREAT, group = config[\\\'TREAT_NAME\\\']), ')
    (162, '           expand("{sample}.{group}.txt", sample = CONTROL, group = config[\\\'CONTROL_NAME\\\'])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule DGE_genome', 'params']
    (163, '        params: ')
    (164, "           treat = config[\\'TREAT_NAME\\'], ")
    (165, "           control = config[\\'CONTROL_NAME\\'], ")
    (166, "           N = config[\\'N\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule DGE_genome', 'output']
    (167, '        output:')
    (168, '           expand("{treat}_{control}_cpm.csv", treat =config[\\\'TREAT_NAME\\\'],control =config[\\\'CONTROL_NAME\\\']),')
    (169, '           expand("{treat}_{control}_dge.csv", treat =config[\\\'TREAT_NAME\\\'],control =config[\\\'CONTROL_NAME\\\'])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule DGE_genome', 'conda']
    (170, "        conda: \\'env/env-dge-genome.yaml\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SherineAwad/BulkRNASeq, file=Snakefile
context_key: ['if LEVEL == "GENOME"', 'rule DGE_genome', 'shell']
    (171, '        shell: ')
    (172, '           """')
    (173, '           Rscript scripts/dge_genome.R {params[0]} {params[1]} {params[2]}  ')
    (174, '           """')
    (175, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PyPSA/pypsa-za, file=Snakefile
context_key: ["if not config[\\'hydro_inflow\\'][\\'disable\\']", 'rule build_inflow_per_country']
    (48, "if not config[\\'hydro_inflow\\'][\\'disable\\']:")
    (49, '    rule build_inflow_per_country:')
    (50, '        input: EIA_hydro_gen="data/EIA_hydro_generation_2011_2014.csv"')
    (51, '        output: "resources/hydro_inflow.csv"')
    (52, '        benchmark: "benchmarks/inflow_per_country"')
    (53, '        threads: 1')
    (54, '        resources: mem_mb=1000')
    (55, '        script: "scripts/build_inflow_per_country.py"')
    (56, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/NCI-GEMSCAN, file=workflow/rules/Snakefile_DeepVariant
context_key: ['if clusterMode == "gcp" or useRemoteFiles']
    (11, 'if clusterMode == "gcp" or useRemoteFiles:')
    (12, '    from snakemake.remote.GS import RemoteProvider as GSRemoteProvider')
    (13, '    GS = GSRemoteProvider()')
    (14, '')
    (15, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/NCI-GEMSCAN, file=workflow/rules/Snakefile_harmonize
context_key: ['if clusterMode == "gcp" or useRemoteFiles']
    (8, 'if clusterMode == "gcp" or useRemoteFiles:')
    (9, '    from snakemake.remote.GS import RemoteProvider as GSRemoteProvider')
    (10, '    GS = GSRemoteProvider()')
    (11, '')
    (12, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/NCI-GEMSCAN, file=workflow/rules/Snakefile_Strelka2
context_key: ['if clusterMode == "gcp" or useRemoteFiles']
    (8, 'if clusterMode == "gcp" or useRemoteFiles:')
    (9, '    from snakemake.remote.GS import RemoteProvider as GSRemoteProvider')
    (10, '    GS = GSRemoteProvider()')
    (11, '')
    (12, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/NCI-GEMSCAN, file=workflow/rules/Snakefile_preprocess
context_key: ['if clusterMode == "gcp"']
    (7, 'if clusterMode == "gcp":')
    (8, '    GS = GSRemoteProvider()      ')
    (9, '        ')
    (10, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/NCI-GEMSCAN, file=workflow/rules/Snakefile_preprocess
context_key: ['if clusterMode == "gcp"']
    (7, 'if clusterMode == "gcp":')
    (8, '    GS = GSRemoteProvider()      ')
    (9, '        ')
    (10, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/NCI-GEMSCAN, file=workflow/Snakefile
context_key: ['if clusterMode == "gcp" or useRemoteFiles']
    (46, 'if clusterMode == "gcp" or useRemoteFiles:')
    (47, '    from snakemake.remote.GS import RemoteProvider as GSRemoteProvider')
    (48, '    GS = GSRemoteProvider()')
    (49, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/NCI-GEMSCAN, file=workflow/Snakefile
context_key: ['if clusterMode == "cluster"']
    (50, 'if clusterMode == "cluster":')
    (51, '   outputDir = "./"')
    (52, '')
    (53, '    ')
    (54, '#including the common functions')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/NCI-GEMSCAN, file=workflow/Snakefile
context_key: ['if dv_mode']
    (58, 'if dv_mode:')
    (59, "    #modelPath = config[\\'modelPath\\']")
    (60, "    #useShards = config[\\'useShards\\']")
    (61, "    glnexus_dv_config = config[\\'glnexus_dv_config\\']")
    (62, "    model_type = config[\\'model_type\\']")
    (63, "    dv_memGB = config[\\'dv_memGB\\']")
    (64, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/NCI-GEMSCAN, file=workflow/Snakefile
context_key: ['if strelka2_mode']
    (65, 'if strelka2_mode:')
    (66, "    bedFileGZ = config[\\'bedFileGZ\\']")
    (67, "    glnexus_strelka2_config = config[\\'glnexus_strelka2_config\\']")
    (68, "    exome_param = config[\\'exome_param\\']")
    (69, "    strelka2_memGB = config[\\'strelka2_memGB\\']")
    (70, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/NCI-GEMSCAN, file=workflow/Snakefile
context_key: ['if hc_mode']
    (71, 'if hc_mode:')
    (72, "    glnexus_gatk_config = config[\\'glnexus_gatk_config\\']")
    (73, '    ')
    (74, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/NCI-GEMSCAN, file=workflow/Snakefile
context_key: ['if strelka2_mode and not dv_mode and not hc_mode', 'rule all', 'input']
    (116, 'if strelka2_mode and not dv_mode and not hc_mode:')
    (117, "    include: \\'rules/Snakefile_Strelka2\\'")
    (118, '    rule all:')
    (119, '        input:')
    (120, "            \\'strelka2/genotyped/strelka2_variants.vcf.gz\\'")
    (121, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/NCI-GEMSCAN, file=workflow/Snakefile
context_key: ['if hc_mode and not dv_mode and not strelka2_mode', 'rule all', 'input']
    (122, 'if hc_mode and not dv_mode and not strelka2_mode:')
    (123, "    include: \\'rules/Snakefile_HaplotypeCaller\\'")
    (124, '    rule all:')
    (125, '        input:')
    (126, "            \\'HaplotypeCaller/genotyped/HC_variants.vcf.gz\\'")
    (127, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/NCI-GEMSCAN, file=workflow/Snakefile
context_key: ['if dv_mode and not hc_mode and not strelka2_mode', 'rule all', 'input']
    (128, 'if dv_mode and not hc_mode and not strelka2_mode:')
    (129, "    include: \\'rules/Snakefile_DeepVariant\\'")
    (130, '    rule all:')
    (131, '        input:')
    (132, "            expand(\\'deepVariant/called/vcfs/{sample}_all_chroms.vcf.gz\\', sample=sampleList) if by_chrom else expand(\\'deepVariant/called_by_sample/{sample}.vcf.gz\\', sample=sampleList),")
    (133, "            \\'deepVariant/genotyped/DV_variants.vcf.gz\\'")
    (134, '            ')
    (135, '            ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/NCI-GEMSCAN, file=workflow/Snakefile
context_key: ['if hc_mode and dv_mode and strelka2_mode and not har_mode', 'rule all', 'input']
    (136, 'if hc_mode and dv_mode and strelka2_mode and not har_mode:')
    (137, "    include: \\'rules/Snakefile_HaplotypeCaller\\'")
    (138, "    include: \\'rules/Snakefile_DeepVariant\\'")
    (139, "    include: \\'rules/Snakefile_Strelka2\\'")
    (140, '    rule all:')
    (141, '        input:')
    (142, "            \\'HaplotypeCaller/genotyped/HC_variants.vcf.gz\\',")
    (143, "            \\'deepVariant/genotyped/DV_variants.vcf.gz\\',   ")
    (144, "            \\'strelka2/genotyped/strelka2_variants.vcf.gz\\'")
    (145, '            ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/NCI-GEMSCAN, file=workflow/Snakefile
context_key: ['if hc_mode and dv_mode and strelka2_mode and har_mode', 'rule all', 'input']
    (146, 'if hc_mode and dv_mode and strelka2_mode and har_mode:')
    (147, "    include: \\'rules/Snakefile_HaplotypeCaller\\'")
    (148, "    include: \\'rules/Snakefile_DeepVariant\\'")
    (149, "    include: \\'rules/Snakefile_Strelka2\\'")
    (150, "    include: \\'rules/Snakefile_harmonize\\'")
    (151, '    rule all:')
    (152, '        input:')
    (153, "            \\'ensemble/all_callers_merged_genotypes.vcf.gz\\',")
    (154, "            \\'ensemble/all_callers_merged_genotypes.vcf.gz.tbi\\'")
    (155, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zwebbs/RNA-Seq-Pipeline, file=Snakefile
context_key: ['if cluster_id == "GARDNER"']
    (20, 'if cluster_id == "GARDNER":')
    (21, '    cfg_schema="CFG_GARDNER_BASIC"')
    (22, '    data_schema="META_GARDNER_SEQ_BASIC"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zwebbs/RNA-Seq-Pipeline, file=Snakefile
context_key: ['elif cluster_id == "MIDWAY2"']
    (23, 'elif cluster_id == "MIDWAY2":')
    (24, '    cfg_schema = None  # TODO: implement gs midway2 schema and replace')
    (25, '    data_schema = None  # TODO: implement gs midway2 schema and replace')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zwebbs/RNA-Seq-Pipeline, file=Snakefile
context_key: ['if Path(genome_index_dir).exists()', 'rule Verify_Index_Contents', 'shell']
    (66, 'if Path(genome_index_dir).exists():')
    (67, '    rule Verify_Index_Contents:')
    (68, '        params: **CH.get_parameters("STAR_Create_Genome_Index")')
    (69, '        output: **DM.get_rule_data("STAR_Create_Genome_Index", ["static_outputs"])')
    (70, '        resources: **CH.get_resources("Verify_Index_Contents")')
    (71, '        shell:')
    (72, '            "check_directory -o {output.rc_out}"')
    (73, '            " {params.genome_index_manifest} {params.genome_index_dir}"')
    (74, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=OSU-SRLab/SCLC-autopsy-genomics, file=Snakefile
context_key: ['if raw_fastq_gz[-3']
    (22, '        if raw_fastq_gz[-3:] == ".gz":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=OSU-SRLab/SCLC-autopsy-genomics, file=Snakefile
context_key: ['else', 'if custom_path is not None']
    (27, '    if custom_path is not None:')
    (28, '        fastqs = []')
    (29, '        for path in raw_fastqs:')
    (30, '            base = os.path.basename(path)')
    (31, '            new_path = os.path.join(custom_path, base)')
    (32, '            fastqs.append(new_path)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmenglund/FEGA-encryption-workflow, file=Snakefile
context_key: ['if platform == "linux" or platform == "linux2"']
    (8, 'if platform == "linux" or platform == "linux2":')
    (9, '    MD5_COMMAND = "md5sum"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmenglund/FEGA-encryption-workflow, file=Snakefile
context_key: ['elif platform == "darwin"']
    (10, 'elif platform == "darwin":  # macOS')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Mar111tiN/RNAseq, file=rules/trim.smk
context_key: ['if (a']
    (12, "    if (a:=u[\\'adapters5\\']):")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Mar111tiN/RNAseq, file=rules/trim.smk
context_key: ['if (a']
    (15, "    if (a:=u[\\'adapters3\\']):")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Mar111tiN/RNAseq, file=rules/trim.smk
context_key: ['if u["fastq2"]']
    (19, '    if u["fastq2"]:')
    (20, '        adapter_list += [f"-G {a}" for a in adapt5] + [f"-A {a}" for a in adapt3]')
    (21, '    adapter_string = " ".join(adapter_list) + " "')
    (22, '')
    (23, '    return adapter_string')
    (24, '')
    (25, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sophsatt/test, file=Snakefile
context_key: ['if not os.path.exists("plots")']
    (8, 'if not os.path.exists("plots"):')
    (9, '    os.makedirs("plots")')
    (10, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sophsatt/test, file=Snakefile
context_key: ['if not os.path.exists("clustering_distance.txt")']
    (11, 'if not os.path.exists("clustering_distance.txt"):')
    (12, '    file = open("clustering_distance.txt", "w")')
    (13, '    file.write("canberra")')
    (14, '    file.close()')
    (15, '')
    (16, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/Snakefile
context_key: ['if "ancestor" not in config']
    (33, 'if "ancestor" not in config:')
    (34, '    assert (')
    (35, '        "reference" in config and "outgroup" in config')
    (36, '    ), "Without an ancestor file a reference file and an outgroup file must be specified!"')
    (37, '')
    (38, '    assert os.path.exists(FAI), "reference fasta index file not found!"')
    (39, '    CHRS = [line.split()[0] for line in open(FAI).readlines()]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/Snakefile
context_key: ['if "stratify" in config']
    (46, 'if "stratify" in config:')
    (47, '    RGNS = config["stratify"].keys()')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_bam', 'input']
    (3, 'if "chain" not in config:')
    (4, '')
    (5, '    rule make_bam:')
    (6, '        input:')
    (7, '            ref=REF,')
    (8, '            outgroup=OUTGROUP,')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_bam', 'output']
    (9, '        output:')
    (10, '            bam=temp("results/chain/out-to-ref.bam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_bam', 'log']
    (11, '        log:')
    (12, '            "logs/chain/bam.log",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_bam', 'conda']
    (13, '        conda:')
    (14, '            "../envs/env.yml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_bam', 'threads']
    (15, '        threads: 8')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_bam', 'shell']
    (16, '        shell:')
    (17, '            """')
    (18, '            minimap2 -ax asm20 -Y --eqx -t {threads} \\\\')
    (19, '                {input.ref} {input.outgroup} \\\\')
    (20, '                    | samtools view -u - \\\\')
    (21, '                    | samtools sort -@ {threads} -m 8G - \\\\')
    (22, '                > {output.bam}')
    (23, '            """')
    (24, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_psl', 'input']
    (25, '    rule make_psl:')
    (26, '        input:')
    (27, '            bam=rules.make_bam.output.bam,')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_psl', 'output']
    (28, '        output:')
    (29, '            psl=temp("results/chain/out-to-ref.psl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_psl', 'log']
    (30, '        log:')
    (31, '            "logs/chain/psl.log",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_psl', 'conda']
    (32, '        conda:')
    (33, '            "../envs/env.yml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_psl', 'shell']
    (34, '        shell:')
    (35, '            """')
    (36, '            bamToPsl {input.bam} {output.psl}  ')
    (37, '            """')
    (38, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_chain', 'input']
    (39, '    rule make_chain:')
    (40, '        input:')
    (41, '            psl=rules.make_psl.output.psl,')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_chain', 'output']
    (42, '        output:')
    (43, '            chain=CHAIN,')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_chain', 'log']
    (44, '        log:')
    (45, '            "logs/chain/chain.log",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_chain', 'conda']
    (46, '        conda:')
    (47, '            "../envs/env.yml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "chain" not in config', 'rule make_chain', 'shell']
    (48, '        shell:')
    (49, '            """')
    (50, '            pslToChain {input.psl} {output.chain}')
    (51, '            """')
    (52, '')
    (53, '')
    (54, '#')
    (55, '#')
    (56, '#')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "ancestor" in config', 'rule prep_ancestor', 'input']
    (96, 'if "ancestor" in config:')
    (97, '')
    (98, '    rule prep_ancestor:')
    (99, '        input:')
    (100, '            ancestor=ANCESTOR_FA,')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "ancestor" in config', 'rule prep_ancestor', 'output']
    (101, '        output:')
    (102, '            fasta=temp("results/ancestral-fasta/{chrm}.fa"),')
    (103, '            fai=temp("results/ancestral-fasta/{chrm}.fa.fai"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "ancestor" in config', 'rule prep_ancestor', 'log']
    (104, '        log:')
    (105, '            "results/ancestral-fasta/{chrm}.log",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "ancestor" in config', 'rule prep_ancestor', 'conda']
    (106, '        conda:')
    (107, '            "../envs/env.yml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mrvollger/mutyper_workflow, file=workflow/rules/mutyper.smk
context_key: ['if "ancestor" in config', 'rule prep_ancestor', 'shell']
    (108, '        shell:')
    (109, '            """')
    (110, '            samtools faidx {input.ancestor} {wildcards.chrm} \\\\')
    (111, '                | seqtk seq -l 60 > {output.fasta}')
    (112, '            samtools faidx {output.fasta}')
    (113, '            """')
    (114, '')
    (115, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/qc.smk
context_key: ['if [ "$(wc $f|awk \\\'{{print $1}}\\\')" != "0" ];the']
    (25, 'if [ "$(wc $f|awk \\\'{{print $1}}\\\')" != "0" ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/align.smk
context_key: ['if [ -d /lscratch/${{SLURM_JOB_ID}} ];the']
    (36, 'if [ -d /lscratch/${{SLURM_JOB_ID}} ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/align.smk
context_key: ['if [ ! -d {params.outdir} ];then mkdir {params.outdir};f']
    (42, 'if [ ! -d {params.outdir} ];then mkdir {params.outdir};fi')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/align.smk
context_key: ['if [ "{params.peorse}" == "PE" ];the']
    (43, 'if [ "{params.peorse}" == "PE" ];then')
    (44, '# paired-end')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/align.smk
context_key: ['if [ -d /lscratch/${{SLURM_JOB_ID}} ];the']
    (237, 'if [ -d /lscratch/${{SLURM_JOB_ID}} ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/align.smk
context_key: ['if [ ! -d {params.outdir} ];then mkdir {params.outdir};f']
    (243, 'if [ ! -d {params.outdir} ];then mkdir {params.outdir};fi')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/align.smk
context_key: ['if [ "$limitSjdbInsertNsj" -lt "400000" ];then limitSjdbInsertNsj="400000";f']
    (245, 'if [ "$limitSjdbInsertNsj" -lt "400000" ];then limitSjdbInsertNsj="400000";fi')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/align.smk
context_key: ['if [ "{params.peorse}" == "PE" ];the']
    (246, 'if [ "{params.peorse}" == "PE" ];then')
    (247, '# paired-end')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/findcircrna.smk
context_key: ['if [ ! -d {params.outdir} ];then mkdir {params.outdir};f']
    (108, 'if [ ! -d {params.outdir} ];then mkdir {params.outdir};fi')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/findcircrna.smk
context_key: ['if [ "{params.peorse}" == "PE" ];the']
    (174, 'if [ "{params.peorse}" == "PE" ];then')
    (175, '    ## paired-end')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/findcircrna.smk
context_key: ['if [[ "$(cat {input.quantfile} | wc -l)" != "0" ]']
    (341, 'if [[ "$(cat {input.quantfile} | wc -l)" != "0" ]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/findcircrna.smk
context_key: ['if [ ! -d $outdir ];then mkdir -p $outdir;f']
    (362, 'if [ ! -d $outdir ];then mkdir -p $outdir;fi')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/findcircrna.smk
context_key: ['if [ -d /lscratch/${{SLURM_JOB_ID}} ];the']
    (421, 'if [ -d /lscratch/${{SLURM_JOB_ID}} ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/findcircrna.smk
context_key: ['if [ "{params.peorse}" == "PE" ];the']
    (430, 'if [ "{params.peorse}" == "PE" ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/findcircrna.smk
context_key: ['if [ -d /lscratch/${{SLURM_JOB_ID}} ];the']
    (547, 'if [ -d /lscratch/${{SLURM_JOB_ID}} ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/findcircrna.smk
context_key: ['if [ "{params.peorse}" == "PE" ];the']
    (556, 'if [ "{params.peorse}" == "PE" ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/findcircrna.smk
context_key: ['if [ -d /lscratch/${{SLURM_JOB_ID}} ];the']
    (612, 'if [ -d /lscratch/${{SLURM_JOB_ID}} ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/findcircrna.smk
context_key: ['if [ -d /lscratch/${{SLURM_JOB_ID}} ];the']
    (674, 'if [ -d /lscratch/${{SLURM_JOB_ID}} ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_align_processing.smk
context_key: ['if [ -d /lscratch/${{SLURM_JOB_ID}} ];the']
    (19, 'if [ -d /lscratch/${{SLURM_JOB_ID}} ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_align_processing.smk
context_key: ['if [ -d /lscratch/${{SLURM_JOB_ID}} ];the']
    (71, 'if [ -d /lscratch/${{SLURM_JOB_ID}} ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_align_processing.smk
context_key: ['if [ -d /lscratch/${{SLURM_JOB_ID}} ];the']
    (100, 'if [ -d /lscratch/${{SLURM_JOB_ID}} ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_align_processing.smk
context_key: ['if [ "{params.peorse}" == "PE" ];the']
    (107, 'if [ "{params.peorse}" == "PE" ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_align_processing.smk
context_key: ['if [ -d /lscratch/${{SLURM_JOB_ID}} ];the']
    (134, 'if [ -d /lscratch/${{SLURM_JOB_ID}} ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_align_processing.smk
context_key: ['if [ "$(wc -l $bdg|awk \\\'{{print $1}}\\\')" != "0" ];the']
    (151, 'if [ "$(wc -l $bdg|awk \\\'{{print $1}}\\\')" != "0" ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_align_processing.smk
context_key: ['if [ -d /lscratch/${{SLURM_JOB_ID}} ];the']
    (179, 'if [ -d /lscratch/${{SLURM_JOB_ID}} ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_align_processing.smk
context_key: ['if [ "$(wc -l $bdg|awk \\\'{{print $1}}\\\')" != "0" ];the']
    (196, 'if [ "$(wc -l $bdg|awk \\\'{{print $1}}\\\')" != "0" ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_findcircrna_processing.smk
context_key: ['if [[ "$(cat /dev/shm/{params.sample}.ciri.lst | wc -l)" != "0" ]];the']
    (21, 'if [[ "$(cat /dev/shm/{params.sample}.ciri.lst | wc -l)" != "0" ]];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_findcircrna_processing.smk
context_key: ['if [[ "$(cat /dev/shm/{params.sample}.circExplorer.lst | wc -l)" != "0" ]];the']
    (22, '    if [[ "$(cat /dev/shm/{params.sample}.circExplorer.lst | wc -l)" != "0" ]];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_findcircrna_processing.smk
context_key: ['if [ -d /lscratch/${{SLURM_JOB_ID}} ];the']
    (86, 'if [ -d /lscratch/${{SLURM_JOB_ID}} ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_findcircrna_processing.smk
context_key: ['if [ "$(wc -l $bdg|awk \\\'{{print $1}}\\\')" != "0" ];the']
    (123, 'if [ "$(wc -l $bdg|awk \\\'{{print $1}}\\\')" != "0" ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_findcircrna_processing.smk
context_key: ['if [ "$(wc -l $plusbdg|awk \\\'{{print $1}}\\\')" != "0" ];the']
    (126, '    if [ "$(wc -l $plusbdg|awk \\\'{{print $1}}\\\')" != "0" ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_findcircrna_processing.smk
context_key: ['if [ "$(wc -l $minusbdg|awk \\\'{{print $1}}\\\')" != "0" ];the']
    (131, '    if [ "$(wc -l $minusbdg|awk \\\'{{print $1}}\\\')" != "0" ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_findcircrna_processing.smk
context_key: ['if [ -d /lscratch/${{SLURM_JOB_ID}} ];the']
    (165, 'if [ -d /lscratch/${{SLURM_JOB_ID}} ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_DAQ, file=workflow/rules/post_findcircrna_processing.smk
context_key: ['if [ -d /lscratch/${{SLURM_JOB_ID}} ];the']
    (211, 'if [ -d /lscratch/${{SLURM_JOB_ID}} ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/classifyReads.smk
context_key: ['if (seq_type == "pe")']
    (16, 'if (seq_type == "pe"):')
    (17, "    paired_string = \\'--paired\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['if [ ! -d {output.map2Ref} ]; then']
    (99, '      if [ ! -d {output.map2Ref} ]; then ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['if [ $(ls -l {input.refSeq} | wc -l) -gt 1 ]; the']
    (102, '      if [ $(ls -l {input.refSeq} | wc -l) -gt 1 ]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['if [[ "{params.mappingTool}" == "bowtie2" ]]; the']
    (106, '          if [[ "{params.mappingTool}" == "bowtie2" ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['if [[ "{params.seq_type}" == "pe" ]]; the']
    (109, '            if [[ "{params.seq_type}" == "pe" ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['elif [[ "{params.seq_type}" == "se" ]]; the']
    (111, '            elif [[ "{params.seq_type}" == "se" ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['if [[ "{params.seq_type}" == "pe" ]]; the']
    (117, '            if [[ "{params.seq_type}" == "pe" ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['elif [[ "{params.seq_type}" == "se" ]]; the']
    (119, '            elif [[ "{params.seq_type}" == "se" ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['if [[ -s {output.map2Ref}/$refName.coverage.txt ]]; the']
    (128, '          if [[ -s {output.map2Ref}/$refName.coverage.txt ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['if [[ -s {output.map2Ref}/$refName.0.bed ]]; then  #if {output.map2Ref}/$refName.0.bed is not empt']
    (140, '            if [[ -s {output.map2Ref}/$refName.0.bed ]]; then  #if {output.map2Ref}/$refName.0.bed is not empty')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['if [ ! -d {output.map2Contig} ]; then']
    (176, '      if [ ! -d {output.map2Contig} ]; then ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['if [ ! -d {output.finalAssembly} ]; then']
    (181, '      if [ ! -d {output.finalAssembly} ]; then ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['if [ ! -d {output.pseudoContig} ]; then']
    (186, '      if [ ! -d {output.pseudoContig} ]; then ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['if [[ -s {input.refNamex} ]]; then   #check if file is not empty']
    (192, '      if [[ -s {input.refNamex} ]]; then   #check if file is not empty  ')
    (193, '        #process each reference')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['if [[ "{params.mappingTool}" == "bowtie2" ]']
    (209, '          if [[ "{params.mappingTool}" == "bowtie2" ]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['if [[ "{params.seq_type}" == "pe" ]']
    (212, '            if [[ "{params.seq_type}" == "pe" ]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['elif [[ "{params.seq_type}" == "se" ]']
    (215, '            elif [[ "{params.seq_type}" == "se" ]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['if [[ "{params.seq_type}" == "pe" ]']
    (221, '            if [[ "{params.seq_type}" == "pe" ]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/mappingReads.smk
context_key: ['elif [[ "{params.seq_type}" == "se" ]']
    (224, '            elif [[ "{params.seq_type}" == "se" ]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/makeReport.smk
context_key: ['if [ ! {strand1} ]; the']
    (211, '        if [ ! {strand1} ]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/makeReport.smk
context_key: ['if [ ! -d {reportDir}/blastnx ]; the']
    (223, '        if [ ! -d {reportDir}/blastnx ]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/makeReport.smk
context_key: ['if [[ $found >0 ]]; the']
    (229, '        if [[ $found >0 ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=Snakefile
context_key: ['try', 'except', 'if os.path.isfile(reportFile)']
    (54, 'if os.path.isfile(reportFile):')
    (55, '    os.remove(reportFile)')
    (56, '')
    (57, '## Check whether input fastq files are compressed or not')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=Snakefile
context_key: ['if  "input_format" in config.keys()']
    (58, 'if  "input_format" in config.keys():')
    (59, '    input_format = config["input_format"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=Snakefile
context_key: ['if fastqDir', 'if not os.path.exists(rawReadDir)']
    (68, 'if fastqDir:')
    (69, '    #set up soft link for fastq files if fastq folder is surpplied')
    (70, '    if not os.path.exists(rawReadDir):')
    (71, '        os.system("ln -sf " + fastqDir + " " + rawReadDir)')
    (72, '    # Check if given path is link')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=Snakefile
context_key: ['if fastqDir', 'if os.path.exists(rawReadDir) and os.path.islink(rawReadDir)']
    (73, '    if os.path.exists(rawReadDir) and os.path.islink(rawReadDir):')
    (74, '        print(fastqDir, " soft link is built." )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=Snakefile
context_key: ['if fastqDir', 'else']
    (75, '    else:')
    (76, '        # soft link broken, copy files')
    (77, '        if not os.path.isdir(rawReadDir): #check a folder exists')
    (78, '            os.mkdir(rawReadDir) #make a folder')
    (79, '        os.system("cp " + fastqDir + "/*." + input_format + " " + rawReadDir + "/")  ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=Snakefile
context_key: ['elif flowCellDir and len(os.listdir(flowCellDir)) != 0']
    (81, 'elif flowCellDir and len(os.listdir(flowCellDir)) != 0 :')
    (82, '    print("Runing bcl2fastq")')
    (83, '    if not os.path.isdir(rawReadDir): #check a folder exists')
    (84, '        os.mkdir(rawReadDir) #make a folder')
    (85, '    os.system("bcl2fastq -R " + flowCellDir + " -o " + rawReadDir + bclOption + ">> " + rawReadDir + "/bcl2fastq.log 2>&1")')
    (86, '    #--sample-sheet SampleSheet.csv')
    (87, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=Snakefile
context_key: ['if not os.listdir(rawReadDir)']
    (88, 'if not os.listdir(rawReadDir): ')
    (89, '    print(rawReadDir + " is empty. Exit!") ')
    (90, '    sys.exit()')
    (91, '')
    (92, '#get sample names from rawReadDir')
    (93, "#print(config[\\'workdir\\'])")
    (94, "#print(os.path.basename(config[\\'run_info\\'][\\'raw_fastq\\']))")
    (95, '#SAMPLES, = glob_wildcards(rawReadDir + "/{sample}_R1_001.fastq.gz")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=Snakefile
context_key: ['if (blastDbType == "all")', 'rule all']
    (170, 'if (blastDbType == "all"):')
    (171, '\\t#Run Blastn againt NCBI nt\\t\\t')
    (172, '\\tallBlastnt = expand(annotateDir + "/{sample}.blastnt.txt", sample=SAMPLES)')
    (173, '\\t#Run Diamond/Blastx againt NCBI nr\\t\\t')
    (174, '\\tallBlastnr = expand(annotateDir + "/{sample}.blastnr.txt", sample=SAMPLES)')
    (175, '\\trunBlast.append(allBlastnt)')
    (176, '\\trunBlast.append(allBlastnr)')
    (177, '')
    (178, 'rule all:')
    (179, '\\tinput: #targets')
    (180, '\\t\\t#QC raw reads')
    (181, '\\t\\trawFastQC,')
    (182, '')
    (183, '\\t\\t#remove host ribosomal RNAs')
    (184, '\\t\\trRNACheck,')
    (185, '')
    (186, '\\t\\t#remove duplicate reads')
    (187, '\\t\\trmDup,')
    (188, '\\t\\t')
    (189, '\\t\\t#remove PhiX174 contaminant')
    (190, '\\t\\trmCtm,')
    (191, '\\t\\t')
    (192, '\\t\\t#Trim reads')
    (193, '\\t\\ttrim,')
    (194, '\\t\\t')
    (195, '\\t\\t#QC trimmed reads')
    (196, '\\t\\ttrimmedFastQC,')
    (197, '')
    (198, '\\t\\t#summary fastqc by multiqc')
    (199, '\\t\\tqcDir + "/multiqc/raw_multiqc.html", #run multiqc for raw reads fastqc')
    (200, '\\t\\tqcDir + "/multiqc/trimmed_multiqc.html", #run multiqc for trimmed reads fastqc')
    (201, '')
    (202, '\\t\\t#kraken-krona')
    (203, '\\t\\texpand(classifyDir + "/{sample}.kraken2.report.html", sample=SAMPLES),')
    (204, '\\t\\t#extract pathogen reads')
    (205, '\\t\\textractPathRead,')
    (206, '\\t\\t')
    (207, '\\t\\t#Kaiju-krona')
    (208, '\\t\\texpand(classifyDir + "/{sample}.kaiju.table.txt", sample=SAMPLES),')
    (209, '\\t\\texpand(classifyDir + "/{sample}.kaiju_krona.html", sample=SAMPLES),')
    (210, '')
    (211, '\\t\\t#assemble pathogen reads')
    (212, '\\t\\texpand(assembleDir + "/{sample}/contigs.fasta", sample=SAMPLES),')
    (213, '\\t\\texpand(qcDir + "/quast/{sample}.quast", sample=SAMPLES),')
    (214, '\\t\\t#quast multiqc')
    (215, '\\t\\tqcDir + "/multiqc/quast_multiqc.html", #run multiqc for quast')
    (216, '')
    (217, '\\t\\t#annotate contigs using blastn and blastx')
    (218, '\\t\\trunBlast,')
    (219, '\\t\\t#reteive references ')
    (220, '\\t\\texpand(annotateDir + "/{sample}.refNamen.txt", sample=SAMPLES),')
    (221, '\\t\\t')
    (222, '\\t\\t#mapping reads to references')
    (223, '\\t\\texpand(logDir + "/checkPoint/{sample}.retrieveRef.done", sample=SAMPLES),')
    (224, '\\t\\t')
    (225, '\\t\\t#mapping reads to contigs, possible novel virus')
    (226, "\\t\\texpand(logDir + \\'/checkPoint/{sample}.map2Contig.done\\', sample=SAMPLES),")
    (227, '')
    (228, '\\t\\t#blastn consensus against NCBI, if failed, run blastx')
    (229, '\\t\\texpand(logDir + "/checkPoint/{sample}.blastn.done", sample=SAMPLES),')
    (230, '\\t\\t')
    (231, '\\t\\treportFile,')
    (232, '\\t\\thtmlReport')
    (233, '')
    (234, '\\tmessage: "Rule all"')
    (235, '\\tshell: "echo Job done    `date \\\'+%Y-%m-%d %H:%M\\\'`"\\t\\t\\t')
    (236, "'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=healthyPlant/PhytoPipe, file=rules/cleanReads.smk
context_key: ['if "_001." in rawFile']
    (25, '  if "_001." in rawFile:')
    (26, '    true001 = 1')
    (27, '    break')
    (28, '')
    (29, '#print(rawFiles)')
    (30, '')
    (31, '#for ambiguous input')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SilasK/FastDrep, file=compare.smk
context_key: ['if not "mem" in r.resources']
    (35, '    if not "mem" in r.resources:')
    (36, '        r.resources["mem_mb"]=config["mem"][\\\'default\\\'] *1000')
    (37, '    if not "time" in r.resources:')
    (38, '        r.resources["time_min"]=config["runtime"]["default"] *60')
    (39, '')
    (40, '#')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ding-lab/cptac_rna_expression, file=Snakefile
context_key: ['if len(SAMPLES) != len(set(SAMPLES))']
    (22, 'if len(SAMPLES) != len(set(SAMPLES)):')
    (23, "    _logger.error(\\'There are duplicated samples in the sample list!\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ding-lab/cptac_rna_expression, file=Snakefile
context_key: ['if sample_name not in SAMPLES']
    (36, '        if sample_name not in SAMPLES:')
    (37, '            continue')
    (38, '        is_genomic_rna_bam = (')
    (39, "            row[\\'experimental_strategy\\'] == \\'RNA-Seq\\' and")
    (40, "            row[\\'data_format\\'] == \\'BAM\\' and")
    (41, "            row[\\'reference\\'] == \\'hg38\\' and")
    (42, "            row[\\'data_path\\'].endswith(\\'.rna_seq.genomic.gdc_realn.bam\\')")
    (43, '        )')
    (44, '        if not is_genomic_rna_bam:')
    (45, "            _logger.warning(f\\'{sample_name} is not a genomic RNA-Seq BAM\\')")
    (46, '')
    (47, "        bam_pth = Path(row[\\'data_path\\'])")
    (48, '        SAMPLE_INFO[sample_name] = SampleInfo(')
    (49, "            row[\\'case\\'], row[\\'disease\\'], row[\\'UUID\\'], bam_pth")
    (50, '        )')
    (51, '')
    (52, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Roett/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'if len(fol_path_spillover_slide_acs) > 0']
    (117, 'if do_compensation:')
    (118, '    if len(file_path_orig_sm) == 0:')
    (119, '        if len(fol_path_spillover_slide_acs) > 0:')
    (120, "            spillover_mode = \\'estimate_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Roett/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'else']
    (121, '        else:')
    (122, '            raise ValueError(\\\'Either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (123, "                             \\'needs to be provided.\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Roett/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'elif len(fol_path_spillover_slide_acs) == 0']
    (124, '    elif len(fol_path_spillover_slide_acs) == 0:')
    (125, "            spillover_mode = \\'copy_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Roett/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'else']
    (126, '    else:')
    (127, '        raise ValueError(\\\'Only provide either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (128, "                         \\'but not both.\\')")
    (129, '')
    (130, '# Produce a list of all cellprofiler output files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Roett/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if not do_compensation']
    (267, 'if not do_compensation:')
    (268, "    config_dict_cp[\\'measuremasks\\'] = {")
    (269, '        \\\'message\\\':  """\\\\')
    (270, '                    Measure masks ')
    (271, '                    ')
    (272, '                    Measure segmentation masks')
    (273, '                    """,')
    (274, "        \\'run_size\\': 1,")
    (275, "        \\'plugins\\': cp_plugins,")
    (276, "        \\'pipeline\\': \\'resources/cp4_pipelines/adapted/3_measure_mask_basic.cppipe\\',")
    (277, "        \\'input_files\\': [")
    (278, '            # Folder containing segmentation masks')
    (279, '            fol_path_cellmasks,')
    (280, '            # Folder containing image stacks to measure')
    (281, '            fol_path_full,')
    (282, '            # Folder containing cell probabilities')
    (283, '            fol_path_probabilities,')
    (284, '            # Acquisition metadata')
    (285, '            file_path_acmeta,')
    (286, '            # Channel metadata')
    (287, '            file_path_channelmeta_full,')
    (288, '            file_path_channelmeta_cellprobab')
    (289, '        ],')
    (290, "        \\'output_patterns\\': {\\'.\\':")
    (291, '                            # Folder containing cellprofiler measurement')
    (292, '                                cp_measurements_output_files,')
    (293, '                            # Subfolder containing segmentation masks used')
    (294, '                            # for the measurements')
    (295, "                            \\'masks\\': directory(fol_path_cp_masks),")
    (296, '                            # Subfolder containing the images used for the')
    (297, '                            # measurements.')
    (298, "                            \\'images\\': directory(fol_path_cp_images)},")
    (299, "        \\'input_folder\\': fol_path_cp_input")
    (300, '    }')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Roett/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'params']
    (454, 'if do_compensation:')
    (455, '    if spillover_mode == "estimate_sm":')
    (456, '        rule get_ss_panel_from_panel:')
    (457, '            input: csv_panel')
    (458, '            output: file_path_ss_panel')
    (459, '            params:')
    (460, '                col_metal=csv_panel_metal,')
    (461, '                col_spillover=csv_panel_spillover')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Roett/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'run']
    (462, '            run:')
    (463, '                dat_csv = pd.read_csv(csv_panel)')
    (464, '                assert params.col_spillover in dat_csv.columns, ValueError(')
    (465, '                    f"The panel csv {csv_panel} should contain a boolean column"')
    (466, '                    f"{params.col_spillover} indicating the metals used for the"')
    (467, '                    f"spillover acquisitions.")')
    (468, '                metals = dat_csv.query(')
    (469, "                    f\\'{params.col_spillover} == 1\\')[params.col_metal].values")
    (470, "                with open(output[0], \\'w\\') as f:")
    (471, '                    for m in metals:')
    (472, "                        f.write(m+\\'\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=WEHIGenomicsRnD/duplex-seq-pipeline, file=workflow/rules/preprocess.smk
context_key: ['if correct_umis', 'rule correct_umis', 'input']
    (82, 'if correct_umis:')
    (83, '')
    (84, '    rule correct_umis:')
    (85, '        input:')
    (86, '            bam="results/unmapped_bams/{sample}_unmapped_uncorrected_wUMI.bam",')
    (87, '            umis=config["umis"],')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=WEHIGenomicsRnD/duplex-seq-pipeline, file=workflow/rules/preprocess.smk
context_key: ['if correct_umis', 'rule correct_umis', 'output']
    (88, '        output:')
    (89, '            "results/unmapped_bams/{sample}_unmapped_wUMI.bam",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=WEHIGenomicsRnD/duplex-seq-pipeline, file=workflow/rules/preprocess.smk
context_key: ['if correct_umis', 'rule correct_umis', 'log']
    (90, '        log:')
    (91, '            "logs/correct_umis_{sample}.log",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=WEHIGenomicsRnD/duplex-seq-pipeline, file=workflow/rules/preprocess.smk
context_key: ['if correct_umis', 'rule correct_umis', 'conda']
    (92, '        conda:')
    (93, '            "../envs/fgbio.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=WEHIGenomicsRnD/duplex-seq-pipeline, file=workflow/rules/preprocess.smk
context_key: ['if correct_umis', 'rule correct_umis', 'threads']
    (94, '        threads: cluster["fgbio"]["threads"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=WEHIGenomicsRnD/duplex-seq-pipeline, file=workflow/rules/preprocess.smk
context_key: ['if correct_umis', 'rule correct_umis', 'resources']
    (95, '        resources:')
    (96, '            mem_mb=cluster["fgbio"]["mem_mb"],')
    (97, '            runtime=cluster["fgbio"]["runtime"],')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=WEHIGenomicsRnD/duplex-seq-pipeline, file=workflow/rules/preprocess.smk
context_key: ['if correct_umis', 'rule correct_umis', 'params']
    (98, '        params:')
    (99, '            max_mismatches=config["max_mismatches"],')
    (100, '            min_distance=config["min_distance"],')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=WEHIGenomicsRnD/duplex-seq-pipeline, file=workflow/rules/preprocess.smk
context_key: ['if correct_umis', 'rule correct_umis', 'shell']
    (101, '        shell:')
    (102, '            """')
    (103, '            fgbio -Xmx{resources.mem_mb}m \\\\')
    (104, '                -Djava.io.tmpdir={resources.tmpdir} \\\\')
    (105, '                CorrectUmis \\\\')
    (106, '                --input={input.bam} \\\\')
    (107, '                --output={output} \\\\')
    (108, '                --max-mismatches={params.max_mismatches} \\\\')
    (109, '                --min-distance={params.min_distance} \\\\')
    (110, '                -t RX -U {input.umis}')
    (111, '            """')
    (112, '')
    (113, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/human_genomics_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no") and (config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\')']
    (186, 'if (config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no") and (config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\'):')
    (187, '    include: "rules/gatk_HaplotypeCaller_single.smk"')
    (188, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/human_genomics_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no") and (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\')']
    (189, 'if (config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no") and (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\'):')
    (190, '    include: "rules/gatk_HaplotypeCaller_cohort.smk"')
    (191, '    include: "rules/gatk_CombineGVCFs.smk"')
    (192, '    include: "rules/gatk_GenotypeGVCFs.smk"')
    (193, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/human_genomics_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'GPU_ACCELERATED\\\'] == "Yes" or config[\\\'GPU_ACCELERATED\\\'] == "yes") and (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\')']
    (197, 'if (config[\\\'GPU_ACCELERATED\\\'] == "Yes" or config[\\\'GPU_ACCELERATED\\\'] == "yes") and (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\'):')
    (198, '    include: "rules/pbrun_triocombinegvcf.smk"')
    (199, '    include: "rules/gatk_GenotypeGVCFs.smk"\'')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if len(genome_IDs) == 0']
    (5, 'if len(genome_IDs) == 0:')
    (6, '    print("\\')
    (7, '    No assembly fasta files were found, is the config.yaml file set appropriately?\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule run_checkm', 'conda']
    (148, 'if not config["is_euk"]:')
    (149, '    rule run_checkm:')
    (150, '        conda:')
    (151, '            "envs/checkm.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule run_checkm', 'params']
    (152, '        params:')
    (153, '            pplacer_cpus = config["gtdb_tk_checkm_pplacer_cpus"],')
    (154, '            genomes_dir = config["genomes_dir"],')
    (155, '            extension = config["assembly_extension"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule run_checkm', 'resources']
    (156, '        resources:')
    (157, '            cpus = config["threads"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule run_checkm', 'output']
    (158, '        output:')
    (159, '            "checkm-summary.tsv"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule run_checkm', 'log']
    (160, '        log:')
    (161, '            config["logs_dir"] + "checkm.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule run_checkm', 'shell']
    (162, '        shell:')
    (163, '            """')
    (164, '            checkm lineage_wf -x {params.extension} -t {resources.cpus} --pplacer_threads {params.pplacer_cpus} --tab_table -f checkm-summary.tsv {params.genomes_dir} checkm-output > {log} 2>&1')
    (165, '            """')
    (166, '')
    (167, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule gtdb_tk_classify', 'conda']
    (168, '    rule gtdb_tk_classify:')
    (169, '        conda:')
    (170, '            "envs/gtdb-tk.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule gtdb_tk_classify', 'input']
    (171, '        input:')
    (172, '            gtdbtk_db_trigger = config["GTDB_DATA_PATH"] + "/" + config["GTDB_TRIGGER_FILE"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule gtdb_tk_classify', 'params']
    (173, '        params:')
    (174, '            pplacer_cpus = config["gtdb_tk_checkm_pplacer_cpus"],')
    (175, '            genomes_dir = config["genomes_dir"],')
    (176, '            extension = config["assembly_extension"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule gtdb_tk_classify', 'resources']
    (177, '        resources:')
    (178, '            cpus = config["threads"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule gtdb_tk_classify', 'output']
    (179, '        output:')
    (180, '            directory("gtdb-tk-out")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule gtdb_tk_classify', 'log']
    (181, '        log:')
    (182, '            config["logs_dir"] + "gtdb.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule gtdb_tk_classify', 'shell']
    (183, '        shell:')
    (184, '            """')
    (185, '            gtdbtk classify_wf -x {params.extension} --genome_dir {params.genomes_dir} --out_dir {output} --cpus {resources.cpus} --pplacer_cpus {params.pplacer_cpus} > {log} 2>&1')
    (186, '            """')
    (187, '')
    (188, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule combine_gtdb_classifications', 'input']
    (189, '    rule combine_gtdb_classifications:')
    (190, '        input:')
    (191, '            "gtdb-tk-out"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule combine_gtdb_classifications', 'output']
    (192, '        output:')
    (193, '            "gtdb-taxonomy.tsv"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule combine_gtdb_classifications', 'shell']
    (194, '        shell:')
    (195, '            "cut -f 1,2 gtdb-tk-out/classify/*summary.tsv > {output}"')
    (196, '')
    (197, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule combine_outputs', 'input']
    (198, '    rule combine_outputs:')
    (199, '        input:')
    (200, '            assembly_stats_tab = "summary-stats.tsv",')
    (201, '            checkm_tab = "checkm-summary.tsv",')
    (202, '            gtdb_tax_tab = "gtdb-taxonomy.tsv"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule combine_outputs', 'output']
    (203, '        output:')
    (204, '            str(config["output_prefix"]) + "-genome-summaries.tsv"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AstrobioMike/genome-assembly-summary, file=Snakefile
context_key: ['if not config["is_euk"]', 'rule combine_outputs', 'shell']
    (205, '        shell:')
    (206, '            "python scripts/combine-outputs.py -s {input.assembly_stats_tab} -c {input.checkm_tab} -t {input.gtdb_tax_tab} -o {output}"')
    (207, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexmsalmeida/genome-funcs, file=Snakefile
context_key: ['if not os.path.exists(dirname)']
    (24, '    if not os.path.exists(dirname):')
    (25, '        os.makedirs(dirname)')
    (26, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexmsalmeida/genome-funcs, file=Snakefile
context_key: ['if not os.path.exists(db_dir+"/amr_finder")']
    (27, 'if not os.path.exists(db_dir+"/amr_finder"):')
    (28, '        os.makedirs(db_dir+"/amr_finder")')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexmsalmeida/genome-funcs, file=Snakefile
context_key: ['if not os.path.exists(db_dir+"/antismash")']
    (30, 'if not os.path.exists(db_dir+"/antismash"):')
    (31, '        os.makedirs(db_dir+"/antismash")')
    (32, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yztxwd/snakemake-pipeline-general, file=rules/coverage.smk
context_key: ['if checkcontrol(samples)', 'rule bamCompare', 'input']
    (56, 'if checkcontrol(samples):')
    (57, '    rule bamCompare:')
    (58, '        input:')
    (59, '            ip="output/mapped/{sample}-{rep}.merge.sort.bam",')
    (60, '            ip_index="output/mapped/{sample}-{rep}.merge.sort.bam.bai",')
    (61, '            input=f"output/mapped/{samples.loc[samples[\\\'condition\\\']==\\\'control\\\', \\\'sample\\\'].iloc[0]}-{{rep}}.merge.sort.bam",')
    (62, '            input_index=f"output/mapped/{samples.loc[samples[\\\'condition\\\']==\\\'control\\\', \\\'sample\\\'].iloc[0]}-{{rep}}.merge.sort.bam.bai"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yztxwd/snakemake-pipeline-general, file=rules/coverage.smk
context_key: ['if checkcontrol(samples)', 'rule bamCompare', 'output']
    (63, '        output:')
    (64, '            "output/coverage/{sample}-{rep}.bamCompare.bw"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yztxwd/snakemake-pipeline-general, file=rules/coverage.smk
context_key: ['if checkcontrol(samples)', 'rule bamCompare', 'params']
    (65, '        params:')
    (66, "            config[\\'bamCompare\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yztxwd/snakemake-pipeline-general, file=rules/coverage.smk
context_key: ['if checkcontrol(samples)', 'rule bamCompare', 'log']
    (67, '        log:')
    (68, '            "logs/bamCompare/{sample}-{rep}.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yztxwd/snakemake-pipeline-general, file=rules/coverage.smk
context_key: ['if checkcontrol(samples)', 'rule bamCompare', 'threads']
    (69, '        threads:')
    (70, "            config[\\'threads\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yztxwd/snakemake-pipeline-general, file=rules/coverage.smk
context_key: ['if checkcontrol(samples)', 'rule bamCompare', 'resources']
    (71, '        resources:')
    (72, "            cpus=config[\\'threads\\'],")
    (73, "            mem=config[\\'mem\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yztxwd/snakemake-pipeline-general, file=rules/coverage.smk
context_key: ['if checkcontrol(samples)', 'rule bamCompare', 'conda']
    (74, '        conda:')
    (75, '            f"{snake_dir}/envs/deeptools.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yztxwd/snakemake-pipeline-general, file=rules/coverage.smk
context_key: ['if checkcontrol(samples)', 'rule bamCompare', 'shell']
    (76, '        shell:')
    (77, '            """')
    (78, '            bamCompare -b1 {input.ip} -b2 {input.input} -o {output} -of bigwig \\\\')
    (79, '                {params} -p {threads}')
    (80, '            """')
    (81, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/misc, file=workflow/rules/common.smk
context_key: ['if not workflow.overwrite_configfiles']
    (20, 'if not workflow.overwrite_configfiles:')
    (21, '    sys.exit("At least one config file must be passed using --configfile/--configfiles, by command line or a profile!")')
    (22, '')
    (23, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cite-seq-alevin-fry-seurat, file=workflow/rules/common.smk
context_key: ['if wildcards.sample == "adt"']
    (1, '    if wildcards.sample == "adt":')
    (2, '        return config["antibodies"]["adt-seqs"]')
    (3, '    elif wildcards.sample == "hto":')
    (4, '        return config["antibodies"]["hto-seqs"]')
    (5, '    elif wildcards.sample == "rna":')
    (6, '        return "resources/reference/rna/transcriptome.fasta"')
    (7, '')
    (8, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=myRNASeq/scRNASeqPipeline, file=Snakefile
context_key: ['if "markers" in config.get("celltype", {})']
    (17, 'if "markers" in config.get("celltype", {}):')
    (18, '    markers = pd.read_csv(config["celltype"]["markers"], sep="\\\\t").set_index("name", drop=False)')
    (19, '    markers.loc[:, "parent"].fillna("root", inplace=True)')
    (20, '')
    (21, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=genxnetwork/grape, file=workflows/remove_relatives/Snakefile
context_key: ["if flow == \\'ibis\\'"]
    (85, "if flow == \\'ibis\\':")
    (86, '    include: "../../rules/relatives_ibis.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=genxnetwork/grape, file=workflows/remove_relatives/Snakefile
context_key: ["elif flow == \\'germline\\'"]
    (87, "elif flow == \\'germline\\':")
    (88, '    include: "../../rules/relatives.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=genxnetwork/grape, file=workflows/remove_relatives/Snakefile
context_key: ["elif flow == \\'ibis_king\\'"]
    (89, "elif flow == \\'ibis_king\\':")
    (90, '    include: "../../rules/relatives_ibis_king.smk"')
    (91, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=BDI-pathogens/ShiverCovid, file=snakemake/Snakefile
context_key: ['else {params.conda_bin}/samtools index {input.bam} >{log} 2>&1; fi']
    (357, 'else {params.conda_bin}/samtools index {input.bam} >{log} 2>&1; fi"')
    (358, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tdayris-perso/bam-msisensor, file=Snakefile
context_key: ['if sys.version_info < (3, 8)']
    (5, 'if sys.version_info < (3, 8):\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmeppley/np_read_clustering, file=Snakefile
context_key: ['if GROUP >= 0']
    (98, 'if GROUP >= 0:')
    (99, '    logger.warning("Processing group: {}".format(GROUP))')
    (100, '')
    (101, '### File Locations and templates')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmeppley/np_read_clustering, file=rules/Snakefile.polish
context_key: ['if kind is None']
    (22, '    if kind is None:')
    (23, '        out_fnames = OUT_FNAMES.values()')
    (24, '    else:')
    (25, '        out_fnames = [OUT_FNAMES[kind],]')
    (26, '')
    (27, '    gr_cl_sc_s = [(group, cluster, subcluster)')
    (28, '                  for group, cluster in get_pre_filtered_clusters() \\\\')
    (29, '                  for subcluster in get_pre_filtered_subclusters(group, cluster) ]')
    (30, '    logger.debug(f"Found {len(gr_cl_sc_s)} subclusters total")')
    (31, '')
    (32, '    return [(SUBCLUSTER_DIR_TEMPLATE + "/" + fname).format(WORK_DIR=WORK_DIR, **locals()) \\\\')
    (33, '            for group, cluster, subcluster in gr_cl_sc_s')
    (34, '            for fname in out_fnames')
    (35, '           ]')
    (36, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmeppley/np_read_clustering, file=rules/Snakefile.subclusters
context_key: ["if \\'pfam_hmm_path\\' in config", 'rule genes_pfam', 'input']
    (54, "if \\'pfam_hmm_path\\' in config:")
    (55, '    rule genes_pfam:')
    (56, '        input:')
    (57, "            faa=\\'{prefix}/{name}.faa\\',")
    (58, "            hmm=config[\\'pfam_hmm_path\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmeppley/np_read_clustering, file=rules/Snakefile.subclusters
context_key: ["if \\'pfam_hmm_path\\' in config", 'rule genes_pfam', 'output']
    (59, '        output: ')
    (60, "            domtbl=\\'{prefix}/{name}.faa.pfam.tbl\\',")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmeppley/np_read_clustering, file=rules/Snakefile.subclusters
context_key: ["if \\'pfam_hmm_path\\' in config", 'rule genes_pfam', 'benchmark']
    (61, "        benchmark: \\'{prefix}/{name}.faa.pfam.tbl.time\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmeppley/np_read_clustering, file=rules/Snakefile.subclusters
context_key: ["if \\'pfam_hmm_path\\' in config", 'rule genes_pfam', 'conda']
    (62, "        conda: \\'../conda/genes.yaml\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmeppley/np_read_clustering, file=rules/Snakefile.subclusters
context_key: ["if \\'pfam_hmm_path\\' in config", 'rule genes_pfam', 'threads']
    (63, '        threads: HMMER_THREADS')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jmeppley/np_read_clustering, file=rules/Snakefile.subclusters
context_key: ["if \\'pfam_hmm_path\\' in config", 'rule genes_pfam', 'shell']
    (64, '        shell: "hmmsearch --cpu {threads} --domtblout {output.domtbl} {input.hmm} {input.faa} > /dev/null"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Zhanmengtao/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'if len(fol_path_spillover_slide_acs) > 0']
    (117, 'if do_compensation:')
    (118, '    if len(file_path_orig_sm) == 0:')
    (119, '        if len(fol_path_spillover_slide_acs) > 0:')
    (120, "            spillover_mode = \\'estimate_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Zhanmengtao/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'else']
    (121, '        else:')
    (122, '            raise ValueError(\\\'Either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (123, "                             \\'needs to be provided.\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Zhanmengtao/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'elif len(fol_path_spillover_slide_acs) == 0']
    (124, '    elif len(fol_path_spillover_slide_acs) == 0:')
    (125, "            spillover_mode = \\'copy_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Zhanmengtao/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'else']
    (126, '    else:')
    (127, '        raise ValueError(\\\'Only provide either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (128, "                         \\'but not both.\\')")
    (129, '')
    (130, '# Produce a list of all cellprofiler output files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Zhanmengtao/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if not do_compensation']
    (267, 'if not do_compensation:')
    (268, "    config_dict_cp[\\'measuremasks\\'] = {")
    (269, '        \\\'message\\\':  """\\\\')
    (270, '                    Measure masks ')
    (271, '                    ')
    (272, '                    Measure segmentation masks')
    (273, '                    """,')
    (274, "        \\'run_size\\': 1,")
    (275, "        \\'plugins\\': cp_plugins,")
    (276, "        \\'pipeline\\': \\'resources/cp4_pipelines/adapted/3_measure_mask_basic.cppipe\\',")
    (277, "        \\'input_files\\': [")
    (278, '            # Folder containing segmentation masks')
    (279, '            fol_path_cellmasks,')
    (280, '            # Folder containing image stacks to measure')
    (281, '            fol_path_full,')
    (282, '            # Folder containing cell probabilities')
    (283, '            fol_path_probabilities,')
    (284, '            # Acquisition metadata')
    (285, '            file_path_acmeta,')
    (286, '            # Channel metadata')
    (287, '            file_path_channelmeta_full,')
    (288, '            file_path_channelmeta_cellprobab')
    (289, '        ],')
    (290, "        \\'output_patterns\\': {\\'.\\':")
    (291, '                            # Folder containing cellprofiler measurement')
    (292, '                                cp_measurements_output_files,')
    (293, '                            # Subfolder containing segmentation masks used')
    (294, '                            # for the measurements')
    (295, "                            \\'masks\\': directory(fol_path_cp_masks),")
    (296, '                            # Subfolder containing the images used for the')
    (297, '                            # measurements.')
    (298, "                            \\'images\\': directory(fol_path_cp_images)},")
    (299, "        \\'input_folder\\': fol_path_cp_input")
    (300, '    }')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Zhanmengtao/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'params']
    (454, 'if do_compensation:')
    (455, '    if spillover_mode == "estimate_sm":')
    (456, '        rule get_ss_panel_from_panel:')
    (457, '            input: csv_panel')
    (458, '            output: file_path_ss_panel')
    (459, '            params:')
    (460, '                col_metal=csv_panel_metal,')
    (461, '                col_spillover=csv_panel_spillover')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Zhanmengtao/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'run']
    (462, '            run:')
    (463, '                dat_csv = pd.read_csv(csv_panel)')
    (464, '                assert params.col_spillover in dat_csv.columns, ValueError(')
    (465, '                    f"The panel csv {csv_panel} should contain a boolean column"')
    (466, '                    f"{params.col_spillover} indicating the metals used for the"')
    (467, '                    f"spillover acquisitions.")')
    (468, '                metals = dat_csv.query(')
    (469, "                    f\\'{params.col_spillover} == 1\\')[params.col_metal].values")
    (470, "                with open(output[0], \\'w\\') as f:")
    (471, '                    for m in metals:')
    (472, "                        f.write(m+\\'\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MetaSUB-CAMP/camp_short-read-assembly, file=workflow/Snakefile
context_key: ["if assembler == \\'rnaSPAdes\\'"]
    (30, "if assembler == \\'rnaSPAdes\\':")
    (31, "    assembler_call = \\'rnaspades.py\\'")
    (32, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MetaSUB-CAMP/camp_short-read-assembly, file=workflow/Snakefile
context_key: ["if assembler == \\'metaviralSPAdes\\'"]
    (33, "if assembler == \\'metaviralSPAdes\\':")
    (34, "    assembler_call = \\'metaviralspades.py\\'")
    (35, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MetaSUB-CAMP/camp_short-read-assembly, file=workflow/Snakefile
context_key: ["if assembler == \\'plasmidSPAdes\\'"]
    (36, "if assembler == \\'plasmidSPAdes\\':")
    (37, "    assembler_call = \\'metaplasmidspades.py\\'")
    (38, '')
    (39, "if assembler == \\'plasmidSPAdes\\':")
    (40, "    assembler_call = \\'metaplasmidspades.py\\'")
    (41, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MetaSUB-CAMP/camp_short-read-assembly, file=workflow/Snakefile
context_key: ["if assembler == \\'SPAdes\\'"]
    (42, "if assembler == \\'SPAdes\\':")
    (43, "    assembler_call = \\'spades.py\\'")
    (44, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MetaSUB-CAMP/camp_short-read-assembly, file=workflow/Snakefile
context_key: ["if \\'SPAdes\\' in assembler"]
    (45, "if \\'SPAdes\\' in assembler:")
    (46, '    ruleorder: run_metaspades > run_megahit')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MetaSUB-CAMP/camp_short-read-assembly, file=workflow/Snakefile
context_key: ["if \\'megahit\\' == assembler"]
    (47, "if \\'megahit\\' == assembler:")
    (48, '    ruleorder: run_megahit > run_metaspades')
    (49, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=williamjeong2/snakemake_RNA-seq, file=Snakefile
context_key: ["if fname.endswith(\\'_1.fastq\\')"]
    (31, "    if fname.endswith(\\'_1.fastq\\'):")
    (32, '        units.loc[[fname.split(\\\'_1.\\\')[0]], \\\'fq1\\\'] = "data/" + fname.split(\\\'_1.\\\')[0] + "_1.fastq"')
    (33, "    if fname.endswith(\\'_2.fastq\\'):")
    (34, '        units.loc[[fname.split(\\\'_2.\\\')[0]], \\\'fq2\\\'] = "data/" + fname.split(\\\'_2.\\\')[0] + "_2.fastq"')
    (35, "    if fname.endswith(\\'_1.fq.gz\\'):")
    (36, '        units.loc[[fname.split(\\\'_1.\\\')[0]], \\\'fq1\\\'] = "data/" + fname.split(\\\'_1.\\\')[0] + "_1.fq.gz"')
    (37, "    if fname.endswith(\\'_2.fq.gz\\'):")
    (38, '        units.loc[[fname.split(\\\'_2.\\\')[0]], \\\'fq2\\\'] = "data/" + fname.split(\\\'_2.\\\')[0] + "_2.fq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=williamjeong2/snakemake_RNA-seq, file=Snakefile_toCount
context_key: ["if fname.endswith(\\'_1.fastq\\')"]
    (31, "    if fname.endswith(\\'_1.fastq\\'):")
    (32, '        units.loc[[fname.split(\\\'_1.\\\')[0]], \\\'fq1\\\'] = "data/" + fname.split(\\\'_1.\\\')[0] + "_1.fastq"')
    (33, "    if fname.endswith(\\'_2.fastq\\'):")
    (34, '        units.loc[[fname.split(\\\'_2.\\\')[0]], \\\'fq2\\\'] = "data/" + fname.split(\\\'_2.\\\')[0] + "_2.fastq"')
    (35, "    if fname.endswith(\\'_1.fq.gz\\'):")
    (36, '        units.loc[[fname.split(\\\'_1.\\\')[0]], \\\'fq1\\\'] = "data/" + fname.split(\\\'_1.\\\')[0] + "_1.fq.gz"')
    (37, "    if fname.endswith(\\'_2.fq.gz\\'):")
    (38, '        units.loc[[fname.split(\\\'_2.\\\')[0]], \\\'fq2\\\'] = "data/" + fname.split(\\\'_2.\\\')[0] + "_2.fq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if mode == "pair"', 'rule Import_sequences', 'input']
    (175, 'if mode == "pair":')
    (176, '')
    (177, '    # Import pe-sequnces to qiime')
    (178, '    # Make sure you have a paired end specific manifest file - see example in examples folder')
    (179, '    rule Import_sequences:')
    (180, '        input: ')
    (181, '           expand(["01.raw_data/{sample}/{sample}_R1.fastq.gz",')
    (182, '                  "01.raw_data/{sample}/{sample}_R2.fastq.gz"], sample=config[\\\'samples\\\']),')
    (183, '           manifest_file=config["MANIFEST"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if mode == "pair"', 'rule Import_sequences', 'output']
    (184, '        output: "03.import/reads.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if mode == "pair"', 'rule Import_sequences', 'log']
    (185, '        log: "logs/Import_sequences/Import_sequences.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if mode == "pair"', 'rule Import_sequences', 'threads']
    (186, '        threads: 5')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if mode == "pair"', 'rule Import_sequences', 'params']
    (187, '        params:')
    (188, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (189, "            seq_dir=lambda w, input: path.dirname(input[0]).split(\\'/\\')[0]")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if mode == "pair"', 'rule Import_sequences', 'shell']
    (190, '        shell:')
    (191, '            """')
    (192, '            set +u')
    (193, '            {params.conda_activate}')
    (194, '            set -u')
    (195, '')
    (196, '            qiime tools import \\\\')
    (197, "                 --type \\'SampleData[PairedEndSequencesWithQuality]\\' \\\\")
    (198, '                 --input-path {input.manifest_file} \\\\')
    (199, '                 --output-path {output} \\\\')
    (200, '                 --input-format PairedEndFastqManifestPhred33')
    (201, '            """')
    (202, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if mode == "pair"', 'rule Trim_primers', 'params']
    (203, '    rule Trim_primers:')
    (204, '        input: rules.Import_sequences.output')
    (205, '        output: "04.Trim_primers/trimmed_reads.qza"')
    (206, '        log: "logs/Trim_primers/Trim_primers.log"')
    (207, '        threads: 10')
    (208, '        params:')
    (209, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (210, "            forward_primer=config[\\'parameters\\'][\\'cutadapt\\'][\\'forward_primer\\'],")
    (211, "            reverse_primer=config[\\'parameters\\'][\\'cutadapt\\'][\\'reverse_primer\\'],")
    (212, "            cores=config[\\'parameters\\'][\\'cutadapt\\'][\\'cores\\'],")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if mode == "pair"', 'rule Trim_primers', 'shell']
    (213, '        shell:')
    (214, '            """')
    (215, '            set +u')
    (216, '            {params.conda_activate}')
    (217, '            set -u')
    (218, '')
    (219, '            qiime cutadapt trim-paired \\\\')
    (220, '                 --i-demultiplexed-sequences {input} \\\\')
    (221, '                 --p-cores {params.cores} \\\\')
    (222, '                 --p-front-f {params.forward_primer} \\\\')
    (223, '                 --p-front-r {params.reverse_primer} \\\\')
    (224, '                 --o-trimmed-sequences {output} \\\\')
    (225, '                 --verbose')
    (226, '')
    (227, '            """')
    (228, '')
    (229, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "single"', 'rule Import_sequences', 'input']
    (230, 'elif mode == "single":')
    (231, '')
    (232, '    # Import se-sequnces to qiime')
    (233, '    rule Import_sequences:')
    (234, '        input: ')
    (235, '            expand("01.raw_data/{sample}.fastq.gz", sample=config[\\\'samples\\\']),')
    (236, '            manifest_file=config["MANIFEST"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "single"', 'rule Import_sequences', 'output']
    (237, '        output: "03.import/reads.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "single"', 'rule Import_sequences', 'log']
    (238, '        log: "logs/Import_sequences/Import_sequences.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "single"', 'rule Import_sequences', 'threads']
    (239, '        threads: 5')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "single"', 'rule Import_sequences', 'params']
    (240, '        params:')
    (241, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (242, '            seq_dir=lambda w, input: path.dirname(input[0])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "single"', 'rule Import_sequences', 'shell']
    (243, '        shell:')
    (244, '            """')
    (245, '            set +u')
    (246, '            {params.conda_activate}')
    (247, '            set -u')
    (248, '')
    (249, '            qiime tools import \\\\')
    (250, "                 --type \\'SampleData[SequencesWithQuality]\\' \\\\")
    (251, '                 --input-path {input.manifest_file} \\\\')
    (252, '                 --output-path {output} \\\\')
    (253, '                 --input-format SingleEndFastqManifestPhred33')
    (254, '            """')
    (255, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "single"', 'rule Trim_primers', 'params']
    (256, '    rule Trim_primers:')
    (257, '        input: rules.Import_sequences.output')
    (258, '        output: "04.Trim_primers/trimmed_reads.qza"')
    (259, '        log: "logs/Trim_primers/Trim_primers.log"')
    (260, '        threads: 10')
    (261, '        params:')
    (262, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (263, "            forward_primer=config[\\'parameters\\'][\\'cutadapt\\'][\\'forward_primer\\'],")
    (264, "            cores=config[\\'parameters\\'][\\'cutadapt\\'][\\'cores\\'],")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "single"', 'rule Trim_primers', 'shell']
    (265, '        shell:')
    (266, '            """')
    (267, '            set +u')
    (268, '            {params.conda_activate}')
    (269, '            set -u')
    (270, '')
    (271, '            qiime cutadapt trim-single \\\\')
    (272, '                 --i-demultiplexed-sequences {input} \\\\')
    (273, '                 --p-cores {params.cores} \\\\')
    (274, '                 --p-front {params.forward_primer} \\\\')
    (275, '                 --o-trimmed-sequences {output} \\\\')
    (276, '                 --verbose')
    (277, '')
    (278, '            """')
    (279, '')
    (280, '')
    (281, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'input']
    (282, 'elif mode == "merge":')
    (283, '')
    (284, '    if merge_method == "pear":')
    (285, '')
    (286, '        # Merge paired-end reads using pear - modifify the -m -t flags of pear before running the workflow    ')
    (287, '        rule Merge_reads:')
    (288, '            input:')
    (289, '                expand(["01.raw_data/{sample}/{sample}_R1.fastq.gz", ')
    (290, '                       "01.raw_data/{sample}/{sample}_R2.fastq.gz"], sample=config[\\\'samples\\\'])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'output']
    (291, '            output:')
    (292, '                expand("02.merge_reads/{sample}/{sample}.fastq.gz", sample=config[\\\'samples\\\'])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'log']
    (293, '            log: "logs/Merge_reads/Merge_reads.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'threads']
    (294, '            threads: 5')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'params']
    (295, '            params:')
    (296, '                out_dir=lambda w, output: path.dirname(output[0]),')
    (297, '                in_dir=lambda w, input: path.dirname(input[0]),')
    (298, "                program=config[\\'programs_path\\'][\\'run_pear\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'shell']
    (299, '            shell:')
    (300, '                """')
    (301, '                # Merge reads then delete unnecessary files')
    (302, '                 {params.program} -o {params.out_dir}/  {params.in_dir}/*.fastq.gz && \\\\')
    (303, '                 rm -rf {params.out_dir}/*.unassembled* {params.out_dir}/*discarded*')
    (304, '         ')
    (305, '               # gzip to save memory')
    (306, '         ')
    (307, '                 gzip {params.out_dir}/*.fastq')
    (308, '')
    (309, '                """')
    (310, '')
    (311, '')
    (312, '        # Import pe-joined reads to qiime')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'input']
    (313, '        rule Import_sequences:')
    (314, '            input: ')
    (315, '                rules.Merge_reads.output,')
    (316, '                manifest_file=config["MANIFEST"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'output']
    (317, '            output: "03.import/reads.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'log']
    (318, '            log: "logs/Import_sequences/Import_sequences.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'threads']
    (319, '            threads: 5')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'params']
    (320, '            params:')
    (321, '                conda_activate=config["conda"]["qiime2"]["env"],')
    (322, '                seq_dir=lambda w, input: path.dirname(input[0])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'shell']
    (323, '            shell:')
    (324, '                """')
    (325, '                set +u')
    (326, '                {params.conda_activate}')
    (327, '                set -u')
    (328, '')
    (329, '                qiime tools import \\\\')
    (330, "                     --type \\'SampleData[SequencesWithQuality]\\' \\\\")
    (331, '                     --input-path {input.manifest_file} \\\\')
    (332, '                     --output-path {output} \\\\')
    (333, '                     --input-format SingleEndFastqManifestPhred33')
    (334, '                """')
    (335, '')
    (336, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'input']
    (337, '    elif merge_method == "vsearch":')
    (338, '')
    (339, '        rule Import_sequences:')
    (340, '            input:')
    (341, '                expand(["01.raw_data/{sample}/{sample}_R1.fastq.gz", ')
    (342, '                       "01.raw_data/{sample}/{sample}_R2.fastq.gz"],sample=config[\\\'samples\\\']),')
    (343, '                manifest_file=config["MANIFEST"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'output']
    (344, '            output: "03.import/reads.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'log']
    (345, '            log: "logs/Import_sequences/Import_sequences.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'threads']
    (346, '            threads: 5')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'params']
    (347, '            params:')
    (348, '                conda_activate=config["conda"]["qiime2"]["env"],')
    (349, '                seq_dir=lambda w, input: path.dirname(input[0])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'shell']
    (350, '            shell:')
    (351, '                """')
    (352, '                set +u')
    (353, '                {params.conda_activate}')
    (354, '                set -u')
    (355, '')
    (356, '                qiime tools import \\\\')
    (357, "                     --type \\'SampleData[\\'PairedEndSequencesWithQuality\\']\\' \\\\")
    (358, '                     --input-path {input.manifest_file} \\\\')
    (359, '                     --output-path {output} ')
    (360, '                """')
    (361, '')
    (362, '        # Merge forwards and reverse reads using vsearch')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Merge_reads', 'shell']
    (363, '        rule Merge_reads:')
    (364, '            input: rules.Import_sequences.output')
    (365, '            output: "02.merge_reads/reads.qza"')
    (366, '            log: "logs/Merge_reads/Merge_reads.log"')
    (367, '            threads: 5')
    (368, '            shell:')
    (369, '                """')
    (370, '                set +u')
    (371, '                {params.conda_activate}')
    (372, '                set -u')
    (373, '')
    (374, '                qiime vsearch join-pairs \\\\')
    (375, '                     --i-demultiplexed-seqs {input} \\\\')
    (376, '                     --p-truncqual {params.trunc_qual} \\\\')
    (377, '                     --p-minlen {params.min_len} \\\\')
    (378, '                     --p-maxns {params.min_ns} \\\\')
    (379, '                     --p-minmergelen {params.men_merge_len} \\\\')
    (380, '                     --p-maxmergelen {params.max_merge_len} \\\\')
    (381, '                     --o-joined-sequences {output}')
    (382, '                """')
    (383, ' ')
    (384, '')
    (385, '# Demultiplex and View reads quality')
    (386, '# Analyze quality scores of 10000 random samples')
    (387, '')
    (388, '# If the merge method is vsearch, the input')
    (389, '# for demultiplexing should be the merged reads after')
    (390, '# Import else use the Imported reads')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if merge_method == "vsearch"', 'rule Qaulity_check', 'params']
    (391, 'if merge_method == "vsearch":')
    (392, '')
    (393, '    rule Qaulity_check:')
    (394, '        input: rules.Merge_reads.output')
    (395, '        output: "04.QC/qual_viz.qzv"')
    (396, '        log: "logs/Qaulity_check/Qaulity_check.log"')
    (397, '        threads: 10')
    (398, '        params:')
    (399, '            conda_activate=config["conda"]["qiime2"]["env"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if merge_method == "vsearch"', 'rule Qaulity_check', 'shell']
    (400, '        shell:')
    (401, '            """')
    (402, '            set +u')
    (403, '            {params.conda_activate}')
    (404, '            set -u')
    (405, '')
    (406, '            qiime demux summarize \\\\')
    (407, '                --p-n 10000 \\\\')
    (408, '                --i-data {input} \\\\')
    (409, '                --o-visualization {output}')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if merge_method == "vsearch"', 'rule Qaulity_check']
    (410, '        """')
    (411, '')
    (412, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if denoise_method == "dada2"', 'rule Denoise_reads', 'output']
    (457, 'if denoise_method == "dada2":')
    (458, '    ')
    (459, '        ')
    (460, '    rule Denoise_reads:')
    (461, '        input: rules.Trim_primers.output #rules.Import_sequences.output')
    (462, '        output: ')
    (463, '            table="05.Denoise_reads/table.qza",')
    (464, '            rep_seqs="05.Denoise_reads/representative_sequences.qza",')
    (465, '            stats="05.Denoise_reads/denoise_stats.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if denoise_method == "dada2"', 'rule Denoise_reads', 'log']
    (466, '        log: "logs/Denoise_reads/Denoise_reads.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if denoise_method == "dada2"', 'rule Denoise_reads', 'threads']
    (467, '        threads: 30')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if denoise_method == "dada2"', 'rule Denoise_reads', 'params']
    (468, '        params:')
    (469, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (470, "            mode=config[\\'parameters\\'][\\'dada2\\'][\\'mode\\'],")
    (471, "            trun_len_forward=config[\\'parameters\\'][\\'dada2\\'][\\'trunc_length_forward\\'],")
    (472, "            trun_len_reverse=config[\\'parameters\\'][\\'dada2\\'][\\'trunc_length_reverse\\'],")
    (473, "            trim_len_forward=config[\\'parameters\\'][\\'dada2\\'][\\'trim_length_forward\\'],")
    (474, "            trim_len_reverse=config[\\'parameters\\'][\\'dada2\\'][\\'trim_length_reverse\\'],")
    (475, "            max_forward_err=config[\\'parameters\\'][\\'dada2\\'][\\'maximum_forward_error\\'],")
    (476, "            max_reverse_err=config[\\'parameters\\'][\\'dada2\\'][\\'maximum_reverse_error\\'],")
    (477, "            threads=config[\\'parameters\\'][\\'dada2\\'][\\'threads\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if denoise_method == "dada2"', 'rule Denoise_reads', 'shell']
    (478, '        shell:')
    (479, '            """')
    (480, '            set +u')
    (481, '            {params.conda_activate}')
    (482, '            set -u')
    (483, '            ')
    (484, '            MODE={params.mode}')
    (485, '')
    (486, '            if [ ${{MODE}} == "paired" ];then')
    (487, '')
    (488, '                # Paired end')
    (489, '                qiime dada2 denoise-paired \\\\')
    (490, '                    --i-demultiplexed-seqs {input} \\\\')
    (491, '                    --o-table {output.table} \\\\')
    (492, '                    --o-representative-sequences {output.rep_seqs} \\\\')
    (493, '                    --o-denoising-stats {output.stats} \\\\')
    (494, '                    --p-trunc-len-f  {params.trun_len_forward} \\\\')
    (495, '                    --p-trunc-len-r {params.trun_len_reverse} \\\\')
    (496, '                    --p-trim-left-f {params.trim_len_forward} \\\\')
    (497, '                    --p-trim-left-r {params.trim_len_reverse} \\\\')
    (498, '                    --p-max-ee-f {params.max_forward_err} \\\\')
    (499, '                    --p-max-ee-r {params.max_reverse_err} \\\\')
    (500, '                    --p-n-threads {params.threads} ')
    (501, '')
    (502, '            else')
    (503, '')
    (504, '                # Single end')
    (505, '                qiime dada2 denoise-single \\\\')
    (506, '                    --i-demultiplexed-seqs {input} \\\\')
    (507, '                    --o-table {output.table} \\\\')
    (508, '                    --o-representative-sequences {output.rep_seqs} \\\\')
    (509, '                    --o-denoising-stats {output.stats} \\\\')
    (510, '                    --p-trunc-len  {params.trun_len_forward} \\\\')
    (511, '                    --p-trim-left {params.trim_len_forward} \\\\')
    (512, '                    --p-max-ee {params.max_forward_err} \\\\')
    (513, '                    --p-n-threads {params.threads}')
    (514, '')
    (515, '            fi')
    (516, '      ')
    (517, '            """')
    (518, '')
    (519, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif ASV_method == "deblur"', 'rule Denoise_reads', 'output']
    (520, 'elif ASV_method == "deblur":')
    (521, '')
    (522, '    # Denoise using deblur')
    (523, '    rule Denoise_reads:')
    (524, '        input: rules.Trim_primers.output #rules.Import_sequences.output')
    (525, '        output:')
    (526, '            filtered_reads="05.Denoise_reads/reads-filtered.qza",')
    (527, '            filter_stats="05.Denoise_reads/reads-filter-stats.qza",')
    (528, '            filter_stats_viz="05.Denoise_reads/reads-filter-stats.qzv",')
    (529, '            table="05.Denoise_reads/table.qza",')
    (530, '            rep_seqs="05.Denoise_reads/representative_sequences.qza",')
    (531, '            stats="05.Denoise_reads/denoise_stats.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif ASV_method == "deblur"', 'rule Denoise_reads', 'log']
    (532, '        log: "logs/Denoise_reads/Denoise_reads.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif ASV_method == "deblur"', 'rule Denoise_reads', 'threads']
    (533, '        threads: 30')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif ASV_method == "deblur"', 'rule Denoise_reads', 'params']
    (534, '        params:')
    (535, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (536, "            trunc_length=config[\\'parameters\\'][\\'deblur\\'][\\'trunc_length\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif ASV_method == "deblur"', 'rule Denoise_reads', 'shell']
    (537, '        shell:')
    (538, '            """')
    (539, '            set +u')
    (540, '            {params.conda_activate}')
    (541, '            set -u')
    (542, '')
    (543, '            # Initial quality filtering process based on quality scores')
    (544, '            qiime quality-filter q-score \\\\')
    (545, '              --i-demux {input} \\\\')
    (546, '              --o-filtered-sequences {output.filtered_reads} \\\\')
    (547, '              --o-filter-stats {output.filter_stats}')
    (548, '')
    (549, '            # Tabulate the filter statistics')
    (550, '            qiime metadata tabulate \\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if denoise_method == "dada2"', 'rule Tabulate_denoise_statistics', 'params']
    (611, 'if denoise_method == "dada2":')
    (612, '')
    (613, '    rule Tabulate_denoise_statistics:')
    (614, '        input: rules.Denoise_reads.output.stats')
    (615, '        output: "05.Denoise_reads/denoise_stats.qzv"')
    (616, '        log: "logs/Tabulate_denoise_statistics/Tabulate_denoise_statistics.log"')
    (617, '        threads: 1')
    (618, '        params:')
    (619, '            conda_activate=config["conda"]["qiime2"]["env"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if denoise_method == "dada2"', 'rule Tabulate_denoise_statistics', 'shell']
    (620, '        shell:')
    (621, '            """')
    (622, '            set +u')
    (623, '            {params.conda_activate}')
    (624, '            set -u')
    (625, '')
    (626, '            # Visualize dada2 denoise stats')
    (627, '            qiime metadata tabulate \\\\')
    (628, '              --m-input-file {input} \\\\')
    (629, '              --o-visualization {output}')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['if denoise_method == "dada2"', 'rule Tabulate_denoise_statistics']
    (630, '        """')
    (631, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif denoise_method == "deblur"', 'rule Tabulate_denoise_statistics', 'params']
    (632, 'elif denoise_method == "deblur":')
    (633, '    rule Tabulate_denoise_statistics:')
    (634, '        input: rules.Denoise_reads.output.stats')
    (635, '        output: "05.Denoise_reads/denoise_stats.qzv"')
    (636, '        log: "logs/Tabulate_denoise_statistics/Tabulate_denoise_statistics.log"')
    (637, '        threads: 1')
    (638, '        params:')
    (639, '            conda_activate=config["conda"]["qiime2"]["env"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.16S
context_key: ['elif denoise_method == "deblur"', 'rule Tabulate_denoise_statistics', 'shell']
    (640, '        shell:')
    (641, '            """')
    (642, '            set +u')
    (643, '            {params.conda_activate}')
    (644, '            set -u')
    (645, '')
    (646, '             # Visualize deblur stats')
    (647, '             qiime deblur visualize-stats \\\\')
    (648, '                 --i-deblur-stats {output.stats} \\\\')
    (649, '                 --o-visualization {output}')
    (650, '            """')
    (651, '')
    (652, '')
    (653, '')
    (654, '# Assign taxonomy to denoised representative sequences')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if mode == "pair"', 'rule Import_sequences', 'input']
    (175, 'if mode == "pair":')
    (176, '')
    (177, '    # Import pe-sequnces to qiime')
    (178, '    # Make sure you have a paired end specific manifest file - see example in examples folder')
    (179, '    rule Import_sequences:')
    (180, '        input: ')
    (181, '           expand(["01.raw_data/{sample}/{sample}_R1.fastq.gz",')
    (182, '                  "01.raw_data/{sample}/{sample}_R2.fastq.gz"], sample=config[\\\'samples\\\']),')
    (183, '           manifest_file=config["MANIFEST"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if mode == "pair"', 'rule Import_sequences', 'output']
    (184, '        output: "03.import/reads.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if mode == "pair"', 'rule Import_sequences', 'log']
    (185, '        log: "logs/Import_sequences/Import_sequences.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if mode == "pair"', 'rule Import_sequences', 'threads']
    (186, '        threads: 5')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if mode == "pair"', 'rule Import_sequences', 'params']
    (187, '        params:')
    (188, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (189, "            seq_dir=lambda w, input: path.dirname(input[0]).split(\\'/\\')[0]")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if mode == "pair"', 'rule Import_sequences', 'shell']
    (190, '        shell:')
    (191, '            """')
    (192, '            set +u')
    (193, '            {params.conda_activate}')
    (194, '            set -u')
    (195, '')
    (196, '            qiime tools import \\\\')
    (197, "                 --type \\'SampleData[PairedEndSequencesWithQuality]\\' \\\\")
    (198, '                 --input-path {input.manifest_file} \\\\')
    (199, '                 --output-path {output} \\\\')
    (200, '                 --input-format PairedEndFastqManifestPhred33')
    (201, '            """')
    (202, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if mode == "pair"', 'rule Trim_primers', 'params']
    (203, '    rule Trim_primers:')
    (204, '        input: rules.Import_sequences.output')
    (205, '        output: "04.Trim_primers/trimmed_reads.qza"')
    (206, '        log: "logs/Trim_primers/Trim_primers.log"')
    (207, '        threads: 10')
    (208, '        params:')
    (209, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (210, "            forward_primer=config[\\'parameters\\'][\\'cutadapt\\'][\\'forward_primer\\'],")
    (211, "            reverse_primer=config[\\'parameters\\'][\\'cutadapt\\'][\\'reverse_primer\\'],")
    (212, "            cores=config[\\'parameters\\'][\\'cutadapt\\'][\\'cores\\'],")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if mode == "pair"', 'rule Trim_primers', 'shell']
    (213, '        shell:')
    (214, '            """')
    (215, '            set +u')
    (216, '            {params.conda_activate}')
    (217, '            set -u')
    (218, '')
    (219, '            qiime cutadapt trim-paired \\\\')
    (220, '                 --i-demultiplexed-sequences {input} \\\\')
    (221, '                 --p-cores {params.cores} \\\\')
    (222, '                 --p-front-f {params.forward_primer} \\\\')
    (223, '                 --p-front-r {params.reverse_primer} \\\\')
    (224, '                 --o-trimmed-sequences {output} \\\\')
    (225, '                 --verbose')
    (226, '')
    (227, '            """')
    (228, '')
    (229, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "single"', 'rule Import_sequences', 'input']
    (230, 'elif mode == "single":')
    (231, '')
    (232, '    # Import se-sequnces to qiime')
    (233, '    rule Import_sequences:')
    (234, '        input: ')
    (235, '            expand("01.raw_data/{sample}.fastq.gz", sample=config[\\\'samples\\\']),')
    (236, '            manifest_file=config["MANIFEST"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "single"', 'rule Import_sequences', 'output']
    (237, '        output: "03.import/reads.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "single"', 'rule Import_sequences', 'log']
    (238, '        log: "logs/Import_sequences/Import_sequences.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "single"', 'rule Import_sequences', 'threads']
    (239, '        threads: 5')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "single"', 'rule Import_sequences', 'params']
    (240, '        params:')
    (241, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (242, '            seq_dir=lambda w, input: path.dirname(input[0])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "single"', 'rule Import_sequences', 'shell']
    (243, '        shell:')
    (244, '            """')
    (245, '            set +u')
    (246, '            {params.conda_activate}')
    (247, '            set -u')
    (248, '')
    (249, '            qiime tools import \\\\')
    (250, "                 --type \\'SampleData[SequencesWithQuality]\\' \\\\")
    (251, '                 --input-path {input.manifest_file} \\\\')
    (252, '                 --output-path {output} \\\\')
    (253, '                 --input-format SingleEndFastqManifestPhred33')
    (254, '            """')
    (255, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "single"', 'rule Trim_primers', 'params']
    (256, '    rule Trim_primers:')
    (257, '        input: rules.Import_sequences.output')
    (258, '        output: "04.Trim_primers/trimmed_reads.qza"')
    (259, '        log: "logs/Trim_primers/Trim_primers.log"')
    (260, '        threads: 10')
    (261, '        params:')
    (262, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (263, "            forward_primer=config[\\'parameters\\'][\\'cutadapt\\'][\\'forward_primer\\'],")
    (264, "            cores=config[\\'parameters\\'][\\'cutadapt\\'][\\'cores\\'],")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "single"', 'rule Trim_primers', 'shell']
    (265, '        shell:')
    (266, '            """')
    (267, '            set +u')
    (268, '            {params.conda_activate}')
    (269, '            set -u')
    (270, '')
    (271, '            qiime cutadapt trim-single \\\\')
    (272, '                 --i-demultiplexed-sequences {input} \\\\')
    (273, '                 --p-cores {params.cores} \\\\')
    (274, '                 --p-front {params.forward_primer} \\\\')
    (275, '                 --o-trimmed-sequences {output} \\\\')
    (276, '                 --verbose')
    (277, '')
    (278, '            """')
    (279, '')
    (280, '')
    (281, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'input']
    (282, 'elif mode == "merge":')
    (283, '')
    (284, '    if merge_method == "pear":')
    (285, '')
    (286, '        # Merge paired-end reads using pear - modifify the -m -t flags of pear before running the workflow    ')
    (287, '        rule Merge_reads:')
    (288, '            input:')
    (289, '                expand(["01.raw_data/{sample}/{sample}_R1.fastq.gz", ')
    (290, '                       "01.raw_data/{sample}/{sample}_R2.fastq.gz"], sample=config[\\\'samples\\\'])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'output']
    (291, '            output:')
    (292, '                expand("02.merge_reads/{sample}/{sample}.fastq.gz", sample=config[\\\'samples\\\'])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'log']
    (293, '            log: "logs/Merge_reads/Merge_reads.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'threads']
    (294, '            threads: 5')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'params']
    (295, '            params:')
    (296, '                out_dir=lambda w, output: path.dirname(output[0]),')
    (297, '                in_dir=lambda w, input: path.dirname(input[0]),')
    (298, "                program=config[\\'programs_path\\'][\\'run_pear\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'shell']
    (299, '            shell:')
    (300, '                """')
    (301, '                # Merge reads then delete unnecessary files')
    (302, '                 {params.program} -o {params.out_dir}/  {params.in_dir}/*.fastq.gz && \\\\')
    (303, '                 rm -rf {params.out_dir}/*.unassembled* {params.out_dir}/*discarded*')
    (304, '         ')
    (305, '               # gzip to save memory')
    (306, '         ')
    (307, '                 gzip {params.out_dir}/*.fastq')
    (308, '')
    (309, '                """')
    (310, '')
    (311, '')
    (312, '        # Import pe-joined reads to qiime')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'input']
    (313, '        rule Import_sequences:')
    (314, '            input: ')
    (315, '                rules.Merge_reads.output,')
    (316, '                manifest_file=config["MANIFEST"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'output']
    (317, '            output: "03.import/reads.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'log']
    (318, '            log: "logs/Import_sequences/Import_sequences.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'threads']
    (319, '            threads: 5')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'params']
    (320, '            params:')
    (321, '                conda_activate=config["conda"]["qiime2"]["env"],')
    (322, '                seq_dir=lambda w, input: path.dirname(input[0])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'shell']
    (323, '            shell:')
    (324, '                """')
    (325, '                set +u')
    (326, '                {params.conda_activate}')
    (327, '                set -u')
    (328, '')
    (329, '                qiime tools import \\\\')
    (330, "                     --type \\'SampleData[SequencesWithQuality]\\' \\\\")
    (331, '                     --input-path {input.manifest_file} \\\\')
    (332, '                     --output-path {output} \\\\')
    (333, '                     --input-format SingleEndFastqManifestPhred33')
    (334, '                """')
    (335, '')
    (336, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'input']
    (337, '    elif merge_method == "vsearch":')
    (338, '')
    (339, '        rule Import_sequences:')
    (340, '            input:')
    (341, '                expand(["01.raw_data/{sample}/{sample}_R1.fastq.gz", ')
    (342, '                       "01.raw_data/{sample}/{sample}_R2.fastq.gz"],sample=config[\\\'samples\\\']),')
    (343, '                manifest_file=config["MANIFEST"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'output']
    (344, '            output: "03.import/reads.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'log']
    (345, '            log: "logs/Import_sequences/Import_sequences.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'threads']
    (346, '            threads: 5')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'params']
    (347, '            params:')
    (348, '                conda_activate=config["conda"]["qiime2"]["env"],')
    (349, '                seq_dir=lambda w, input: path.dirname(input[0])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'shell']
    (350, '            shell:')
    (351, '                """')
    (352, '                set +u')
    (353, '                {params.conda_activate}')
    (354, '                set -u')
    (355, '')
    (356, '                qiime tools import \\\\')
    (357, "                     --type \\'SampleData[\\'PairedEndSequencesWithQuality\\']\\' \\\\")
    (358, '                     --input-path {input.manifest_file} \\\\')
    (359, '                     --output-path {output} ')
    (360, '                """')
    (361, '')
    (362, '        # Merge forwards and reverse reads using vsearch')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Merge_reads', 'shell']
    (363, '        rule Merge_reads:')
    (364, '            input: rules.Import_sequences.output')
    (365, '            output: "02.merge_reads/reads.qza"')
    (366, '            log: "logs/Merge_reads/Merge_reads.log"')
    (367, '            threads: 5')
    (368, '            shell:')
    (369, '                """')
    (370, '                set +u')
    (371, '                {params.conda_activate}')
    (372, '                set -u')
    (373, '')
    (374, '                qiime vsearch join-pairs \\\\')
    (375, '                     --i-demultiplexed-seqs {input} \\\\')
    (376, '                     --p-truncqual {params.trunc_qual} \\\\')
    (377, '                     --p-minlen {params.min_len} \\\\')
    (378, '                     --p-maxns {params.min_ns} \\\\')
    (379, '                     --p-minmergelen {params.men_merge_len} \\\\')
    (380, '                     --p-maxmergelen {params.max_merge_len} \\\\')
    (381, '                     --o-joined-sequences {output}')
    (382, '                """')
    (383, ' ')
    (384, '')
    (385, '# Demultiplex and View reads quality')
    (386, '# Analyze quality scores of 10000 random samples')
    (387, '')
    (388, '# If the merge method is vsearch, the input')
    (389, '# for demultiplexing should be the merged reads after')
    (390, '# Import else use the Imported reads')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if merge_method == "vsearch"', 'rule Qaulity_check', 'params']
    (391, 'if merge_method == "vsearch":')
    (392, '')
    (393, '    rule Qaulity_check:')
    (394, '        input: rules.Merge_reads.output')
    (395, '        output: "04.QC/qual_viz.qzv"')
    (396, '        log: "logs/Qaulity_check/Qaulity_check.log"')
    (397, '        threads: 10')
    (398, '        params:')
    (399, '            conda_activate=config["conda"]["qiime2"]["env"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if merge_method == "vsearch"', 'rule Qaulity_check', 'shell']
    (400, '        shell:')
    (401, '            """')
    (402, '            set +u')
    (403, '            {params.conda_activate}')
    (404, '            set -u')
    (405, '')
    (406, '            qiime demux summarize \\\\')
    (407, '                --p-n 10000 \\\\')
    (408, '                --i-data {input} \\\\')
    (409, '                --o-visualization {output}')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if merge_method == "vsearch"', 'rule Qaulity_check']
    (410, '        """')
    (411, '')
    (412, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if denoise_method == "dada2"', 'rule Denoise_reads', 'output']
    (457, 'if denoise_method == "dada2":')
    (458, '    ')
    (459, '        ')
    (460, '    rule Denoise_reads:')
    (461, '        input: rules.Trim_primers.output #rules.Import_sequences.output')
    (462, '        output: ')
    (463, '            table="05.Denoise_reads/table.qza",')
    (464, '            rep_seqs="05.Denoise_reads/representative_sequences.qza",')
    (465, '            stats="05.Denoise_reads/denoise_stats.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if denoise_method == "dada2"', 'rule Denoise_reads', 'log']
    (466, '        log: "logs/Denoise_reads/Denoise_reads.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if denoise_method == "dada2"', 'rule Denoise_reads', 'threads']
    (467, '        threads: 30')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if denoise_method == "dada2"', 'rule Denoise_reads', 'params']
    (468, '        params:')
    (469, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (470, "            mode=config[\\'parameters\\'][\\'dada2\\'][\\'mode\\'],")
    (471, "            trun_len_forward=config[\\'parameters\\'][\\'dada2\\'][\\'trunc_length_forward\\'],")
    (472, "            trun_len_reverse=config[\\'parameters\\'][\\'dada2\\'][\\'trunc_length_reverse\\'],")
    (473, "            trim_len_forward=config[\\'parameters\\'][\\'dada2\\'][\\'trim_length_forward\\'],")
    (474, "            trim_len_reverse=config[\\'parameters\\'][\\'dada2\\'][\\'trim_length_reverse\\'],")
    (475, "            max_forward_err=config[\\'parameters\\'][\\'dada2\\'][\\'maximum_forward_error\\'],")
    (476, "            max_reverse_err=config[\\'parameters\\'][\\'dada2\\'][\\'maximum_reverse_error\\'],")
    (477, "            threads=config[\\'parameters\\'][\\'dada2\\'][\\'threads\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if denoise_method == "dada2"', 'rule Denoise_reads', 'shell']
    (478, '        shell:')
    (479, '            """')
    (480, '            set +u')
    (481, '            {params.conda_activate}')
    (482, '            set -u')
    (483, '            ')
    (484, '            MODE={params.mode}')
    (485, '')
    (486, '            if [ ${{MODE}} == "paired" ];then')
    (487, '')
    (488, '                # Paired end')
    (489, '                qiime dada2 denoise-paired \\\\')
    (490, '                    --i-demultiplexed-seqs {input} \\\\')
    (491, '                    --o-table {output.table} \\\\')
    (492, '                    --o-representative-sequences {output.rep_seqs} \\\\')
    (493, '                    --o-denoising-stats {output.stats} \\\\')
    (494, '                    --p-trunc-len-f  {params.trun_len_forward} \\\\')
    (495, '                    --p-trunc-len-r {params.trun_len_reverse} \\\\')
    (496, '                    --p-trim-left-f {params.trim_len_forward} \\\\')
    (497, '                    --p-trim-left-r {params.trim_len_reverse} \\\\')
    (498, '                    --p-max-ee-f {params.max_forward_err} \\\\')
    (499, '                    --p-max-ee-r {params.max_reverse_err} \\\\')
    (500, '                    --p-n-threads {params.threads} ')
    (501, '')
    (502, '            else')
    (503, '')
    (504, '                # Single end')
    (505, '                qiime dada2 denoise-single \\\\')
    (506, '                    --i-demultiplexed-seqs {input} \\\\')
    (507, '                    --o-table {output.table} \\\\')
    (508, '                    --o-representative-sequences {output.rep_seqs} \\\\')
    (509, '                    --o-denoising-stats {output.stats} \\\\')
    (510, '                    --p-trunc-len  {params.trun_len_forward} \\\\')
    (511, '                    --p-trim-left {params.trim_len_forward} \\\\')
    (512, '                    --p-max-ee {params.max_forward_err} \\\\')
    (513, '                    --p-n-threads {params.threads}')
    (514, '')
    (515, '            fi')
    (516, '      ')
    (517, '            """')
    (518, '')
    (519, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif ASV_method == "deblur"', 'rule Denoise_reads', 'output']
    (520, 'elif ASV_method == "deblur":')
    (521, '')
    (522, '    # Denoise using deblur')
    (523, '    rule Denoise_reads:')
    (524, '        input: rules.Trim_primers.output #rules.Import_sequences.output')
    (525, '        output:')
    (526, '            filtered_reads="05.Denoise_reads/reads-filtered.qza",')
    (527, '            filter_stats="05.Denoise_reads/reads-filter-stats.qza",')
    (528, '            filter_stats_viz="05.Denoise_reads/reads-filter-stats.qzv",')
    (529, '            table="05.Denoise_reads/table.qza",')
    (530, '            rep_seqs="05.Denoise_reads/representative_sequences.qza",')
    (531, '            stats="05.Denoise_reads/denoise_stats.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif ASV_method == "deblur"', 'rule Denoise_reads', 'log']
    (532, '        log: "logs/Denoise_reads/Denoise_reads.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif ASV_method == "deblur"', 'rule Denoise_reads', 'threads']
    (533, '        threads: 30')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif ASV_method == "deblur"', 'rule Denoise_reads', 'params']
    (534, '        params:')
    (535, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (536, "            trunc_length=config[\\'parameters\\'][\\'deblur\\'][\\'trunc_length\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif ASV_method == "deblur"', 'rule Denoise_reads', 'shell']
    (537, '        shell:')
    (538, '            """')
    (539, '            set +u')
    (540, '            {params.conda_activate}')
    (541, '            set -u')
    (542, '')
    (543, '            # Initial quality filtering process based on quality scores')
    (544, '            qiime quality-filter q-score \\\\')
    (545, '              --i-demux {input} \\\\')
    (546, '              --o-filtered-sequences {output.filtered_reads} \\\\')
    (547, '              --o-filter-stats {output.filter_stats}')
    (548, '')
    (549, '            # Tabulate the filter statistics')
    (550, '            qiime metadata tabulate \\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if denoise_method == "dada2"', 'rule Tabulate_denoise_statistics', 'params']
    (611, 'if denoise_method == "dada2":')
    (612, '')
    (613, '    rule Tabulate_denoise_statistics:')
    (614, '        input: rules.Denoise_reads.output.stats')
    (615, '        output: "05.Denoise_reads/denoise_stats.qzv"')
    (616, '        log: "logs/Tabulate_denoise_statistics/Tabulate_denoise_statistics.log"')
    (617, '        threads: 1')
    (618, '        params:')
    (619, '            conda_activate=config["conda"]["qiime2"]["env"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if denoise_method == "dada2"', 'rule Tabulate_denoise_statistics', 'shell']
    (620, '        shell:')
    (621, '            """')
    (622, '            set +u')
    (623, '            {params.conda_activate}')
    (624, '            set -u')
    (625, '')
    (626, '            # Visualize dada2 denoise stats')
    (627, '            qiime metadata tabulate \\\\')
    (628, '              --m-input-file {input} \\\\')
    (629, '              --o-visualization {output}')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['if denoise_method == "dada2"', 'rule Tabulate_denoise_statistics']
    (630, '        """')
    (631, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif denoise_method == "deblur"', 'rule Tabulate_denoise_statistics', 'params']
    (632, 'elif denoise_method == "deblur":')
    (633, '    rule Tabulate_denoise_statistics:')
    (634, '        input: rules.Denoise_reads.output.stats')
    (635, '        output: "05.Denoise_reads/denoise_stats.qzv"')
    (636, '        log: "logs/Tabulate_denoise_statistics/Tabulate_denoise_statistics.log"')
    (637, '        threads: 1')
    (638, '        params:')
    (639, '            conda_activate=config["conda"]["qiime2"]["env"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile.ITS
context_key: ['elif denoise_method == "deblur"', 'rule Tabulate_denoise_statistics', 'shell']
    (640, '        shell:')
    (641, '            """')
    (642, '            set +u')
    (643, '            {params.conda_activate}')
    (644, '            set -u')
    (645, '')
    (646, '             # Visualize deblur stats')
    (647, '             qiime deblur visualize-stats \\\\')
    (648, '                 --i-deblur-stats {output.stats} \\\\')
    (649, '                 --o-visualization {output}')
    (650, '            """')
    (651, '')
    (652, '')
    (653, '')
    (654, '# Assign taxonomy to denoised representative sequences')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if mode == "pair" or mode == "merge"']
    (41, 'if mode == "pair" or mode == "merge":')
    (42, '')
    (43, '    READS=expand(["01.raw_data/{sample}/{sample}_R1.fastq.gz", "01.raw_data/{sample}/{sample}_R2.fastq.gz"], sample=config[\\\'samples\\\'])')
    (44, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if mode == "pair"', 'rule Import_sequences', 'input']
    (195, 'if mode == "pair":')
    (196, '')
    (197, '    # Import pe-sequnces to qiime')
    (198, '    # Make sure you have a paired end specific manifest file - see example in examples folder')
    (199, '    rule Import_sequences:')
    (200, '        input: ')
    (201, '           expand(["01.raw_data/{sample}/{sample}_R1.fastq.gz",')
    (202, '                  "01.raw_data/{sample}/{sample}_R2.fastq.gz"], sample=config[\\\'samples\\\']),')
    (203, '           manifest_file=config["MANIFEST"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if mode == "pair"', 'rule Import_sequences', 'output']
    (204, '        output: "03.import/reads.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if mode == "pair"', 'rule Import_sequences', 'log']
    (205, '        log: "logs/Import_sequences/Import_sequences.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if mode == "pair"', 'rule Import_sequences', 'threads']
    (206, '        threads: 5')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if mode == "pair"', 'rule Import_sequences', 'params']
    (207, '        params:')
    (208, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (209, "            seq_dir=lambda w, input: path.dirname(input[0]).split(\\'/\\')[0]")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if mode == "pair"', 'rule Import_sequences', 'shell']
    (210, '        shell:')
    (211, '            """')
    (212, '            set +u')
    (213, '            {params.conda_activate}')
    (214, '            set -u')
    (215, '')
    (216, '            qiime tools import \\\\')
    (217, "                 --type \\'SampleData[PairedEndSequencesWithQuality]\\' \\\\")
    (218, '                 --input-path {input.manifest_file} \\\\')
    (219, '                 --output-path {output} \\\\')
    (220, '                 --input-format PairedEndFastqManifestPhred33')
    (221, '            """')
    (222, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if mode == "pair"', 'rule Trim_primers', 'params']
    (223, '    rule Trim_primers:')
    (224, '        input: rules.Import_sequences.output')
    (225, '        output: "04.Trim_primers/trimmed_reads.qza"')
    (226, '        log: "logs/Trim_primers/Trim_primers.log"')
    (227, '        threads: 10')
    (228, '        params:')
    (229, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (230, "            forward_primer=config[\\'parameters\\'][\\'cutadapt\\'][\\'forward_primer\\'],")
    (231, "            reverse_primer=config[\\'parameters\\'][\\'cutadapt\\'][\\'reverse_primer\\'],")
    (232, "            cores=config[\\'parameters\\'][\\'cutadapt\\'][\\'cores\\'],")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if mode == "pair"', 'rule Trim_primers', 'shell']
    (233, '        shell:')
    (234, '            """')
    (235, '            set +u')
    (236, '            {params.conda_activate}')
    (237, '            set -u')
    (238, '')
    (239, '            qiime cutadapt trim-paired \\\\')
    (240, '                 --i-demultiplexed-sequences {input} \\\\')
    (241, '                 --p-cores {params.cores} \\\\')
    (242, '                 --p-front-f {params.forward_primer} \\\\')
    (243, '                 --p-front-r {params.reverse_primer} \\\\')
    (244, '                 --o-trimmed-sequences {output} \\\\')
    (245, '                 --verbose')
    (246, '')
    (247, '            """')
    (248, '')
    (249, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "single"', 'rule Import_sequences', 'input']
    (250, 'elif mode == "single":')
    (251, '')
    (252, '    # Import se-sequnces to qiime')
    (253, '    rule Import_sequences:')
    (254, '        input: ')
    (255, '            expand("01.raw_data/{sample}.fastq.gz", sample=config[\\\'samples\\\']),')
    (256, '            manifest_file=config["MANIFEST"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "single"', 'rule Import_sequences', 'output']
    (257, '        output: "03.import/reads.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "single"', 'rule Import_sequences', 'log']
    (258, '        log: "logs/Import_sequences/Import_sequences.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "single"', 'rule Import_sequences', 'threads']
    (259, '        threads: 5')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "single"', 'rule Import_sequences', 'params']
    (260, '        params:')
    (261, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (262, '            seq_dir=lambda w, input: path.dirname(input[0])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "single"', 'rule Import_sequences', 'shell']
    (263, '        shell:')
    (264, '            """')
    (265, '            set +u')
    (266, '            {params.conda_activate}')
    (267, '            set -u')
    (268, '')
    (269, '            qiime tools import \\\\')
    (270, "                 --type \\'SampleData[SequencesWithQuality]\\' \\\\")
    (271, '                 --input-path {input.manifest_file} \\\\')
    (272, '                 --output-path {output} \\\\')
    (273, '                 --input-format SingleEndFastqManifestPhred33')
    (274, '            """')
    (275, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "single"', 'rule Trim_primers', 'params']
    (276, '    rule Trim_primers:')
    (277, '        input: rules.Import_sequences.output')
    (278, '        output: "04.Trim_primers/trimmed_reads.qza"')
    (279, '        log: "logs/Trim_primers/Trim_primers.log"')
    (280, '        threads: 10')
    (281, '        params:')
    (282, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (283, "            forward_primer=config[\\'parameters\\'][\\'cutadapt\\'][\\'forward_primer\\'],")
    (284, "            cores=config[\\'parameters\\'][\\'cutadapt\\'][\\'cores\\'],")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "single"', 'rule Trim_primers', 'shell']
    (285, '        shell:')
    (286, '            """')
    (287, '            set +u')
    (288, '            {params.conda_activate}')
    (289, '            set -u')
    (290, '')
    (291, '            qiime cutadapt trim-single \\\\')
    (292, '                 --i-demultiplexed-sequences {input} \\\\')
    (293, '                 --p-cores {params.cores} \\\\')
    (294, '                 --p-front {params.forward_primer} \\\\')
    (295, '                 --o-trimmed-sequences {output} \\\\')
    (296, '                 --verbose')
    (297, '')
    (298, '            """')
    (299, '')
    (300, '')
    (301, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'input']
    (302, 'elif mode == "merge":')
    (303, '')
    (304, '    if merge_method == "pear":')
    (305, '')
    (306, '#        # Merge paired-end reads using pear - modifify the -m -t flags of pear before running the workflow    ')
    (307, '#        rule Merge_reads:')
    (308, '#            input:')
    (309, '#                expand(["01.raw_data/{sample}/{sample}_R1.fastq.gz", ')
    (310, '#                       "01.raw_data/{sample}/{sample}_R2.fastq.gz"], sample=config[\\\'samples\\\'])')
    (311, '#            output:')
    (312, '#                expand("02.merge_reads/{sample}/{sample}.fastq.gz", sample=config[\\\'samples\\\'])')
    (313, '#            log: "logs/Merge_reads/Merge_reads.log"')
    (314, '#            threads: 5')
    (315, '#            params:')
    (316, '#                out_dir=lambda w, output: path.dirname(output[0]),')
    (317, '#                in_dir=lambda w, input: path.dirname(input[0]).dirname,')
    (318, "#                program=config[\\'programs_path\\'][\\'run_pear\\'],")
    (319, "#                conda_activate=config[\\'conda\\'][\\'bioinfo\\'][\\'env\\'],")
    (320, "#                PERL5LIB=config[\\'conda\\'][\\'bioinfo\\'][\\'perl5lib\\']")
    (321, '#')
    (322, '#            shell:')
    (323, '#                """')
    (324, '#                set +u')
    (325, '#                {params.conda_activate}')
    (326, '#                {params.PERL5LIB}')
    (327, '#                set -u')
    (328, '#                ')
    (329, '#')
    (330, '#                FILES=$(find)')
    (331, '#                # Merge reads then delete unnecessary files')
    (332, '#                 {params.program} -o {params.out_dir}/  {params.in_dir}/*.fastq.gz && \\\\')
    (333, '#                 rm -rf {params.out_dir}/*.unassembled* {params.out_dir}/*discarded*')
    (334, '#         ')
    (335, '#               # gzip to save memory')
    (336, '#         ')
    (337, '#                 gzip {params.out_dir}/*.fastq')
    (338, '#')
    (339, '#                """')
    (340, '        # Merge paired-end reads using pear - modifify the -m -t flags of pear before running the workflow    ')
    (341, '        rule Merge_reads:')
    (342, '            input:')
    (343, '                forward="01.raw_data/{sample}/{sample}_R1.fastq.gz",')
    (344, '                rev="01.raw_data/{sample}/{sample}_R2.fastq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'output']
    (345, '            output: "02.merge_reads/{sample}/{sample}.fastq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'log']
    (346, '            log: "logs/Merge_reads/{sample}/{sample}.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'threads']
    (347, '            threads: 5')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'params']
    (348, '            params:')
    (349, '                out_dir=lambda w, output: path.dirname(output[0]),')
    (350, "                program=config[\\'programs_path\\'][\\'run_pear\\'],")
    (351, "                conda_activate=config[\\'conda\\'][\\'pear\\'][\\'env\\'],")
    (352, "                min=config[\\'parameters\\'][\\'pear\\'][\\'min_assembly\\'],")
    (353, "                max=config[\\'parameters\\'][\\'pear\\'][\\'max_assembly\\'],")
    (354, "                min_trim=config[\\'parameters\\'][\\'pear\\'][\\'min_trim\\'],")
    (355, "                threads=config[\\'parameters\\'][\\'pear\\'][\\'threads\\']")
    (356, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Merge_reads', 'shell']
    (357, '            shell:')
    (358, '                """')
    (359, '                {params.conda_activate}')
    (360, '               ')
    (361, '                 [ -d {params.out_dir} ] ||  mkdir -p {params.out_dir}')
    (362, '                 # Merge reads then delete unnecessary files')
    (363, '                 {params.program} \\\\')
    (364, '                    -f {input.forward} \\\\')
    (365, '                    -r {input.rev} \\\\')
    (366, '                    -j {params.threads} \\\\')
    (367, '                    -o {params.out_dir}/{wildcards.sample} \\\\')
    (368, '                    -m {params.max} \\\\')
    (369, '                    -n {params.min} \\\\')
    (370, '                    -t {params.min_trim} > {log} 2>&1')
    (371, '')
    (372, '')
    (373, '                 rm -rf \\\\')
    (374, '                   {params.out_dir}/{wildcards.sample}.discarded.fastq \\\\')
    (375, '                   {params.out_dir}/{wildcards.sample}.unassembled.forward.fastq \\\\')
    (376, '                   {params.out_dir}/{wildcards.sample}.unassembled.reverse.fastq ')
    (377, '')
    (378, '                 mv {params.out_dir}/{wildcards.sample}.assembled.fastq {params.out_dir}/{wildcards.sample}.fastq')
    (379, '         ')
    (380, '                 # gzip to save memory')
    (381, '         ')
    (382, '                 gzip {params.out_dir}/{wildcards.sample}.fastq')
    (383, '')
    (384, '               """')
    (385, '')
    (386, '')
    (387, '')
    (388, '        # Import pe-joined reads to qiime')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'input']
    (389, '        rule Import_sequences:')
    (390, '            input: ')
    (391, '                expand("02.merge_reads/{sample}/{sample}.fastq.gz", sample=config[\\\'samples\\\']),')
    (392, '                manifest_file=config["MANIFEST"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'output']
    (393, '            output: "03.import/reads.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'log']
    (394, '            log: "logs/Import_sequences/Import_sequences.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'threads']
    (395, '            threads: 5')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'params']
    (396, '            params:')
    (397, '                conda_activate=config["conda"]["qiime2"]["env"],')
    (398, '                seq_dir=lambda w, input: path.dirname(input[0])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Import_sequences', 'shell']
    (399, '            shell:')
    (400, '                """')
    (401, '                set +u')
    (402, '                {params.conda_activate}')
    (403, '                set -u')
    (404, '')
    (405, '                qiime tools import \\\\')
    (406, "                     --type \\'SampleData[SequencesWithQuality]\\' \\\\")
    (407, '                     --input-path {input.manifest_file} \\\\')
    (408, '                     --output-path {output} \\\\')
    (409, '                     --input-format SingleEndFastqManifestPhred33')
    (410, '                """')
    (411, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Trim_primers', 'params']
    (412, '        rule Trim_primers:')
    (413, '            input: rules.Import_sequences.output')
    (414, '            output: "04.Trim_primers/trimmed_reads.qza"')
    (415, '            log: "logs/Trim_primers/Trim_primers.log"')
    (416, '            threads: 10')
    (417, '            params:')
    (418, '                conda_activate=config["conda"]["qiime2"]["env"],')
    (419, "                forward_primer=config[\\'parameters\\'][\\'cutadapt\\'][\\'forward_primer\\'],")
    (420, "                cores=config[\\'parameters\\'][\\'cutadapt\\'][\\'cores\\'],")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'if merge_method == "pear"', 'rule Trim_primers', 'shell']
    (421, '            shell:')
    (422, '                """')
    (423, '                set +u')
    (424, '                {params.conda_activate}')
    (425, '                set -u')
    (426, '')
    (427, '                qiime cutadapt trim-single \\\\')
    (428, '                     --i-demultiplexed-sequences {input} \\\\')
    (429, '                     --p-cores {params.cores} \\\\')
    (430, '                     --p-front {params.forward_primer} \\\\')
    (431, '                     --o-trimmed-sequences {output} \\\\')
    (432, '                     --verbose')
    (433, '                """')
    (434, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'input']
    (435, '    elif merge_method == "vsearch":')
    (436, '')
    (437, '        rule Import_sequences:')
    (438, '            input:')
    (439, '               expand(["01.raw_data/{sample}/{sample}_R1.fastq.gz",')
    (440, '                      "01.raw_data/{sample}/{sample}_R2.fastq.gz"], sample=config[\\\'samples\\\']),')
    (441, '               manifest_file=config["MANIFEST"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'output']
    (442, '            output: "03.import/reads.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'log']
    (443, '            log: "logs/Import_sequences/Import_sequences.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'threads']
    (444, '            threads: 5')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'params']
    (445, '            params:')
    (446, '                conda_activate=config["conda"]["qiime2"]["env"],')
    (447, "                seq_dir=lambda w, input: path.dirname(input[0]).split(\\'/\\')[0]")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Import_sequences', 'shell']
    (448, '            shell:')
    (449, '                """')
    (450, '                set +u')
    (451, '                {params.conda_activate}')
    (452, '                set -u')
    (453, '')
    (454, '                qiime tools import \\\\')
    (455, "                     --type \\'SampleData[PairedEndSequencesWithQuality]\\' \\\\")
    (456, '                     --input-path {input.manifest_file} \\\\')
    (457, '                     --output-path {output} \\\\')
    (458, '                     --input-format PairedEndFastqManifestPhred33')
    (459, '                """')
    (460, '')
    (461, '')
    (462, '        # Merge forwards and reverse reads using vsearch')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Merge_reads', 'shell']
    (463, '        rule Merge_reads:')
    (464, '            input: rules.Import_sequences.output')
    (465, '            output: "02.merge_reads/reads.qza"')
    (466, '            log: "logs/Merge_reads/Merge_reads.log"')
    (467, '            threads: 5')
    (468, '            shell:')
    (469, '                """')
    (470, '                set +u')
    (471, '                {params.conda_activate}')
    (472, '                set -u')
    (473, '')
    (474, '                qiime vsearch join-pairs \\\\')
    (475, '                     --i-demultiplexed-seqs {input} \\\\')
    (476, '                     --p-truncqual {params.trunc_qual} \\\\')
    (477, '                     --p-minlen {params.min_len} \\\\')
    (478, '                     --p-maxns {params.min_ns} \\\\')
    (479, '                     --p-minmergelen {params.men_merge_len} \\\\')
    (480, '                     --p-maxmergelen {params.max_merge_len} \\\\')
    (481, '                     --o-joined-sequences {output}')
    (482, '                """')
    (483, ' ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Trim_primers', 'params']
    (484, '        rule Trim_primers:')
    (485, '            input: rules.Merge_reads.output')
    (486, '            output: "04.Trim_primers/trimmed_reads.qza"')
    (487, '            log: "logs/Trim_primers/Trim_primers.log"')
    (488, '            threads: 10')
    (489, '            params:')
    (490, '                conda_activate=config["conda"]["qiime2"]["env"],')
    (491, "                forward_primer=config[\\'parameters\\'][\\'cutadapt\\'][\\'forward_primer\\'],")
    (492, "                cores=config[\\'parameters\\'][\\'cutadapt\\'][\\'cores\\'],")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif mode == "merge"', 'elif merge_method == "vsearch"', 'rule Trim_primers', 'shell']
    (493, '            shell:')
    (494, '                """')
    (495, '                set +u')
    (496, '                {params.conda_activate}')
    (497, '                set -u')
    (498, '            ')
    (499, '                qiime cutadapt trim-single \\\\')
    (500, '                     --i-demultiplexed-sequences {input} \\\\')
    (501, '                     --p-cores {params.cores} \\\\')
    (502, '                     --p-front {params.forward_primer} \\\\')
    (503, '                     --o-trimmed-sequences {output} \\\\')
    (504, '                     --verbose')
    (505, '                """')
    (506, '')
    (507, '')
    (508, '')
    (509, '# Demultiplex and View reads quality')
    (510, '# Analyze quality scores of 10000 random samples')
    (511, '')
    (512, '# If the merge method is vsearch, the input')
    (513, '# for demultiplexing should be the merged reads after')
    (514, '# Import else use the Imported reads')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if merge_method == "vsearch"', 'rule Qaulity_check', 'params']
    (515, 'if merge_method == "vsearch":')
    (516, '')
    (517, '    rule Qaulity_check:')
    (518, '        input: rules.Merge_reads.output')
    (519, '        output: "04.QC/qual_viz.qzv"')
    (520, '        log: "logs/Qaulity_check/Qaulity_check.log"')
    (521, '        threads: 10')
    (522, '        params:')
    (523, '            conda_activate=config["conda"]["qiime2"]["env"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if merge_method == "vsearch"', 'rule Qaulity_check', 'shell']
    (524, '        shell:')
    (525, '            """')
    (526, '            set +u')
    (527, '            {params.conda_activate}')
    (528, '            set -u')
    (529, '')
    (530, '            qiime demux summarize \\\\')
    (531, '                --p-n 10000 \\\\')
    (532, '                --i-data {input} \\\\')
    (533, '                --o-visualization {output}')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if merge_method == "vsearch"', 'rule Qaulity_check']
    (534, '        """')
    (535, '')
    (536, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if denoise_method == "dada2"', 'rule Denoise_reads', 'output']
    (581, 'if denoise_method == "dada2":')
    (582, '    ')
    (583, '        ')
    (584, '    rule Denoise_reads:')
    (585, '        input: rules.Trim_primers.output #rules.Import_sequences.output')
    (586, '        output: ')
    (587, '            table="05.Denoise_reads/table.qza",')
    (588, '            rep_seqs="05.Denoise_reads/representative_sequences.qza",')
    (589, '            stats="05.Denoise_reads/denoise_stats.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if denoise_method == "dada2"', 'rule Denoise_reads', 'log']
    (590, '        log: "logs/Denoise_reads/Denoise_reads.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if denoise_method == "dada2"', 'rule Denoise_reads', 'threads']
    (591, '        threads: 30')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if denoise_method == "dada2"', 'rule Denoise_reads', 'params']
    (592, '        params:')
    (593, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (594, "            mode=config[\\'parameters\\'][\\'dada2\\'][\\'mode\\'],")
    (595, "            trun_len_forward=config[\\'parameters\\'][\\'dada2\\'][\\'trunc_length_forward\\'],")
    (596, "            trun_len_reverse=config[\\'parameters\\'][\\'dada2\\'][\\'trunc_length_reverse\\'],")
    (597, "            trim_len_forward=config[\\'parameters\\'][\\'dada2\\'][\\'trim_length_forward\\'],")
    (598, "            trim_len_reverse=config[\\'parameters\\'][\\'dada2\\'][\\'trim_length_reverse\\'],")
    (599, "            max_forward_err=config[\\'parameters\\'][\\'dada2\\'][\\'maximum_forward_error\\'],")
    (600, "            max_reverse_err=config[\\'parameters\\'][\\'dada2\\'][\\'maximum_reverse_error\\'],")
    (601, "            threads=config[\\'parameters\\'][\\'dada2\\'][\\'threads\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if denoise_method == "dada2"', 'rule Denoise_reads', 'shell']
    (602, '        shell:')
    (603, '            """')
    (604, '            set +u')
    (605, '            {params.conda_activate}')
    (606, '            set -u')
    (607, '            ')
    (608, '            MODE={params.mode}')
    (609, '')
    (610, '            if [ ${{MODE}} == "paired" ];then')
    (611, '')
    (612, '                # Paired end')
    (613, '                qiime dada2 denoise-paired \\\\')
    (614, '                    --i-demultiplexed-seqs {input} \\\\')
    (615, '                    --o-table {output.table} \\\\')
    (616, '                    --o-representative-sequences {output.rep_seqs} \\\\')
    (617, '                    --o-denoising-stats {output.stats} \\\\')
    (618, '                    --p-trunc-len-f  {params.trun_len_forward} \\\\')
    (619, '                    --p-trunc-len-r {params.trun_len_reverse} \\\\')
    (620, '                    --p-trim-left-f {params.trim_len_forward} \\\\')
    (621, '                    --p-trim-left-r {params.trim_len_reverse} \\\\')
    (622, '                    --p-max-ee-f {params.max_forward_err} \\\\')
    (623, '                    --p-max-ee-r {params.max_reverse_err} \\\\')
    (624, '                    --p-n-threads {params.threads} ')
    (625, '')
    (626, '            else')
    (627, '')
    (628, '                # Single end')
    (629, '                qiime dada2 denoise-single \\\\')
    (630, '                    --i-demultiplexed-seqs {input} \\\\')
    (631, '                    --o-table {output.table} \\\\')
    (632, '                    --o-representative-sequences {output.rep_seqs} \\\\')
    (633, '                    --o-denoising-stats {output.stats} \\\\')
    (634, '                    --p-trunc-len  {params.trun_len_forward} \\\\')
    (635, '                    --p-trim-left {params.trim_len_forward} \\\\')
    (636, '                    --p-max-ee {params.max_forward_err} \\\\')
    (637, '                    --p-n-threads {params.threads}')
    (638, '')
    (639, '            fi')
    (640, '      ')
    (641, '            """')
    (642, '')
    (643, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif ASV_method == "deblur"', 'rule Denoise_reads', 'output']
    (644, 'elif ASV_method == "deblur":')
    (645, '')
    (646, '    # Denoise using deblur')
    (647, '    rule Denoise_reads:')
    (648, '        input: rules.Trim_primers.output #rules.Import_sequences.output')
    (649, '        output:')
    (650, '            filtered_reads="05.Denoise_reads/reads-filtered.qza",')
    (651, '            filter_stats="05.Denoise_reads/reads-filter-stats.qza",')
    (652, '            filter_stats_viz="05.Denoise_reads/reads-filter-stats.qzv",')
    (653, '            table="05.Denoise_reads/table.qza",')
    (654, '            rep_seqs="05.Denoise_reads/representative_sequences.qza",')
    (655, '            stats="05.Denoise_reads/denoise_stats.qza"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif ASV_method == "deblur"', 'rule Denoise_reads', 'log']
    (656, '        log: "logs/Denoise_reads/Denoise_reads.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif ASV_method == "deblur"', 'rule Denoise_reads', 'threads']
    (657, '        threads: 30')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif ASV_method == "deblur"', 'rule Denoise_reads', 'params']
    (658, '        params:')
    (659, '            conda_activate=config["conda"]["qiime2"]["env"],')
    (660, "            trunc_length=config[\\'parameters\\'][\\'deblur\\'][\\'trunc_length\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif ASV_method == "deblur"', 'rule Denoise_reads', 'shell']
    (661, '        shell:')
    (662, '            """')
    (663, '            set +u')
    (664, '            {params.conda_activate}')
    (665, '            set -u')
    (666, '')
    (667, '            # Initial quality filtering process based on quality scores')
    (668, '            qiime quality-filter q-score \\\\')
    (669, '              --i-demux {input} \\\\')
    (670, '              --o-filtered-sequences {output.filtered_reads} \\\\')
    (671, '              --o-filter-stats {output.filter_stats}')
    (672, '')
    (673, '            # Tabulate the filter statistics')
    (674, '            qiime metadata tabulate \\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if denoise_method == "dada2"', 'rule Tabulate_denoise_statistics', 'params']
    (735, 'if denoise_method == "dada2":')
    (736, '')
    (737, '    rule Tabulate_denoise_statistics:')
    (738, '        input: rules.Denoise_reads.output.stats')
    (739, '        output: "05.Denoise_reads/denoise_stats.qzv"')
    (740, '        log: "logs/Tabulate_denoise_statistics/Tabulate_denoise_statistics.log"')
    (741, '        threads: 1')
    (742, '        params:')
    (743, '            conda_activate=config["conda"]["qiime2"]["env"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if denoise_method == "dada2"', 'rule Tabulate_denoise_statistics', 'shell']
    (744, '        shell:')
    (745, '            """')
    (746, '            set +u')
    (747, '            {params.conda_activate}')
    (748, '            set -u')
    (749, '')
    (750, '            # Visualize dada2 denoise stats')
    (751, '            qiime metadata tabulate \\\\')
    (752, '              --m-input-file {input} \\\\')
    (753, '              --o-visualization {output}')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if denoise_method == "dada2"', 'rule Tabulate_denoise_statistics']
    (754, '        """')
    (755, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif denoise_method == "deblur"', 'rule Tabulate_denoise_statistics', 'params']
    (756, 'elif denoise_method == "deblur":')
    (757, '    rule Tabulate_denoise_statistics:')
    (758, '        input: rules.Denoise_reads.output.stats')
    (759, '        output: "05.Denoise_reads/denoise_stats.qzv"')
    (760, '        log: "logs/Tabulate_denoise_statistics/Tabulate_denoise_statistics.log"')
    (761, '        threads: 1')
    (762, '        params:')
    (763, '            conda_activate=config["conda"]["qiime2"]["env"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['elif denoise_method == "deblur"', 'rule Tabulate_denoise_statistics', 'shell']
    (764, '        shell:')
    (765, '            """')
    (766, '            set +u')
    (767, '            {params.conda_activate}')
    (768, '            set -u')
    (769, '')
    (770, '             # Visualize deblur stats')
    (771, '             qiime deblur visualize-stats \\\\')
    (772, '                 --i-deblur-stats {output.stats} \\\\')
    (773, '                 --o-visualization {output}')
    (774, '            """')
    (775, '')
    (776, '')
    (777, '')
    (778, '# Assign taxonomy to denoised representative sequences')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ramay/dada2_snakemake_workflow, file=Snakefile
context_key: ['if PAIRED_END']
    (10, "if PAIRED_END: FRACTIONS+= [\\'R2\\']")
    (11, '')
    (12, '')
    (13, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lrgr/sigma, file=Snakefile
context_key: ["if not ('samples' in config)"]
    (18, "if not ('samples' in config):")
    (19, '    import json')
    (20, "    with open(config.get('mutations_file'), 'r') as IN:")
    (21, "        config['samples'] = json.load(IN).get('samples')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lrgr/sigma, file=Snakefile
context_key: ["elif type(config['samples']) != type([])"]
    (22, "elif type(config['samples']) != type([]):")
    (23, "    config['samples'] = [config['samples']]")
    (24, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lrgr/sigma, file=Snakefile
context_key: ["if len(config.get('active_signatures')) == 0"]
    (31, "if len(config.get('active_signatures')) == 0:")
    (32, "    ACTIVE_SIGNATURES_PARAM = ''")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sevyharris/qe_workflow, file=workflow/Snakefile
context_key: ["if metal == \\'Cu\\' or metal == \\'Ni\\'"]
    (21, "if metal == \\'Cu\\' or metal == \\'Ni\\':")
    (22, "    mixing_beta = \\'low\\'    ")
    (23, '')
    (24, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=allytrope/variant-analysis, file=workflow/rules/hard_filter.smk
context_key: ['if mode == "SNP"']
    (5, '    if mode == "SNP":')
    (6, '        return """-filter "QUAL < 30.0" --filter-name "QUAL30" \\\\')
    (7, '                  -filter "QD < 2.0" --filter-name "QD2" \\\\')
    (8, '                  -filter "SOR > 3.0" --filter-name "SOR3" \\\\')
    (9, '                  -filter "FS > 60.0" --filter-name "FS60" \\\\')
    (10, '                  -filter "MQ < 40.0" --filter-name "MQ40" \\\\')
    (11, '                  -filter "MQRankSum < -12.5" --filter-name "MQRankSum-12.5" \\\\')
    (12, '                  -filter "ReadPosRankSum < -8.0" --filter-name "ReadPosRankSum-8" """')
    (13, '    elif mode == "indel":')
    (14, '        return """-filter "QUAL < 30.0" --filter-name "QUAL30" \\\\')
    (15, '                  -filter "QD < 2.0" --filter-name "QD2" \\\\')
    (16, '                  -filter "FS > 200.0" --filter-name "FS200" \\\\')
    (17, '                  -filter "ReadPosRankSum < -20.0" --filter-name "ReadPosRankSum-20" """')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=allytrope/variant-analysis, file=workflow/Snakefile
context_key: ['if Path(config["results"] + "db/chromosomes.list").is_file()']
    (23, 'if Path(config["results"] + "db/chromosomes.list").is_file():')
    (24, '    with open(config["results"] + "db/chromosomes.list") as f:')
    (25, '        CHROMOSOMES = f.read().splitlines()')
    (26, '    print(CHROMOSOMES)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jiarong/xander-assembly-pipeline, file=Snakefile
context_key: ['if GROUP not in DF.columns']
    (21, 'if GROUP not in DF.columns:')
    (22, '    mes = \\\'*** {} is not a head in {}, please change "GROUP" in config.yaml\\\'')
    (23, "    print(mes.format(GROUP, os.path.basename(config[\\'METADATA\\'])))")
    (24, '    sys.exit(1)')
    (25, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jiarong/xander-assembly-pipeline, file=Snakefile
context_key: ['if PATH not in DF.columns']
    (26, 'if PATH not in DF.columns:')
    (27, '    mes = \\\'*** {} is not a head in {}, please change "PATH" in config.yaml\\\'')
    (28, "    print(mes.format(PATH, os.path.basename(config[\\'METADATA\\'])))")
    (29, '    sys.exit(1)')
    (30, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jiarong/xander-assembly-pipeline, file=Snakefile
context_key: ['if [[ ! -f Xander_assembler/bin/run_xander_skel.sh ]]; then']
    (41, '     if [[ ! -f Xander_assembler/bin/run_xander_skel.sh ]]; then ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sheucke/rna-seq-star-deseq2, file=rules/diffexp.smk
context_key: ['if "strandedness" in units.columns']
    (1, '    if "strandedness" in units.columns:')
    (2, '        return units["strandedness"].tolist()')
    (3, '    else:')
    (4, '        strand_list=["none"]')
    (5, '        return strand_list*units.shape[0]')
    (6, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "T" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] != "SE"', 'rule init_structure', 'input']
    (29, 'if len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "T" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] != "SE":')
    (30, '    rule init_structure:')
    (31, '        input:')
    (32, '            fw = config["fw_reads"],')
    (33, '            rv = config["rv_reads"],')
    (34, '            metadata = config["metadata"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "T" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] != "SE"', 'rule init_structure', 'output']
    (35, '        output:')
    (36, '            r1="{PROJECT}/samples/{sample}/rawdata/fw.fastq"  if config["gzip_input"] == "F" else "{PROJECT}/samples/{sample}/rawdata/fw.fastq.gz",')
    (37, '            r2="{PROJECT}/samples/{sample}/rawdata/rv.fastq"  if config["gzip_input"] == "F" else "{PROJECT}/samples/{sample}/rawdata/rv.fastq.gz",')
    (38, '            metadata="{PROJECT}/metadata/sampleList_mergedBarcodes_{sample}.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "T" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] != "SE"', 'rule init_structure', 'shell']
    (39, '        shell:')
    (40, '            "Scripts/init_sample.sh "+config["PROJECT"]+" "+config["LIBRARY"][0]+" {input.metadata} {input.fw} {input.rv}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "T" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] == "SE"', 'rule init_structure', 'input']
    (41, 'elif len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "T" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] == "SE":')
    (42, '    rule init_structure:')
    (43, '        input:')
    (44, '            fw = config["fw_reads"],')
    (45, '            metadata = config["metadata"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "T" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] == "SE"', 'rule init_structure', 'output']
    (46, '        output:')
    (47, '            r1="{PROJECT}/samples/{sample}/rawdata/fw.fastq"  if config["gzip_input"] == "F" else "{PROJECT}/samples/{sample}/rawdata/fw.fastq.gz",')
    (48, '            metadata="{PROJECT}/metadata/sampleList_mergedBarcodes_{sample}.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "T" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] == "SE"', 'rule init_structure', 'shell']
    (49, '        shell:')
    (50, '            "Scripts/init_sample_SE.sh "+config["PROJECT"]+" "+config["LIBRARY"][0]+" {input.metadata} {input.fw}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "F" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] != "SE"', 'rule init_structure', 'input']
    (51, 'elif len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "F" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] != "SE":')
    (52, '    rule init_structure:')
    (53, '        input:')
    (54, '            fw = config["fw_reads"],')
    (55, '            rv = config["rv_reads"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "F" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] != "SE"', 'rule init_structure', 'output']
    (56, '        output:')
    (57, '            r1="{PROJECT}/samples/{sample}/rawdata/fw.fastq" if config["gzip_input"] == "F" else "{PROJECT}/samples/{sample}/rawdata/fw.fastq.gz",')
    (58, '            r2="{PROJECT}/samples/{sample}/rawdata/rv.fastq" if config["gzip_input"] == "F" else "{PROJECT}/samples/{sample}/rawdata/rv.fastq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "F" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] != "SE"', 'rule init_structure', 'shell']
    (59, '        shell:')
    (60, '            "Scripts/init_sample_dmx.sh "+config["PROJECT"]+" "+config["LIBRARY"][0]+"  {input.fw} {input.rv}"')
    (61, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "F" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] == "SE"', 'rule init_structure', 'input']
    (62, 'elif len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "F" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] == "SE":')
    (63, '    rule init_structure:')
    (64, '        input:')
    (65, '            fw = config["fw_reads"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "F" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] == "SE"', 'rule init_structure', 'output']
    (66, '        output:')
    (67, '            r1="{PROJECT}/samples/{sample}/rawdata/fw.fastq" if config["gzip_input"] == "F" else "{PROJECT}/samples/{sample}/rawdata/fw.fastq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif len(config["LIBRARY"])==1 and config["demultiplexing"]["demultiplex"] == "F" and len(config["input_files"])<2 and config["LIBRARY_LAYOUT"] == "SE"', 'rule init_structure', 'shell']
    (68, '        shell:')
    (69, '            "Scripts/init_sample_dmx_SE.sh "+config["PROJECT"]+" "+config["LIBRARY"][0]+"  {input.fw}"')
    (70, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if (config["derep"]["dereplicate"] == "T" and config["pickOTU"]["m"] != "usearch") or config["pickOTU"]["m"] == "swarm"', 'rule dereplicate', 'input']
    (1365, 'if (config["derep"]["dereplicate"] == "T" and config["pickOTU"]["m"] != "usearch") or config["pickOTU"]["m"] == "swarm" :')
    (1366, '#here make two steps')
    (1367, '    rule dereplicate:')
    (1368, '        input:')
    (1369, '            "{PROJECT}/runs/{run}/seqs_fw_rev_combined.fasta"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if (config["derep"]["dereplicate"] == "T" and config["pickOTU"]["m"] != "usearch") or config["pickOTU"]["m"] == "swarm"', 'rule dereplicate', 'output']
    (1370, '        output:')
    (1371, '            #"{PROJECT}/runs/{run}/derep/seqs_fw_rev_combined_otus.txt", <- picking OTUs method')
    (1372, '            "{PROJECT}/runs/{run}/derep/seqs_fw_rev_combined_derep.fasta",')
    (1373, '            "{PROJECT}/runs/{run}/derep/seqs_fw_rev_combined_derep.uc"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if (config["derep"]["dereplicate"] == "T" and config["pickOTU"]["m"] != "usearch") or config["pickOTU"]["m"] == "swarm"', 'rule dereplicate', 'params']
    (1374, '        params:')
    (1375, '            "{PROJECT}/runs/{run}/derep/"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if (config["derep"]["dereplicate"] == "T" and config["pickOTU"]["m"] != "usearch") or config["pickOTU"]["m"] == "swarm"', 'rule dereplicate', 'benchmark']
    (1376, '        benchmark:')
    (1377, '            "{PROJECT}/runs/{run}/derep/derep.benchmark"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if (config["derep"]["dereplicate"] == "T" and config["pickOTU"]["m"] != "usearch") or config["pickOTU"]["m"] == "swarm"', 'rule dereplicate', 'shell']
    (1378, '        shell:')
    (1379, '            "{config[derep][vsearch_cmd]} --derep_fulllength {input} --output {output[0]} --uc {output[1]} --strand {config[derep][strand]} "')
    (1380, '            "--fasta_width 0 --minuniquesize {config[derep][min_abundance]} --sizeout" if config["pickOTU"]["m"] == "swarm"')
    (1381, '            else "{config[derep][vsearch_cmd]} --derep_fulllength {input} --output {output[0]} --uc {output[1]} --strand {config[derep][strand]} "')
    (1382, '            "--fasta_width 0 --minuniquesize {config[derep][min_abundance]}" ')
    (1383, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["pickOTU"]["m"] == "swarm"', 'rule cluster_OTUs', 'input']
    (1384, 'if  config["pickOTU"]["m"] == "swarm" :')
    (1385, '    rule cluster_OTUs:')
    (1386, '        input:')
    (1387, '            "{PROJECT}/runs/{run}/derep/seqs_fw_rev_combined_derep.fasta" ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["pickOTU"]["m"] == "swarm"', 'rule cluster_OTUs', 'output']
    (1388, '        output:')
    (1389, '            swarms="{PROJECT}/runs/{run}/otu/seqs_fw_rev_combined_derep_otus.txt",')
    (1390, '           # rep_seqs="{PROJECT}/runs/{run}/otu/representative_seq_set_swarm.fasta", (in case we want to generate reps -w {output.rep_seqs})')
    (1391, '            uc="{PROJECT}/runs/{run}/otu/swarms.uc" ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["pickOTU"]["m"] == "swarm"', 'rule cluster_OTUs', 'params']
    (1392, '        params:')
    (1393, '            otuDir="{PROJECT}/runs/{run}/otu/"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["pickOTU"]["m"] == "swarm"', 'rule cluster_OTUs', 'benchmark']
    (1394, '        benchmark:')
    (1395, '            "{PROJECT}/runs/{run}/otu.benchmark"')
    (1396, '        #-i  {params.otuDir}swarm.struct')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["pickOTU"]["m"] == "swarm"', 'rule cluster_OTUs', 'shell']
    (1397, '        shell:')
    (1398, '            "swarm  -s {params.otuDir}swarm.stats -d {config[pickOTU][s]} -z "')
    (1399, '            "-o {output.swarms}  -u {output.uc}  -t {config[pickOTU][cpus]} "')
    (1400, '            "{config[pickOTU][extra_params]} < {input} "')
    (1401, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["assignTaxonomy"]["tool"] == "blast"', 'rule assign_taxonomy', 'input']
    (1468, 'if  config["assignTaxonomy"]["tool"] == "blast":')
    (1469, '    rule assign_taxonomy:')
    (1470, '        input:')
    (1471, '            "{PROJECT}/runs/{run}/otu/representative_seq_set.fasta"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["assignTaxonomy"]["tool"] == "blast"', 'rule assign_taxonomy', 'output']
    (1472, '        output:')
    (1473, '            temp("{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/representative_seq_set_tax_blastn.out")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["assignTaxonomy"]["tool"] == "blast"', 'rule assign_taxonomy', 'params']
    (1474, '        params:')
    (1475, '            reference="-db " +config["assignTaxonomy"]["blast"]["blast_db"] if len(str(config["assignTaxonomy"]["blast"]["blast_db"])) > 1')
    (1476, '            else "-subject " +config["assignTaxonomy"]["blast"]["fasta_db"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["assignTaxonomy"]["tool"] == "blast"', 'rule assign_taxonomy', 'benchmark']
    (1477, '        benchmark:')
    (1478, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/assign_taxa.benchmark"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["assignTaxonomy"]["tool"] == "blast"', 'rule assign_taxonomy', 'shell']
    (1479, '        shell:')
    (1480, '            "{config[assignTaxonomy][blast][command]} {params.reference} -query {input} -evalue {config[assignTaxonomy][blast][evalue]} "')
    (1481, '            "-outfmt \\\'6 qseqid sseqid pident qcovs evalue bitscore\\\' -num_threads {config[assignTaxonomy][blast][jobs]} "')
    (1482, '            "-max_target_seqs {config[assignTaxonomy][blast][max_target_seqs]} -perc_identity {config[assignTaxonomy][blast][identity]} "')
    (1483, '            "{config[assignTaxonomy][blast][extra_params]} -out {output[0]} "')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["assignTaxonomy"]["tool"] == "blast"', 'rule complete_blast_out']
    (1484, '    rule complete_blast_out:')
    (1485, '        """')
    (1486, '         Add missing ids to blast out')
    (1487, '        """')
    (1488, '        input:')
    (1489, '            blastout="{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/representative_seq_set_tax_blastn.out",')
    (1490, '            otus="{PROJECT}/runs/{run}/otu/seqs_fw_rev_combined_derep_otus.txt" if (config["derep"]["dereplicate"] == "T" and config["pickOTU"]["m"] != "usearch")')
    (1491, '            or config["pickOTU"]["m"] == "swarm" else "{PROJECT}/runs/{run}/otu/seqs_fw_rev_combined_otus.txt"')
    (1492, '        output:')
    (1493, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/representative_seq_set_tax_blastn.complete.out"')
    (1494, '        benchmark:')
    (1495, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/complte_blast.assign_taxa.benchmark"')
    (1496, '        shell:')
    (1497, '            "cat {input.blastout} | cut -f1 | sort | uniq | grep -v -w -F -f - {input.otus} "')
    (1498, '            "| awk \\\'{{print $1\\\\"\\\\\\\\tUnassigned\\\\\\\\t-\\\\\\\\t-\\\\\\\\t-\\\\\\\\t-\\\\"}}\\\' | cat {input.blastout} - > {output}"')
    (1499, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["assignTaxonomy"]["tool"] == "blast"', 'rule prepare_blast_for_stampa']
    (1500, '    rule prepare_blast_for_stampa:')
    (1501, '        """')
    (1502, '         Take completed blast output file and make some reformat in order to')
    (1503, '         fulfill stampa format.')
    (1504, '        """')
    (1505, '        input:')
    (1506, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/representative_seq_set_tax_blastn.complete.out"')
    (1507, '        output:')
    (1508, '            temp("{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/hits.blast.out")')
    (1509, '        benchmark:')
    (1510, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/assign_taxa.hits.benchmark"')
    (1511, '        shell:')
    (1512, '            "cat {input}  | cut -f2 | sort | uniq | grep -F -w -f -  {config[assignTaxonomy][blast][mapFile]} | "')
    (1513, '            "awk \\\'NR==FNR {{h[$1] = $2; next}} {{print $1\\\\"\\\\\\\\t\\\\"$3\\\\"\\\\\\\\t\\\\"$2\\\\" \\\\"h[$2]}}\\\' FS=\\\\"\\\\\\\\t\\\\" - FS=\\\\"\\\\\\\\t\\\\" {input} "')
    (1514, '            " > {output}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["assignTaxonomy"]["tool"] == "blast"', 'rule skip_stampa']
    (1515, '    rule skip_stampa:')
    (1516, '        """')
    (1517, '         skip stampa by selecting the best hit. It keeps the accesions from the other hits')
    (1518, '         and add them as metadata with the identity. If we use stampa the identities are the same')
    (1519, '         for all the best hits, for blast the identity can be different and we only report the best one.')
    (1520, '        """')
    (1521, '        input:')
    (1522, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/hits.blast.out"')
    (1523, '        output:')
    (1524, '            temp("{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/results.blast.no_stampa.out")')
    (1525, '        benchmark:')
    (1526, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/stampa.benchmark"')
    (1527, '        shell:')
    (1528, '            #"cat {input}  | sed \\\'s/ /\\\\\\\\t/1\\\' | awk -F\\\'\\\\\\\\t\\\' \\\'BEGIN{{OFS=\\\\"\\\\\\\\t\\\\"}}{{if(!h[$1]){{if($3 == \\\\"*\\\\"){{print $1,1,$2,\\\\"Unassigned\\\\",$3;h[$1]=$1}}else{{print $1,1,$2,$4,$3;h[$1]=$1}}}}}}\\\' > {output}"')
    (1529, '            "cat {input}  | sed \\\'s/ /\\\\t/1\\\' | awk -F\\\'\\\\\\\\t\\\' -v current=\\\'\\\' \\\'BEGIN{{OFS=\\\\"\\\\\\\\t\\\\"}}{{if(length(current)>0 && current != $1 && length(line)> 0){{print line\\\\"\\\\\\\\t\\\\"h[current];line=\\\\"\\\\";}};if(!h[$1]){{if($3 == \\\\"*\\\\"){{print $1,1,$2,\\\\"Unassigned\\\\",$3;h[$1]=$3;}}else{{line=$1\\\\"\\\\\\\\t1\\\\\\\\t\\\\"$2\\\\"\\\\\\\\t\\\\"$4;h[$1]=$3;current=$1}}}}else{{h[$1]=h[$1]\\\\";\\\\"$3}}}}END{{if(length(line)>0){{print line\\\\"\\\\\\\\t\\\\"h[current]}}}}\\\' > {output}"')
    (1530, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["assignTaxonomy"]["tool"] == "blast"', 'rule run_stampa']
    (1531, '    rule run_stampa:')
    (1532, '        """')
    (1533, '         compute lca using stampa merge script')
    (1534, '        """')
    (1535, '        input:')
    (1536, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/hits.blast.out"')
    (1537, '        params:')
    (1538, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/"')
    (1539, '        output:')
    (1540, '            temp("{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/results.blast.out")')
    (1541, '        benchmark:')
    (1542, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/stampa.benchmark"')
    (1543, '        shell:')
    (1544, '            "Scripts/stampa_merge.py {params} {config[assignTaxonomy][blast][taxo_separator]}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["assignTaxonomy"]["tool"] == "blast"', 'rule normalize_taxo_out']
    (1545, '    rule normalize_taxo_out:')
    (1546, '        """')
    (1547, '         Normalize the output in terms of its format and names in order to be able')
    (1548, '         to continue with the pipeline')
    (1549, '        """')
    (1550, '        input:')
    (1551, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/results.blast.out" if config["assignTaxonomy"]["map_lca"].lower() == "t"')
    (1552, '            else "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/results.blast.no_stampa.out" ')
    (1553, '        output:')
    (1554, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/representative_seq_set_tax_assignments.txt"')
    (1555, '        benchmark:')
    (1556, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/stampa.benchmark"')
    (1557, '        shell:')
    (1558, '            "cat {input} |  awk -F\\\\"\\\\\\\\t\\\\" \\\'{{print $1\\\\"\\\\\\\\t\\\\"$4\\\\"\\\\\\\\t\\\\"$3\\\\"\\\\\\\\t\\\\"$5}}\\\' | sed \\\'s/N;o;_;h;i;t/Unassigned/\\\' > {output}"')
    (1559, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif  config["assignTaxonomy"]["tool"] == "vsearch"', 'rule assign_taxonomy', 'input']
    (1560, 'elif  config["assignTaxonomy"]["tool"] == "vsearch":')
    (1561, '    rule assign_taxonomy:')
    (1562, '        input:')
    (1563, '            "{PROJECT}/runs/{run}/otu/representative_seq_set.fasta"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif  config["assignTaxonomy"]["tool"] == "vsearch"', 'rule assign_taxonomy', 'output']
    (1564, '        output:')
    (1565, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/representative_seq_set_tax_vsearch.out"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif  config["assignTaxonomy"]["tool"] == "vsearch"', 'rule assign_taxonomy', 'params']
    (1566, '        params:')
    (1567, '            reference="-db " +config["assignTaxonomy"]["blast"]["blast_db"] if len(str(config["assignTaxonomy"]["blast"]["blast_db"])) > 1')
    (1568, '            else "-subject " +config["assignTaxonomy"]["blast"]["fasta_db"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif  config["assignTaxonomy"]["tool"] == "vsearch"', 'rule assign_taxonomy', 'benchmark']
    (1569, '        benchmark:')
    (1570, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/assign_taxa.benchmark"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif  config["assignTaxonomy"]["tool"] == "vsearch"', 'rule assign_taxonomy', 'shell']
    (1571, '        shell:')
    (1572, '            "{config[assignTaxonomy][vsearch][command]}  --usearch_global {input} --db {config[assignTaxonomy][vsearch][db_file]} "')
    (1573, '            "--dbmask none --qmask none --rowlen 0 --id {config[assignTaxonomy][vsearch][identity]} "')
    (1574, '            "--iddef {config[assignTaxonomy][vsearch][identity_definition]}  --userfields query+id{config[assignTaxonomy][vsearch][identity_definition]}+target "')
    (1575, '            "--threads {config[assignTaxonomy][vsearch][jobs]} {config[assignTaxonomy][vsearch][extra_params]} "')
    (1576, '            " --maxaccepts {config[assignTaxonomy][vsearch][max_target_seqs]} --output_no_hits --userout {output[0]} "')
    (1577, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif  config["assignTaxonomy"]["tool"] == "vsearch"', 'rule mapp_vsearch_taxo']
    (1578, '    rule mapp_vsearch_taxo:')
    (1579, '        """')
    (1580, '         Take vsearch output and make some reformat in order to')
    (1581, '         fulfill stampa format.')
    (1582, '        """')
    (1583, '        input:')
    (1584, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/representative_seq_set_tax_vsearch.out"')
    (1585, '        output:')
    (1586, '            temp("{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/taxons.txt")')
    (1587, '        benchmark:')
    (1588, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/assign_taxa.map.benchmark"')
    (1589, '        shell:')
    (1590, '            "cat {input}  | cut -f3 | sort | uniq | grep -F -w -f -  {config[assignTaxonomy][vsearch][mapFile]} > {output} "')
    (1591, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif  config["assignTaxonomy"]["tool"] == "vsearch"', 'rule prepare_vsearch_for_stampa']
    (1592, '    rule prepare_vsearch_for_stampa:')
    (1593, '        """')
    (1594, '         Take completed blast output file and make some reformat in order to')
    (1595, '         fulfill stampa format.')
    (1596, '        """')
    (1597, '        input:')
    (1598, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/taxons.txt",')
    (1599, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/representative_seq_set_tax_vsearch.out"')
    (1600, '        output:')
    (1601, '            temp("{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/hits.vsearch.out")')
    (1602, '        benchmark:')
    (1603, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/assign_taxa.hits.benchmark"')
    (1604, '        shell:')
    (1605, '            "echo \\\'*\\\\\\\\tUnassigned\\\' | cat {input[0]} - | awk \\\'NR==FNR {{h[$1] = $2; next}} {{print $1\\\\"\\\\\\\\t\\\\"$2\\\\"\\\\\\\\t\\\\"$3\\\\" \\\\"h[$3]}}\\\' FS=\\\\"\\\\\\\\t\\\\" - FS=\\\\"\\\\\\\\t\\\\" {input[1]} "')
    (1606, '            " > {output}"')
    (1607, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif  config["assignTaxonomy"]["tool"] == "vsearch"', 'rule skip_stampa']
    (1608, '    rule skip_stampa:')
    (1609, '        """')
    (1610, '         skip stampa by selecting the best hit. It keeps the accesions from the other hits')
    (1611, '         and add them as metadata with the identity. If we use stampa the identities are the same ')
    (1612, '         for all the best hits, for blast the identity can be different and we only report the best one. ')
    (1613, '        """')
    (1614, '        input:')
    (1615, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/hits.vsearch.out"')
    (1616, '        output:')
    (1617, '            temp("{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/results.vsearch.no_stampa.out")')
    (1618, '        benchmark:')
    (1619, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/stampa.benchmark"')
    (1620, '        shell:')
    (1621, '            #"cat {input}  | sed \\\'s/ /\\\\\\\\t/1\\\' | awk -F\\\'\\\\\\\\t\\\' \\\'BEGIN{{OFS=\\\\"\\\\\\\\t\\\\"}}{{if(!h[$1]){{if($3 == \\\\"*\\\\"){{print $1,1,$2,\\\\"Unassigned\\\\",$3;h[$1]=$1}}else{{print $1,1,$2,$4,$3;h[$1]=$1}}}}}}\\\' > {output}"')
    (1622, '            "cat {input}  | sed \\\'s/ /\\\\t/1\\\' | awk -F\\\'\\\\\\\\t\\\' -v current=\\\'\\\' \\\'BEGIN{{OFS=\\\\"\\\\\\\\t\\\\"}}{{if(length(current)>0 && current != $1 && length(line)> 0){{print line\\\\"\\\\\\\\t\\\\"h[current];line=\\\\"\\\\";}};if(!h[$1]){{if($3 == \\\\"*\\\\"){{print $1,1,$2,\\\\"Unassigned\\\\",$3;h[$1]=$3;}}else{{line=$1\\\\"\\\\\\\\t1\\\\\\\\t\\\\"$2\\\\"\\\\\\\\t\\\\"$4;h[$1]=$3;current=$1}}}}else{{h[$1]=h[$1]\\\\";\\\\"$3}}}}END{{if(length(line)>0){{print line\\\\"\\\\\\\\t\\\\"h[current]}}}}\\\' > {output}"')
    (1623, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif  config["assignTaxonomy"]["tool"] == "vsearch"', 'rule run_stampa']
    (1624, '    rule run_stampa:')
    (1625, '        """')
    (1626, '         compute lca using stampa merge script')
    (1627, '        """')
    (1628, '        input:')
    (1629, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/hits.vsearch.out"')
    (1630, '        params:')
    (1631, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/"')
    (1632, '        output:')
    (1633, '            temp("{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/results.vsearch.out")')
    (1634, '        benchmark:')
    (1635, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/stampa.benchmark"')
    (1636, '        shell:')
    (1637, '            "Scripts/stampa_merge.py {params} {config[assignTaxonomy][vsearch][taxo_separator]}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['elif  config["assignTaxonomy"]["tool"] == "vsearch"', 'rule normalize_taxo_out']
    (1638, '    rule normalize_taxo_out:')
    (1639, '        """')
    (1640, '         Normalize the output in terms oif format and names in order to be able')
    (1641, '         to continue with the pipeline')
    (1642, '        """')
    (1643, '        input:')
    (1644, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/results.vsearch.out" if config["assignTaxonomy"]["map_lca"].lower() == "t"')
    (1645, '            else "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/results.vsearch.no_stampa.out"  ')
    (1646, '        output:')
    (1647, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/representative_seq_set_tax_assignments.txt"')
    (1648, '        benchmark:')
    (1649, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/stampa.benchmark"')
    (1650, '        shell:')
    (1651, '            "cat {input} |  awk -F\\\\"\\\\\\\\t\\\\" \\\'{{print $1\\\\"\\\\\\\\t\\\\"$4\\\\"\\\\\\\\t\\\\"$3\\\\"\\\\\\\\t\\\\"$5}}\\\' | sed \\\'s/N;o;_;h;i;t/Unassigned/\\\' > {output}"')
    (1652, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["krona"]["report"].casefold() == "t" or config["krona"]["report"].casefold() == "true"', 'rule krona_report', 'input']
    (1798, 'if  config["krona"]["report"].casefold() == "t" or config["krona"]["report"].casefold() == "true":')
    (1799, '    rule krona_report:')
    (1800, '        input:')
    (1801, '            "{PROJECT}/runs/{run}/asv/taxonomy_dada2/asvTable_noSingletons.txt" if config["ANALYSIS_TYPE"] == "ASV" and config["krona"]["otu_table"].casefold() != "singletons"  ')
    (1802, '            else "{PROJECT}/runs/{run}/asv/taxonomy_dada2/asvTable.txt" ')
    (1803, '            if config["ANALYSIS_TYPE"] == "ASV"')
    (1804, '            else')
    (1805, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/otuTable_noSingletons.txt" ')
    (1806, '            if config["krona"]["otu_table"].casefold() != "singletons"')
    (1807, '            else "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/otuTable.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["krona"]["report"].casefold() == "t" or config["krona"]["report"].casefold() == "true"', 'rule krona_report', 'output']
    (1808, '        output:')
    (1809, '            "{PROJECT}/runs/{run}/report_files/krona_report.dada2.html"')
    (1810, '            if config["ANALYSIS_TYPE"] == "ASV" else')
    (1811, '            "{PROJECT}/runs/{run}/report_files/krona_report."+config["assignTaxonomy"]["tool"]+".html"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["krona"]["report"].casefold() == "t" or config["krona"]["report"].casefold() == "true"', 'rule krona_report', 'params']
    (1812, '        params:')
    (1813, '            "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["krona"]["report"].casefold() == "t" or config["krona"]["report"].casefold() == "true"', 'rule krona_report', 'benchmark']
    (1814, '        benchmark:')
    (1815, '              "{PROJECT}/runs/{run}/otu/taxonomy_"+config["assignTaxonomy"]["tool"]+"/krona_report.benchmark"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlejandroAb/CASCABEL, file=Snakefile
context_key: ['if  config["krona"]["report"].casefold() == "t" or config["krona"]["report"].casefold() == "true"', 'rule krona_report', 'script']
    (1816, '        script:')
    (1817, '            "Scripts/otu2krona.py"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_4.smk
context_key: ['if exp_type == 4', 'if not os.path.isdir("tmp")']
    (25, 'if exp_type == 4:')
    (26, '    # Make tmp directory for kmc')
    (27, '    if not os.path.isdir("tmp"):')
    (28, '        os.mkdir("tmp")')
    (29, '')
    (30, '    # Parse out a pivot from the downloaded dataset')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_4.smk
context_key: ['if exp_type == 4', 'if not os.path.isdir("input_type4/")', 'if not os.path.isdir(f"input_type4/rest_of_set/dataset_{i}")']
    (31, '    if not os.path.isdir("input_type4/"):')
    (32, '        for i in range(1, num_datasets+1):')
    (33, '            dir_prefix = f"{database_root}/trial_{curr_trial}/exp0_nonpivot_genomes/dataset_{i}/"')
    (34, '            list_of_files = [dir_prefix + x for x in os.listdir(dir_prefix)]')
    (35, '')
    (36, '            if not os.path.isdir(f"input_type4/rest_of_set/dataset_{i}"):')
    (37, '                os.makedirs(f"input_type4/rest_of_set/dataset_{i}")')
    (38, '')
    (39, '            # Copy over all the files at first')
    (40, '            for data_file in list_of_files:')
    (41, '                shutil.copy(data_file, f"input_type4/rest_of_set/dataset_{i}")')
    (42, '            ')
    (43, '            # Take the pivot and copy to needed folder')
    (44, '            pivot = f"{database_root}/trial_{curr_trial}/exp0_pivot_genomes/dataset_{i}/pivot_{i}.fna.gz"')
    (45, '            if not os.path.isdir(f"input_type4/pivot/"):')
    (46, '                os.makedirs(f"input_type4/pivot/")')
    (47, '            shutil.copy(pivot, f"input_type4/pivot/pivot_{i}.fna.gz")')
    (48, '')
    (49, '            # Added pivot to rest_of_set if doing in-pivot')
    (50, '            if not out_pivot_exp4:')
    (51, '                shutil.copy(pivot, f"input_type4/rest_of_set/dataset_{i}")')
    (52, '    ')
    (53, '    # Initialize complex ops directory to start writing files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_4.smk
context_key: ['if exp_type == 4', 'if not os.path.isdir("complex_ops_type_4")']
    (54, '    if not os.path.isdir("complex_ops_type_4"):')
    (55, '        os.makedirs("complex_ops_type_4")')
    (56, '')
    (57, '    # Build the complex ops files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_4.smk
context_key: ['if exp_type == 4', 'if not os.path.isdir(f"complex_ops_type_4/k_{k}")']
    (58, '    for k in k_values:')
    (59, '        if not os.path.isdir(f"complex_ops_type_4/k_{k}"):')
    (60, '            os.mkdir(f"complex_ops_type_4/k_{k}")')
    (61, '        ')
    (62, '        for num in range(1, num_datasets+1):')
    (63, '            kmc_input_files = []')
    (64, '')
    (65, '            for data_file in os.listdir(f"input_type4/rest_of_set/dataset_{num}"):')
    (66, '                if data_file.endswith(".fna.gz"):')
    (67, '                    base_name = data_file.split(".fna.gz")[0]')
    (68, '                    kmc_input_files.append(f"genome_sets_type_4/rest_of_set/k_{k}/dataset_{num}/{base_name}.transformed")')
    (69, '            ')
    (70, '            if not os.path.isdir(f"complex_ops_type_4/k_{k}/dataset_{num}/"):')
    (71, '                os.mkdir(f"complex_ops_type_4/k_{k}/dataset_{num}/")')
    (72, '')
    (73, '            with open(f"complex_ops_type_4/k_{k}/dataset_{num}/ops_{num}.txt", "w") as fd:')
    (74, '                fd.write("INPUT:\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_4.smk
context_key: ['if not os.path.isdir(f"filelists_type_4/k_{k}")']
    (92, '        if not os.path.isdir(f"filelists_type_4/k_{k}"):')
    (93, '            os.makedirs(f"filelists_type_4/k_{k}")')
    (94, '        intersect_files = []')
    (95, '        pivot_files = []')
    (96, '        for pivot in range(1, num_datasets+1):')
    (97, '            pivot_files.append("/text_dump_type_4/k_{k}/pivot/pivot_{pivot}.txt".format(k = k, pivot = pivot))')
    (98, '            for num in range(1, num_datasets+1):')
    (99, '                intersect_files.append("/text_dump_type_4/k_{k}/intersection/pivot_{pivot}/pivot_{pivot}_intersect_dataset_{num}.txt".format(k = k, pivot = pivot, num = num))')
    (100, '        with open(f"filelists_type_4/k_{k}/pivots_filelist.txt", "w") as fd:')
    (101, '            for f in pivot_files:')
    (102, '                fd.write(base_dir+f+"\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_5.smk
context_key: ['if exp_type == 5', 'if not os.path.isdir("non_pivot_type_5/")', 'if not os.path.isdir(f"non_pivot_type_5/dataset_{i}")']
    (18, 'if exp_type == 5:')
    (19, '    if not os.path.isdir("non_pivot_type_5/"):')
    (20, '        for i in range(1, num_datasets+1):')
    (21, '            dir_prefix = f"data/dataset_{i}/"')
    (22, '            list_of_files = [dir_prefix + x for x in os.listdir(dir_prefix)]')
    (23, '')
    (24, '            if not os.path.isdir(f"non_pivot_type_5/dataset_{i}"):')
    (25, '                os.makedirs(f"non_pivot_type_5/dataset_{i}")')
    (26, '')
    (27, '            # Copy over all the files at first')
    (28, '            for data_file in list_of_files:')
    (29, '                shutil.copy(data_file, f"non_pivot_type_5/dataset_{i}")')
    (30, '            ')
    (31, '            # Extract the pivot and move to new directory')
    (32, '            dir_prefix = f"non_pivot_type_5/dataset_{i}/"')
    (33, '            list_of_files = [dir_prefix + x for x in os.listdir(dir_prefix)]')
    (34, '')
    (35, '            print(list_of_files)')
    (36, '')
    (37, '            pivot = random.choice(list_of_files)')
    (38, '            if not os.path.isdir(f"pivot_ms_type_5/pivot_{i}"):')
    (39, '                os.makedirs(f"pivot_ms_type_5/pivot_{i}")')
    (40, '            ')
    (41, '            # Remove pivot from other set of genomes')
    (42, '            shutil.copy(pivot, f"pivot_ms_type_5/pivot_{i}/pivot_{i}.fna")')
    (43, '            os.remove(pivot)')
    (44, '            ')
    (45, '####################################################')
    (46, '# Section 2: Helper functions needed for these')
    (47, '#            experiment rules.')
    (48, '####################################################')
    (49, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_6.smk
context_key: ['if exp_type == 6', 'if not os.path.isdir("tmp")']
    (23, 'if exp_type == 6:')
    (24, '    # Make tmp directory for kmc')
    (25, '    if not os.path.isdir("tmp"):')
    (26, '        os.mkdir("tmp")')
    (27, '    ')
    (28, '    # Initialize complex ops directory to start writing files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_6.smk
context_key: ['if exp_type == 6', 'if not os.path.isdir("exp6_complex_ops")']
    (29, '    if not os.path.isdir("exp6_complex_ops"):')
    (30, '        os.makedirs("exp6_complex_ops")')
    (31, '    ')
    (32, '    # Create filelists that will be used for merging text dumps')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_6.smk
context_key: ['if exp_type == 6', 'if not os.path.isdir(f"exp6_filelists/k_{k}")']
    (33, '    read_types = ["illumina","ont"]')
    (34, '    ')
    (35, '    for k in k_values:')
    (36, '        if not os.path.isdir(f"exp6_filelists/k_{k}"):')
    (37, '            os.makedirs(f"exp6_filelists/k_{k}")')
    (38, '        for read_type in read_types:')
    (39, '            if not os.path.isdir(f"exp6_filelists/k_{k}/{read_type}"):')
    (40, '                os.makedirs(f"exp6_filelists/k_{k}/{read_type}")')
    (41, '            intersect_files = []')
    (42, '            pivot_files = []')
    (43, '            for pivot in range(1, num_datasets+1):')
    (44, '                pivot_files.append("/exp6_text_dump/k_{k}/{read_type}/pivot/pivot_{pivot}.txt".format(k = k, pivot = pivot, read_type = read_type))')
    (45, '                for num in range(1, num_datasets+1):')
    (46, '                    intersect_files.append("/exp6_text_dump/k_{k}/{read_type}/intersection/pivot_{pivot}/pivot_{pivot}_intersect_dataset_{num}.txt".format(k = k, pivot = pivot, num = num, read_type = read_type))')
    (47, '            with open(f"exp6_filelists/k_{k}/{read_type}/pivots_filelist.txt", "w") as fd:')
    (48, '                for f in pivot_files:')
    (49, '                    fd.write(base_dir+f+"\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_3.smk
context_key: ['if exp_type == 3', 'if not os.path.isdir("tmp")']
    (26, 'if exp_type == 3:')
    (27, '    # Make tmp directory for kmc')
    (28, '    if not os.path.isdir("tmp"):')
    (29, '        os.mkdir("tmp")')
    (30, '    ')
    (31, '    # Parse out a pivot from the downloaded dataset')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_3.smk
context_key: ['if exp_type == 3', 'if not os.path.isdir("input_type3/")', 'if not os.path.isdir(f"input_type3/rest_of_set/dataset_{i}")']
    (32, '    if not os.path.isdir("input_type3/"):')
    (33, '        for i in range(1, num_datasets+1):')
    (34, '            dir_prefix = f"data/dataset_{i}/"')
    (35, '            list_of_files = [dir_prefix + x for x in os.listdir(dir_prefix)]')
    (36, '')
    (37, '            if not os.path.isdir(f"input_type3/rest_of_set/dataset_{i}"):')
    (38, '                os.makedirs(f"input_type3/rest_of_set/dataset_{i}")')
    (39, '')
    (40, '            # Copy over all the files at first')
    (41, '            for data_file in list_of_files:')
    (42, '                shutil.copy(data_file, f"input_type3/rest_of_set/dataset_{i}")')
    (43, '            ')
    (44, '            # Extract the pivot and move to new directory')
    (45, '            dir_prefix = f"input_type3/rest_of_set/dataset_{i}/"')
    (46, '            list_of_files = [dir_prefix + x for x in os.listdir(dir_prefix)]')
    (47, '')
    (48, '            pivot = random.choice(list_of_files)')
    (49, '            if not os.path.isdir(f"input_type3/pivot/dataset_{i}"):')
    (50, '                os.makedirs(f"input_type3/pivot/dataset_{i}")')
    (51, '            ')
    (52, '            # Remove pivot from other set of genomes')
    (53, '            shutil.copy(pivot, f"input_type3/pivot/dataset_{i}/pivot_{i}.fna.gz")')
    (54, '            os.remove(pivot)')
    (55, '')
    (56, '    # Initialize complex ops directory to start writing files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_3.smk
context_key: ['if exp_type == 3', 'if not os.path.isdir("complex_ops")']
    (57, '    if not os.path.isdir("complex_ops"):')
    (58, '        os.makedirs("complex_ops/within_groups")')
    (59, '')
    (60, '    # Build the complex ops files for within groups')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_3.smk
context_key: ['if exp_type == 3', 'if not os.path.isdir(f"complex_ops/within_groups/k_{k}")']
    (61, '    for k in k_values:')
    (62, '        if not os.path.isdir(f"complex_ops/within_groups/k_{k}"):')
    (63, '            os.mkdir(f"complex_ops/within_groups/k_{k}")')
    (64, '        ')
    (65, '        for num in range(1, num_datasets+1):')
    (66, '            kmc_input_files = []')
    (67, '')
    (68, '            for data_file in os.listdir(f"input_type3/rest_of_set/dataset_{num}"):')
    (69, '                if data_file.endswith(".fna.gz"):')
    (70, '                    base_name = data_file.split(".fna.gz")[0]')
    (71, '                    kmc_input_files.append(f"genome_sets_type3/rest_of_set/k_{k}/dataset_{num}/{base_name}.transformed")')
    (72, '            ')
    (73, '            if not os.path.isdir(f"complex_ops/within_groups/k_{k}/dataset_{num}/"):')
    (74, '                os.mkdir(f"complex_ops/within_groups/k_{k}/dataset_{num}/")')
    (75, '        ')
    (76, '            with open(f"complex_ops/within_groups/k_{k}/dataset_{num}/within_dataset_{num}.txt", "w") as fd:')
    (77, '                fd.write("INPUT:\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_8.smk
context_key: ['if exp_type == 8', 'if not os.path.isdir("non_pivot_type_8/")', 'if not os.path.isdir(f"non_pivot_type_8/dataset_{i}")']
    (17, 'if exp_type == 8:')
    (18, '    if not os.path.isdir("non_pivot_type_8/"):')
    (19, '        for i in range(1, num_datasets+1):')
    (20, '            dir_prefix = f"data/dataset_{i}/"')
    (21, '            list_of_files = [dir_prefix + x for x in os.listdir(dir_prefix)]')
    (22, '')
    (23, '            if not os.path.isdir(f"non_pivot_type_8/dataset_{i}"):')
    (24, '                os.makedirs(f"non_pivot_type_8/dataset_{i}")')
    (25, '')
    (26, '            # Copy over all the files at first')
    (27, '            for data_file in list_of_files:')
    (28, '                shutil.copy(data_file, f"non_pivot_type_8/dataset_{i}")')
    (29, '            ')
    (30, '            # Extract the pivot and move to new directory')
    (31, '            dir_prefix = f"non_pivot_type_8/dataset_{i}/"')
    (32, '            list_of_files = [dir_prefix + x for x in os.listdir(dir_prefix)]')
    (33, '')
    (34, '            print(list_of_files)')
    (35, '')
    (36, '            pivot = random.choice(list_of_files)')
    (37, '            if not os.path.isdir(f"pivot_type_8/pivot_ref/"):')
    (38, '                os.makedirs(f"pivot_type_8/pivot_ref/")')
    (39, '            ')
    (40, '            # Remove pivot from other set of genomes')
    (41, '            shutil.copy(pivot, f"pivot_type_8/pivot_ref/pivot_{i}.fna")')
    (42, '            os.remove(pivot)')
    (43, '        ')
    (44, '####################################################')
    (45, '# Section 2: Helper functions needed for these')
    (46, '#            experiment rules.')
    (47, '####################################################')
    (48, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/prepare_data.smk
context_key: ['if (( $num_lines > $max_lines )); the']
    (167, '            if (( $num_lines > $max_lines )); then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_1.smk
context_key: ['if exp_type == 1', 'if not os.path.isdir("tmp")']
    (25, 'if exp_type == 1:')
    (26, '    # Make tmp directory for kmc')
    (27, '    if not os.path.isdir("tmp"):')
    (28, '        os.mkdir("tmp")')
    (29, '')
    (30, '    # Initialize complex_ops directory to start writing files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_1.smk
context_key: ['if exp_type == 1', 'if not os.path.isdir("complex_ops")']
    (31, '    if not os.path.isdir("complex_ops"):')
    (32, '        os.mkdir("complex_ops")')
    (33, '        os.mkdir("complex_ops/within_groups")')
    (34, '')
    (35, '    # This loop builds the complex_ops files for within groups ...')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_1.smk
context_key: ['if exp_type == 1', 'if not os.path.isdir(f"complex_ops/within_groups/k_{k}")']
    (36, '    for k in k_values:')
    (37, '        if not os.path.isdir(f"complex_ops/within_groups/k_{k}"):')
    (38, '            os.mkdir(f"complex_ops/within_groups/k_{k}")')
    (39, '')
    (40, '        for num in range(1, num_datasets+1):')
    (41, '            kmc_input_files = []')
    (42, '')
    (43, '            for data_file in os.listdir(f"data/dataset_{num}"):')
    (44, '                if data_file.endswith(".fna.gz"):')
    (45, '                    base_name = data_file.split(".fna.gz")[0]')
    (46, '                    kmc_input_files.append(f"step_2/k_{k}/dataset_{num}/{base_name}.transformed")')
    (47, '')
    (48, '            if not os.path.isdir(f"complex_ops/within_groups/k_{k}/dataset_{num}/"):')
    (49, '                os.mkdir(f"complex_ops/within_groups/k_{k}/dataset_{num}")')
    (50, '')
    (51, '            with open(f"complex_ops/within_groups/k_{k}/dataset_{num}/within_dataset_{num}.txt", "w") as fd:')
    (52, '                fd.write("INPUT:\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_1.smk
context_key: ['if not os.path.isdir("complex_ops/across_groups")']
    (68, '    if not os.path.isdir("complex_ops/across_groups"):')
    (69, '        os.mkdir("complex_ops/across_groups")')
    (70, '')
    (71, '    # This loop builds the complex_ops across groups ...')
    (72, '    for k in k_values:')
    (73, '        if not os.path.isdir(f"complex_ops/across_groups/k_{k}"):')
    (74, '            os.mkdir(f"complex_ops/across_groups/k_{k}")')
    (75, '        ')
    (76, '        kmc_input_files = []')
    (77, '        for i in range(1, num_datasets+1):')
    (78, '            kmc_input_files.append(f"step_6/k_{k}/dataset_{i}/dataset_{i}.transformed.combined.transformed")')
    (79, '        ')
    (80, '        with open(f"complex_ops/across_groups/k_{k}/across_all_datasets.txt", "w") as fd:')
    (81, '            fd.write("INPUT:\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_1.smk
context_key: ['if metrics[0] == id_str']
    (237, '                    if metrics[0] == id_str:')
    (238, '                        metrics.append(round(metrics[8]/max_ratio, 4))')
    (239, '            ')
    (240, '            # Print all the values to csv file')
    (241, '            for metrics in all_metrics:')
    (242, '                metrics_str = ",".join([str(x) for x in metrics])')
    (243, '                out_fd.write(f"{metrics_str}\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_2.smk
context_key: ['if exp_type == 2', 'if not os.path.isdir("tmp")']
    (25, 'if exp_type == 2:')
    (26, '    # Make tmp directory for kmc')
    (27, '    if not os.path.isdir("tmp"):')
    (28, '        os.mkdir("tmp")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_2.smk
context_key: ['if exp_type == 2', 'if not os.path.isdir("input_type_2/")', 'if not os.path.isdir(f"input_type_2/rest_of_set/dataset_{i}")']
    (29, '    """')
    (30, '    # Parse out a pivot from the downloaded dataset')
    (31, '    if not os.path.isdir("input_type_2/"):')
    (32, '        for i in range(1, num_datasets+1):')
    (33, '            dir_prefix = f"data/dataset_{i}/"')
    (34, '            list_of_files = [dir_prefix + x for x in os.listdir(dir_prefix)]')
    (35, '')
    (36, '            if not os.path.isdir(f"input_type_2/rest_of_set/dataset_{i}"):')
    (37, '                os.makedirs(f"input_type_2/rest_of_set/dataset_{i}")')
    (38, '')
    (39, '            # Copy over all the files at first')
    (40, '            for data_file in list_of_files:')
    (41, '                shutil.copy(data_file, f"input_type_2/rest_of_set/dataset_{i}")')
    (42, '            ')
    (43, '            # Extract the pivot and move to new directory')
    (44, '            dir_prefix = f"input_type_2/rest_of_set/dataset_{i}/"')
    (45, '            list_of_files = [dir_prefix + x for x in os.listdir(dir_prefix)]')
    (46, '')
    (47, '            pivot = random.choice(list_of_files)')
    (48, '            if not os.path.isdir(f"input_type_2/pivot/dataset_{i}"):')
    (49, '                os.makedirs(f"input_type_2/pivot/dataset_{i}")')
    (50, '            ')
    (51, '            # Remove pivot from other set of genomes')
    (52, '            shutil.copy(pivot, f"input_type_2/pivot/dataset_{i}/pivot_{i}.fna.gz")')
    (53, '            os.remove(pivot)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_2.smk
context_key: ['if exp_type == 2', 'if not os.path.isdir("complex_ops_type_2/")']
    (54, '    """')
    (55, '    """')
    (56, '    # Initialize complex ops directory to start writing files')
    (57, '    if not os.path.isdir("complex_ops_type_2/"):')
    (58, '        os.makedirs("complex_ops_type_2/within_groups")')
    (59, '    ')
    (60, '    # Build the complex ops files for within groups')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_2.smk
context_key: ['if exp_type == 2', 'if not os.path.isdir(f"complex_ops_type_2/within_groups/k_{k}")']
    (61, '    for k in k_values:')
    (62, '        if not os.path.isdir(f"complex_ops_type_2/within_groups/k_{k}"):')
    (63, '            os.mkdir(f"complex_ops_type_2/within_groups/k_{k}")')
    (64, '        ')
    (65, '        for num in range(1, num_datasets+1):')
    (66, '            kmc_input_files = []')
    (67, '')
    (68, '            for data_file in os.listdir(f"input_type_2/rest_of_set/dataset_{num}"):')
    (69, '                if data_file.endswith(".fna.gz"):')
    (70, '                    base_name = data_file.split(".fna.gz")[0]')
    (71, '                    kmc_input_files.append(f"genome_sets_type_2/rest_of_set/k_{k}/dataset_{num}/{base_name}.transformed")')
    (72, '            ')
    (73, '            if not os.path.isdir(f"complex_ops_type_2/within_groups/k_{k}/dataset_{num}/"):')
    (74, '                os.mkdir(f"complex_ops_type_2/within_groups/k_{k}/dataset_{num}/")')
    (75, '        ')
    (76, '            with open(f"complex_ops_type_2/within_groups/k_{k}/dataset_{num}/within_dataset_{num}.txt", "w") as fd:')
    (77, '                fd.write("INPUT:\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_2.smk
context_key: ['if not os.path.isdir("complex_ops_type_2/across_groups")']
    (93, '    if not os.path.isdir("complex_ops_type_2/across_groups"):')
    (94, '        os.mkdir("complex_ops_type_2/across_groups")')
    (95, '')
    (96, '    # This loop builds the across_groups operation files')
    (97, '    for k in k_values:')
    (98, '        if not os.path.isdir(f"complex_ops_type_2/across_groups/k_{k}"):')
    (99, '            os.mkdir(f"complex_ops_type_2/across_groups/k_{k}")')
    (100, '        ')
    (101, '        # Build a file for each pivot ')
    (102, '        for pivot_num in range(1, num_datasets+1):')
    (103, '            if not os.path.isdir(f"complex_ops_type_2/across_groups/k_{k}/pivot_{pivot_num}"):')
    (104, '                os.mkdir(f"complex_ops_type_2/across_groups/k_{k}/pivot_{pivot_num}")')
    (105, '')
    (106, '            kmc_input_files = []')
    (107, '            for i in range(1, num_datasets+1):')
    (108, '                if i != pivot_num:')
    (109, '                    kmc_input_files.append(f"within_databases_type_2/rest_of_set/k_{k}/dataset_{i}/dataset_{i}.transformed.combined.transformed")')
    (110, '')
    (111, '            with open(f"complex_ops_type_2/across_groups/k_{k}/pivot_{pivot_num}/across_datasets_pivot_{pivot_num}.txt", "w") as fd:')
    (112, '                fd.write("INPUT:\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_2.smk
context_key: ['if metrics[0] == id_str']
    (464, '                    if metrics[0] == id_str:')
    (465, '                        metrics.append(round(metrics[8]/max_ratio, 4))')
    (466, '            ')
    (467, '            # Print all the values to csv file')
    (468, '            for metrics in all_metrics:')
    (469, '                metrics_str = ",".join([str(x) for x in metrics])')
    (470, '                output_fd.write(f"{metrics_str}\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_2.smk
context_key: ['if metrics[0] == id_str']
    (583, '                    if metrics[0] == id_str:')
    (584, '                        metrics.append(round(metrics[8]/max_ratio, 4))')
    (585, '            ')
    (586, '            # Print all the values to csv file')
    (587, '            for metrics in all_metrics:')
    (588, '                metrics_str = ",".join([str(x) for x in metrics])')
    (589, '                output_fd.write(f"{metrics_str}\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_7.smk
context_key: ['if exp_type == 7', 'if not os.path.isdir("exp7_non_pivot_data/")', 'if not os.path.isdir(f"exp7_non_pivot_data/dataset_{i}")']
    (18, 'if exp_type == 7:')
    (19, '    """ --> Commented out when created prepare_data.smk')
    (20, '    if not os.path.isdir("exp7_non_pivot_data/"):')
    (21, '        for i in range(1, num_datasets+1):')
    (22, '            dir_prefix = f"data/dataset_{i}/"')
    (23, '            list_of_files = [dir_prefix + x for x in os.listdir(dir_prefix)]')
    (24, '')
    (25, '            if not os.path.isdir(f"exp7_non_pivot_data/dataset_{i}"):')
    (26, '                os.makedirs(f"exp7_non_pivot_data/dataset_{i}")')
    (27, '')
    (28, '            # Copy over all the files at first')
    (29, '            for data_file in list_of_files:')
    (30, '                shutil.copy(data_file, f"exp7_non_pivot_data/dataset_{i}")')
    (31, '            ')
    (32, '            # Extract the pivot and move to new directory')
    (33, '            dir_prefix = f"exp7_non_pivot_data/dataset_{i}/"')
    (34, '            list_of_files = [dir_prefix + x for x in os.listdir(dir_prefix)]')
    (35, '')
    (36, '            pivot = random.choice(list_of_files)')
    (37, '')
    (38, '            # Print pivot name to list in trial folder')
    (39, '            with open(trial_info_dir + "/exp7_pivot_names.txt", \\\'a\\\') as fd:')
    (40, '                fd.write(pivot+"\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=oma219/khoice, file=workflow/rules/exp_type_7.smk
context_key: ['if not os.path.isdir(f"exp7_pivot_data/pivot_ref/")']
    (43, '            if not os.path.isdir(f"exp7_pivot_data/pivot_ref/"):')
    (44, '                os.makedirs(f"exp7_pivot_data/pivot_ref/")')
    (45, '            ')
    (46, '            # Remove pivot from other set of genomes')
    (47, '            shutil.copy(pivot, f"exp7_pivot_data/pivot_ref/pivot_{i}.fna")')
    (48, '            os.remove(pivot)')
    (49, '    """')
    (50, '')
    (51, '####################################################')
    (52, '# Section 2: Helper functions needed for these')
    (53, '#            experiment rules.')
    (54, '####################################################')
    (55, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snaktest, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'if len(fol_path_spillover_slide_acs) > 0']
    (117, 'if do_compensation:')
    (118, '    if len(file_path_orig_sm) == 0:')
    (119, '        if len(fol_path_spillover_slide_acs) > 0:')
    (120, "            spillover_mode = \\'estimate_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snaktest, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'else']
    (121, '        else:')
    (122, '            raise ValueError(\\\'Either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (123, "                             \\'needs to be provided.\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snaktest, file=workflow/Snakefile
context_key: ['if do_compensation', 'elif len(fol_path_spillover_slide_acs) == 0']
    (124, '    elif len(fol_path_spillover_slide_acs) == 0:')
    (125, "            spillover_mode = \\'copy_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snaktest, file=workflow/Snakefile
context_key: ['if do_compensation', 'else']
    (126, '    else:')
    (127, '        raise ValueError(\\\'Only provide either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (128, "                         \\'but not both.\\')")
    (129, '')
    (130, '# Produce a list of all cellprofiler output files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snaktest, file=workflow/Snakefile
context_key: ['if not do_compensation']
    (267, 'if not do_compensation:')
    (268, "    config_dict_cp[\\'measuremasks\\'] = {")
    (269, '        \\\'message\\\':  """\\\\')
    (270, '                    Measure masks ')
    (271, '                    ')
    (272, '                    Measure segmentation masks')
    (273, '                    """,')
    (274, "        \\'run_size\\': 1,")
    (275, "        \\'plugins\\': cp_plugins,")
    (276, "        \\'pipeline\\': \\'resources/cp4_pipelines/adapted/3_measure_mask_basic.cppipe\\',")
    (277, "        \\'input_files\\': [")
    (278, '            # Folder containing segmentation masks')
    (279, '            fol_path_cellmasks,')
    (280, '            # Folder containing image stacks to measure')
    (281, '            fol_path_full,')
    (282, '            # Folder containing cell probabilities')
    (283, '            fol_path_probabilities,')
    (284, '            # Acquisition metadata')
    (285, '            file_path_acmeta,')
    (286, '            # Channel metadata')
    (287, '            file_path_channelmeta_full,')
    (288, '            file_path_channelmeta_cellprobab')
    (289, '        ],')
    (290, "        \\'output_patterns\\': {\\'.\\':")
    (291, '                            # Folder containing cellprofiler measurement')
    (292, '                                cp_measurements_output_files,')
    (293, '                            # Subfolder containing segmentation masks used')
    (294, '                            # for the measurements')
    (295, "                            \\'masks\\': directory(fol_path_cp_masks),")
    (296, '                            # Subfolder containing the images used for the')
    (297, '                            # measurements.')
    (298, "                            \\'images\\': directory(fol_path_cp_images)},")
    (299, "        \\'input_folder\\': fol_path_cp_input")
    (300, '    }')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snaktest, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'params']
    (454, 'if do_compensation:')
    (455, '    if spillover_mode == "estimate_sm":')
    (456, '        rule get_ss_panel_from_panel:')
    (457, '            input: csv_panel')
    (458, '            output: file_path_ss_panel')
    (459, '            params:')
    (460, '                col_metal=csv_panel_metal,')
    (461, '                col_spillover=csv_panel_spillover')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snaktest, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'run']
    (462, '            run:')
    (463, '                dat_csv = pd.read_csv(csv_panel)')
    (464, '                assert params.col_spillover in dat_csv.columns, ValueError(')
    (465, '                    f"The panel csv {csv_panel} should contain a boolean column"')
    (466, '                    f"{params.col_spillover} indicating the metals used for the"')
    (467, '                    f"spillover acquisitions.")')
    (468, '                metals = dat_csv.query(')
    (469, "                    f\\'{params.col_spillover} == 1\\')[params.col_metal].values")
    (470, "                with open(output[0], \\'w\\') as f:")
    (471, '                    for m in metals:')
    (472, "                        f.write(m+\\'\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=imgag/ont_tools, file=workflow/Snakefile
context_key: ["if  os.path.isfile(config[\\'sample_file\\'])", 'if len(row) == 0']
    (31, "if  os.path.isfile(config[\\'sample_file\\']):")
    (32, "    with open(config[\\'sample_file\\']) as f:")
    (33, '        reader=csv.reader(f, delimiter="\\\\t")')
    (34, '        for row in reader:')
    (35, '            if len(row) == 0:')
    (36, '                pass')
    (37, '            elif len(row) == 2:')
    (38, '                ID_samples.add(row[0])')
    (39, '                ID_folders.add(row[1])')
    (40, '                map_samples_folder[row[0]].append(row[1])')
    (41, '            # Add barcodes to folder (tuple)')
    (42, '            elif len(row) == 3:')
    (43, '                ID_samples.add(row[0])')
    (44, '                ID_folders.add(row[1])')
    (45, '                ID_barcode_folders.add(row[1])')
    (46, '                map_samples_barcode[row[0]].append((row[1], row[2]))')
    (47, '            else :')
    (48, '                print("Wrong number of columns in row: ", row, len(row))')
    (49, '                pass')
    (50, '')
    (51, '# If no FOFN available, check dir for  Folders with "Sample_" Prefix')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=imgag/ont_tools, file=workflow/Snakefile
context_key: ['else', 'if not ID_samples']
    (57, '    if not ID_samples: ')
    (58, '        print("NO samples ... quitting")')
    (59, '        exit(1)')
    (60, '')
    (61, '# Create run names from the Folder_IDs. IE only `20211213_1214_2-A7-D7_PAI80186_4d463244_02064` ')
    (62, '# Regex for run name:   "^\\\\d{8}_\\\\d{4}_\\\\w+-\\\\w+-\\\\w+_\\\\D{3}\\\\d{5}_\\\\w{8}_\\\\w{5}$"')
    (63, '# New folder names: "QIGMI017AF_21061LRa001_02013" or "21061LRa001_02013"')
    (64, '# Regex for new run name:   "^(?:Q\\\\w{9}_)?\\\\d{5}\\\\w*_\\\\d{5}$"')
    (65, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=uaauaguga/cfRNA-pipe, file=Snakefile
context_key: ["if not os.path.exists(config[\\'sample_ids\\'])"]
    (10, "if not os.path.exists(config[\\'sample_ids\\']):")
    (11, '    print(f"Specified sample id path {config[\\\'sample_ids\\\']} does not exists")')
    (12, '    sys.exit(1)')
    (13, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=uaauaguga/cfRNA-pipe, file=Snakefile
context_key: ["if \\'default_config\\' not in config"]
    (14, "if \\'default_config\\' not in config:")
    (15, "    config[\\'default_config\\'] = \\'config/default.yaml\\'")
    (16, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=uaauaguga/cfRNA-pipe, file=Snakefile
context_key: ["if not os.path.exists(config[\\'default_config\\'])"]
    (17, "if not os.path.exists(config[\\'default_config\\']):")
    (18, '    print(f"Specified default config file {config[\\\'default_config\\\']} does not exists")')
    (19, '    sys.exit(2)')
    (20, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=uaauaguga/cfRNA-pipe, file=Snakefile
context_key: ['if key not in config']
    (26, '    if key not in config:')
    (27, '        config[key] = default_config[key]')
    (28, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=myRNASeq/FF2, file=rules/cell-type.smk
context_key: ['if wildcards.parent != "root"']
    (1, '    if wildcards.parent != "root":')
    (2, '        parent = markers.loc[wildcards.parent, "parent"]')
    (3, '        return f"analysis/cellassign.{parent}.rds"')
    (4, '    else:')
    (5, '        return []')
    (6, '')
    (7, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=myRNASeq/FF2, file=Snakefile
context_key: ['if "markers" in config.get("celltype", {})']
    (17, 'if "markers" in config.get("celltype", {}):')
    (18, '    markers = pd.read_csv(config["celltype"]["markers"], sep="\\\\t").set_index("name", drop=False)')
    (19, '    markers.loc[:, "parent"].fillna("root", inplace=True)')
    (20, '')
    (21, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bcgsc/pori_graphkb_loader, file=Snakefile
context_key: ['if not os.path.exists(DATA_DIR)']
    (7, 'if not os.path.exists(DATA_DIR):')
    (8, '    os.mkdir(DATA_DIR)')
    (9, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bcgsc/pori_graphkb_loader, file=Snakefile
context_key: ['if not os.path.exists(LOGS_DIR)']
    (10, 'if not os.path.exists(LOGS_DIR):')
    (11, '    os.mkdir(LOGS_DIR)')
    (12, '')
    (13, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bcgsc/pori_graphkb_loader, file=Snakefile
context_key: ['if USE_FDA_UNII', 'rule download_ncit_fda']
    (57, 'if USE_FDA_UNII:')
    (58, '    rule download_ncit_fda:')
    (59, "        output: f\\'{DATA_DIR}/ncit/FDA-UNII_NCIt_Subsets.txt\\'")
    (60, "        shell: dedent(f\\'\\'\\'\\\\")
    (61, '            cd {DATA_DIR}/ncit')
    (62, "            wget https://evs.nci.nih.gov/ftp1/FDA/UNII/FDA-UNII_NCIt_Subsets.txt\\'\\'\\')")
    (63, '')
    (64, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bcgsc/pori_graphkb_loader, file=Snakefile
context_key: ['if USE_FDA_UNII', 'rule download_fda_srs']
    (74, 'if USE_FDA_UNII:')
    (75, '    rule download_fda_srs:')
    (76, "        output: f\\'{DATA_DIR}/fda/UNII_Records.txt\\'")
    (77, "        shell: dedent(f\\'\\'\\'\\\\")
    (78, '            cd {DATA_DIR}/fda')
    (79, '            wget https://fdasis.nlm.nih.gov/srs/download/srs/UNII_Data.zip')
    (80, '            unzip UNII_Data.zip')
    (81, '            rm UNII_Data.zip')
    (82, '')
    (83, '            mv UNII*.txt UNII_Records.txt')
    (84, "            \\'\\'\\')")
    (85, '')
    (86, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bcgsc/pori_graphkb_loader, file=Snakefile
context_key: ['if USE_FDA_UNII', 'rule load_fda_srs']
    (230, 'if USE_FDA_UNII:')
    (231, '    rule load_fda_srs:')
    (232, "        input: expand(rules.load_local.output, local=[\\'vocab\\']),")
    (233, "            data=f\\'{DATA_DIR}/fda/UNII_Records.txt\\'")
    (234, '        container: CONTAINER')
    (235, "        log: f\\'{LOGS_DIR}/fdaSrs.logs.txt\\'")
    (236, "        output: f\\'{DATA_DIR}/fdaSrs.COMPLETE\\'")
    (237, "        shell: LOADER_COMMAND + \\' file fdaSrs {input.data} &> {log}; cp {log} {output}\\'")
    (238, '')
    (239, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bcgsc/pori_graphkb_loader, file=Snakefile
context_key: ['if USE_FDA_UNII', 'rule load_ncit_fda']
    (240, '    rule load_ncit_fda:')
    (241, '        input: rules.load_ncit.output,')
    (242, '            rules.load_fda_srs.output,')
    (243, '            data=rules.download_ncit_fda.output')
    (244, '        container: CONTAINER')
    (245, "        log: f\\'{LOGS_DIR}/ncitFdaXref.logs.txt\\'")
    (246, "        output: f\\'{DATA_DIR}/ncitFdaXref.COMPLETE\\'")
    (247, "        shell: LOADER_COMMAND + \\' file ncitFdaXref {input.data} &> {log}; cp {log} {output}\\'")
    (248, '')
    (249, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile
context_key: ['if "chromosomes" not in config', 'if config["genome_name"] in config["chromosome_defaults"].keys()']
    (24, 'if "chromosomes" not in config:')
    (25, '    if config["genome_name"] in config["chromosome_defaults"].keys():')
    (26, '        chromosomes = config["chromosome_defaults"][config["genome_name"]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile
context_key: ['if "chromosomes" not in config', 'else']
    (27, '    else:')
    (28, '        chromosomes = []')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile
context_key: ['if len(chromosomes) == 0']
    (32, 'if len(chromosomes) == 0:')
    (33, '    raise(WorkflowSetupError("No valid chromosomes found. If you\\\'re using a non-standard genome (not hg38 or hg19), you must explicitly specify chromosome names in the config"))')
    (34, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile
context_key: ['if "genome" in config', 'if config["genome_name"] in config["genome"]']
    (35, 'if "genome" in config:')
    (36, '    if config["genome_name"] in config["genome"]:')
    (37, '        genome_path = config["genome"][config["genome_name"]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile
context_key: ['if "genome" in config', 'else']
    (38, '    else:')
    (39, '        genome_path = "failed"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile
context_key: ['if "genome" in config', 'if not os.path.exists(genome_path)']
    (40, '    if not os.path.exists(genome_path):')
    (41, '        genome_path = "failed"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile
context_key: ['if genome_path == "failed"', 'if connec.status_code == 200']
    (45, 'if genome_path == "failed":')
    (46, '    hgver = config["genome_name"]')
    (47, '    connec = requests.get(f"http://hgdownload.cse.ucsc.edu/goldenpath/{hgver}/database/cytoBand.txt.gz")')
    (48, '    if connec.status_code == 200:')
    (49, '        genome_path = f"resources/{hgver}/genome.fa"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile
context_key: ['if genome_path == "failed"', 'else']
    (50, '    else:')
    (51, '        raise(WorkflowSetupError("Genome fasta file not found in config or on UCSC. Specify a path to your genome fasta"))')
    (52, '')
    (53, '')
    (54, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile
context_key: ['if os.path.exists(config["array_positions"][config["genome_name"]]']
    (60, '    if os.path.exists(config["array_positions"][config["genome_name"]])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile
context_key: ['else os.path.join']
    (61, '    else os.path.join(')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile
context_key: ['if str(chromosome) not in snp_chrs']
    (72, '        if str(chromosome) not in snp_chrs:')
    (73, '            raise(WorkflowSetupError(f"Chromosome {chromosome} specified in config was not found in provided snp position data"))')
    (74, '    lens = [len(line) for line in snp_positions]')
    (75, '    print(lens[0])')
    (76, '    if any(leng > 2 for leng in lens):')
    (77, '        raise(WorkflowSetupError("More than two columns detected in snp position data - file must contain only chromosome and position columns"))')
    (78, '')
    (79, '')
    (80, '')
    (81, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile
context_key: ['if cyto_path == "auto"', 'if connec.status_code == 200']
    (83, 'if cyto_path == "auto":')
    (84, '    ## Try to automatically get cytobands based on genome name')
    (85, '    hgver = config["genome_name"]')
    (86, '    connec = requests.get(f"http://hgdownload.cse.ucsc.edu/goldenpath/{hgver}/database/cytoBand.txt.gz")')
    (87, '    if connec.status_code == 200:')
    (88, '        cyto_path = f"resources/{hgver}/cytobands.txt"')
    (89, '        cyto_arg = f"-c {cyto_path}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile
context_key: ['if cyto_path == "auto"', 'else']
    (90, '    else:')
    (91, '        raise WorkflowSetupError("Cytoband file is set to auto-detect, but could not download cytoband file. Make sure you didn\\\'t misspell the genome file, leave the cyto_path blank in the config, or explicitly set a path for it")')
    (92, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile
context_key: ['if not os.path.exists(bam)']
    (112, '        if not os.path.exists(bam):')
    (113, '            raise WorkflowSetupError(f"Input file {bam} could not be found. Ensure that the file is spelled correctly, and that you\\\'ve correctly bound the directory if using singularity")')
    (114, '    combinations = expand("{somatic}_{normal}", somatic=somatics, normal=normals)')
    (115, '    outs = [os.path.join(output_dir, sample, comb, "cna.txt") for comb in combinations]')
    (116, '    output_list.extend(outs)')
    (117, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile.gsc.smk
context_key: ['if "id" in config.keys() and']
    (34, 'if "id" in config.keys() and (')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile.gsc.smk
context_key: ['if "biopsy" in config.keys()']
    (44, 'if "biopsy" in config.keys():')
    (45, '    config["tumour_lib"], config["normal_lib"] = get_biopsy_dna_tumour_normal(')
    (46, '        patient_id=config["id"], biopsy=config["biopsy"]')
    (47, '    )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile.gsc.smk
context_key: ['if output_di']
    (63, '    if output_dir')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile.gsc.smk
context_key: ['else get_gsc_output_folder']
    (64, '    else get_gsc_output_folder(')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile.gsc.smk
context_key: ['if "gsc_config_filename" in config.keys()']
    (75, 'if "gsc_config_filename" in config.keys():')
    (76, '    gsc_config_filename = config["gsc_config_filename"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile.gsc.smk
context_key: ['if not os.path.exists(gsc_config_filename)']
    (81, 'if not os.path.exists(gsc_config_filename):')
    (82, '    print(f"Creating {gsc_config_filename}")')
    (83, '    args = SimpleNamespace(**config)')
    (84, '    args.pipeline_ver = pipeline_ver')
    (85, '    args.output_file = gsc_config_filename')
    (86, '    args.patient_id = args.id')
    (87, '    build_config(args=args)')
    (88, '')
    (89, '# Run the standard Snakemake pipeline with the appropriate config')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if not valid']
    (18, 'if not valid:')
    (19, '    raise Exception("provided configuration is not valid")')
    (20, '')
    (21, '#################################################################')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "langs" in config']
    (49, 'if "langs" in config:')
    (50, '    LANGS = set(config["langs"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "lang1" in config']
    (51, 'if "lang1" in config:')
    (52, '    LANGS.add(LANG1)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "lang2" in config']
    (53, 'if "lang2" in config:')
    (54, '    LANGS.add(LANG2)')
    (55, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if VERBOSE and is_first_snakemake_execution(TMPDIR)']
    (59, 'if VERBOSE and is_first_snakemake_execution(TMPDIR):')
    (60, '    # provided config')
    (61, '    sys.stderr.write("Config: \\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "crawler" in config', 'if CRAWLTARGET == "linguacrawl"']
    (99, 'if "crawler" in config:')
    (100, '    if CRAWLTARGET == "linguacrawl":')
    (101, '        # TODO should we change this default value, as we have done with wget, as well?')
    (102, '        CRAWLFILETYPES = ["text/html", "application/pdf"]')
    (103, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "crawlerUserAgent" in config']
    (108, 'if "crawlerUserAgent" in config:')
    (109, '    USERAGENT = config["crawlerUserAgent"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "crawlBlackListURL" in config']
    (110, 'if "crawlBlackListURL" in config:')
    (111, '    CRAWLBLACKLISTURL = config["crawlBlackListURL"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "crawlPrefixFilter" in config']
    (112, 'if "crawlPrefixFilter" in config:')
    (113, '    CRAWLPREFIXFILTER = config["crawlPrefixFilter"]')
    (114, '')
    (115, '#################################################################')
    (116, '# PREPROCESS')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "writeHTML" in config and config["writeHTML"]']
    (122, 'if "writeHTML" in config and config["writeHTML"]:')
    (123, '    HTML_FILE = "html.gz"')
    (124, '    PPROC_FILES.append("html.gz")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "preprocessor" in config', 'if config["preprocessor"] == "warc2preprocess"']
    (125, 'if "preprocessor" in config:')
    (126, '    if config["preprocessor"] == "warc2preprocess":')
    (127, '        PPROC = "w2p"')
    (128, '        PPROC_FILES = ["plain_text.gz", "url.gz", "mime.gz", "normalized_html.gz", "deboilerplate_html.gz"]')
    (129, '        TEXT_FILE = "plain_text.gz"')
    (130, '        HTML_FILE = "deboilerplate_html.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "preprocessor" in config', 'else']
    (131, '    else:')
    (132, '        PPROC = config["preprocessor"]')
    (133, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "parser" in config']
    (148, 'if "parser" in config:')
    (149, '    PARSER = f"--parser {config[\\\'parser\\\']}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "PDFextract" in config and config["PDFextract"]', 'if "PDFextract_configfile" in config and config["PDFextract_configfile"]']
    (150, 'if "PDFextract" in config and config["PDFextract"]:')
    (151, '    PDFEXTRACT_CF = ""')
    (152, '    PDFEXTRACT_SJ = ""')
    (153, '    PDFEXTRACT_KL = ""')
    (154, '')
    (155, '    if "PDFextract_configfile" in config and config["PDFextract_configfile"]:')
    (156, '        PDFEXTRACT_CF = f" --pe_configfile {config[\\\'PDFextract_configfile\\\']}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "PDFextract" in config and config["PDFextract"]', 'if "PDFextract_sentence_join_path" in config and config["PDFextract_sentence_join_path"]']
    (157, '    if "PDFextract_sentence_join_path" in config and config["PDFextract_sentence_join_path"]:')
    (158, '        PDFEXTRACT_SJ = f" --sentence_join_path {config[\\\'PDFextract_sentence_join_path\\\']}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "PDFextract" in config and config["PDFextract"]', 'if "PDFextract_kenlm_path" in config and config["PDFextract_kenlm_path"]']
    (159, '    if "PDFextract_kenlm_path" in config and config["PDFextract_kenlm_path"]:')
    (160, '        PDFEXTRACT_KL = f" --kenlm_path {config[\\\'PDFextract_kenlm_path\\\']}"')
    (161, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "PDFextract" in config and config["PDFextract"]']
    (162, '    PDFEXTRACT = f"--pdfextract {PDFEXTRACT_CF} {PDFEXTRACT_SJ} {PDFEXTRACT_KL}"')
    (163, '')
    (164, '# sentence splitting and tokenisation')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if WORDTOKS']
    (173, 'if WORDTOKS:')
    (174, '    WORDTOK1 = get_lang_or_default(WORDTOKS, LANG1)')
    (175, '    WORDTOK2 = get_lang_or_default(WORDTOKS, LANG2)')
    (176, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "translationDirection" in config and config["translationDirection"] == f"{LANG2}2{LANG1}"']
    (199, 'if "translationDirection" in config and config["translationDirection"] == f"{LANG2}2{LANG1}":')
    (200, '    # Swap necessary variables')
    (201, '    SRC_LANG, TRG_LANG = LANG2, LANG1')
    (202, '    WORDTOK1, WORDTOK2 = WORDTOK2, WORDTOK1')
    (203, '')
    (204, '# dic-based docalign')
    (205, '# snakemake/include/dic-docsegalign')
    (206, '#################################################################')
    (207, '# dic')
    (208, '# snakemake/include/dic-generation')
    (209, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if BICLEANER', 'if is_flavour_ai']
    (246, 'if BICLEANER:')
    (247, '    is_flavour_ai = BICLEANER_FLAVOUR == "ai"')
    (248, '    path_exists_f=os.path.isdir if is_flavour_ai else os.path.isfile')
    (249, '')
    (250, '    # Create soft links based on the flavour if necessary')
    (251, '    if is_flavour_ai:')
    (252, '        # Get directory path from bicleaner model path')
    (253, "        BICLEANER_MODEL = \\'/\\'.join(BICLEANER_MODEL.split(\\'/\\')[:-1])")
    (254, '')
    (255, '    # BICLEANER_MODEL will be a directory if is_flavour_ai else will be a file')
    (256, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "granularity" in config', 'if  "documents" in config["granularity"]']
    (267, 'if "granularity" in config:')
    (268, '    if  "documents" in config["granularity"]:')
    (269, '        OUTPUT_FILES.append("documents")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if TMX']
    (272, 'if TMX:')
    (273, '    OUTPUT_FILES.append("not-deduped.tmx")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if DEDUPED']
    (274, 'if DEDUPED:')
    (275, '    OUTPUT_FILES.append("deduped.tmx")')
    (276, '    OUTPUT_FILES.append("deduped.txt")')
    (277, '    STATS_FILES.append("deduped")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if BIROAMER', 'if TMX']
    (278, 'if BIROAMER:')
    (279, '    if TMX:')
    (280, '        OUTPUT_FILES.append("not-deduped.roamed.tmx")')
    (281, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if BIROAMER', 'if DEDUPED']
    (282, '    if DEDUPED:')
    (283, '        OUTPUT_FILES.append("deduped.roamed.tmx")')
    (284, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if BIROAMER', 'if "biroamerMixFiles" in config and len(config["biroamerMixFiles"]) > 0']
    (285, '    if "biroamerMixFiles" in config and len(config["biroamerMixFiles"]) > 0:')
    (286, '        BIROAMER_MIX_FILES = " ".join(config["biroamerMixFiles"])')
    (287, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if BIROAMER', 'if "biroamerImproveAlignmentCorpus" in config']
    (288, '    if "biroamerImproveAlignmentCorpus" in config:')
    (289, '        BIROAMER_ALIGNMENT_CORPUS = f"-a {config[\\\'biroamerImproveAlignmentCorpus\\\']}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if not BICLEANER_GENERATE_MODEL']
    (290, 'if not BICLEANER_GENERATE_MODEL:')
    (291, '    BICLEANER_TRAINING_CORPUS_PREFIX = []')
    (292, '    BICLEANER_AI_MONO_CORPUS_PREFIX = []')
    (293, '    BICLEANER_AI_DEV_CORPUS_PREFIX = []')
    (294, '')
    (295, '')
    (296, '#################################################################')
    (297, '# DATASOURCES')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "warcs" in config']
    (302, 'if "warcs" in config:')
    (303, '    WARCS = WARCS.union(config["warcs"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "hosts" in config']
    (304, 'if "hosts" in config:')
    (305, '    HOSTS = HOSTS.union(config["hosts"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "preverticals" in config']
    (306, 'if "preverticals" in config:')
    (307, '    PREVERTICALS = PREVERTICALS.union(config["preverticals"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "hostsFile" in config']
    (308, 'if "hostsFile" in config:')
    (309, '    with open_xz_or_gzip_or_plain(config["hostsFile"]) as f:')
    (310, '        for line in f:')
    (311, '            HOSTS.add(line.strip())')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "warcsFile" in config']
    (312, 'if "warcsFile" in config:')
    (313, '    with open_xz_or_gzip_or_plain(config["warcsFile"]) as f:')
    (314, '        for line in f:')
    (315, '            WARCS.add(line.strip())')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "preverticalsFile" in config']
    (316, 'if "preverticalsFile" in config:')
    (317, '    with open_xz_or_gzip_or_plain(config["preverticalsFile"]) as f:')
    (318, '        for line in f:')
    (319, '            PREVERTICALS.add(line.strip())')
    (320, '')
    (321, '# sort in order to avoid different results across executions')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "parallelWorkers" in config']
    (369, 'if "parallelWorkers" in config:')
    (370, '    for k in config["parallelWorkers"]:')
    (371, '        THREADS[k] = config["parallelWorkers"][k]')
    (372, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if "parallelJobs" in config', 'if job_resource_name not in workflow.global_resources']
    (373, 'if "parallelJobs" in config:')
    (374, '    for k in config["parallelJobs"]:')
    (375, '        JOB_THREADS[k] = config["parallelJobs"][k]')
    (376, '')
    (377, '        # Modify the snakemake resources if they were not provided')
    (378, '        job_resource_name = f"job_{k}"')
    (379, '')
    (380, '        if job_resource_name not in workflow.global_resources:')
    (381, '            # Since we want to run as parallel jobs as configured, the resources')
    (382, '            #  are squared in order to leave available the exact number of resources')
    (383, '            #  for each job.')
    (384, '            # E.g., if parallelJobs["bicleaner"] = 4, we want 4 jobs at max. to run')
    (385, '            #  in parallel, so we need 4 "job_bicleaner" resources to be free for each')
    (386, '            #  "bicleaner" job, so 4 * 4 = 16 necessary "job_bicleaner" resources')
    (387, '            workflow.global_resources[job_resource_name] = JOB_THREADS[k] ** 2')
    (388, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if UNTIL not in UNTIL_RULES_ALL_LANGS', 'if len(LANGS) > 2', 'if not UNTIL']
    (393, 'if UNTIL not in UNTIL_RULES_ALL_LANGS:')
    (394, '    # At some point, not all languages from LANGS will be taken into account but only LANG1 and LANG2')
    (395, '')
    (396, '    if len(LANGS) > 2:')
    (397, '        # There are different languages to process, not only LANG1 and LANG2')
    (398, '')
    (399, '        # Warn about the fact that not all languages will be processed at some point')
    (400, '        if not UNTIL:')
    (401, '            snakemake.logger.logger.warning("Not all languages will be processed until the end of the pipeline")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if UNTIL not in UNTIL_RULES_ALL_LANGS', 'if len(LANGS) > 2', 'else']
    (402, '        else:')
    (403, '            snakemake.logger.logger.warning(f"Not all languages will be processed until \\\'{UNTIL}\\\'")')
    (404, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if not UNTIL']
    (405, 'if not UNTIL: # config["until"] not provided -> run the whole pipeline')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if DOCALIGN == "externalMT"']
    (425, '    if DOCALIGN == "externalMT":')
    (426, '        OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}")')
    (427, '    elif DOCALIGN in ("DIC", "NDA"):')
    (428, '        OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{SRC_LANG}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['elif UNTIL == "crawl"', 'if CRAWLTARGET == "linguacrawl"']
    (429, 'elif UNTIL == "crawl":')
    (430, '    if CRAWLTARGET == "linguacrawl":')
    (431, '        OUTPUT.append(f"{DATADIR}/warc/linguacrawl.finished")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['elif UNTIL == "crawl"', 'else']
    (432, '    else:')
    (433, '        for domain, hosts in DOMAIN_2_HOSTS.items():')
    (434, '            for host in hosts:')
    (435, '                OUTPUT.append(f"{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['elif UNTIL == "preprocess"']
    (436, 'elif UNTIL == "preprocess":')
    (437, '    OUTPUT = expand(')
    (438, '        "{datadir}/preprocess/{target}/{pproc}/{lang}/{pproc_file}",')
    (439, '        datadir=DATADIR,')
    (440, '        target=TARGET_2_WARCS,')
    (441, '        pproc=PPROC,')
    (442, '        lang=LANGS,')
    (443, '        pproc_file=PPROC_FILES,')
    (444, '    )')
    (445, '    OUTPUT.extend(expand(')
    (446, '        "{datadir}/preprocess/{target}/prevertical2text/{lang}/{pproc_file}",')
    (447, '        datadir=DATADIR,')
    (448, '        target=TARGET_2_PREVERTICALS,')
    (449, '        lang=LANGS,')
    (450, '        pproc_file=PPROC_FILES,')
    (451, '    ))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['elif UNTIL == "shard"']
    (452, 'elif UNTIL == "shard":')
    (453, '    OUTPUT = expand("{datadir}/shards/02.batches.{lang}", datadir=DATADIR, lang=LANGS)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['elif UNTIL == "split"']
    (454, 'elif UNTIL == "split":')
    (455, '    OUTPUT = expand("{datadir}/shards/03.split.{lang}", datadir=DATADIR, lang=LANGS)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['elif UNTIL == "tokenise"']
    (456, 'elif UNTIL == "tokenise":')
    (457, '    OUTPUT = expand("{datadir}/shards/05.tokenise.{lang}", datadir=DATADIR, lang=LANGS)')
    (458, '')
    (459, '# TODO should we implement all languages provided in LANGS?')
    (460, '# SRC_LANG and TRG_LANG dependent')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['elif UNTIL == "translate"']
    (461, 'elif UNTIL == "translate":')
    (462, '    OUTPUT = f"{DATADIR}/shards/04.translate.{SRC_LANG}2{TRG_LANG}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['elif UNTIL == "tokenise_trg"']
    (463, 'elif UNTIL == "tokenise_trg":')
    (464, '    OUTPUT = f"{DATADIR}/shards/05.tokenise.{TRG_LANG}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['elif UNTIL == "tokenise_src"', 'if DOCALIGN == "externalMT"']
    (465, 'elif UNTIL == "tokenise_src":')
    (466, '    if DOCALIGN == "externalMT":')
    (467, '        OUTPUT = f"{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['elif UNTIL == "tokenise_src"', 'elif DOCALIGN in ("DIC", "NDA")']
    (468, '    elif DOCALIGN in ("DIC", "NDA"):')
    (469, '        OUTPUT = f"{DATADIR}/shards/05.tokenise.{SRC_LANG}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['else', 'if UNTIL == "docalign"']
    (473, '    if UNTIL == "docalign":')
    (474, '        OUTPUT.append(f"{TRANSIENT}/06_01.docalign.{LANG1}_{LANG2}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['else', 'elif UNTIL == "segalign"']
    (475, '    elif UNTIL == "segalign":')
    (476, '        OUTPUT.append(f"{TRANSIENT}/06_02.segalign.{LANG1}_{LANG2}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['else', 'elif UNTIL == "bifixer"']
    (477, '    elif UNTIL == "bifixer":')
    (478, '        OUTPUT.append(f"{TRANSIENT}/07_01.bifixer.{LANG1}_{LANG2}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['else', 'elif UNTIL == "bicleaner"']
    (479, '    elif UNTIL == "bicleaner":')
    (480, '        OUTPUT.append(f"{TRANSIENT}/07_02.bicleaner.{LANG1}_{LANG2}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['else', 'elif UNTIL == "filter"']
    (481, '    elif UNTIL == "filter":')
    (482, '        OUTPUT.append(f"{TRANSIENT}/07_03.filter.{LANG1}_{LANG2}")')
    (483, '')
    (484, '#################################################################')
    (485, '# OTHER')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if PARAGRAPH_IDENTIFICATION']
    (488, 'if PARAGRAPH_IDENTIFICATION:')
    (489, '    BASE64_FILTER_CMD = f"python3 {WORKFLOW}/utils/apply_command_b64_doc.py \\\'cut -f 1\\\'"')
    (490, '')
    (491, '# Create mark')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if BIFIXER']
    (496, 'if BIFIXER:')
    (497, '    check_nltk_model.check("tokenizers/punkt", "punkt")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if BIROAMER']
    (498, 'if BIROAMER:')
    (499, '    check_nltk_model.check("misc/perluniprops", "perluniprops")')
    (500, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if LANG1 or LANG2']
    (667, '        if LANG1 or LANG2:')
    (668, '            yaml_content += f"mandatory_lang: \\\'{LANG1}\\\'\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if not params.crawl_cat']
    (724, '            if not params.crawl_cat:')
    (725, '                warcs = "\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['else', 'if params.crawl_cat_max <= 0']
    (729, '                if params.crawl_cat_max <= 0:')
    (730, '                    warcs = " ".join(warcs)')
    (731, '                    shell(')
    (732, '                        f"""')
    (733, '                        cat {warcs} > {params.folder}/linguacrawl.warc.gz')
    (734, '                        echo "{params.folder}/linguacrawl.warc.gz" > {{output[0]}}')
    (735, '                        """')
    (736, '                    )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['else', 'else', 'if current > {params.crawl_cat_max}']
    (744, '                        if current > {params.crawl_cat_max}:')
    (745, '                            current = 0')
    (746, '                            blocks += 1')
    (747, '                            files.append(f"{params.folder}/linguacrawl{blocks}.warc.gz")')
    (748, '')
    (749, '                        shell(f"cat {warc} >> {files[-1]}")')
    (750, '')
    (751, '                        du_command = subprocess.Popen(("du", "-b", warc), stdout=subprocess.PIPE)')
    (752, '')
    (753, '                        try:')
    (754, '                            du_warc = subprocess.check_output(("awk", "{print $1}"), stdin=du_command.stdout)')
    (755, '                            du_command.wait()')
    (756, '')
    (757, '                            current += int(du_warc)')
    (758, '                        except:')
    (759, '                            sys.stderr.write(f"WARNING: could not retrieve the size of {warc}\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['except', 'if len(WARCS) == 0']
    (767, '            if len(WARCS) == 0:')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if SEGALIGN == "hunalign"']
    (1418, 'if SEGALIGN == "hunalign":')
    (1419, '    split_input_filename = "hunalign.06_02.segalign"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['elif SEGALIGN == "vecalign"']
    (1420, 'elif SEGALIGN == "vecalign":')
    (1421, '    split_input_filename = "vecalign.06_02.segalign"')
    (1422, '')
    (1423, '# split segalign results into balanced chunks')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['input', 'run', 'if len(input) == 0']
    (1447, '        if len(input) == 0:')
    (1448, '            raise Exception("Didn\\\'t get any input file")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if [ -z "$(ls -A {params.folder})" ]; the']
    (1463, '                if [ -z "$(ls -A {params.folder})" ]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if BICLEANER_GENERATE_MODEL and BICLEANER_FLAVOUR == "classic"', 'rule bicleaner_train_model', 'input']
    (1513, 'if BICLEANER_GENERATE_MODEL and BICLEANER_FLAVOUR == "classic":')
    (1514, '    rule bicleaner_train_model:')
    (1515, '        """')
    (1516, '        Train bicleaner model (flavour: classic)')
    (1517, '        """')
    (1518, '        input:')
    (1519, '            corpus_l1=expand("{dataset}.{lang}.gz", dataset=BICLEANER_TRAINING_CORPUS_PREFIX, lang=SRC_LANG),')
    (1520, '            corpus_l2=expand("{dataset}.{lang}.gz", dataset=BICLEANER_TRAINING_CORPUS_PREFIX, lang=TRG_LANG),')
    (1521, '            # \\\'optional_str_input\\\' is needed because if \\\'config["generateDic"] == True and not BICLEANER_GENERATE_MODEL\\\' then')
    (1522, '            #   will trigger this rule ("reason: Input files updated by another job") even if the output already exists')
    (1523, '            e2f=optional_str_input(f"{DIC}.lex.e2f.gz", BICLEANER_GENERATE_MODEL),')
    (1524, '            f2e=optional_str_input(f"{DIC}.lex.f2e.gz", BICLEANER_GENERATE_MODEL),')
    (1525, '            vcb1=optional_str_input(f"{mgizaModelDir}/corpus.{SRC_LANG}.filtered.vcb.gz", BICLEANER_GENERATE_MODEL),')
    (1526, '            vcb2=optional_str_input(f"{mgizaModelDir}/corpus.{TRG_LANG}.filtered.vcb.gz", BICLEANER_GENERATE_MODEL),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if BICLEANER_GENERATE_MODEL and BICLEANER_FLAVOUR == "classic"', 'rule bicleaner_train_model', 'output']
    (1527, '        output:')
    (1528, '            model=BICLEANER_MODEL,')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if BICLEANER_GENERATE_MODEL and BICLEANER_FLAVOUR == "classic"', 'rule bicleaner_train_model', 'params']
    (1529, '        params:')
    (1530, '            training_corpus=temp(f"{TMPDIR}/bicleaner_train/training_parallel_corpus.{SRC_LANG}_{TRG_LANG}"),')
    (1531, '            classifier=f"{\\\'/\\\'.join(BICLEANER_MODEL.split(\\\'/\\\')[:-1])}/{SRC_LANG}-{TRG_LANG}.classifier",')
    (1532, '            tokenizer_l1=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, SRC_LANG), \\\'-S "{}"\\\'),')
    (1533, '            tokenizer_l2=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, TRG_LANG), \\\'-T "{}"\\\'),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if BICLEANER_GENERATE_MODEL and BICLEANER_FLAVOUR == "classic"', 'rule bicleaner_train_model', 'shell']
    (1534, '        shell:')
    (1535, '            """')
    (1536, '            echo "Training corpus: {params.training_corpus}"')
    (1537, '            mkdir -p "{TMPDIR}/bicleaner_train"')
    (1538, '            paste <(zcat {input.corpus_l1}) <(zcat {input.corpus_l2}) > {params.training_corpus}')
    (1539, '')
    (1540, '            {PROFILING} bicleaner-train {params.training_corpus} {params.tokenizer_l1} {params.tokenizer_l2} --treat_oovs \\\\')
    (1541, '                --normalize_by_length -s {SRC_LANG} -t {TRG_LANG} -d {input.e2f} -D {input.f2e} -f {input.vcb1} \\\\')
    (1542, '                -F {input.vcb2} -c {params.classifier} -m {output.model} --classifier_type random_forest \\\\')
    (1543, '                --seed 71213')
    (1544, '            """')
    (1545, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if BICLEANER_GENERATE_MODEL and BICLEANER_FLAVOUR == "ai"', 'rule bicleaner_train_model', 'input']
    (1546, 'if BICLEANER_GENERATE_MODEL and BICLEANER_FLAVOUR == "ai":')
    (1547, '    rule bicleaner_train_model:')
    (1548, '        """')
    (1549, '        Train bicleaner model (flavour: ai)')
    (1550, '        """')
    (1551, '        input:')
    (1552, '            corpus_l1=expand("{dataset}.{lang}.gz", dataset=BICLEANER_TRAINING_CORPUS_PREFIX, lang=SRC_LANG),')
    (1553, '            corpus_l2=expand("{dataset}.{lang}.gz", dataset=BICLEANER_TRAINING_CORPUS_PREFIX, lang=TRG_LANG),')
    (1554, '            mono_l1=\\\' \\\'.join(expand("{dataset}.{lang}.gz", dataset=BICLEANER_AI_MONO_CORPUS_PREFIX, lang=SRC_LANG)),')
    (1555, '            mono_l2=\\\' \\\'.join(expand("{dataset}.{lang}.gz", dataset=BICLEANER_AI_MONO_CORPUS_PREFIX, lang=TRG_LANG)),')
    (1556, '            dev_corpus_l1=\\\' \\\'.join(expand("{dataset}.{lang}.gz", dataset=BICLEANER_AI_DEV_CORPUS_PREFIX, lang=SRC_LANG)),')
    (1557, '            dev_corpus_l2=\\\' \\\'.join(expand("{dataset}.{lang}.gz", dataset=BICLEANER_AI_DEV_CORPUS_PREFIX, lang=TRG_LANG)),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if BICLEANER_GENERATE_MODEL and BICLEANER_FLAVOUR == "ai"', 'rule bicleaner_train_model', 'output']
    (1558, '        output:')
    (1559, '            model=BICLEANER_MODEL,')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if BICLEANER_GENERATE_MODEL and BICLEANER_FLAVOUR == "ai"', 'rule bicleaner_train_model', 'params']
    (1560, '        params:')
    (1561, '            training_corpus=temp(f"{TMPDIR}/bicleaner_train/training_parallel_corpus.{SRC_LANG}_{TRG_LANG}"),')
    (1562, '            tfreq=temp(f"{TMPDIR}/bicleaner_train/bicleaner-ai/wordfreq_{TRG_LANG}.gz"),')
    (1563, '            dev_corpus=temp(f"{TMPDIR}/bicleaner_train/bicleaner-ai/dev_parallel_corpus.{SRC_LANG}_{TRG_LANG}"),')
    (1564, '            mono_corpus=temp(f"{TMPDIR}/bicleaner_train/bicleaner-ai/mono_corpus.{SRC_LANG}_{TRG_LANG}"),')
    (1565, '            metadata_dir=BICLEANER_MODEL,')
    (1566, '            ai_metadata=f"{BICLEANER_MODEL}/metadata.yaml",')
    (1567, '            model=BICLEANER_MODEL,')
    (1568, '            tokenizer_l1=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, SRC_LANG), \\\'-S "{}"\\\'),')
    (1569, '            tokenizer_l2=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, TRG_LANG), \\\'-T "{}"\\\'),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if BICLEANER_GENERATE_MODEL and BICLEANER_FLAVOUR == "ai"', 'rule bicleaner_train_model', 'shell']
    (1570, '        shell:')
    (1571, '            """')
    (1572, '            echo "Training corpus: {params.training_corpus}"')
    (1573, '            mkdir -p "{TMPDIR}/bicleaner_train/bicleaner-ai"')
    (1574, '            paste <(zcat {input.corpus_l1}) <(zcat {input.corpus_l2}) > {params.training_corpus}')
    (1575, '')
    (1576, '            # Target lang frequency')
    (1577, '            echo "Frequency file (lang={TRG_LANG}): {params.tfreq}"')
    (1578, '            zcat {input.mono_l2} \\\\')
    (1579, '                | {WORDTOK2} \\\\')
    (1580, "                | awk \\'{{print tolower($0)}}\\' \\\\")
    (1581, "                | tr \\' \\' \\'\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if [[ -z "$bicleaner_src_lang" ]] || [[ -z "$bicleaner_trg_lang" ]]; the']
    (1641, '        if [[ -z "$bicleaner_src_lang" ]] || [[ -z "$bicleaner_trg_lang" ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if [[ "$bicleaner_src_lang" == "{SRC_LANG}" ]] && [[ "$bicleaner_trg_lang" == "{TRG_LANG}" ]]; the']
    (1646, '            if [[ "$bicleaner_src_lang" == "{SRC_LANG}" ]] && [[ "$bicleaner_trg_lang" == "{TRG_LANG}" ]]; then')
    (1647, '                # Ok')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['elif [[ "$bicleaner_src_lang" == "{TRG_LANG}" ]] && [[ "$bicleaner_trg_lang" == "{SRC_LANG}" ]]; the']
    (1649, '            elif [[ "$bicleaner_src_lang" == "{TRG_LANG}" ]] && [[ "$bicleaner_trg_lang" == "{SRC_LANG}" ]]; then')
    (1650, '                # Change src and trg')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if [[ "$first" == "0" ]]; the']
    (1738, '            if [[ "$first" == "0" ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if [[ "$d" != "0" ]]; the']
    (1745, '                if [[ "$d" != "0" ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/Snakefile
context_key: ['if [[ ! -s "{output}" ]]; the']
    (1804, '        if [[ ! -s "{output}" ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/rules/dic_generation.smk
context_key: ['if "initCorpusTrainingPrefix" in config']
    (12, 'if "initCorpusTrainingPrefix" in config:')
    (13, '    TRAIN_PREFIXES = config["initCorpusTrainingPrefix"]')
    (14, '')
    (15, '#################################################################')
    (16, '### RULES #######################################################')
    (17, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/rules/dic_generation.smk
context_key: ['if DIC_GENERATE', 'rule dic_generation_symmetrise_dic', 'input']
    (150, 'if DIC_GENERATE:')
    (151, '    # Obtaining the harmonic probability of each pair of words in both directions and filtering out those with less than p=0.2; printing the dictionary')
    (152, '    rule dic_generation_symmetrise_dic:')
    (153, '        input:')
    (154, '            vcb1=f"{mgizaModelDir}/corpus.{SRC_LANG}.filtered.vcb",')
    (155, '            vcb2=f"{mgizaModelDir}/corpus.{TRG_LANG}.filtered.vcb",')
    (156, '            t3_1=f"{mgizaModelDir}/corpus.{SRC_LANG}-{TRG_LANG}.t3.final",')
    (157, '            t3_2=f"{mgizaModelDir}/corpus.{TRG_LANG}-{SRC_LANG}.t3.final",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/rules/dic_generation.smk
context_key: ['if DIC_GENERATE', 'rule dic_generation_symmetrise_dic', 'output']
    (158, '        output:')
    (159, '            expand("{dic}", dic=DIC),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/rules/dic_generation.smk
context_key: ['if DIC_GENERATE', 'rule dic_generation_symmetrise_dic', 'run']
    (160, '        run:')
    (161, '            svocabulary = {}')
    (162, '            tvocabulary = {}')
    (163, '            svcb = open(input.vcb1, "r")')
    (164, '            tvcb = open(input.vcb2, "r")')
    (165, '            for line in svcb:')
    (166, '                item = line.strip().split(" ")')
    (167, '                svocabulary[item[0]] = item[1]')
    (168, '')
    (169, '            for line in tvcb:')
    (170, '                item = line.strip().split(" ")')
    (171, '                tvocabulary[item[0]] = item[1]')
    (172, '')
    (173, '            t3dic = {}')
    (174, '            t3s = open(input.t3_1, "r")')
    (175, '            t3t = open(input.t3_2, "r")')
    (176, '            for line in t3t:')
    (177, '                item = line.strip().split(" ")')
    (178, '                if item[1] in t3dic:')
    (179, '                    t3dic[item[1]][item[0]] = item[2]')
    (180, '                else:')
    (181, '                    t3dic[item[1]] = {}')
    (182, '                    t3dic[item[1]][item[0]] = item[2]')
    (183, '')
    (184, '            dic = open(output[0], "wt")')
    (185, '            dic.write(f"{SRC_LANG}\\\\t{TRG_LANG}\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/rules/dic_generation.smk
context_key: ['if item[0] in t3dic', 'if item[1] in t3dic[item[0]]', 'if hmean > 0.1', 'if item[1] in svocabulary and item[0] in tvocabulary', 'if word1.isalpha() or word2.isalpha()']
    (189, '                if item[0] in t3dic:')
    (190, '                    if item[1] in t3dic[item[0]]:')
    (191, '                        value1 = float(t3dic[item[0]][item[1]])')
    (192, '                        value2 = float(item[2])')
    (193, '                        hmean = 2 / ((1 / value1) + (1 / value2))')
    (194, '')
    (195, '                        if hmean > 0.1:')
    (196, '                            if item[1] in svocabulary and item[0] in tvocabulary:')
    (197, '                                word1 = svocabulary[item[1]]')
    (198, '                                word2 = tvocabulary[item[0]]')
    (199, '                                if word1.isalpha() or word2.isalpha():')
    (200, '                                    dic.write("{0}\\\\t{1}\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor, file=bitextor/rules/dic_generation.smk
context_key: ['if value > 0.1', 'if item[1] in svocabulary and item[0] in tvocabulary']
    (249, '            if value > 0.1:')
    (250, '                if item[1] in svocabulary and item[0] in tvocabulary:')
    (251, '                    dicf2e.write("{0} {1} {2}\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=juniper-lake/trio-assembly-snakemake, file=Snakefile
context_key: ['if "trio_id" in config']
    (12, 'if "trio_id" in config: trio_id = config[\\\'trio_id\\\']')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=juniper-lake/trio-assembly-snakemake, file=Snakefile
context_key: ['if reads_path.endswith(extension)']
    (36, '            if reads_path.endswith(extension):')
    (37, '                hifi_dict[child][extension][os.path.basename(reads_path)[:-len(extension)]] = reads_path')
    (38, '                hifi_prefixes[child].append(os.path.basename(reads_path)[:-len(extension)])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=juniper-lake/trio-assembly-snakemake, file=Snakefile
context_key: ["if \\'parent_reads\\' in config[trio_id] and config[trio_id][\\'parent_reads\\'] == \\'hifi\\'", 'if reads_path.endswith(extension)']
    (42, "        if \\'parent_reads\\' in config[trio_id] and config[trio_id][\\'parent_reads\\'] == \\'hifi\\':")
    (43, "            for extension in [\\'.fastq.gz\\', \\'.bam\\']:")
    (44, '                if reads_path.endswith(extension):')
    (45, '                    hifi_dict[parent_id][extension][os.path.basename(reads_path)[:-len(extension)]] = reads_path')
    (46, '                    hifi_prefixes[parent_id].append(os.path.basename(reads_path)[:-len(extension)])')
    (47, '        else:')
    (48, "            for extension in [\\'.fastq.gz\\', \\'.bam\\', \\'.cram\\']:")
    (49, '                if reads_path.endswith(extension):')
    (50, '                    sr_dict[parent_id][extension][os.path.basename(reads_path)[:-len(extension)]] = reads_path')
    (51, "                    prefix = os.path.basename(reads_path)[:-(len(extension)+3)] if extension == \\'.fastq.gz\\' else os.path.basename(reads_path)[:-len(extension)]")
    (52, '                    sr_prefixes[parent_id].append(prefix)')
    (53, '')
    (54, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=juniper-lake/trio-assembly-snakemake, file=Snakefile
context_key: ["if \\'kmers\\' in config[\\'targets\\']"]
    (74, "if \\'kmers\\' in config[\\'targets\\']:")
    (75, '    targets.extend([f"{output_dir}/{trio_id}/yak/{parent}.yak"')
    (76, "        for parent in [config[trio_id][\\'father\\'][\\'id\\'], config[trio_id][\\'mother\\'][\\'id\\']]])")
    (77, '# assembly and stats')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=juniper-lake/trio-assembly-snakemake, file=Snakefile
context_key: ["if \\'assembly\\' in config[\\'targets\\']"]
    (78, "if \\'assembly\\' in config[\\'targets\\']:")
    (79, '    targets.extend([f"{output_dir}/{trio_id}/hifiasm/{child}.asm.dip.{infix}.{suffix}"')
    (80, "        for suffix in [\\'fasta.gz\\', \\'fasta.stats.txt\\', \\'trioeval.txt\\']")
    (81, "        for infix in [\\'hap1.p_ctg\\', \\'hap2.p_ctg\\']])")
    (82, '# assembly alignments')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=juniper-lake/trio-assembly-snakemake, file=Snakefile
context_key: ["if \\'alignment\\' in config[\\'targets\\']"]
    (83, "if \\'alignment\\' in config[\\'targets\\']:")
    (84, '    targets.extend([f"{output_dir}/{trio_id}/hifiasm/{child}.{hap}.{ref}.{suffix}"')
    (85, "        for hap in [\\'hap1\\', \\'hap2\\']")
    (86, "        for suffix in [\\'bam\\', \\'bam.bai\\']])")
    (87, '')
    (88, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if i<10']
    (69, '                        if i<10:')
    (70, '                            f_out.write(line.split("\\\\t")[0]+"\\\\t")')
    (71, '                            i += 1')
    (72, '                        else:')
    (73, '                            f_out.write(line.split("\\\\t")[0]+"\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if line[']
    (117, '                        if line[:13] == "SN\\\\tsequences:":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if line[']
    (120, '                        if line[:16] == "SN\\\\treads mapped:":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if line[']
    (123, '                        if line[:16] == "SN\\\\tbases mapped:":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if line[']
    (127, '                        if line[:18] == "SN\\\\taverage length:":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if i<10']
    (194, '                        if i<10:')
    (195, '                            f_out.write(line.split("\\\\t")[0]+"\\\\t")')
    (196, '                            i += 1')
    (197, '                        else:')
    (198, '                            f_out.write(line.split("\\\\t")[0]+"\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if line[']
    (242, '                        if line[:13] == "SN\\\\tsequences:":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if line[']
    (245, '                        if line[:16] == "SN\\\\treads mapped:":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if line[']
    (248, '                        if line[:16] == "SN\\\\tbases mapped:":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if line[']
    (252, '                        if line[:18] == "SN\\\\taverage length:":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if line[']
    (321, '        if line[:5] == "study":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if len(indv2) < len(indv)']
    (325, '        if len(indv2) < len(indv):')
    (326, '            indv = indv2')
    (327, '        bam = line.split("\\\\t")[7].split(";")[0]')
    (328, '        SCHUENEMANN2017_INDIVIDUALS_TO_BAM[indv] = bam')
    (329, '        INDIVIDUALS_SCHUENEMANN2017.append(indv)')
    (330, '        POPULATION_SCHUENEMANN2017[indv] = "AncientEgyptian"')
    (331, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if not indv in ["Psor-092_S15","Psor-028_S13","Psor-102_S25"]']
    (371, '        if not indv in ["Psor-092_S15","Psor-028_S13","Psor-102_S25"]:')
    (372, '            INDIVIDUALS_SUDAN.append(indv)')
    (373, '            POPULATION_SUDAN[indv] = pop')
    (374, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if i<10']
    (403, '                        if i<10:')
    (404, '                            f_out.write(line.split("\\\\t")[0]+"\\\\t")')
    (405, '                            i += 1')
    (406, '                        else:')
    (407, '                            f_out.write(line.split("\\\\t")[0]+"\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if line[']
    (463, '                        if line[:13] == "SN\\\\tsequences:":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if line[']
    (466, '                        if line[:16] == "SN\\\\treads mapped:":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if line[']
    (469, '                        if line[:16] == "SN\\\\tbases mapped:":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if line[']
    (473, '                        if line[:18] == "SN\\\\taverage length:":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if line[']
    (584, '        if line[:3] == "url":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ['if alt == major']
    (874, '                if alt == major:')
    (875, '                    major_variants["_".join(s[1:4])] = True')
    (876, '        with open(input[0],"r") as f_in, open(output[0],"w") as f_out:')
    (877, '            for line in f_in: ')
    (878, '                if line[0] == "#":')
    (879, '                    f_out.write(line)')
    (880, '                    continue')
    (881, '                s = line.strip("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2020_mt_analyses, file=Snakefile
context_key: ["if s[4] == \\'*\\'"]
    (885, "                if s[4] == \\'*\\':")
    (886, "                    s[4] = \\'D\\'")
    (887, '                if "_".join([s[1],s[3],s[4]]) in major_variants:')
    (888, '                    f_out.write(line)')
    (889, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SilasK/16S-dada2, file=Snakefile
context_key: ['if not os.path.exists(sample_table_file)']
    (7, 'if not os.path.exists(sample_table_file):')
    (8, '')
    (9, '    logger.critical("Couldn\\\'t find sampletable!"')
    (10, '                    f"I looked for {sample_table_file} relativ to working directory.\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SilasK/16S-dada2, file=Snakefile
context_key: ['if PAIRED_END']
    (28, "if PAIRED_END: FRACTIONS+= [\\'R2\\']")
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JieqiongDai/Single-Cell-CNV-Evolution, file=Snakefile
context_key: ['if (config["group"]=="ready" and config["patient"]=="ready")', 'rule all', 'input']
    (40, 'if (config["group"]=="ready" and config["patient"]=="ready"):')
    (41, '   include: "modules/Snakefile_gen"')
    (42, '   include: "modules/Snakefile_group"')
    (43, '   include: "modules/Snakefile_patient"')
    (44, '   rule all:')
    (45, '       input:')
    (46, '             expand(out + "link/summary/{sample}_web_summary.html",sample=sample),')
    (47, '             expand(out + "reanalysis/link/loup/{sample}_dloupe.dloupe",sample=sample),')
    (48, '             "redundant_removed.txt",')
    (49, '             expand(out + "MEDALT/{sample}/medalt.pdf",sample=sample),')
    (50, '             expand(out + "reanalysis/link/tsne/{sample}_plotly_tsne.html",sample=sample),')
    (51, '             expand(out + "MEDALT_group/{sample}/singlecell.tree.pdf",sample=sample),')
    (52, '             expand(out + "MEDALT_group/{sample}/medalt.group.force.directed.pdf",sample=sample),')
    (53, '             expand(out + "reanalysis/link/tsne/{sample}_plotly_tsne_group.html",sample=sample),')
    (54, '             expand(out + "MEDALT_patient/{patient}/singlecell.tree.pdf",patient=patient),')
    (55, '             expand(out + "MEDALT_patient/{patient}/medalt.patient.force.directed.pdf",patient=patient)')
    (56, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=williamrowell/hifi-deepvariant-snakemake, file=Snakefile
context_key: ['if ubam_match']
    (33, '    if ubam_match:')
    (34, '        # create a dict-of-dict to link samples to movie context to uBAM filenames')
    (35, '        ubam_dict[ubam_match.group("movie")] = str(infile)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=williamrowell/hifi-deepvariant-snakemake, file=Snakefile
context_key: ['if fastq_match']
    (38, '    if fastq_match:')
    (39, '        # create a dict-of-dict to link samples to movie context to FASTQ filenames')
    (40, '        fastq_dict[fastq_match.group("movie")] = str(infile)')
    (41, '# create a list of movies from the uBAMs and FASTQs')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/pipeline-transcriptome-de, file=Snakefile
context_key: ['if not workflow.overwrite_configfiles']
    (5, 'if not workflow.overwrite_configfiles:')
    (6, '    configfile: "config.yml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhklab/eacon_cnv_pipeline, file=Snakefile
context_key: ['if (os.path.exists(os.path.join(f"")))']
    (21, 'if (os.path.exists(os.path.join(f""))):')
    (22, '    pairs_df = pd.read_csv(os.path.join(metadata, pairs_file), sep="\\\\t")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhklab/eacon_cnv_pipeline, file=Snakefile
context_key: ['else', 'if re.match("snp6", config["array_type"])']
    (24, '    if re.match("snp6", config["array_type"]):')
    (25, '        cel_paths = glob.glob(f"{rawdata}/**/*CEL", recursive=True)')
    (26, '        pairs_df = pd.DataFrame({')
    (27, '            "cel_files": cel_paths,')
    (28, '            "SampleName": [re.split(r"\\\\/|\\\\\\\\", path)[1] for path in cel_paths]')
    (29, '        })')
    (30, '        pairs_df.to_csv(os.path.join(metadata, pairs_file), sep="\\\\t")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=racng/snakemake-merge-wgs, file=rules/merge.smk
context_key: ['if len(config["include"]) > 0']
    (8, 'if len(config["include"]) > 0:')
    (9, '    samples = config["include"]')
    (10, '')
    (11, '# Parse regions')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/pyflow-ATACseq, file=Snakefile
context_key: ['if total_reads > target_reads']
    (215, '        if total_reads > target_reads:')
    (216, '            down_rate = target_reads/total_reads')
    (217, '        else:')
    (218, '            down_rate = 1')
    (219, '')
    (220, '        shell("sambamba view -f bam -t 5 --subsampling-seed=3 -s {rate} {inbam} | samtools sort -m 2G -@ 5 -T {outbam}.tmp > {outbam} 2> {log}".format(rate = down_rate, inbam = input[0], outbam = output[0], log = log))')
    (221, '')
    (222, '        shell("samtools index {outbam}".format(outbam = output[0]))')
    (223, '')
    (224, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/gffutils.smk
context_key: ['if is_human_genome()', 'input']
    (17, 'if is_human_genome():')
    (18, '    use rule Create_Gffutils_DB as Create_Gffutils_DB_MANE with:')
    (19, '        input:')
    (20, '            "resources/ensembl/MANE.GRCh38.v1.0.gtf.gz"')
    (21, '        output:')
    (22, '            "resources/ensembl/mane-gffutils.db"')
    (23, '        log:')
    (24, '            "logs/create_gffutils_db_mane.log"\'')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/cds_regions.smk
context_key: ['if is_human_genome()', 'input']
    (16, 'if is_human_genome():')
    (17, '    use rule CDS_Regions as CDS_Regions_MANE with:')
    (18, '        input:')
    (19, '            db="resources/ensembl/mane-gffutils.db",')
    (20, '        output:')
    (21, '            tsv="resources/ensembl/mane-cds_regions.tsv",')
    (22, '        log:')
    (23, '            "logs/create_gffutils_db_mane.log"\'')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Fetch_VCF_Annotation', 'output']
    (15, 'if is_human_genome():        ')
    (16, '    rule Fetch_VCF_Annotation:')
    (17, '        """Downloads an Ensembl VCF annotation file')
    (18, '        """')
    (19, '        output:')
    (20, '            "resources/ensembl/{vcf_type}_variants.vcf.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Fetch_VCF_Annotation', 'conda']
    (21, '        conda:')
    (22, '            "../envs/curl.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Fetch_VCF_Annotation', 'log']
    (23, '        log:')
    (24, '            "logs/ensembl/download-vcf-{vcf_type}.log",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Fetch_VCF_Annotation', 'params']
    (25, '        params:')
    (26, '            # --insecure used because https fails cert validation')
    (27, '            # and http times out for larger files')
    (28, '            curl="--insecure --retry 3 --retry-connrefused --show-error --silent",')
    (29, '            base_url=get_ensembl_url("variation/vcf"),')
    (30, '            vcf=lambda wc: f"{config[\\\'ref\\\'][\\\'species\\\']}_{wc.vcf_type}.vcf.gz",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Fetch_VCF_Annotation', 'wildcard_constraints']
    (31, '        wildcard_constraints:')
    (32, '            vcf_type="|".join(')
    (33, '                [')
    (34, '                    "clinically_associated",')
    (35, '                    "phenotype_associated",')
    (36, '                ]')
    (37, '            ),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Fetch_VCF_Annotation', 'cache']
    (38, '        cache: True')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Fetch_VCF_Annotation', 'retries']
    (39, '        retries: 3')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Fetch_VCF_Annotation', 'shell']
    (40, '        shell:')
    (41, '            "curl -o {output[0]} {params.curl} {params.base_url}/{params.vcf} &> {log}"')
    (42, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Fetch_ClinVar_Variants', 'output']
    (43, '    rule Fetch_ClinVar_Variants:')
    (44, '        output:')
    (45, '            "resources/clinvar/clinvar.vcf.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Fetch_ClinVar_Variants', 'params']
    (46, '        params:')
    (47, '            curl="--insecure --retry 3 --retry-connrefused --show-error --silent",')
    (48, '            vcf=f"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_{HUMAN_ACC}/clinvar.vcf.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Fetch_ClinVar_Variants', 'conda']
    (49, '        conda: "../envs/curl.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Fetch_ClinVar_Variants', 'log']
    (50, '        log: "logs/ensembl/download-clinvar-vcf.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Fetch_ClinVar_Variants', 'cache']
    (51, '        cache: True')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Fetch_ClinVar_Variants', 'retries']
    (52, '        retries: 3')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Fetch_ClinVar_Variants', 'shell']
    (53, '        shell:')
    (54, '            "curl -o {output[0]} {params.curl} {params.vcf} &> {log}"')
    (55, '        ')
    (56, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Filter_Variants', 'input']
    (57, '    rule Filter_Variants:')
    (58, '        """Remove unusual variant types that cause errors in bedtools.')
    (59, '        """')
    (60, '        input:')
    (61, '            "resources/{vcf_file}.vcf.gz",')
    (62, '            "resources/{vcf_file}.vcf.gz.csi",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Filter_Variants', 'output']
    (63, '        output:')
    (64, '            "resources/{vcf_file}.filtered.vcf.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Filter_Variants', 'conda']
    (65, '        conda:')
    (66, '            "../envs/bcftools.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Filter_Variants', 'cache']
    (67, '        cache: True')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Filter_Variants', 'threads']
    (68, '        threads: 2')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-ref_genomes, file=workflow/rules/download_variants.smk
context_key: ['if is_human_genome()', 'rule Filter_Variants', 'shell']
    (69, '        shell:')
    (70, '            "bcftools view -O z9 -o {output} --threads {threads} -V other {input}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/filtering, file=workflow/rules/common.smk
context_key: ['if not workflow.overwrite_configfiles']
    (17, 'if not workflow.overwrite_configfiles:')
    (18, '    sys.exit("At least one config file must be passed using --configfile/--configfiles, by command line or a profile!")')
    (19, '')
    (20, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/DTR-phage-pipeline, file=Snakefile
context_key: ['else DTR_READS_FASTA if pre_filter == "DTR" \\']
    (60, '                                 else DTR_READS_FASTA if pre_filter == "DTR" \\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/DTR-phage-pipeline, file=Snakefile
context_key: ['else READS_IMPORT_FAST']
    (61, '                                 else READS_IMPORT_FASTA')
    (62, '')
    (63, '')
    (64, '###################################')
    (65, '# KAIJU CLASSIFICATION            #')
    (66, '###################################')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/DTR-phage-pipeline, file=Snakefile
context_key: ['if os.path.exists(KAIJU_DB_DIR)', 'rule all_kaiju', 'input']
    (287, 'if os.path.exists(KAIJU_DB_DIR):')
    (288, '    rule all_kaiju:')
    (289, '        input:')
    (290, '            KAIJU_RESULTS_KRONA_HTML,')
    (291, '            expand(str(KMER_FREQS_UMAP_TAX), database=DATABASE_NAME, rank=TAX_RANK),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/WUR_mapping-variant-calling, file=Snakefile
context_key: ['if "OUTDIR" in config']
    (16, 'if "OUTDIR" in config:')
    (17, '    # print("\\')
    (18, 'Saving to " + config["OUTDIR"] + "\\')
    (19, '")')
    (20, '    workdir: config["OUTDIR"]')
    (21, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/wf-bulk-indexing, file=Snakefile
context_key: ['if [ -f /bin/micromamba ]; the']
    (19, '                 if [ -f /bin/micromamba ]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/wf-bulk-indexing, file=Snakefile
context_key: ['if [ -f $done_accessions ]; the']
    (29, '                         if [ -f $done_accessions ]; then')
    (30, "                             #find lines only in input_accessions (that haven\\'t been done)")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/wf-bulk-indexing, file=Snakefile
context_key: ['if ( [[ -s $failed_accessions_output ]] && ((status != 0)) ) || (( status == 0 )); the']
    (377, '            if ( [[ -s $failed_accessions_output ]] && ((status != 0)) ) || (( status == 0 )); then')
    (378, '                # accessions done now, input accessions that are not in failed -- append to all done accessions')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/wf-bulk-indexing, file=Snakefile
context_key: ['if ( [[ -s $failed_accessions_output ]] && ((status != 0)) ) || (( status == 0 )); the']
    (465, '            if ( [[ -s $failed_accessions_output ]] && ((status != 0)) ) || (( status == 0 )); then')
    (466, '                # accessions done now, input accessions that are not in failed -- append to all done accessions')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/wf-bulk-indexing, file=Snakefile
context_key: ['if ( [[ -s $failed_accessions_output ]] && ((status != 0)) ) || (( status == 0 )); the']
    (744, '            if ( [[ -s $failed_accessions_output ]] && ((status != 0)) ) || (( status == 0 )); then')
    (745, '                # accessions done now, input accessions that are not in failed -- append to all done accessions')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/wf-bulk-indexing, file=Snakefile
context_key: ['if ( [[ -s $failed_accessions_output ]] && ((status != 0)) ) || (( status == 0 )); the']
    (814, '            if ( [[ -s $failed_accessions_output ]] && ((status != 0)) ) || (( status == 0 )); then')
    (815, '                # accessions done now, input accessions that are not in failed -- append to all done accessions')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexmsalmeida/magscreen, file=Snakefile
context_key: ['if (compl - 5*cont) > 50']
    (21, '        if (compl - 5*cont) > 50:')
    (22, '            genomes_qc.add(cols[0].split(".fa")[0])')
    (23, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexmsalmeida/magscreen, file=Snakefile
context_key: ['if genome_name in genomes_qc']
    (27, '    if genome_name in genomes_qc:')
    (28, '        genomes_files.add(genome_name)')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexmsalmeida/magscreen, file=Snakefile
context_key: ['if not os.path.exists(OUTPUT_DIR+"/logs")']
    (30, 'if not os.path.exists(OUTPUT_DIR+"/logs"):')
    (31, '    os.makedirs(OUTPUT_DIR+"/logs")')
    (32, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pmenzel/score-assemblies, file=Snakefile
context_key: ['if len(assemblies) == 0', 'if len(references) > 0', 'if len(references_protein) > 0', 'if run_bakta == 1', 'rule all', 'rule assess_assembly', 'rule assess_homopolymers_minimap', 'rule assess_homopolymers', 'rule gather_stats_pomoxis', 'rule busco', 'rule busco2tsv', 'rule gather_stats_busco', 'rule quast', 'rule dnadiff', 'rule gather_stats_dnadiff', 'rule nucdiff', 'rule gather_stats_nucdiff', 'rule download_uniprot', 'rule diamond_makedb', 'rule prodigal', 'rule prodigal_stats']
    (22, 'if len(assemblies) == 0:')
    (23, '\\tprint("Found no *.fa files in folder assemblies/")')
    (24, '\\tquit()')
    (25, '')
    (26, 'references, = glob_wildcards("references/{ref,[^/\\\\\\\\\\\\\\\\]+}.fa")')
    (27, 'references_protein, = glob_wildcards("references-protein/{ref,[^/\\\\\\\\\\\\\\\\]+}.faa")')
    (28, '')
    (29, 'list_assess_assembly_summ = []')
    (30, 'list_assess_assembly_meanQ_tsv = []')
    (31, 'list_assess_assembly_meanQ_pdf = []')
    (32, 'list_assess_homopolymers_rel_len = []')
    (33, 'list_assess_homopolymers_correct_len_tsv = []')
    (34, 'list_assess_homopolymers_correct_len_pdf = []')
    (35, 'list_pomoxis = []')
    (36, 'list_quast_report = []')
    (37, 'list_dnadiff_report = []')
    (38, 'list_dnadiff = []')
    (39, 'list_dnadiff_tsv = []')
    (40, 'list_dnadiff_pdf = []')
    (41, 'list_nucdiff_stat = []')
    (42, 'list_nucdiff_tsv = []')
    (43, 'list_nucdiff = []')
    (44, 'list_nucdiff_pdf = []')
    (45, 'list_ideel_ref_tsv = []')
    (46, 'list_ideel_ref_pdf = []')
    (47, 'if len(references) > 0:')
    (48, '\\tlist_assess_assembly_summ = expand(out_dir + "/pomoxis/{id}/assess_assembly/{id}_{ref}_summ.txt", id=assemblies, ref=references)')
    (49, '\\tlist_assess_assembly_meanQ_tsv = expand(out_dir + "/pomoxis/{id}/assess_assembly/{id}_{ref}_scores.tsv", id=assemblies, ref=references)')
    (50, '\\tlist_assess_assembly_meanQ_pdf = expand(out_dir + "/pomoxis/{ref}_assess_assembly_all_meanQ.pdf", ref=references)')
    (51, '\\tlist_assess_homopolymers_rel_len = expand(out_dir + "/pomoxis/{id}/assess_homopolymers/{id}_{ref}_rel_len.tsv", id=assemblies, ref=references)')
    (52, '\\tlist_assess_homopolymers_correct_len_tsv = expand(out_dir + "/pomoxis/{id}/assess_homopolymers/{id}_{ref}_correct_len.tsv", id=assemblies, ref=references)')
    (53, '\\tlist_pomoxis = [out_dir + "/pomoxis/assess_assembly_all_scores.tsv", out_dir + "/pomoxis/assess_homopolymers_all_rel_len.tsv", out_dir + "/pomoxis/assess_homopolymers_all_correct_len.tsv" ]')
    (54, '\\tlist_quast_report = expand(out_dir + "/quast/{ref}/report.html", ref=references)')
    (55, '\\tlist_dnadiff_report = expand(out_dir + "/dnadiff/{ref}/{id}-dnadiff.report", id=assemblies, ref=references)')
    (56, '\\tlist_dnadiff = [ out_dir + "/dnadiff/all_stats.tsv" ]')
    (57, '\\tlist_dnadiff_tsv = expand(out_dir + "/dnadiff/{ref}/{id}-dnadiff-stats.tsv", id=assemblies, ref=references)')
    (58, '\\tlist_dnadiff_pdf = expand(out_dir + "/dnadiff/{ref}_dnadiff_stats.pdf", ref=references)')
    (59, '\\tlist_nucdiff_stat = expand(out_dir + "/nucdiff/{ref}/{id}-nucdiff/results/nucdiff_stat.out", id=assemblies, ref=references)')
    (60, '\\tlist_nucdiff_tsv = expand(out_dir + "/nucdiff/{ref}/{id}-nucdiff/nucdiff.tsv", id=assemblies, ref=references)')
    (61, '\\tlist_nucdiff = [ out_dir + "/nucdiff/all_stats.tsv" ]')
    (62, '\\tlist_nucdiff_pdf = expand(out_dir + "/nucdiff/{ref}_nucdiff_stats.pdf", ref=references)')
    (63, '')
    (64, 'if len(references_protein) > 0:')
    (65, '\\tlist_ideel_ref_tsv = expand(out_dir + "/ideel/diamond-ref/{ref}/{id}_{ref}.tsv", id=assemblies, ref=references_protein)')
    (66, '\\tlist_ideel_ref_pdf = expand(out_dir + "/ideel/{ref}_ideel_{type}.pdf", ref=references_protein, type=[\\\'histograms\\\', \\\'boxplots\\\'])')
    (67, '')
    (68, 'list_busco_out = expand(out_dir + "/busco/{id}/short_summary.specific.{blin}_odb10.{id}.txt", id=assemblies, blin=busco_lineage)')
    (69, 'list_busco_tsv = expand(out_dir + "/busco/{id}/short_summary.specific.{blin}_odb10.{id}.tsv", id=assemblies, blin=busco_lineage)')
    (70, '')
    (71, 'list_prodigal_proteins = expand(out_dir + "/ideel/prodigal/{id}.faa", id=assemblies)')
    (72, 'list_ideel_uniprot_tsv = expand(out_dir + "/ideel/diamond/{id}.tsv", id=assemblies)')
    (73, '')
    (74, 'list_bakta_out = []')
    (75, 'if run_bakta == 1:')
    (76, '\\tlist_bakta_out = expand(out_dir + "/bakta/{id}/{id}.txt", id=assemblies)')
    (77, '')
    (78, 'rule all:')
    (79, '\\tinput:')
    (80, '\\t\\tlist_assess_assembly_summ,')
    (81, '\\t\\tlist_assess_assembly_meanQ_tsv,')
    (82, '\\t\\tlist_assess_assembly_meanQ_pdf,')
    (83, '\\t\\tlist_assess_homopolymers_rel_len,')
    (84, '\\t\\tlist_assess_homopolymers_correct_len_tsv,')
    (85, '\\t\\tlist_pomoxis,')
    (86, '\\t\\tlist_busco_out,')
    (87, '\\t\\tlist_busco_tsv,')
    (88, '\\t\\tout_dir + "/busco/all_stats.tsv",')
    (89, '\\t\\tout_dir + "/busco/busco_stats.pdf",')
    (90, '\\t\\tlist_bakta_out,')
    (91, '\\t\\tlist_quast_report,')
    (92, '\\t\\tlist_dnadiff_report,')
    (93, '\\t\\tlist_dnadiff,')
    (94, '\\t\\tlist_dnadiff_tsv,')
    (95, '\\t\\tlist_dnadiff_pdf,')
    (96, '\\t\\tlist_nucdiff_stat,')
    (97, '\\t\\tlist_nucdiff_tsv,')
    (98, '\\t\\tlist_nucdiff,')
    (99, '\\t\\tlist_nucdiff_pdf,')
    (100, '\\t\\tout_dir + "/ideel/uniprot/uniprot_sprot.fasta.gz",')
    (101, '\\t\\tout_dir + "/ideel/uniprot/uniprot_sprot.dmnd",')
    (102, '\\t\\tlist_prodigal_proteins,')
    (103, '\\t\\tout_dir + "/ideel/prodigal_stats.tsv",')
    (104, '\\t\\tlist_ideel_uniprot_tsv,')
    (105, '\\t\\tout_dir + "/ideel/ideel_uniprot_histograms.pdf",')
    (106, '\\t\\tout_dir + "/ideel/ideel_uniprot_boxplots.pdf",')
    (107, '\\t\\tlist_ideel_ref_tsv,')
    (108, '\\t\\tlist_ideel_ref_pdf,')
    (109, '\\t\\t"score-assemblies-report.html"')
    (110, '')
    (111, '# -------------------------------------------------------------------------------------------------------------------------------------------')
    (112, '# pomoxis')
    (113, '# -------------------------------------------------------------------------------------------------------------------------------------------')
    (114, '')
    (115, 'rule assess_assembly:')
    (116, '\\tinput:')
    (117, '\\t\\tassembly = "assemblies/{id}.fa",')
    (118, '\\t\\tref = "references/{ref}.fa"')
    (119, '\\toutput:')
    (120, '\\t\\tsumm = out_dir + "/pomoxis/{id}/assess_assembly/{id}_{ref}_summ.txt",')
    (121, '\\t\\ttsv = out_dir + "/pomoxis/{id}/assess_assembly/{id}_{ref}_scores.tsv"')
    (122, '\\tlog: log_dir + "/pomoxis/{id}/assess_assembly/{id}_{ref}_log.txt"')
    (123, '\\tshell:')
    (124, '\\t\\t"""')
    (125, '\\t\\tassess_assembly -r {input.ref} -i {input.assembly} -p {out_dir}/pomoxis/{wildcards.id}/assess_assembly/{wildcards.id}_{wildcards.ref} >{log} 2>&1')
    (126, "\\t\\tmeanQ=$(grep -A2 \\'#  Q Scores\\' {output.summ} | tail -n1 | awk \\'{{print $2}}\\')")
    (127, "\\t\\tpercErr=$(grep -A2 \\'#  Percentage Errors\\' {output.summ} | tail -n1 | awk \\'{{print $2}}\\')")
    (128, '\\t\\techo "{wildcards.id}\\\\t{wildcards.ref}\\\\t$meanQ\\\\t$percErr" > {output.tsv}')
    (129, '\\t\\t"""')
    (130, '')
    (131, 'rule assess_homopolymers_minimap:')
    (132, '\\tthreads: 5')
    (133, '\\tinput:')
    (134, '\\t\\tassembly = "assemblies/{id}.fa",')
    (135, '\\t\\tref = "references/{ref}.fa"')
    (136, '\\toutput: out_dir + "/pomoxis/{id}/assess_homopolymers/{id}_{ref}.bam"')
    (137, '\\tlog: log_dir + "/pomoxis/{id}/assess_homopolymers/{id}_{ref}.minimap2.log"')
    (138, '\\tshell:')
    (139, '\\t\\t"""')
    (140, '\\t\\tminimap2 -x asm5 -t {threads} --MD -a {input.ref} {input.assembly} 2>{log} | samtools sort -o {output} - >>{log} 2>&1')
    (141, '\\t\\t"""')
    (142, '')
    (143, 'rule assess_homopolymers:')
    (144, '\\tinput: out_dir + "/pomoxis/{id}/assess_homopolymers/{id}_{ref}.bam"')
    (145, '\\toutput:')
    (146, '\\t\\thp_count = out_dir + "/pomoxis/{id}/assess_homopolymers/{id}_{ref}_count/hp_counts.pkl",')
    (147, '\\t\\trel_len = out_dir + "/pomoxis/{id}/assess_homopolymers/{id}_{ref}_analyse/hp_rel_len_counts.txt",')
    (148, '\\t\\trel_len2 = out_dir + "/pomoxis/{id}/assess_homopolymers/{id}_{ref}_rel_len.tsv",')
    (149, '\\t\\tcorrect_len = out_dir + "/pomoxis/{id}/assess_homopolymers/{id}_{ref}_analyse/hp_correct_vs_len.txt",')
    (150, '\\t\\tcorrect_len2 = out_dir + "/pomoxis/{id}/assess_homopolymers/{id}_{ref}_correct_len.tsv"')
    (151, '\\tparams:')
    (152, '\\t\\tcount_dir = directory(out_dir + "/pomoxis/{id}/assess_homopolymers/{id}_{ref}_count"),')
    (153, '\\t\\tanalyse_dir = directory(out_dir + "/pomoxis/{id}/assess_homopolymers/{id}_{ref}_analyse")')
    (154, '\\tlog: log_dir + "/pomoxis/{id}/assess_homopolymers/{id}_{ref}_log.txt"')
    (155, '\\tshell:')
    (156, '\\t\\t"""')
    (157, '\\t\\trm -rf {params.count_dir} {params.analyse_dir}')
    (158, '\\t\\tassess_homopolymers count -o {params.count_dir} {input} >{log} 2>&1')
    (159, '\\t\\tassess_homopolymers analyse -o {params.analyse_dir} {output.hp_count} >>{log} 2>&1')
    (160, '')
    (161, "\\t\\tgrep -v count {output.rel_len} | sed \\'s/^/{wildcards.id}\\\\t{wildcards.ref}\\\\t/\\' > {output.rel_len2}")
    (162, "\\t\\tgrep -v rlen {output.correct_len} | sed \\'s/^/{wildcards.id}\\\\t{wildcards.ref}\\\\t/\\' > {output.correct_len2}")
    (163, '\\t\\t"""')
    (164, '')
    (165, 'rule gather_stats_pomoxis:')
    (166, '\\tinput:')
    (167, '\\t\\taa_meanQ = list_assess_assembly_meanQ_tsv,')
    (168, '\\t\\thp_rel_len = list_assess_homopolymers_rel_len,')
    (169, '\\t\\thp_correct_len = list_assess_homopolymers_correct_len_tsv')
    (170, '\\toutput:')
    (171, '\\t\\tall_aa_meanQ = out_dir + "/pomoxis/assess_assembly_all_scores.tsv",')
    (172, '\\t\\tall_hp_rel_len = out_dir + "/pomoxis/assess_homopolymers_all_rel_len.tsv",')
    (173, '\\t\\tall_hp_correct_len = out_dir + "/pomoxis/assess_homopolymers_all_correct_len.tsv"')
    (174, '\\tshell:')
    (175, '\\t\\t"""')
    (176, '\\t\\t#filter inf values, which happen when comparing identical assemblies')
    (177, "\\t\\tcat {input.aa_meanQ} | grep -v -w \\'inf\\' > {output.all_aa_meanQ}")
    (178, '')
    (179, '\\t\\tcat {input.hp_rel_len}  > {output.all_hp_rel_len}')
    (180, '\\t\\tcat {input.hp_correct_len}  > {output.all_hp_correct_len}')
    (181, '\\t\\t"""')
    (182, '')
    (183, '# -------------------------------------------------------------------------------------------------------------------------------------------')
    (184, '# BUSCO')
    (185, '# -------------------------------------------------------------------------------------------------------------------------------------------')
    (186, '')
    (187, 'rule busco:')
    (188, '\\tthreads: 5')
    (189, '\\tresources: wget_busco=1')
    (190, '\\tinput:')
    (191, '\\t\\tassembly = "assemblies/{id}.fa"')
    (192, '\\toutput:')
    (193, '\\t\\tout_dir + "/busco/{id}/short_summary.specific.{busco_lineage}_odb10.{id}.txt",')
    (194, '\\tlog: log_dir + "/busco/{id}/busco_{busco_lineage}.log"')
    (195, '\\tshell:')
    (196, '\\t\\t"""')
    (197, '\\t\\tcd {out_dir}/busco && busco -q -c {threads} -f -m genome -l {busco_lineage} -o {wildcards.id} -i ../../{input} >../../{log} 2>&1')
    (198, '\\t\\t"""')
    (199, '')
    (200, 'rule busco2tsv:')
    (201, '\\tthreads: 1')
    (202, '\\tinput:')
    (203, '\\t\\tout_dir + "/busco/{id}/short_summary.specific.{busco_lineage}_odb10.{id}.txt",')
    (204, '\\toutput:')
    (205, '\\t\\tout_dir + "/busco/{id}/short_summary.specific.{busco_lineage}_odb10.{id}.tsv"')
    (206, '\\tshell:')
    (207, '\\t\\t"""')
    (208, '\\t\\tperl -lne \\\'print "{wildcards.id}\\\\t$1\\\\t$2\\\\t$3\\\\t$4" if /C:([\\\\-\\\\d\\\\.]+).*F:([\\\\-\\\\d\\\\.]+).*M:([\\\\-\\\\d\\\\.]+).*n:(\\\\d+)/\\\' {input} > {output}')
    (209, '\\t\\t"""')
    (210, '')
    (211, 'rule gather_stats_busco:')
    (212, '\\tinput:')
    (213, '\\t\\tlist_busco_tsv')
    (214, '\\toutput:')
    (215, '\\t\\tout_dir + "/busco/all_stats.tsv"')
    (216, '\\tshell:')
    (217, '\\t\\t"""')
    (218, '\\t\\tcat {input} > {output}')
    (219, '\\t\\t"""')
    (220, '')
    (221, '# -------------------------------------------------------------------------------------------------------------------------------------------')
    (222, '# QUAST')
    (223, '# -------------------------------------------------------------------------------------------------------------------------------------------')
    (224, '')
    (225, 'rule quast:')
    (226, '\\tthreads: 5')
    (227, '\\tinput:')
    (228, '\\t\\treference = "references/{ref}.fa",')
    (229, '\\t\\tfa = assemblies_fa')
    (230, '\\toutput:')
    (231, '\\t\\treport = out_dir + "/quast/{ref}/report.html"')
    (232, '\\tlog: log_dir + "/quast/{ref}/quast.log"')
    (233, '\\tshell:')
    (234, '\\t\\t"""')
    (235, '\\t\\tquast -t {threads} --glimmer -o {out_dir}/quast/{wildcards.ref} -r {input.reference} {input.fa} >{log} 2>&1')
    (236, '\\t\\t"""')
    (237, '')
    (238, '# -------------------------------------------------------------------------------------------------------------------------------------------')
    (239, '# dnadiff')
    (240, '# -------------------------------------------------------------------------------------------------------------------------------------------')
    (241, '')
    (242, 'rule dnadiff:')
    (243, '\\tthreads: 1')
    (244, '\\tinput:')
    (245, '\\t\\treference = "references/{ref}.fa",')
    (246, '\\t\\tassembly = "assemblies/{id}.fa"')
    (247, '\\toutput:')
    (248, '\\t\\tdnadiff_report = out_dir + "/dnadiff/{ref}/{id}-dnadiff.report",')
    (249, '\\t\\tstats_tsv = out_dir + "/dnadiff/{ref}/{id}-dnadiff-stats.tsv"')
    (250, '\\tlog: log_dir + "/dnadiff/{ref}/{id}-dnadiff.log"')
    (251, '\\tshell:')
    (252, '\\t\\t"""')
    (253, '\\t\\tdnadiff -p {out_dir}/dnadiff/{wildcards.ref}/{wildcards.id}-dnadiff {input.reference} {input.assembly} >{log} 2>&1')
    (254, "\\t\\tcat {output.dnadiff_report} | grep -A3 \\'1-to-1\\' | grep \\'AvgIdentity\\' | sed -e \\'s/^/{wildcards.id}\\\\t{wildcards.ref}\\\\t/\\' | perl -lpne \\'s/\\\\s+/\\\\t/g\\' > {output.stats_tsv}")
    (255, "\\t\\tgrep TotalIndels {output.dnadiff_report} | sed -e \\'s/^/{wildcards.id}\\\\t{wildcards.ref}\\\\t/\\' | perl -lpne \\'s/\\\\s+/\\\\t/g\\' >> {output.stats_tsv}")
    (256, '\\t\\t"""')
    (257, '')
    (258, 'rule gather_stats_dnadiff:')
    (259, '\\tinput:')
    (260, '\\t\\tall_reports = list_dnadiff_tsv')
    (261, '\\toutput:')
    (262, '\\t\\tall_dnadiff_report_tsv = out_dir + "/dnadiff/all_stats.tsv"')
    (263, '\\tshell:')
    (264, '\\t\\t"""')
    (265, '\\t\\tcat {input} > {output}')
    (266, '\\t\\t"""')
    (267, '')
    (268, '# -------------------------------------------------------------------------------------------------------------------------------------------')
    (269, '# nucdiff')
    (270, '# -------------------------------------------------------------------------------------------------------------------------------------------')
    (271, '')
    (272, 'rule nucdiff:')
    (273, '\\tthreads: 1')
    (274, '\\tinput:')
    (275, '\\t\\treference = "references/{ref}.fa",')
    (276, '\\t\\tassembly = "assemblies/{id}.fa"')
    (277, '\\toutput:')
    (278, '\\t\\tnucdiff_stat = out_dir + "/nucdiff/{ref}/{id}-nucdiff/results/nucdiff_stat.out",')
    (279, '\\t\\tnucdiff_tsv = out_dir + "/nucdiff/{ref}/{id}-nucdiff/nucdiff.tsv"')
    (280, '\\tparams:')
    (281, '\\t\\tnucdiff_dir = directory(out_dir + "/nucdiff/{ref}/{id}-nucdiff/")')
    (282, '\\tlog: log_dir + "/nucdiff/{ref}/{id}-nucdiff.log"')
    (283, '\\tshell:')
    (284, '\\t\\t"""')
    (285, '\\t\\tnucdiff {input.reference} {input.assembly} {params.nucdiff_dir} nucdiff >{log} 2>&1')
    (286, "\\t\\tcat {output.nucdiff_stat} | grep \\'Insertions\\\\|Deletions\\\\|Substitutions\\' | sed -e \\'s/^/{wildcards.id}\\\\t{wildcards.ref}\\\\t/\\' > {output.nucdiff_tsv}")
    (287, '\\t\\t"""')
    (288, '')
    (289, 'rule gather_stats_nucdiff:')
    (290, '\\tinput:')
    (291, '\\t\\tall_reports = list_nucdiff_tsv')
    (292, '\\toutput:')
    (293, '\\t\\tall_nucdiff_tsv = out_dir + "/nucdiff/all_stats.tsv"')
    (294, '\\tshell:')
    (295, '\\t\\t"""')
    (296, '\\t\\tcat {input} > {output}')
    (297, '\\t\\t"""')
    (298, '')
    (299, '')
    (300, '# -------------------------------------------------------------------------------------------------------------------------------------------')
    (301, '# ideel with uniprot')
    (302, '# -------------------------------------------------------------------------------------------------------------------------------------------')
    (303, '')
    (304, 'rule download_uniprot:')
    (305, '\\tpriority: 10')
    (306, '\\toutput:')
    (307, '\\t\\tout_dir + "/ideel/uniprot/uniprot_sprot.fasta.gz"')
    (308, '\\tlog: log_dir + "/ideel/uniprot/download.log"')
    (309, '\\tshell:')
    (310, '\\t\\t"""')
    (311, '\\t\\twget -P {out_dir}/ideel/uniprot ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz >{log} 2>&1')
    (312, '\\t\\t"""')
    (313, '')
    (314, 'rule diamond_makedb:')
    (315, '\\tinput:')
    (316, '\\t\\tout_dir + "/ideel/uniprot/uniprot_sprot.fasta.gz"')
    (317, '\\toutput:')
    (318, '\\t\\tout_dir + "/ideel/uniprot/uniprot_sprot.dmnd"')
    (319, '\\tlog: log_dir + "/diamond-makedb-uniprot.log"')
    (320, '\\tshell:')
    (321, '\\t\\t"""')
    (322, '\\t\\tdiamond makedb --db {out_dir}/ideel/uniprot/uniprot_sprot --in {input} >{log} 2>&1')
    (323, '\\t\\t"""')
    (324, '')
    (325, 'rule prodigal:')
    (326, '\\tinput: "assemblies/{id}.fa"')
    (327, '\\toutput: out_dir + "/ideel/prodigal/{id}.faa"')
    (328, '\\tlog: log_dir + "/ideel/prodigal/{id}.log"')
    (329, '\\tshell:')
    (330, '\\t\\t"""')
    (331, '\\t\\tprodigal -a {output} -i {input} >{log} 2>&1')
    (332, '\\t\\t# remove stop codon (*) from end of sequences')
    (333, "\\t\\tsed -i \\'s/*$//\\' {output}")
    (334, '\\t\\t"""')
    (335, '')
    (336, 'rule prodigal_stats:')
    (337, '  threads: 1')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=maarten-k/realignment, file=workflow/rules/original.smk
context_key: ['if [ -f ${{wrk}}/chr1/{wildcards.SM}.chr1.g.vcf.gz']
    (128, '            if [ -f ${{wrk}}/chr1/{wildcards.SM}.chr1.g.vcf.gz ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bihealth/varfish-data-igsr, file=workflow/Snakefile
context_key: ['if sample[COL_GENDER] == "SEX_MALE"']
    (134, '    if sample[COL_GENDER] == "SEX_MALE":')
    (135, '        result += FILES_CHRY')
    (136, '    return [f"igsr-data/{x}" for x in result]')
    (137, '')
    (138, '')
    (139, '# Split out chromosomes')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MGXlab/phap, file=workflow/rules/common.smk
context_key: ['if "tools" in config', 'if config["tools"] == "all"']
    (46, 'if "tools" in config:')
    (47, '    if config["tools"] == "all":')
    (48, '        TOOLS = [')
    (49, '            "vhulk",')
    (50, '            "rafah",')
    (51, '            "vhmnet",')
    (52, '            "wish",')
    (53, '            "htp",')
    (54, '            "crispropendb",')
    (55, '            "phist"')
    (56, '            ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MGXlab/phap, file=workflow/rules/common.smk
context_key: ['if "tools" in config', 'else']
    (57, '    else:')
    (58, '        TOOLS = [t for t in config["tools"]]')
    (59, '')
    (60, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tniranjan1/RRBS_PIPELINE, file=workflow/Snakefile
context_key: ['if not os.path.isfile(config_path)']
    (20, 'if not os.path.isfile(config_path):')
    (21, "    print(\\'Configuration file ({}) not found. Workflow aborted.\\'.format(config_path))")
    (22, '    sys.exit()')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dilworthlab/CnT_pipeline_snakemake, file=Snakefile
context_key: ['if os.path.join(home, ".multiqc_config.yaml") in files', "if \\'log_filesize_limit\\' in str(values)"]
    (29, 'if os.path.join(home, ".multiqc_config.yaml") in files:')
    (30, "    print(\\'~/.multiqc_config.yaml is present\\')")
    (31, '    with open(os.path.join(home, ".multiqc_config.yaml"), \\\'r\\\') as file:')
    (32, '        values = yaml.safe_load(file)')
    (33, '')
    (34, "        if \\'log_filesize_limit\\' in str(values):")
    (35, "            print(\\'log_filesize_limit is already set\\')")
    (36, '        if values is None:')
    (37, '            with open(os.path.join(home, ".multiqc_config.yaml"), \\\'w\\\') as file:')
    (38, '                docs = yaml.dump(datadict, file)')
    (39, "                print(\\'added log_filesize_limit to multiqc log file\\')")
    (40, '        else:')
    (41, '            with open(os.path.join(home, ".multiqc_config.yaml"), \\\'r\\\') as file:')
    (42, '                new_yaml = yaml.safe_load(file)')
    (43, '                print(type(new_yaml))')
    (44, '                new_yaml.update(datadict)')
    (45, '')
    (46, '')
    (47, '            with open(os.path.join(home, ".multiqc_config.yaml"),\\\'w\\\') as file:')
    (48, '                yaml.safe_dump(new_yaml, file)')
    (49, "                print(\\'log_filesize_limit: 2000000000 is set\\')")
    (50, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dilworthlab/CnT_pipeline_snakemake, file=Snakefile
context_key: ['if file_.endswith(".fastq.gz")']
    (63, '        if file_.endswith(".fastq.gz"):')
    (64, '            READS_DIR = filepath')
    (65, '            ROOT_DIR = os.path.relpath(filepath, wdir)')
    (66, '')
    (67, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/ncov-phylogeography, file=workflow/Snakefile
context_key: ['if not config']
    (25, 'if not config:')
    (26, '    #print("ERROR: Please specify --configfile")')
    (27, '    #quit(1)')
    (28, '    print("WARNING: Using default configfile results/config/snakemake.yaml")')
    (29, '    configfile: os.path.join("results", "config", "snakemake.yaml")')
    (30, '')
    (31, '# Pipeline directories')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/ncov-phylogeography, file=workflow/Snakefile
context_key: ['if workflow.conda_prefix']
    (58, 'if workflow.conda_prefix:')
    (59, '    os.environ["CONDA_CACHEDIR"] =  workflow.conda_prefix')
    (60, '    os.environ["NXF_CONDA_CACHEDIR"] = workflow.conda_prefix')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/ncov-phylogeography, file=workflow/Snakefile
context_key: ['if "LANGUAGE" not in os.environ']
    (66, 'if "LANGUAGE" not in os.environ:')
    (67, '    os.environ["LANGUAGE"] = "en_US.UTF-8"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/ncov-phylogeography, file=workflow/Snakefile
context_key: ['if "LANG" not in os.environ']
    (68, 'if "LANG" not in os.environ:')
    (69, '    os.environ["LANG"] = "en_US.UTF-8"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/ncov-phylogeography, file=workflow/Snakefile
context_key: ['if "LC_ALL" not in os.environ']
    (70, 'if "LC_ALL" not in os.environ:')
    (71, '    os.environ["LC_ALL"] = "en_US.UTF-8"')
    (72, '')
    (73, '')
    (74, '# -----------------------------------------------------------------------------#')
    (75, '#                                Utility Functions                             #')
    (76, '# -----------------------------------------------------------------------------#')
    (77, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/ncov-phylogeography, file=workflow/Snakefile
context_key: ['if [[ {params.reroot} ]]; the']
    (305, '        if [[ {params.reroot} ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bakeronit/snakemake-gatk4-non-model, file=Snakefile
context_key: ['if line.startswith(">")']
    (13, '        if line.startswith(">"):')
    (14, '            line = line.split(" ")[0]')
    (15, '            scaffolds.append(line[1:])')
    (16, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=metagenome-atlas/genome_atlas, file=Snakefile
context_key: ['if not "mem" in r.resources']
    (34, '    if not "mem" in r.resources:')
    (35, '        r.resources["mem"]=config["mem"]')
    (36, '    if not "time" in r.resources:')
    (37, '        r.resources["time"]=config["runtime"]["default"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sinanugur/sncRNA-workflow, file=workflow/Snakefile
context_key: ['if route == "bowtie2"', 'elif route == "bowtie1"', 'elif route == "stats" or route == "statistics"', 'elif route == "devel"', 'else']
    (43, 'if route == "bowtie2":')
    (44, '\\trule all:')
    (45, '\\t\\tinput:')
    (46, '\\t\\t\\t#expand("results/count_tables/{gene}.tsv",gene=gene),')
    (47, '\\t\\t\\texpand(["results/count_tables/" + x + ".tsv" for x in ["tRNA","miRNA_precursor","piRNA","miRNA","lncRNA","misc_RNA","protein_coding","snoRNA","snRNA","scaRNA"]]),')
    (48, '\\t\\t\\t"results/file_statistics.csv",')
    (49, '\\t\\t\\texpand("analyses/fastqc/{sample}.trimmed.FastQC/{sample}.trimmed_fastqc.zip",sample=files),')
    (50, '\\t\\t\\t"results/multiqc_report.html",')
    (51, '\\t\\t\\t"results/count_tables/tRF.tsv",')
    (52, '\\t\\t\\t"results/mirtrace/mirtrace-report.html",')
    (53, '\\t\\t\\t"results/count_tables/isomiR.tsv"')
    (54, '')
    (55, '\\t\\t\\t')
    (56, 'elif route == "bowtie1":')
    (57, '\\tprint("Bowtie1 module has been selected.")')
    (58, '\\trule all_bowtie1:')
    (59, '\\t\\tinput:')
    (60, '\\t\\t\\texpand("analyses/bowtie1_mappings/{sample}.sorted.bam",sample=files)')
    (61, '')
    (62, '')
    (63, 'elif route == "stats" or route == "statistics":')
    (64, '\\tprint("Statistics module has been selected.")')
    (65, '\\trule statistics:')
    (66, '\\t\\tinput:')
    (67, '\\t\\t\\texpand("results/statistics/{gene}.tsv",gene=["gencode.uniq.exon","piRNA.uniq","tRNA.uniq"])')
    (68, '\\t')
    (69, '')
    (70, 'elif route == "devel":')
    (71, '\\tpass')
    (72, '')
    (73, 'else:')
    (74, '\\tprint("Please select a correct route...")')
    (75, '\\tprint("bowtie1/bowtie2/stats")')
    (76, '')
    (77, "'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=samnooij/FECBUD_reference-free_analysis, file=Snakefile
context_key: ['if not REFERENCE_DIR.endswith("/")']
    (38, 'if not REFERENCE_DIR.endswith("/"):')
    (39, '    REFERENCE_DIR = REFERENCE_DIR + "/"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bioconda/bioconda-paper, file=Snakefile
context_key: ['if "GITHUB_TOKEN" in os.environ', 'rule plot_adddel', 'output']
    (144, 'if "GITHUB_TOKEN" in os.environ:')
    (145, '    rule plot_adddel:')
    (146, '        output:')
    (147, '            "plots/add+del.svg"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bioconda/bioconda-paper, file=Snakefile
context_key: ['if "GITHUB_TOKEN" in os.environ', 'rule plot_adddel', 'conda']
    (148, '        conda:')
    (149, '            "envs/analysis.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bioconda/bioconda-paper, file=Snakefile
context_key: ['if "GITHUB_TOKEN" in os.environ', 'rule plot_adddel', 'script']
    (150, '        script:')
    (151, '            "scripts/plot-add-del.py"')
    (152, '')
    (153, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "langs" in config']
    (31, 'if "langs" in config:')
    (32, '    LANGS = set(config["langs"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "lang1" in config']
    (33, 'if "lang1" in config:')
    (34, '    LANG1 = config["lang1"]')
    (35, '    LANGS.add(LANG1)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "lang2" in config']
    (36, 'if "lang2" in config:')
    (37, '    LANG2 = config["lang2"]')
    (38, '    LANGS.add(LANG2)')
    (39, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "profiling" in config and config["profiling"]']
    (41, 'if "profiling" in config and config["profiling"]:')
    (42, '    PROFILING = "/usr/bin/time -v"')
    (43, '')
    (44, '#################################################################')
    (45, '# CRAWLING')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "crawlerUserAgent" in config']
    (52, 'if "crawlerUserAgent" in config:')
    (53, '    USERAGENT = f\\\'-a "{config["crawlerUserAgent"]}"\\\'')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "crawlTimeLimit" in config']
    (54, 'if "crawlTimeLimit" in config:')
    (55, '    CRAWLTIMELIMIT = f\\\'-t {config["crawlTimeLimit"]}\\\'')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "crawlWait" in config']
    (56, 'if "crawlWait" in config:')
    (57, '    CRAWLWAIT = f\\\'--wait {config["crawlWait"]}\\\'')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "crawlFileTypes" in config']
    (58, 'if "crawlFileTypes" in config:')
    (59, '    CRAWLFILETYPES = f\\\'-f {config["crawlFileTypes"]}\\\'')
    (60, '')
    (61, '#################################################################')
    (62, '# PREPROCESS')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "preprocessor" in config', 'if config["preprocessor"] == "warc2preprocess"']
    (67, 'if "preprocessor" in config:')
    (68, '    if config["preprocessor"] == "warc2preprocess":')
    (69, '        PPROC = "w2p"')
    (70, '        PPROC_FILES = ["plain_text.gz", "url.gz", "mime.gz", "normalized_html.gz", "deboilerplate_html.gz"]')
    (71, '        TEXT_FILE = "plain_text.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "preprocessor" in config', 'else']
    (72, '    else:')
    (73, '        PPROC = config["preprocessor"]')
    (74, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "cleanHTML" in config and config["cleanHTML"]']
    (88, 'if "cleanHTML" in config and config["cleanHTML"]:')
    (89, '    CLEANHTML = "--cleanhtml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "ftfy" in config and config["ftfy"]']
    (90, 'if "ftfy" in config and config["ftfy"]:')
    (91, '    FTFY = "--ftfy"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "langID" in config']
    (92, 'if "langID" in config:')
    (93, '    LANGID = config["langID"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "parser" in config']
    (94, 'if "parser" in config:')
    (95, '    PARSER = f"--parser {config[\\\'parser\\\']}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "PDFextract" in config and config["PDFextract"]', 'if "PDFextract_configfile" in config and config["PDFextract_configfile"]']
    (96, 'if "PDFextract" in config and config["PDFextract"]:')
    (97, '    PDFEXTRACT_CF = ""')
    (98, '    PDFEXTRACT_SJ = ""')
    (99, '    PDFEXTRACT_KL = ""')
    (100, '    if "PDFextract_configfile" in config and config["PDFextract_configfile"]:')
    (101, '        PDFEXTRACT_CF = f" --pe_configfile {config[\\\'PDFextract_configfile\\\']}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "PDFextract" in config and config["PDFextract"]', 'if "PDFextract_sentence_join_path" in config and config["PDFextract_sentence_join_path"]']
    (102, '    if "PDFextract_sentence_join_path" in config and config["PDFextract_sentence_join_path"]:')
    (103, '        PDFEXTRACT_SJ = f" --sentence_join_path {config[\\\'PDFextract_sentence_join_path\\\']}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "PDFextract" in config and config["PDFextract"]', 'if "PDFextract_kenlm_path" in config and config["PDFextract_kenlm_path"]']
    (104, '    if "PDFextract_kenlm_path" in config and config["PDFextract_kenlm_path"]:')
    (105, '        PDFEXTRACT_KL = f" --kenlm_path {config[\\\'PDFextract_kenlm_path\\\']}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "PDFextract" in config and config["PDFextract"]']
    (106, '    PDFEXTRACT = f"--pdfextract {PDFEXTRACT_CF} {PDFEXTRACT_SJ} {PDFEXTRACT_KL}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "html5lib" in config and config["html5lib"]']
    (107, 'if "html5lib" in config and config["html5lib"]:')
    (108, '    HTML5LIB = "--html5lib"')
    (109, '')
    (110, '# sentence splitting and tokenisation')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "documentAlignerThreshold" in config']
    (130, 'if "documentAlignerThreshold" in config:')
    (131, '    DOC_THRESHOLD = config["documentAlignerThreshold"]')
    (132, '')
    (133, '#################################################################')
    (134, '# SEGALIGN')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "sentenceAlignerThreshold" in config']
    (138, 'if "sentenceAlignerThreshold" in config:')
    (139, '    SEGALIGN_THRESHOLD = config["sentenceAlignerThreshold"]')
    (140, '')
    (141, '#################################################################')
    (142, '# CLEANING')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "bifixer" in config and config["bifixer"]']
    (162, 'if "bifixer" in config and config["bifixer"]:')
    (163, '    BIFIXER = True')
    (164, '    BIFIXER_FIELDS = ["bifixerhash", "bifixerscore"]')
    (165, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "aggressiveDedup" in config and not config["aggressiveDedup"]']
    (166, 'if "aggressiveDedup" in config and not config["aggressiveDedup"]:')
    (167, '    AGGRESSIVE_DEDUP = ""')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "bicleaner" in config']
    (168, 'if "bicleaner" in config:')
    (169, '    BICLEANER = True')
    (170, '    BICLEANER_MODEL = config["bicleaner"]')
    (171, '    BICLEANER_FIELDS = ["bicleaner"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "bicleanerThreshold" in config']
    (172, 'if "bicleanerThreshold" in config:')
    (173, '    BICLEANER_THRESHOLD = config["bicleanerThreshold"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "elrc" in config and config["elrc"]']
    (174, 'if "elrc" in config and config["elrc"]:')
    (175, '    ELRC = True')
    (176, '    ELRC_FIELDS = ["lengthratio", "numTokensSL", "numTokensTL"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "tmx" in config and config["tmx"]']
    (177, 'if "tmx" in config and config["tmx"]:')
    (178, '    TMX = True')
    (179, '    OUTPUT_FILES.append("not-deduped.tmx")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "deduped" in config and config["deduped"]']
    (180, 'if "deduped" in config and config["deduped"]:')
    (181, '    DEDUPED = True')
    (182, '    OUTPUT_FILES.append("deduped.tmx")')
    (183, '    OUTPUT_FILES.append("deduped.txt")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "biroamer" in config and config["biroamer"]', 'if "deduped.tmx" in OUTPUT_FILES']
    (184, 'if "biroamer" in config and config["biroamer"]:')
    (185, '    BIROAMER = True')
    (186, '')
    (187, '    if "deduped.tmx" in OUTPUT_FILES:')
    (188, '        OUTPUT_FILES.append("deduped-roamed.tmx")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "biroamer" in config and config["biroamer"]', 'else']
    (189, '    else:')
    (190, '        OUTPUT_FILES.append("not-deduped-roamed.tmx")')
    (191, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "biroamer" in config and config["biroamer"]', 'if "biroamerMixFiles" in config and len(config["biroamerMixFiles"]) > 0']
    (192, '    if "biroamerMixFiles" in config and len(config["biroamerMixFiles"]) > 0:')
    (193, '        BIROAMER_MIX_FILES = " ".join(config["biroamerMixFiles"])')
    (194, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "biroamer" in config and config["biroamer"]', 'if "biroamerImproveAlignmentCorpus" in config']
    (195, '    if "biroamerImproveAlignmentCorpus" in config:')
    (196, '        BIROAMER_ALIGNMENT_CORPUS = f"-a {config[\\\'biroamerImproveAlignmentCorpus\\\']}"')
    (197, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "biroamer" in config and config["biroamer"]', 'if "biroamerOmitRandomSentences" in config and config["biroamerOmitRandomSentences"]']
    (198, '    if "biroamerOmitRandomSentences" in config and config["biroamerOmitRandomSentences"]:')
    (199, '        BIROAMER_OMIT = "-o"')
    (200, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "bifixerhash" in BEFORE_ELRC_FIELDS']
    (207, 'if "bifixerhash" in BEFORE_ELRC_FIELDS:')
    (208, '    i = BEFORE_ELRC_FIELDS.index("bifixerhash")')
    (209, '    i = i + 1  # sort counts from 1, not 0')
    (210, '    FILTER_SORT_FIELDS = f"-k{i},{i} -k{i+1},{i+1}nr"')
    (211, '    TMX_DEDUP_FIELDS = "bifixerhash"')
    (212, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "warcs" in config']
    (220, 'if "warcs" in config:')
    (221, '    WARCS = WARCS.union(config["warcs"])')
    (222, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "hosts" in config']
    (223, 'if "hosts" in config:')
    (224, '    HOSTS = HOSTS.union(config["hosts"])')
    (225, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "hostsFile" in config']
    (226, 'if "hostsFile" in config:')
    (227, '    with open_xz_or_gzip_or_plain(config["hostsFile"]) as f:')
    (228, '        for line in f:')
    (229, '            HOSTS.add(line.strip())')
    (230, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "warcsFile" in config']
    (231, 'if "warcsFile" in config:')
    (232, '    with open_xz_or_gzip_or_plain(config["warcsFile"]) as f:')
    (233, '        for line in f:')
    (234, '            WARCS.add(line.strip())')
    (235, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "parallelWorkers" in config']
    (259, 'if "parallelWorkers" in config:')
    (260, '    for k in config["parallelWorkers"]:')
    (261, '        THREADS[k] = config["parallelWorkers"][k]')
    (262, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if "until" not in config']
    (265, 'if "until" not in config:')
    (266, '    OUTPUT = expand(')
    (267, '        "{permanent}/{lang1}-{lang2}.{output_file}.gz",')
    (268, '        permanent=PERMANENT,')
    (269, '        target=TARGETS,')
    (270, '        lang1=LANG1,')
    (271, '        lang2=LANG2,')
    (272, '        output_file=OUTPUT_FILES,')
    (273, '    )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['elif UNTIL == "crawl"']
    (274, 'elif UNTIL == "crawl":')
    (275, '    for domain, hosts in DOMAIN_2_HOSTS.items():')
    (276, '        for host in hosts:')
    (277, '            OUTPUT.append(f"{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['elif UNTIL == "preprocess"']
    (278, 'elif UNTIL == "preprocess":')
    (279, '    OUTPUT = expand(')
    (280, '        "{datadir}/preprocess/{target}/{pproc}/{lang}/{pproc_file}",')
    (281, '        datadir=DATADIR,')
    (282, '        target=TARGETS,')
    (283, '        pproc=PPROC,')
    (284, '        lang=LANGS,')
    (285, '        pproc_file=PPROC_FILES,')
    (286, '    )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['elif UNTIL == "shard"']
    (287, 'elif UNTIL == "shard":')
    (288, '    OUTPUT = expand("{datadir}/shards/02.batches.{lang}", datadir=DATADIR, lang=LANGS)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['elif UNTIL == "split"']
    (289, 'elif UNTIL == "split":')
    (290, '    OUTPUT = expand("{datadir}/shards/03.split.{lang}", datadir=DATADIR, lang=LANGS)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['else', 'if UNTIL == "docalign"']
    (292, '    if UNTIL == "docalign":')
    (293, '        OUTPUT.append(f"{TRANSIENT}/06_01.docalign.{LANG1}_{LANG2}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['else', 'elif UNTIL == "segalign"']
    (294, '    elif UNTIL == "segalign":')
    (295, '        OUTPUT.append(f"{TRANSIENT}/06_02.segalign.{LANG1}_{LANG2}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['else', 'elif UNTIL == "bifixer"']
    (296, '    elif UNTIL == "bifixer":')
    (297, '        OUTPUT.append(f"{TRANSIENT}/07_01.bifixer.{LANG1}_{LANG2}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['else', 'elif UNTIL == "bicleaner"']
    (298, '    elif UNTIL == "bicleaner":')
    (299, '        OUTPUT.append(f"{TRANSIENT}/07_02.bicleaner.{LANG1}_{LANG2}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['else', 'elif UNTIL == "filter"']
    (300, '    elif UNTIL == "filter":')
    (301, '        OUTPUT.append(f"{TRANSIENT}/07_03.filter.{LANG1}_{LANG2}")')
    (302, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if SEGALIGN == "hunalign"']
    (623, 'if SEGALIGN == "hunalign":')
    (624, '    split_input_filename = "hunalign.06_02.segalign"')
    (625, '    split_input_extension = ".xz"')
    (626, '')
    (627, '')
    (628, '# split segalign results into balanced chunks')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['input', 'run', 'if len(input) == 0']
    (644, '        if len(input) == 0:')
    (645, '            shell(')
    (646, '                f"""')
    (647, '                >&2 echo "INFO: no data to work with. Stopping execution"')
    (648, '                touch {output}')
    (649, '                # Kill only the current Snakemake, not all of them (might be running multiple instances, and we only want to stop this one)')
    (650, '                # https://snakemake.readthedocs.io/en/stable/project_info/faq.html#how-do-i-exit-a-running-snakemake-workflow')
    (651, '                pid_to_kill="{snake_no_more_race_get_pgid()}"')
    (652, '                if [[ "$pid_to_kill" == "" ]]; then')
    (653, '                    echo "ERROR: could not stop the execution with a normal status. Forcing snakemake to stop"')
    (654, '                    exit 1')
    (655, '                else')
    (656, '                    kill -TERM "$pid_to_kill" || exit 1')
    (657, '                fi')
    (658, '                """')
    (659, '            )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['input', 'run', 'else', 'if [[ {input[0]} == *.gz ]]; the']
    (666, '                if [[ {input[0]} == *.gz ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['input', 'run', 'else', 'elif [[ {input[0]} == *.xz ]]; the']
    (668, '                elif [[ {input[0]} == *.xz ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['input', 'run', 'else', 'if [ -z "$(ls -A {params.folder})" ]; the']
    (674, '                if [ -z "$(ls -A {params.folder})" ]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if not BIFIXER']
    (721, 'if not BIFIXER:')
    (722, '    bicleaner_input = rules.bifixer.input.segalign')
    (723, '')
    (724, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if not BICLEANER']
    (770, 'if not BICLEANER:')
    (771, '    filter_input = rules.bicleaner.input.bifixer')
    (772, '')
    (773, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=bitextor/Snakefile
context_key: ['if raw_input_filename in ["07_02.bicleaner", "06_02.segalign", "hunalign.06_02.segalign"]']
    (808, 'if raw_input_filename in ["07_02.bicleaner", "06_02.segalign", "hunalign.06_02.segalign"]:')
    (809, '    extension = ".gz"')
    (810, '')
    (811, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "langs" in config']
    (31, 'if "langs" in config:')
    (32, '    LANGS = set(config["langs"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "lang1" in config']
    (33, 'if "lang1" in config:')
    (34, '    LANG1 = config["lang1"]')
    (35, '    LANGS.add(LANG1)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "lang2" in config']
    (36, 'if "lang2" in config:')
    (37, '    LANG2 = config["lang2"]')
    (38, '    LANGS.add(LANG2)')
    (39, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "profiling" in config and config["profiling"]']
    (41, 'if "profiling" in config and config["profiling"]:')
    (42, '    PROFILING = "/usr/bin/time -v"')
    (43, '')
    (44, '#################################################################')
    (45, '# CRAWLING')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "crawlerUserAgent" in config']
    (52, 'if "crawlerUserAgent" in config:')
    (53, '    USERAGENT = f\\\'-a "{config["crawlerUserAgent"]}"\\\'')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "crawlTimeLimit" in config']
    (54, 'if "crawlTimeLimit" in config:')
    (55, '    CRAWLTIMELIMIT = f\\\'-t {config["crawlTimeLimit"]}\\\'')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "crawlWait" in config']
    (56, 'if "crawlWait" in config:')
    (57, '    CRAWLWAIT = f\\\'--wait {config["crawlWait"]}\\\'')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "crawlFileTypes" in config']
    (58, 'if "crawlFileTypes" in config:')
    (59, '    CRAWLFILETYPES = f\\\'-f {config["crawlFileTypes"]}\\\'')
    (60, '')
    (61, '#################################################################')
    (62, '# PREPROCESS')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "documentAlignerThreshold" in config']
    (90, 'if "documentAlignerThreshold" in config:')
    (91, '    DOC_THRESHOLD = config["documentAlignerThreshold"]')
    (92, '')
    (93, '#################################################################')
    (94, '# SEGALIGN')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "sentenceAlignerThreshold" in config']
    (98, 'if "sentenceAlignerThreshold" in config:')
    (99, '    SEGALIGN_THRESHOLD = config["sentenceAlignerThreshold"]')
    (100, '')
    (101, '#################################################################')
    (102, '# CLEANING')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "bifixer" in config and config["bifixer"]']
    (122, 'if "bifixer" in config and config["bifixer"]:')
    (123, '    BIFIXER = True')
    (124, '    BIFIXER_FIELDS = ["bifixerhash", "bifixerscore"]')
    (125, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "aggressiveDedup" in config and not config["aggressiveDedup"]']
    (126, 'if "aggressiveDedup" in config and not config["aggressiveDedup"]:')
    (127, '    AGGRESSIVE_DEDUP = ""')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "bicleaner" in config']
    (128, 'if "bicleaner" in config:')
    (129, '    BICLEANER = True')
    (130, '    BICLEANER_MODEL = config["bicleaner"]')
    (131, '    BICLEANER_FIELDS = ["bicleaner"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "bicleanerThreshold" in config']
    (132, 'if "bicleanerThreshold" in config:')
    (133, '    BICLEANER_THRESHOLD = config["bicleanerThreshold"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "elrc" in config and config["elrc"]']
    (134, 'if "elrc" in config and config["elrc"]:')
    (135, '    ELRC = True')
    (136, '    ELRC_FIELDS = ["lengthratio", "numTokensSL", "numTokensTL"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "tmx" in config and config["tmx"]']
    (137, 'if "tmx" in config and config["tmx"]:')
    (138, '    TMX = True')
    (139, '    OUTPUT_FILES.append("not-deduped.tmx")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "deduped" in config and config["deduped"]']
    (140, 'if "deduped" in config and config["deduped"]:')
    (141, '    DEDUPED = True')
    (142, '    OUTPUT_FILES.append("deduped.tmx")')
    (143, '    OUTPUT_FILES.append("deduped.txt")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "biroamer" in config and config["biroamer"]', 'if "deduped.tmx" in OUTPUT_FILES']
    (144, 'if "biroamer" in config and config["biroamer"]:')
    (145, '    BIROAMER = True')
    (146, '')
    (147, '    if "deduped.tmx" in OUTPUT_FILES:')
    (148, '        OUTPUT_FILES.append("deduped-roamed.tmx")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "biroamer" in config and config["biroamer"]', 'else']
    (149, '    else:')
    (150, '        OUTPUT_FILES.append("not-deduped-roamed.tmx")')
    (151, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "biroamer" in config and config["biroamer"]', 'if "biroamerMixFiles" in config and len(config["biroamerMixFiles"]) > 0']
    (152, '    if "biroamerMixFiles" in config and len(config["biroamerMixFiles"]) > 0:')
    (153, '        BIROAMER_MIX_FILES = " ".join(config["biroamerMixFiles"])')
    (154, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "biroamer" in config and config["biroamer"]', 'if "biroamerImproveAlignmentCorpus" in config']
    (155, '    if "biroamerImproveAlignmentCorpus" in config:')
    (156, '        BIROAMER_ALIGNMENT_CORPUS = f"-a {config[\\\'biroamerImproveAlignmentCorpus\\\']}"')
    (157, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "biroamer" in config and config["biroamer"]', 'if "biroamerOmitRandomSentences" in config and config["biroamerOmitRandomSentences"]']
    (158, '    if "biroamerOmitRandomSentences" in config and config["biroamerOmitRandomSentences"]:')
    (159, '        BIROAMER_OMIT = "-o"')
    (160, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "bifixerhash" in BEFORE_ELRC_FIELDS']
    (167, 'if "bifixerhash" in BEFORE_ELRC_FIELDS:')
    (168, '    i = BEFORE_ELRC_FIELDS.index("bifixerhash")')
    (169, '    i = i + 1  # sort counts from 1, not 0')
    (170, '    FILTER_SORT_FIELDS = f"-k{i},{i} -k{i+1},{i+1}nr"')
    (171, '    TMX_DEDUP_FIELDS = "bifixerhash"')
    (172, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "warcs" in config']
    (180, 'if "warcs" in config:')
    (181, '    WARCS = WARCS.union(config["warcs"])')
    (182, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "hosts" in config']
    (183, 'if "hosts" in config:')
    (184, '    HOSTS = HOSTS.union(config["hosts"])')
    (185, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "hostsFile" in config']
    (186, 'if "hostsFile" in config:')
    (187, '    with open_xz_or_gzip_or_plain(config["hostsFile"]) as f:')
    (188, '        for line in f:')
    (189, '            HOSTS.add(line.strip())')
    (190, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "warcsFile" in config']
    (191, 'if "warcsFile" in config:')
    (192, '    with open_xz_or_gzip_or_plain(config["warcsFile"]) as f:')
    (193, '        for line in f:')
    (194, '            WARCS.add(line.strip())')
    (195, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "parallelWorkers" in config']
    (219, 'if "parallelWorkers" in config:')
    (220, '    for k in config["parallelWorkers"]:')
    (221, '        THREADS[k] = config["parallelWorkers"][k]')
    (222, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if "until" not in config']
    (225, 'if "until" not in config:')
    (226, '    OUTPUT = expand(')
    (227, '        "{permanent}/{lang1}-{lang2}.{output_file}.gz",')
    (228, '        permanent=PERMANENT,')
    (229, '        target=TARGETS,')
    (230, '        lang1=LANG1,')
    (231, '        lang2=LANG2,')
    (232, '        output_file=OUTPUT_FILES,')
    (233, '    )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['elif UNTIL == "crawl"']
    (234, 'elif UNTIL == "crawl":')
    (235, '    for domain, hosts in DOMAIN_2_HOSTS.items():')
    (236, '        for host in hosts:')
    (237, '            OUTPUT.append(f"{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['elif UNTIL == "preprocess"']
    (238, 'elif UNTIL == "preprocess":')
    (239, '    OUTPUT = expand(')
    (240, '        "{datadir}/preprocess/{target}/{pproc}/{lang}/{pproc_file}",')
    (241, '        datadir=DATADIR,')
    (242, '        target=TARGETS,')
    (243, '        pproc=PPROC,')
    (244, '        lang=LANGS,')
    (245, '        pproc_file=PPROC_FILES,')
    (246, '    )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['elif UNTIL == "shard"']
    (247, 'elif UNTIL == "shard":')
    (248, '    OUTPUT = expand("{datadir}/shards/02.batches.{lang}", datadir=DATADIR, lang=LANGS)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['elif UNTIL == "split"']
    (249, 'elif UNTIL == "split":')
    (250, '    OUTPUT = expand("{datadir}/shards/03.split.{lang}", datadir=DATADIR, lang=LANGS)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['else', 'if UNTIL == "docalign"']
    (252, '    if UNTIL == "docalign":')
    (253, '        OUTPUT.append(f"{TRANSIENT}/06_01.docalign.{LANG1}_{LANG2}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['else', 'elif UNTIL == "segalign"']
    (254, '    elif UNTIL == "segalign":')
    (255, '        OUTPUT.append(f"{TRANSIENT}/06_02.segalign.{LANG1}_{LANG2}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['else', 'elif UNTIL == "bifixer"']
    (256, '    elif UNTIL == "bifixer":')
    (257, '        OUTPUT.append(f"{TRANSIENT}/07_01.bifixer.{LANG1}_{LANG2}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['else', 'elif UNTIL == "bicleaner"']
    (258, '    elif UNTIL == "bicleaner":')
    (259, '        OUTPUT.append(f"{TRANSIENT}/07_02.bicleaner.{LANG1}_{LANG2}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['else', 'elif UNTIL == "filter"']
    (260, '    elif UNTIL == "filter":')
    (261, '        OUTPUT.append(f"{TRANSIENT}/07_03.filter.{LANG1}_{LANG2}")')
    (262, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if SEGALIGN == "hunalign"']
    (542, 'if SEGALIGN == "hunalign":')
    (543, '    split_input_filename = "hunalign.06_02.segalign"')
    (544, '    split_input_extension = ".xz"')
    (545, '')
    (546, '')
    (547, '# split segalign results into balanced chunks')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['input', 'run', 'if len(input) == 0']
    (563, '        if len(input) == 0:')
    (564, '            shell(')
    (565, '                f"""')
    (566, '                >&2 echo "INFO: no data to work with. Stopping execution"')
    (567, '                touch {output}')
    (568, '                # Kill only the current Snakemake, not all of them (might be running multiple instances, and we only want to stop this one)')
    (569, '                # https://snakemake.readthedocs.io/en/stable/project_info/faq.html#how-do-i-exit-a-running-snakemake-workflow')
    (570, '                pid_to_kill="{snake_no_more_race_get_pgid()}"')
    (571, '                if [[ "$pid_to_kill" == "" ]]; then')
    (572, '                    echo "ERROR: could not stop the execution with a normal status. Forcing snakemake to stop"')
    (573, '                    exit 1')
    (574, '                else')
    (575, '                    kill -TERM "$pid_to_kill" || exit 1')
    (576, '                fi')
    (577, '                """')
    (578, '            )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['input', 'run', 'else', 'if [[ {input[0]} == *.gz ]]; the']
    (585, '                if [[ {input[0]} == *.gz ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['input', 'run', 'else', 'elif [[ {input[0]} == *.xz ]]; the']
    (587, '                elif [[ {input[0]} == *.xz ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['input', 'run', 'else', 'if [ -z "$(ls -A {params.folder})" ]; the']
    (593, '                if [ -z "$(ls -A {params.folder})" ]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if not BIFIXER']
    (640, 'if not BIFIXER:')
    (641, '    bicleaner_input = rules.bifixer.input.segalign')
    (642, '')
    (643, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if not BICLEANER']
    (689, 'if not BICLEANER:')
    (690, '    filter_input = rules.bicleaner.input.bifixer')
    (691, '')
    (692, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bitextor/bitextor-neural, file=workflow/Snakefile
context_key: ['if raw_input_filename in ["07_02.bicleaner", "06_02.segalign", "hunalign.06_02.segalign"]']
    (727, 'if raw_input_filename in ["07_02.bicleaner", "06_02.segalign", "hunalign.06_02.segalign"]:')
    (728, '    extension = ".gz"')
    (729, '')
    (730, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/Peaks_workflow, file=Snakefile
context_key: ["if mito_chrom not in fai_parsed[\\'chr\\'].values"]
    (25, "if mito_chrom not in fai_parsed[\\'chr\\'].values:")
    (26, "    raise Exception(\\'{mito} not found in reference fai file.\\'.format(mito=mito_chrom))")
    (27, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/Peaks_workflow, file=Snakefile
context_key: ["if not samples[\\'sample\\'].is_unique"]
    (40, "if not samples[\\'sample\\'].is_unique:")
    (41, "    raise Exception(\\'A sample has more than one combination of control, sample_group, enriched_factor, and/or se_or_pe.\\')")
    (42, '')
    (43, '# Filter for sample rows that are not controls')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/Peaks_workflow, file=Snakefile
context_key: ["if (config[\\'atacseq\\'] and units[\\'fq2\\'].isnull().any())"]
    (49, "if (config[\\'atacseq\\'] and units[\\'fq2\\'].isnull().any()):")
    (50, "    raise Exception(\\'SE ATAC-seq not supported.\\')")
    (51, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/Peaks_workflow, file=Snakefile
context_key: ['if not os.path.exists(tmp_dir)']
    (56, 'if not os.path.exists(tmp_dir):')
    (57, '    os.mkdir(tmp_dir)')
    (58, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=welfare-state-analytics/pyriksprot_tagger, file=workflow/Snakefile
context_key: ['if PACKAGE_PATH not in sys.path']
    (18, 'if PACKAGE_PATH not in sys.path:')
    (19, '    sys.path.insert(0, PACKAGE_PATH)')
    (20, '')
    (21, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=welfare-state-analytics/pyriksprot_tagger, file=workflow/Snakefile
context_key: ['if sys.platform not in ["win32"]']
    (28, 'if sys.platform not in ["win32"]:')
    (29, '    shell.prefix("set -o pipefail; ")')
    (30, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sebschmi/snakemake-turso, file=Snakefile
context_key: ['if "datadir" in config']
    (13, 'if "datadir" in config:')
    (14, '    DATADIR = config["datadir"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sebschmi/snakemake-turso, file=Snakefile
context_key: ['if "programdir" in config']
    (19, 'if "programdir" in config:')
    (20, '    PROGRAMDIR = config["programdir"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sebschmi/snakemake-turso, file=Snakefile
context_key: ['if "reportdir" in config']
    (25, 'if "reportdir" in config:')
    (26, '    REPORTDIR = config["reportdir"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan', 'input']
    (80, "if OPT == \\'vulcan\\':")
    (81, '    rule Vulcan:')
    (82, '        input:')
    (83, '            ref = ref_genome,')
    (84, '            fastq="data/NanoQ/PCNQ_read.fastq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan', 'output']
    (85, '        output:')
    (86, '            "data/Vulcan/vulcan_PCNQ_90.bam"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan', 'params']
    (87, '        params:')
    (88, '            prefix="data/Vulcan/vulcan_PCNQ"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan', 'threads']
    (89, '        threads: 10')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan', 'benchmark']
    (90, '        benchmark:')
    (91, '            "benchmarks/Vulcan.tsv"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan', 'shell']
    (92, '        shell:')
    (93, '            "vulcan -ont -t {threads} -r {input.ref} -i {input.fastq} -o {params.prefix}"')
    (94, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_bam_index', 'input']
    (95, '    rule Vulcan_bam_index:')
    (96, '        input:')
    (97, '            "data/Vulcan/vulcan_PCNQ_90.bam",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_bam_index', 'output']
    (98, '        output:')
    (99, '            "data/Vulcan/vulcan_PCNQ_90.bam.bai"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_bam_index', 'shell']
    (100, '        shell:')
    (101, '            "samtools index {input}"')
    (102, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_NanoPlot', 'input']
    (103, '    rule Vulcan_NanoPlot:')
    (104, '        input:')
    (105, '            "data/Vulcan/vulcan_PCNQ_90.bam"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_NanoPlot', 'output']
    (106, '        output:')
    (107, '            "report/Post-alignment_NanoPlot/vulcan_PCNQ_NanoPlot-report.html",')
    (108, '            "report/Post-alignment_NanoPlot/vulcan_PCNQ_NanoStats.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_NanoPlot', 'params']
    (109, '        params:')
    (110, '            prefix="vulcan_PCNQ_",')
    (111, '            outdir="report/Post-alignment_NanoPlot"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_NanoPlot', 'threads']
    (112, '        threads: 8')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_NanoPlot', 'benchmark']
    (113, '        benchmark:')
    (114, '            "benchmarks/Post-alignment_NanoPlot.tsv"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_NanoPlot', 'shell']
    (115, '        shell:')
    (116, '            "NanoPlot -t {threads} --N50 --bam {input} -p {params.prefix} -o {params.outdir}"')
    (117, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Samstats', 'input']
    (118, '    rule Samstats:')
    (119, '        input:  ')
    (120, '            "data/Vulcan/vulcan_PCNQ_90.bam"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Samstats', 'output']
    (121, '        output:')
    (122, '            "report/Samtools_Stats/vulcan_PCNQ_90.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Samstats', 'shell']
    (123, '        shell:')
    (124, '            "samtools stats {input} | grep ^SN | cut -f 2- > {output}"')
    (125, '        ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Samstats_final', 'input']
    (126, '    rule Samstats_final:')
    (127, '        input:')
    (128, '            "report/Samtools_Stats/vulcan_PCNQ_90.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Samstats_final', 'output']
    (129, '        output:')
    (130, '            "report/Samtools_Stats/Aligned_readsamstat.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule Samstats_final', 'shell']
    (131, '        shell:')
    (132, '            "mv {input} {output}"')
    (133, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule NanoPlot_final', 'input']
    (134, '    rule NanoPlot_final:')
    (135, '        input:')
    (136, '            "report/Post-alignment_NanoPlot/vulcan_PCNQ_NanoPlot-report.html"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule NanoPlot_final', 'output']
    (137, '        output:')
    (138, '            "report/Post-alignment_NanoPlot/Aligned_NanoPlot-report.html"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule NanoPlot_final', 'shell']
    (139, '        shell:')
    (140, '            "mv {input} {output}"')
    (141, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule vulcan_plot_chart', 'input']
    (142, '    rule vulcan_plot_chart:')
    (143, '        input:')
    (144, '            "benchmarks/Pre-alignment_NanoPlot.tsv",')
    (145, '            "benchmarks/Post-alignment_NanoPlot.tsv",')
    (146, '            "benchmarks/FastQC.tsv"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule vulcan_plot_chart', 'output']
    (147, '        output:')
    (148, '            "report/benchmark.png"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule vulcan_plot_chart', 'script']
    (149, '        script:')
    (150, '            "scripts/vulcan_plot_benchmark.py"')
    (151, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule MultiQC', 'input']
    (152, '    rule MultiQC:')
    (153, '        input:')
    (154, '            "report/FastQC/PCNQ_read_fastqc.zip",')
    (155, '            # "report/"')
    (156, '            "report/Samtools_Stats/Aligned_readsamstat.txt",')
    (157, '            "report/Post-alignment_NanoPlot/vulcan_PCNQ_NanoStats.txt",')
    (158, '            "report/Pre-alignment_NanoPlot/PCNQ_read_NanoStats.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule MultiQC', 'output']
    (159, '        output:')
    (160, '            "report/MultiQC/multiqc_report.html"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/snakefile_graphmaptest
context_key: ["if OPT == \\'vulcan\\'", 'rule MultiQC', 'shell']
    (161, '        shell:')
    (162, '            "multiqc {input} -o report/MultiQC"')
    (163, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan', 'input']
    (82, "if OPT == \\'vulcan\\':")
    (83, '    rule Vulcan:')
    (84, '        input:')
    (85, '            ref = ref_genome,')
    (86, '            fastq="data/NanoQ/PCNQ_read.fastq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan', 'output']
    (87, '        output:')
    (88, '            "data/Vulcan/vulcan_PCNQ_90.bam"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan', 'params']
    (89, '        params:')
    (90, '            prefix="data/Vulcan/vulcan_PCNQ"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan', 'threads']
    (91, '        threads: config["vulcan_threads"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan', 'benchmark']
    (92, '        benchmark:')
    (93, '            "benchmarks/Vulcan.tsv"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan', 'shell']
    (94, '        shell:')
    (95, '            "vulcan -ont -t {threads} -r {input.ref} -i {input.fastq} -o {params.prefix}"')
    (96, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_bam_index', 'input']
    (97, '    rule Vulcan_bam_index:')
    (98, '        input:')
    (99, '            "data/Vulcan/vulcan_PCNQ_90.bam",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_bam_index', 'output']
    (100, '        output:')
    (101, '            "data/Vulcan/vulcan_PCNQ_90.bam.bai"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_bam_index', 'shell']
    (102, '        shell:')
    (103, '            "samtools index {input}"')
    (104, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_NanoPlot', 'input']
    (105, '    rule Vulcan_NanoPlot:')
    (106, '        input:')
    (107, '            "data/Vulcan/vulcan_PCNQ_90.bam"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_NanoPlot', 'output']
    (108, '        output:')
    (109, '            "report/Post-alignment_NanoPlot/vulcan_PCNQ_NanoPlot-report.html",')
    (110, '            "report/Post-alignment_NanoPlot/vulcan_PCNQ_NanoStats.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_NanoPlot', 'params']
    (111, '        params:')
    (112, '            prefix="vulcan_PCNQ_",')
    (113, '            outdir="report/Post-alignment_NanoPlot"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_NanoPlot', 'threads']
    (114, '        threads: config["aligned_nanoplot_threads"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_NanoPlot', 'benchmark']
    (115, '        benchmark:')
    (116, '            "benchmarks/Post-alignment_NanoPlot.tsv"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Vulcan_NanoPlot', 'shell']
    (117, '        shell:')
    (118, '            "NanoPlot -t {threads} --N50 --bam {input} -p {params.prefix} -o {params.outdir}"')
    (119, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule genomecov', 'input']
    (120, '    rule genomecov:')
    (121, '        input:')
    (122, '            "data/Vulcan/vulcan_PCNQ_90.bam"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule genomecov', 'output']
    (123, '        output:')
    (124, '            "data/GenomeCov/vulcan_PCNQ_90.bdg"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule genomecov', 'shell']
    (125, '        shell:')
    (126, '            "bedtools genomecov -ibam {input} -bg > {output}"')
    (127, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule chrsize', 'input']
    (128, '    rule chrsize:')
    (129, '        input:')
    (130, '            ref_genome')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule chrsize', 'output']
    (131, '        output:')
    (132, '            "data/GenomeCov/chromSizes.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule chrsize', 'shell']
    (133, '        shell:')
    (134, '            "faSize -detailed -tab {input} > {output}"')
    (135, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule bedGraphtobigWig', 'input']
    (136, '    rule bedGraphtobigWig:')
    (137, '        input:')
    (138, '            chrsize = "data/GenomeCov/chromSizes.txt",')
    (139, '            bedgraph = "data/GenomeCov/vulcan_PCNQ_90.bdg"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule bedGraphtobigWig', 'output']
    (140, '        output:')
    (141, '            "data/GenomeCov/Aligned_bigwig.bw"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule bedGraphtobigWig', 'shell']
    (142, '        shell:')
    (143, '            "bedGraphToBigWig {input.bedgraph} {input.chrsize} {output}"')
    (144, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Samstats', 'input']
    (145, '    rule Samstats:')
    (146, '        input:  ')
    (147, '            "data/Vulcan/vulcan_PCNQ_90.bam"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Samstats', 'output']
    (148, '        output:')
    (149, '            "report/Samtools_Stats/vulcan_PCNQ_90.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Samstats', 'shell']
    (150, '        shell:')
    (151, '            "samtools stats {input} | grep ^SN | cut -f 2- > {output}"')
    (152, '        ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Samstats_final', 'input']
    (153, '    rule Samstats_final:')
    (154, '        input:')
    (155, '            "report/Samtools_Stats/vulcan_PCNQ_90.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Samstats_final', 'output']
    (156, '        output:')
    (157, '            "report/Samtools_Stats/Aligned_readsamstat.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule Samstats_final', 'shell']
    (158, '        shell:')
    (159, '            "mv {input} {output}"')
    (160, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule NanoPlot_final', 'input']
    (161, '    rule NanoPlot_final:')
    (162, '        input:')
    (163, '            "report/Post-alignment_NanoPlot/vulcan_PCNQ_NanoPlot-report.html"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule NanoPlot_final', 'output']
    (164, '        output:')
    (165, '            "report/Post-alignment_NanoPlot/Aligned_NanoPlot-report.html"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule NanoPlot_final', 'shell']
    (166, '        shell:')
    (167, '            "mv {input} {output}"')
    (168, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule vulcan_plot_chart', 'input']
    (169, '    rule vulcan_plot_chart:')
    (170, '        input:')
    (171, '            "benchmarks/Pre-alignment_NanoPlot.tsv",')
    (172, '            "benchmarks/Post-alignment_NanoPlot.tsv",')
    (173, '            "benchmarks/FastQC.tsv"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule vulcan_plot_chart', 'output']
    (174, '        output:')
    (175, '            "report/benchmark.png"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule vulcan_plot_chart', 'script']
    (176, '        script:')
    (177, '            "scripts/vulcan_plot_benchmark.py"')
    (178, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule MultiQC', 'input']
    (179, '    rule MultiQC:')
    (180, '        input:')
    (181, '            "report/FastQC/PCNQ_read_fastqc.zip",')
    (182, '            # "report/"')
    (183, '            "report/Samtools_Stats/Aligned_readsamstat.txt",')
    (184, '            "report/Post-alignment_NanoPlot/vulcan_PCNQ_NanoStats.txt",')
    (185, '            "report/Pre-alignment_NanoPlot/PCNQ_read_NanoStats.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule MultiQC', 'output']
    (186, '        output:')
    (187, '            "report/MultiQC/multiqc_report.html"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kaiseriskera/FABLE, file=workflow/Snakefile
context_key: ["if OPT == \\'vulcan\\'", 'rule MultiQC', 'shell']
    (188, '        shell:')
    (189, '            "multiqc {input} -o report/MultiQC"')
    (190, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=steveped/snakemake_hic, file=Snakefile
context_key: ['if not rc == 0']
    (19, 'if not rc == 0:')
    (20, '    print("HiC-Pro not installed")')
    (21, '    sys.exit(1)')
    (22, '')
    (23, '##################')
    (24, '# Define Samples #')
    (25, '##################')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=millerv2/NanoseqSnakemake, file=workflow/rules/dataviz.smk
context_key: ['input', 'if [ ! -f {params.out_dir} ]; then mkdir -p {params.out_dir};f']
    (19, '    if [ ! -f {params.out_dir} ]; then mkdir -p {params.out_dir};fi')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\') and (config[\\\'PREPARE_FOR_SCOUT\\\'] == "No" or config[\\\'PREPARE_FOR_SCOUT\\\'] == \\\'no\\\')', 'rule all', 'input']
    (113, 'if (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\') and (config[\\\'PREPARE_FOR_SCOUT\\\'] == "No" or config[\\\'PREPARE_FOR_SCOUT\\\'] == \\\'no\\\'):')
    (114, '    rule all:')
    (115, '           input:')
    (116, '               expand("../results/filtered/{sample}_filtered.vcf", sample = SAMPLES),')
    (117, '               expand("../results/annotated/{sample}_filtered_annotated.vcf", sample = SAMPLES)')
    (118, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\') and (config[\\\'PREPARE_FOR_SCOUT\\\'] == "Yes" or config[\\\'PREPARE_FOR_SCOUT\\\'] == \\\'yes\\\')', 'rule all', 'input']
    (119, 'if (config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\') and (config[\\\'PREPARE_FOR_SCOUT\\\'] == "Yes" or config[\\\'PREPARE_FOR_SCOUT\\\'] == \\\'yes\\\'):')
    (120, '    rule all:')
    (121, '           input:')
    (122, '               expand("../results/filtered/{sample}_filtered.vcf", sample = SAMPLES),')
    (123, '               expand("../results/annotated/{sample}_filtered_annotated.vcf", sample = SAMPLES),')
    (124, '               expand("../results/readyforscout/{sample}_filtered_annotated_readyforscout.vcf.gz", sample = SAMPLES)')
    (125, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\') and (config[\\\'PREPARE_FOR_SCOUT\\\'] == "Yes" or config[\\\'PREPARE_FOR_SCOUT\\\'] == \\\'yes\\\')', 'rule all', 'input']
    (126, 'if (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\') and (config[\\\'PREPARE_FOR_SCOUT\\\'] == "Yes" or config[\\\'PREPARE_FOR_SCOUT\\\'] == \\\'yes\\\'):')
    (127, '    rule all:')
    (128, '           input:')
    (129, '               expand("../results/filtered/{sample}_filtered.vcf", sample = SAMPLES),')
    (130, '               expand("../results/annotated/{sample}_filtered_annotated.vcf", sample = SAMPLES),')
    (131, '               expand("../results/readyforscout/{sample}_filtered_annotated_readyforscout.vcf", sample = SAMPLES)')
    (132, '')
    (133, '##### Load rules #####')
    (134, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\') and (config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no")']
    (140, 'if (config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\') and (config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no"):')
    (141, '    include: "rules/gatk_CNNScoreVariants.smk"')
    (142, '    include: "rules/gatk_FilterVariantTranches.smk"')
    (143, '        ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\') and (config[\\\'GPU_ACCELERATED\\\'] == "Yes" or config[\\\'GPU_ACCELERATED\\\'] == "yes")']
    (144, 'if (config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\') and (config[\\\'GPU_ACCELERATED\\\'] == "Yes" or config[\\\'GPU_ACCELERATED\\\'] == "yes"):')
    (145, '    include: "rules/pbrun_cnnscorevariants.smk"')
    (146, '    include: "rules/gatk_FilterVariantTranches.smk"')
    (147, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\') and (config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no")']
    (148, 'if (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\') and (config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no"):')
    (149, '    include: "rules/gatk_VariantRecalibrator_indel.smk"')
    (150, '    include: "rules/gatk_VariantRecalibrator_snp.smk"')
    (151, '    include: "rules/gatk_ApplyVQSR_indel.smk"')
    (152, '    include: "rules/gatk_ApplyVQSR_snp.smk"')
    (153, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\') and (config[\\\'GPU_ACCELERATED\\\'] == "Yes" or config[\\\'GPU_ACCELERATED\\\'] == "yes")']
    (154, 'if (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\') and (config[\\\'GPU_ACCELERATED\\\'] == "Yes" or config[\\\'GPU_ACCELERATED\\\'] == "yes"):')
    (155, '    include: "rules/pbrun_vqsr_indel.smk"')
    (156, '    include: "rules/pbrun_vqsr_snp.smk"')
    (157, '')
    (158, '# Annotation')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\')']
    (165, 'if (config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\'):')
    (166, '    include: "rules/SnpSift_annotate_dbSNP_single.smk"')
    (167, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\')']
    (168, 'if (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\'):')
    (169, '    include: "rules/SnpSift_annotate_dbSNP_cohort.smk"')
    (170, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\') and (config[\\\'PREPARE_FOR_SCOUT\\\'] == "Yes" or config[\\\'PREPARE_FOR_SCOUT\\\'] == \\\'yes\\\')']
    (177, 'if (config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\') and (config[\\\'PREPARE_FOR_SCOUT\\\'] == "Yes" or config[\\\'PREPARE_FOR_SCOUT\\\'] == \\\'yes\\\'):')
    (178, '    include: "rules/bcftools_view_multiallelicsites.smk"')
    (179, '    include: "rules/genmod_score_single.smk"')
    (180, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\') and (config[\\\'PREPARE_FOR_SCOUT\\\'] == "Yes" or config[\\\'PREPARE_FOR_SCOUT\\\'] == \\\'yes\\\')']
    (181, 'if (config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\') and (config[\\\'PREPARE_FOR_SCOUT\\\'] == "Yes" or config[\\\'PREPARE_FOR_SCOUT\\\'] == \\\'yes\\\'):')
    (182, '    include: "rules/bcftools_view_multiallelicsites.smk"')
    (183, '    include: "rules/gunzip.smk"')
    (184, '    include: "rules/SnpSift_filter_proband.smk"')
    (185, '    include: "rules/genmod_score_cohort.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PyPSA/pypsa-eur-sec, file=Snakefile
context_key: ['if not exists("config.yaml")']
    (7, 'if not exists("config.yaml"):')
    (8, '    copyfile("config.default.yaml", "config.yaml")')
    (9, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nioo-knaw/PhyloFunDB, file=Snakefile
context_key: ["if not config[\\'update\\']", 'rule iqtree', 'input']
    (334, "if not config[\\'update\\']:")
    (335, '     rule iqtree:')
    (336, '         input:')
    (337, '            expand("interm/{gene}.aligned.good.filter.unique.pick.good.filter.an.{cutoff_otu}.rep.fasta", gene=config["gene"], cutoff_otu=config["cutoff_otu"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nioo-knaw/PhyloFunDB, file=Snakefile
context_key: ["if not config[\\'update\\']", 'rule iqtree', 'output']
    (338, '         output:')
    (339, '            "results/{gene}.treefile"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nioo-knaw/PhyloFunDB, file=Snakefile
context_key: ["if not config[\\'update\\']", 'rule iqtree', 'params']
    (340, '         params:')
    (341, '            "results/{gene}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nioo-knaw/PhyloFunDB, file=Snakefile
context_key: ["if not config[\\'update\\']", 'rule iqtree', 'conda']
    (342, '         conda:')
    (343, '            "envs/iqtree.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nioo-knaw/PhyloFunDB, file=Snakefile
context_key: ["if not config[\\'update\\']", 'rule iqtree', 'threads']
    (344, '         threads:10')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nioo-knaw/PhyloFunDB, file=Snakefile
context_key: ["if not config[\\'update\\']", 'rule iqtree', 'shell']
    (345, '         shell:')
    (346, '            "iqtree -s {input}  -m MFP -alrt 1000 -bb 1000 -nt {threads} -pre {params}"')
    (347, '            ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Mira0507/snakemake_star, file=Snakefile
context_key: ['input', 'run', 'if len(input.fastq) == 2']
    (67, '        if len(input.fastq) == 2:   # if paired-end')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/Genrich_compare, file=Snakefile
context_key: ['if CONTROL']
    (27, 'if CONTROL:')
    (28, '    CONTROLS = [sample for sample in MARK_SAMPLES if CONTROL in sample]')
    (29, '    CASES = [sample for sample in MARK_SAMPLES if CONTROL not in sample]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/Genrich_compare, file=Snakefile
context_key: ['if CONTROL', 'if control in CONTROLS']
    (41, '    if CONTROL:')
    (42, '        control = sample + "_" + CONTROL')
    (43, '        if control in CONTROLS:')
    (44, '            ALL_MACS_PEAKS.append("09peak_macs2/{}_vs_{}_macs2_peaks.xls".format(case, control))')
    (45, '            ALL_GENRICH_PEAKS.append("09peak_Genrich/{}_vs_{}_Genrich_peaks.xls".format(case, control))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/Genrich_compare, file=Snakefile
context_key: ['if CONTROL', 'else']
    (46, '    else:')
    (47, '        ALL_MACS_PEAKS.append("09peak_macs2/{}_macs2_peaks.xls".format(case))')
    (48, '        ALL_GENRICH_PEAKS.append("09peak_Genrich/{}_Genrich_peaks.bed".format(case))')
    (49, '')
    (50, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/Genrich_compare, file=Snakefile
context_key: ['if CONTROL', 'rule call_peaks_macs2', 'params']
    (274, 'if CONTROL:')
    (275, '    rule call_peaks_macs2:')
    (276, '        input: control = "04aln_downsample/{control}-downsample.sorted.bam", case="04aln_downsample/{case}-downsample.sorted.bam"')
    (277, '        output: bed = "09peak_macs2/{case}_vs_{control}_macs2_peaks.xls"')
    (278, '        log: "00log/{case}_vs_{control}_call_peaks_macs2.log"')
    (279, '        params:')
    (280, '            name = "{case}_vs_{control}_macs2",')
    (281, '            jobname = "{case}_vs{control}_macs2",')
    (282, '            custom = config.get("macs2_args")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/Genrich_compare, file=Snakefile
context_key: ['if CONTROL', 'rule call_peaks_macs2', 'message']
    (283, '        message: "call_peaks macs2 {input}: {threads} threads"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/Genrich_compare, file=Snakefile
context_key: ['if CONTROL', 'rule call_peaks_macs2', 'shell']
    (284, '        shell:')
    (285, '            """')
    (286, '           ## for macs2, when nomodel is set, --extsize is default to 200bp, this is the same as 2 * shift-size in macs14.')
    (287, '            source activate macs2')
    (288, '            macs2 callpeak -t {input.case} \\\\')
    (289, '                -c {input.control} -g {config[macs2_g]} \\\\')
    (290, '                --outdir 09peak_macs2 -n {params.name} -p {config[macs2_pvalue]} {params.custom} &> {log}')
    (291, '            """')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/Genrich_compare, file=Snakefile
context_key: ['if CONTROL', 'rule call_peaks_Genrich', 'params']
    (328, 'if CONTROL:')
    (329, '    rule call_peaks_Genrich:')
    (330, '        input: control = "05name_sorted_bam/{control}.name.sorted.bam", case="05name_sorted_bam/{case}.name.sorted.bam"')
    (331, '        output: bed = "09peak_Genrich/{case}_vs_{control}_Genrich_peaks.bed"')
    (332, '        log: "00log/{case}_vs_{control}_call_peaks_Genrich.log"')
    (333, '        params:')
    (334, '            name = "{case}_vs_{control}_Genrich",')
    (335, '            jobname = "{case}_vs{control}_Genrich",')
    (336, '            custom = config.get("Genrich_args")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/Genrich_compare, file=Snakefile
context_key: ['if CONTROL', 'rule call_peaks_Genrich', 'message']
    (337, '        message: "call_peaks Genrich {input}: {threads} threads"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/Genrich_compare, file=Snakefile
context_key: ['if CONTROL', 'rule call_peaks_Genrich', 'shell']
    (338, '        shell:')
    (339, '            """')
    (340, '            {config[Genrich_path]} -t {input.case} \\\\')
    (341, '            -c {input.control} {params.custom} -o {output} &> {log}')
    (342, '            """')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tdayris/cel-cnv-eacon, file=Snakefile
context_key: ['if sys.version_info < (3, 7)']
    (4, 'if sys.version_info < (3, 7):')
    (5, '    raise SystemError("Please use Python 3.7 or later.")')
    (6, '')
    (7, '# Snakemake 5.4.2 at least is required.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RachelRodgers/demux-htcf-snake, file=workflow/Snakefile
context_key: ['if rev_bc1 == True', 'else', 'rule all']
    (48, 'if rev_bc1 == True:')
    (49, '\\tcmdStr = "extract_barcodes.py --input_type barcode_paired_end -f " + barcode1 + " -r " + barcode2 + " --bc1_len " + str(bc1_len) + " --bc2_len " + str(bc2_len) + " -o " + barcodesOut + " --rev_comp_bc1"')
    (50, 'else:')
    (51, '\\tcmdStr = "extract_barcodes.py --input_type barcode_paired_end -f " + barcode1 + " -r " + barcode2 + " --bc1_len " + str(bc1_len) + " --bc2_len " + str(bc2_len) + " -o " + barcodesOut\\t')
    (52, '')
    (53, 'extractCommand.write(cmdStr)')
    (54, 'extractCommand.close()')
    (55, '')
    (56, '#----- Rename input files -----#')
    (57, '')
    (58, 'rename_files(config)')
    (59, '')
    (60, 'SAMPLES, = glob_wildcards(os.path.join(READDIR, "renamed", "{sample}_R1.fastq.gz"))')
    (61, '')
    (62, 'PATTERN_R1 = "{sample}_R1"')
    (63, 'PATTERN_R2 = "{sample}_R2"')
    (64, 'PATTERN_I1 = "{sample}_I1"')
    (65, 'PATTERN_I2 = "{sample}_I2"')
    (66, '')
    (67, '#----- Get New Sample Names -----#')
    (68, '')
    (69, 'NEW = get_new_names(config) ')
    (70, 'print(NEW)')
    (71, '')
    (72, '#----- Snakemake Workflow -----#')
    (73, '')
    (74, 'rule all:')
    (75, '\\tinput:')
    (76, '\\t\\tr1 = expand(os.path.join("results", "compressed", "{new}_R1.fastq.gz"), new = NEW),')
    (77, '                r2 = expand(os.path.join("results", "compressed", "{new}_R2.fastq.gz"), new = NEW)')
    (78, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CellProfiling/SingleCellProteogenomics, file=workflow/Snakefile
context_key: ['if os.path.basename(os.path.abspath(os.curdir)) == "workflow"']
    (5, 'if os.path.basename(os.path.abspath(os.curdir)) == "workflow":')
    (6, "    os.chdir(\\'..\\')")
    (7, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ["if \\'output_directory\\' in config", 'if input_path_config in config']
    (12, "if \\'output_directory\\' in config:")
    (13, "    for input_path_config in [\\'data_root\\', \\'primer_bed\\', \\'reference_genome\\', \\'tree_include_consensus\\']:")
    (14, '        if input_path_config in config:')
    (15, '            config[input_path_config] = str(Path(config[input_path_config]).absolute())')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ["if \\'output_directory\\' in config", 'workdir']
    (16, "    workdir: config[\\'output_directory\\']")
    (17, '')
    (18, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=scottdbrown/minion-plasmid-consensus, file=Snakefile
context_key: ['if [[ $(wc -l <{input.pairs}) -gt 0 ]]\\']
    (259, '        if [[ $(wc -l <{input.pairs}) -gt 0 ]]\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=scottdbrown/minion-plasmid-consensus, file=Snakefile
context_key: ['if line.startswith(">")']
    (336, '                if line.startswith(">"):\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=scottdbrown/minion-plasmid-consensus, file=Snakefile
context_key: ['if len(seqs) < i']
    (338, '                    if len(seqs) < i:\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=scottdbrown/minion-plasmid-consensus, file=Snakefile
context_key: ['if line.startswith(">")']
    (368, '                if line.startswith(">"):\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=scottdbrown/minion-plasmid-consensus, file=Snakefile
context_key: ['if line.startswith(">")']
    (446, '                if line.startswith(">"):\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=scottdbrown/minion-plasmid-consensus, file=Snakefile
context_key: ['if line.startswith(">")']
    (483, '                if line.startswith(">"):\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=scottdbrown/minion-plasmid-consensus, file=Snakefile
context_key: ['elif not WRITTEN_5PRIME']
    (485, '                elif not WRITTEN_5PRIME:\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexmsalmeida/amr-profiler, file=Snakefile
context_key: ['if not os.path.exists(OUTPUT_DIR)']
    (8, 'if not os.path.exists(OUTPUT_DIR):')
    (9, '    os.makedirs(OUTPUT_DIR)')
    (10, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jaleezyy/covid-19-signal, file=Snakefile
context_key: ["if \\'--configfile\\' in sys.argv", 'if not os.path.exists(config_filename)']
    (28, "if \\'--configfile\\' in sys.argv:")
    (29, "    config_filename = os.path.abspath(sys.argv[sys.argv.index(\\'--configfile\\')+1])")
    (30, "    # arguments don\\'t line up")
    (31, '    if not os.path.exists(config_filename):')
    (32, '        print("Invalid filepath for configfile. Looking for default config.yaml")')
    (33, '        configfile: "config.yaml"')
    (34, '        config_filename = os.path.join(os.getcwd(), "config.yaml")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jaleezyy/covid-19-signal, file=Snakefile
context_key: ["if \\'--configfile\\' in sys.argv", 'else']
    (35, '    else:')
    (36, '        configfile: config_filename')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jaleezyy/covid-19-signal, file=Snakefile
context_key: ['try', "if os.path.exists(config[\\'breseq_reference\\'])"]
    (49, "    if os.path.exists(config[\\'breseq_reference\\']):")
    (50, "        breseq_ref = config[\\'breseq_reference\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jaleezyy/covid-19-signal, file=Snakefile
context_key: ["elif not config[\\'run_breseq\\'] and config[\\'run_freebayes\\']", 'rule variant_calling', 'input']
    (189, "elif not config[\\'run_breseq\\'] and config[\\'run_freebayes\\']:")
    (190, '    rule variant_calling:')
    (191, '        input:')
    (192, '            rules.freebayes.input,')
    (193, '            rules.ivar_variants.input,')
    (194, '            rules.consensus.input')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=LCrossman/ribosomal_snakemake, file=Snakefile
context_key: ["if \\'protein\\' in params.type"]
    (87, "                   if \\'protein\\' in params.type:")
    (88, '                       shell("python workflow/scripts/gbk2faa.py {file}")')
    (89, "                   elif \\'dna\\' in params.type:")
    (90, '                       shell("python workflow/scripts/gbk2ffn.py {file}")')
    (91, '           shell("touch \\\'logs/conversion_complete.txt\\\'")')
    (92, '')
    (93, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=LCrossman/ribosomal_snakemake, file=workflow/rules/download_snake.smk
context_key: ['input', 'run', "if \\'yes\\' in params.required"]
    (10, "           if \\'yes\\' in params.required:")
    (11, "               for gen in [line.rstrip() for line in open(\\'genus.txt\\', \\'r\\')]:")
    (12, '                   shell(f"ncbi-genome-download -g {gen} bacteria --parallel {threads}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=LCrossman/ribosomal_snakemake, file=workflow/rules/blast_missing_seqs.smk
context_key: ['output', 'run', "if os.stat(\\'strains_missing_ribos.txt\\').st_size == 0"]
    (22, "          if os.stat(\\'strains_missing_ribos.txt\\').st_size == 0:")
    (23, '               shell("touch \\\'logs/blast_complete.txt\\\'")')
    (24, '               shell("touch \\\'logs/collect_hits.log\\\'")')
    (25, '               shell("touch \\\'logs/concatenate.log\\\'")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=LCrossman/ribosomal_snakemake, file=workflow/rules/blast_missing_seqs.smk
context_key: ['output', 'run', 'else', "if params.protein_dna == \\'protein\\'"]
    (27, "              if params.protein_dna == \\'protein\\':")
    (28, "                   for ribo, name in [(ribo, name) for ribo in SAMPLES for name in [line.rstrip() for line in open(\\'strains_missing_ribos.txt\\', \\'r\\')]]:")
    (29, '                       shell("diamond blastp --query {0}.faa --db {1} --outfmt 6 --max-target-seqs 1 --out {1}.{0}.out".format(name, ribo))')
    (30, '                   shell("touch \\\'logs/blast_complete.txt\\\'")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=LCrossman/ribosomal_snakemake, file=workflow/Snakefile
context_key: ["if \\'protein\\' in params.type"]
    (87, "                   if \\'protein\\' in params.type:")
    (88, '                       shell("python workflow/scripts/gbk2faa.py {file}")')
    (89, "                   elif \\'dna\\' in params.type:")
    (90, '                       shell("python workflow/scripts/gbk2ffn.py {file}")')
    (91, '           shell("touch \\\'logs/conversion_complete.txt\\\'")')
    (92, '')
    (93, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/fusions, file=workflow/rules/common.smk
context_key: ['if not workflow.overwrite_configfiles']
    (17, 'if not workflow.overwrite_configfiles:')
    (18, '    sys.exit("At least one config file must be passed using --configfile/--configfiles, by command line or a profile!")')
    (19, '')
    (20, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lyijin/bismsmark, file=workflow/Snakefile
context_key: ["if line[0][0] == \\'#\\'"]
    (44, "    if line[0][0] == \\'#\\': continue")
    (45, '    ')
    (46, '    # non-comment lines: populate sample_details')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lyijin/bismsmark, file=workflow/Snakefile
context_key: ['if sample_id not in sample_details']
    (59, '    if sample_id not in sample_details:')
    (60, "        sample_details[sample_id] = {\\'short_id\\': short_id,")
    (61, "                                     \\'library_type\\': library_type,")
    (62, "                                     \\'R1_file\\': R1_file,")
    (63, "                                     \\'R2_file\\': R2_file,")
    (64, "                                     \\'mapped_genome\\': {mapped_genome: score_min}}")
    (65, '    else:')
    (66, "        assert short_id == sample_details[sample_id][\\'short_id\\']")
    (67, "        assert library_type == sample_details[sample_id][\\'library_type\\']")
    (68, "        assert R1_file == sample_details[sample_id][\\'R1_file\\']")
    (69, "        assert R2_file == sample_details[sample_id][\\'R2_file\\']")
    (70, "        assert mapped_genome not in sample_details[sample_id][\\'mapped_genome\\']")
    (71, '        ')
    (72, "        sample_details[sample_id][\\'mapped_genome\\'][mapped_genome] = score_min")
    (73, '')
    (74, '# define a couple of vars that will be used in "rule all"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Mira0507/snakemake_alignment, file=Snakefile
context_key: ['input', 'run', 'if len(input.fastq) == 2']
    (104, '        if len(input.fastq) == 2:    # if paired-end')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Mira0507/snakemake_alignment, file=Snakefile
context_key: ['input', 'run', 'if len(input.fastq) == 2']
    (136, '        if len(input.fastq) == 2:   # if paired-end')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=FrancescoFerrari88/ams_umc, file=Snakefile
context_key: ['onstart', 'if "verbose" in config and config["verbose"]']
    (25, '    if "verbose" in config and config["verbose"]:')
    (26, '        print("--- Workflow parameters --------------------------------------------------------")')
    (27, '        print("samples:", samples)')
    (28, '        print("paired:", pairedEnd)')
    (29, '        print("read extension:", reads)')
    (30, '        print("maximum insert size (Bowtie2 -X):", insertSizeMax)')
    (31, '        print("-" * 80, "\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=FrancescoFerrari88/ams_umc, file=Snakefile
context_key: ['onsuccess', 'if "verbose" in config and config["verbose"]']
    (53, '    if "verbose" in config and config["verbose"]:')
    (54, '        print("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=FrancescoFerrari88/ams_umc, file=internals.smk
context_key: ['if infiles == []']
    (8, 'if infiles == []:')
    (9, '    sys.exit("Error! Samples extension in {} are not {}. "')
    (10, '             "Please change the extensions to it or update the config.yaml file "')
    (11, '             "with your desired extension.".format(indir,ext))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=FrancescoFerrari88/ams_umc, file=internals.smk
context_key: ['if not samples']
    (18, 'if not samples:')
    (19, '    sys.exit("\\')
    (20, '  Error! NO samples found in dir "+str(indir or \\\'\\\')+"!!!\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=FrancescoFerrari88/ams_umc, file=internals.smk
context_key: ['if pairedEnd']
    (25, 'if pairedEnd:')
    (26, '    idxRange = 2')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not config']
    (13, 'if not config:')
    (14, '    raise SystemExit(')
    (15, '        "No config file specified. Feel free to use the test config as a"')
    (16, '        "template to generate a config file, and specify with --configfile")')
    (17, '')
    (18, '# Working paths')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not os.path.isdir(ROOT_DIR)']
    (33, 'if not os.path.isdir(ROOT_DIR):')
    (34, '    raise SystemExit("\\')
    (35, '  Path to iGUIDE is not found. Check environmental variables.\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not os.path.isdir(config["Seq_Path"])']
    (39, 'if not os.path.isdir(config["Seq_Path"]):')
    (40, '    raise SystemExit("\\')
    (41, '  Path to sequencing files is not found (Seq_Path). Check your config file.\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not os.path.isfile(RUN_DIR + "/config.yml")']
    (45, 'if not os.path.isfile(RUN_DIR + "/config.yml"):')
    (46, '    raise SystemExit("\\')
    (47, "  Path to symbolic config is not present. Check to make sure you\\'ve run \\'iguide setup\\' first.\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "Sample_Info" in config']
    (51, 'if not "Sample_Info" in config:')
    (52, '    raise SystemExit("\\')
    (53, '  Sample_Info parameter missing in config file. Please specify before continuing.\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if ".csv" in config["Sample_Info"]']
    (75, 'if ".csv" in config["Sample_Info"]:')
    (76, '    delim = ","')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['elif ".tsv" in config["Sample_Info"]']
    (77, 'elif ".tsv" in config["Sample_Info"]:')
    (78, '    delim = "\\\\t"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "maxNcount" in config']
    (85, 'if not "maxNcount" in config:')
    (86, '    config["maxNcount"] = 1')
    (87, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "demultiCores" in config']
    (88, 'if not "demultiCores" in config: ')
    (89, '    demulti_cores = snakemake.utils.available_cpu_count()')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "skipDemultiplexing" in config']
    (95, 'if not "skipDemultiplexing" in config:')
    (96, '    config["skipDemultiplexing"] = False')
    (97, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "Alternate_UMI_Method" in config']
    (98, 'if not "Alternate_UMI_Method" in config:')
    (99, '    config["Alternate_UMI_Method"] = False')
    (100, '    ')
    (101, '')
    (102, '# Sample information')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "demultiMB" in config']
    (126, 'if not "demultiMB" in config:')
    (127, '    config["demultiMB"] = 16000')
    (128, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "trimMB" in config']
    (129, 'if not "trimMB" in config:')
    (130, '    config["trimMB"] = 4000')
    (131, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "filtMB" in config']
    (132, 'if not "filtMB" in config:')
    (133, '    config["filtMB"] = 4000')
    (134, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "consolMB" in config']
    (135, 'if not "consolMB" in config:')
    (136, '    config["consolMB"] = 4000')
    (137, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "alignMB" in config']
    (138, 'if not "alignMB" in config:')
    (139, '    config["alignMB"] = 4000')
    (140, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "qualCtrlMB" in config']
    (141, 'if not "qualCtrlMB" in config:')
    (142, '    config["qualCtrlMB"] = 8000')
    (143, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "assimilateMB" in config']
    (144, 'if not "assimilateMB" in config:')
    (145, '    config["assimilateMB"] = 4000')
    (146, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "evaluateMB" in config']
    (147, 'if not "evaluateMB" in config:')
    (148, '    config["evaluateMB"] = 4000')
    (149, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "reportMB" in config']
    (150, 'if not "reportMB" in config:')
    (151, '    config["reportMB"] = 4000')
    (152, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "bins" in config']
    (153, 'if not "bins" in config:')
    (154, '    config["bins"] = 5')
    (155, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "level" in config']
    (156, 'if not "level" in config:')
    (157, '    config["level"] = 300000')
    (158, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if not "readNamePattern" in config']
    (159, 'if not "readNamePattern" in config:')
    (160, '    config["readNamePattern"] = str("\\\'[\\\\\\\\w\\\\\\\\:\\\\\\\\-\\\\\\\\+]+\\\'")')
    (161, '')
    (162, '')
    (163, '# Define BINS')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if (config["Alternate_UMI_Method"])']
    (187, 'if (config["Alternate_UMI_Method"]):')
    (188, '    include: "rules/arch.umi_alt_method.rules"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if (config["skipDemultiplexing"])']
    (193, 'if (config["skipDemultiplexing"]):')
    (194, '    include: "rules/skip_demulti.rules"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if (config["Alternate_UMI_Method"])']
    (200, 'if (config["Alternate_UMI_Method"]):')
    (201, '    include: "rules/trim.umi_alt_method.rules"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if (config["UMItags"])', 'if (config["Alternate_UMI_Method"])']
    (205, 'if (config["UMItags"]):')
    (206, '    if (config["Alternate_UMI_Method"]):')
    (207, '        include: "rules/umitag.umi_alt_method.rules"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if (config["UMItags"])', 'else']
    (208, '    else:')
    (209, '        include: "rules/umitag.rules"')
    (210, '        UMIseqs = sampleInfo["barcode2"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if (config["Aligner"] == "BLAT" or config["Aligner"] == "blat")', 'if (config["Alternate_UMI_Method"])']
    (216, 'if (config["Aligner"] == "BLAT" or config["Aligner"] == "blat"):')
    (217, '    include: "rules/consol.rules"')
    (218, '    include: "rules/align.blat.rules"')
    (219, '    if (config["Alternate_UMI_Method"]):')
    (220, '        include: "rules/quality.blat.umi_alt_method.rules"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['if (config["Aligner"] == "BLAT" or config["Aligner"] == "blat")', 'else']
    (221, '    else:')
    (222, '        include: "rules/quality.blat.rules"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['elif (config["Aligner"] == "BWA" or config["Aligner"] == "bwa")', 'if (config["Alternate_UMI_Method"])']
    (223, 'elif (config["Aligner"] == "BWA" or config["Aligner"] == "bwa"):')
    (224, '    include: "rules/consol_stub.rules"')
    (225, '    if (config["Alternate_UMI_Method"]):')
    (226, '        include: "rules/align.bwa.umi_alt_method.rules"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['elif (config["Aligner"] == "BWA" or config["Aligner"] == "bwa")', 'else']
    (227, '    else:')
    (228, '        include: "rules/align.bwa.rules"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnobles/iGUIDE, file=Snakefile
context_key: ['elif (config["Aligner"] == "BWA" or config["Aligner"] == "bwa")', 'include']
    (229, '    include: "rules/quality.sam.rules"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nixonlab/HERV_scGTEX, file=workflow/rules/references.smk
context_key: ["if l[2] == \\'gene\\'", "if d[\\'gene_id\\'] in g_sym"]
    (77, "            if l[2] == \\'gene\\':")
    (78, "                if d[\\'gene_id\\'] in g_sym:")
    (79, '                    assert g_sym[d[\\\'gene_id\\\']] == d[\\\'gene_name\\\'], "Gene name mismatch: %s %s" % (d[\\\'gene_name\\\'], g_sym[d[\\\'gene_id\\\']])')
    (80, "                g_sym[d[\\'gene_id\\']] = d[\\'gene_name\\']")
    (81, '')
    (82, "            if l[2] == \\'transcript\\':")
    (83, "                if d[\\'transcript_id\\'] in tx_g:")
    (84, '                    assert tx_g[d[\\\'transcript_id\\\']] == d[\\\'gene_id\\\'], "Gene ID mismatch: %s %s" % (d[\\\'gene_id\\\'], tx_g[d[\\\'transcript_id\\\']])')
    (85, "                tx_g[d[\\'transcript_id\\']] = d[\\'gene_id\\']")
    (86, '')
    (87, '        # Generate ttg')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nixonlab/HERV_scGTEX, file=workflow/rules/references.smk
context_key: ["if l[2] == \\'gene\\'"]
    (88, "        with open(output[0], \\'w\\') as outh:")
    (89, "            print(\\'TXNAME\\\\tGENEID\\', file=outh)")
    (90, "            txlist = (l.strip(\\'\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hivlab/sarscov2-variation, file=workflow/Snakefile
context_key: ['if os.path.exists(v if v else ""']
    (326, '    if os.path.exists(v if v else "")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/gbs-genetic-fitness, file=Snakefile
context_key: ['if "COMPARISON_GENOME" in config']
    (17, 'if "COMPARISON_GENOME" in config:')
    (18, '    comp_genome_results = expand("results/genome_alignment/{prefix}_{species}.png", prefix=PREFIX, species = config["COMPARISON_GENOME"].keys())')
    (19, '    MIN_ALIGNMENT_LENGTH = config["MIN_ALIGNMENT_LENGTH"]')
    (20, '    MIN_QUERY_LENGTH = config["MIN_QUERY_LENGTH"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/gbs-genetic-fitness, file=Snakefile
context_key: ['if RUN_SCAFFOLDING == "N"']
    (24, 'if RUN_SCAFFOLDING == "N":')
    (25, '    extra_genome_stats = []')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/gbs-genetic-fitness, file=Snakefile
context_key: ['elif RUN_SCAFFOLDING == "Y"']
    (26, 'elif RUN_SCAFFOLDING == "Y":')
    (27, '    extra_genome_stats = f"results/assembly_stats_{PREFIX}_new.txt"')
    (28, '')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=msabrysarhan/Barfusser_microbiome, file=Snakefile
context_key: ['if new_sample not in samples']
    (12, '    if new_sample not in samples:')
    (13, '        samples.append(new_sample)')
    (14, '')
    (15, '##Snakemake rules')
    (16, '#hallo')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/Struo, file=Snakefile
context_key: ["if not os.path.isfile(config['samples_file'])"]
    (20, "if not os.path.isfile(config['samples_file']):")
    (21, "    raise IOError('Cannot find file: {}'.format(config['samples_file']))")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/Struo, file=Snakefile
context_key: ["if f not in config['samples'].columns"]
    (24, "    if f not in config['samples'].columns:")
    (25, "        raise ValueError('Cannot find column: {}'.format(f))")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snystrom/cutNrun-pipeline, file=Snakefile
context_key: ['if os.path.exists(file_info_path) == False', 'if type(REFGENOME) is not str', 'if type(SPIKEGENOME) is str', 'if type(SPIKEGENOME) is not list', 'rule all', 'rule combine_technical_reps', 'rule fastQC', 'rule trim_adapter', 'rule fastQC_trim', 'rule fastqScreen', 'rule bowtie2index']
    (22, 'if os.path.exists(file_info_path) == False:')
    (23, "\\tsys.exit(\\'Error: {name} does not exist. Be sure to set `sampleInfo` in config.json.\\'.format(name = file_info_path))")
    (24, '')
    (25, 'if type(REFGENOME) is not str:')
    (26, "\\tsys.exit(\\'Error: refGenome must be a string. Currently set to: {}. Double check `refGenome` in config.json.\\'.format(REFGENOME))")
    (27, '')
    (28, 'if type(SPIKEGENOME) is str:')
    (29, '\\t# Convert spikegenome to list if not already')
    (30, '\\t# allows users to pass single genomes as e.g. "spikeGenome" = "sacCer3" in config.json')
    (31, '\\t# hopefully cutting down on errors for forgetting []')
    (32, '\\t# Also ensures type safety for SPIKEGENOME')
    (33, '\\tSPIKEGENOME = [SPIKEGENOME]')
    (34, '')
    (35, 'if type(SPIKEGENOME) is not list:')
    (36, "\\tsys.exit(\\'Error: spikeGenome entry in config.json is invalid. Entry must be a string or an array of strings. Currently set to: {}\\'.format(SPIKEGENOME))")
    (37, '')
    (38, "#TODO: check that spikegenomes are inside config[\\'genome\\'] & has all necessary files")
    (39, "#TODO: check that refgenome is inside config[\\'genome\\'] & has all necessary files")
    (40, '')
    (41, '#########')
    (42, '# Generating sampleSheet outputs')
    (43, 'speciesList  = [REFGENOME] + SPIKEGENOME')
    (44, '#speciesList.append(SPIKEGENOME)')
    (45, '')
    (46, "combinedGenome = \\'-\\'.join(speciesList)")
    (47, '#TODO: remove')
    (48, '#indexDict    = {REFGENOME: REFGENOMEPATH, SPIKEGENOME: SPIKEGENOMEPATH}')
    (49, '')
    (50, '# TODO: fix _spikeNorm -> _{species}-spikeNorm')
    (51, "fragTypes    = [\\'allFrags\\', \\'20to120\\', \\'150to700\\']")
    (52, 'normTypeList = [\\\'\\\', \\\'_rpgcNorm\\\'] + ["_{}-spikeNorm".format(species) for species in SPIKEGENOME]')
    (53, '')
    (54, 'sampleInfo, sampleSheet = pre.makeSampleSheets(file_info_path, basename_columns, "-", fileDelimiter = config[\\\'sampleInfoDelimiter\\\'])')
    (55, 'poolSampleSheet = sampleSheet.copy()')
    (56, '')
    (57, 'sampleSheet[\\\'fastq_trim_r1\\\'] = expand("Fastq/{sample}_R{num}_trim.fastq.gz", sample = sampleSheet.baseName, num = [\\\'1\\\'])')
    (58, 'sampleSheet[\\\'fastq_trim_r2\\\'] = expand("Fastq/{sample}_R{num}_trim.fastq.gz", sample = sampleSheet.baseName, num = [\\\'2\\\'])')
    (59, 'sampleSheet[\\\'bam\\\']           = expand("Bam/{sample}_{species}_trim_q5_dupsRemoved.{ftype}", sample = sampleSheet.baseName, species = REFGENOME, ftype = {"bam"})')
    (60, '')
    (61, 'for frag, norm in zip(fragTypes, normTypeList):')
    (62, '\\t# Add column per peak call list')
    (63, "\\tpeak_colName = \\'peak_{frag}\\'.format(frag = frag)")
    (64, '\\tsampleSheet[peak_colName] = expand("Peaks/{sample}_{species}_trim_q5_dupsRemoved_{fragType}_peaks.narrowPeak", sample = sampleSheet.baseName, species = REFGENOME, fragType = frag)')
    (65, '\\t')
    (66, "\\tbed_colName = \\'bed_{frag}\\'.format(frag = frag)")
    (67, "\\tsampleSheet[bed_colName] = expand(\\'Bed/{sample}_{species}_trim_q5_dupsRemoved_{fragType}.bed\\', sample = sampleSheet.baseName, species = REFGENOME, fragType = frag)")
    (68, '\\t')
    (69, '')
    (70, '# Add columns for all combinations of fragType & normType')
    (71, '## Create column names:')
    (72, 'fragTypeCombn = fragTypes * len(normTypeList)')
    (73, 'fragTypeCombn.sort()')
    (74, '')
    (75, 'normTypeCombn = normTypeList * len(fragTypes)')
    (76, '')
    (77, "fragNormCombn = [\\'\\'.join(frag + norm) for frag, norm in zip(fragTypeCombn, normTypeCombn)]")
    (78, '')
    (79, '## Add columns to sampleSheet')
    (80, 'for fragNorm in fragNormCombn:')
    (81, '')
    (82, '\\t# Add column per bigwig')
    (83, '\\tbw_colName = \\\'bigwig_{fragNorm}\\\'.format(fragNorm = fragNorm.replace("-", "_"))')
    (84, '\\tsampleSheet[bw_colName] = expand("BigWig/{sample}_{species}_trim_q5_dupsRemoved_{fragNorm}.bw", sample = sampleSheet.baseName, species = REFGENOME, fragNorm = fragNorm) ')
    (85, '')
    (86, '\\t# Add column per zNorm bigwig')
    (87, '\\tbw_colName = \\\'zNorm_bigwig_{fragNorm}\\\'.format(fragNorm = fragNorm.replace("-", "_"))')
    (88, '\\tsampleSheet[bw_colName] = expand("BigWig/{sample}_{species}_trim_q5_dupsRemoved_{fragNorm}_zNorm.bw", sample = sampleSheet.baseName, species = REFGENOME, fragNorm = fragNorm) ')
    (89, '')
    (90, '\\t# Threshold peakcalls:')
    (91, '\\tthresh_colName = \\\'threshold_peaks_{fragNorm}\\\'.format(fragNorm = fragNorm.replace("-", "_"))')
    (92, "\\tsampleSheet[thresh_colName] = expand(\\'Threshold_PeakCalls/{sample}_{species}_trim_q5_dupsRemoved_{fragNorm}_thresholdPeaks.bed\\', sample = sampleSheet.baseName, species = REFGENOME, fragNorm = fragNorm)")
    (93, '')
    (94, 'sampleSheet.to_csv(\\\'sampleSheet.tsv\\\', sep = "\\\\t", index = False)')
    (95, '')
    (96, '')
    (97, '####')
    (98, '# BEGIN PIPELINE:')
    (99, '####')
    (100, '')
    (101, '# TODO: remove')
    (102, 'localrules: all, collect_genome_align_stats')
    (103, '')
    (104, 'rule all:')
    (105, '\\tinput:')
    (106, '\\t\\texpand("Fastq/{sample}_R{num}_trim.fastq.gz", sample = sampleSheet.baseName, num = [\\\'1\\\',\\\'2\\\']),')
    (107, '\\t\\texpand("Sam/{sample}_{species}_trim.sam", sample = sampleSheet.baseName, species = combinedGenome),')
    (108, '\\t\\texpand("Bam/{sample}_{species}_trim_q5_dupsRemoved.{ftype}", sample = sampleSheet.baseName, species = speciesList, ftype = {"bam", "bam.bai"}),')
    (109, '\\t\\texpand("Logs/{sample}_{species}_trim_q5_dupsRemoved_genomeStats.tsv", sample = sampleSheet.baseName, species = combinedGenome),')
    (110, '\\t\\texpand("BigWig/{sample}_{species}_trim_q5_dupsRemoved_{fragType}{normType}.{ftype}", sample = sampleSheet.baseName, species = REFGENOME, fragType = fragTypes, normType = normTypeList, ftype = {"bw", "bg"}),')
    (111, '\\t\\texpand("Peaks/{sample}_{species}_trim_q5_dupsRemoved_{fragType}_peaks.narrowPeak", sample = sampleSheet.baseName, species = REFGENOME, fragType = fragTypes),')
    (112, "\\t\\texpand(\\'Threshold_PeakCalls/{sample}_{species}_trim_q5_dupsRemoved_{fragType}{normType}_thresholdPeaks.bed\\', sample = sampleSheet.baseName, species = REFGENOME, fragType = fragTypes, normType = normTypeList),")
    (113, "\\t\\texpand(\\'FastQC/{sample}_R1_fastqc.html\\', sample = sampleSheet.baseName),")
    (114, "\\t\\texpand(\\'FastQC/{sample}_R1_trim_fastqc.html\\', sample = sampleSheet.baseName),")
    (115, "\\t\\texpand(\\'FQscreen/{sample}_R1_trim_screen.txt\\', sample = sampleSheet.baseName),")
    (116, "\\t\\texpand(\\'FQscreen/{sample}_R1_trim_screen.html\\', sample = sampleSheet.baseName),")
    (117, '\\t\\t"multiqc_report.html",')
    (118, "\\t\\texpand(\\'Plots/FragDistInPeaks/{sample}_{REFGENOME}_trim_q5_allFrags_fragDistPlot.png\\', sample = sampleSheet.baseName, REFGENOME = REFGENOME),")
    (119, "\\t\\texpand(\\'BigWig/{sample}_{REFGENOME}_trim_q5_dupsRemoved_{fragType}_rpgcNorm_zNorm.bw\\', sample = sampleSheet.baseName, REFGENOME = REFGENOME, fragType = fragTypes),")
    (120, '\\t\\texpand("AlignmentStats/{sample}_{species}_trim.tsv", sample = sampleSheet.baseName, species = combinedGenome),')
    (121, '\\t\\texpand("AlignmentStats/{sample}_{species}_trim_q5.tsv", sample = sampleSheet.baseName, species = combinedGenome),')
    (122, '\\t\\texpand("AlignmentStats/{sample}_{species}_trim_q5_dupsRemoved.tsv", sample = sampleSheet.baseName, species = combinedGenome)')
    (123, '')
    (124, '')
    (125, 'rule combine_technical_reps:')
    (126, '\\tinput:')
    (127, '\\t\\tr1 = lambda wildcards : sampleInfo[sampleInfo.baseName == wildcards.sample].fastq_r1,')
    (128, '\\t\\tr2 = lambda wildcards : sampleInfo[sampleInfo.baseName == wildcards.sample].fastq_r2')
    (129, '\\toutput:')
    (130, "\\t\\tr1 = \\'Fastq/{sample}_R1.fastq.gz\\',")
    (131, "\\t\\tr2 = \\'Fastq/{sample}_R2.fastq.gz\\'")
    (132, '\\tshell:')
    (133, '\\t\\t"""')
    (134, '\\t\\tcat {input.r1} > {output.r1} &&')
    (135, '\\t\\tcat {input.r2} > {output.r2}')
    (136, '\\t\\t"""')
    (137, '')
    (138, 'rule fastQC:')
    (139, '\\tinput:')
    (140, "\\t\\t\\'Fastq/{sample}_R1.fastq.gz\\',")
    (141, '\\toutput:')
    (142, "\\t\\t\\'FastQC/{sample}_R1_fastqc.html\\'")
    (143, '\\tenvmodules:')
    (144, "\\t\\tmodules[\\'fastqcVer\\']")
    (145, '\\tshell:')
    (146, '\\t\\t"""')
    (147, '\\t\\tfastqc -o ./FastQC/ -f fastq {input}')
    (148, '\\t\\t"""')
    (149, '')
    (150, 'rule trim_adapter:')
    (151, '\\tinput:')
    (152, '\\t\\tr1 = "Fastq/{sample}_R1.fastq.gz",')
    (153, '\\t\\tr2 = "Fastq/{sample}_R2.fastq.gz"')
    (154, '\\toutput:')
    (155, '\\t\\tr1 = "Fastq/{sample}_R1_trim.fastq.gz",')
    (156, '\\t\\tr2 = "Fastq/{sample}_R2_trim.fastq.gz"')
    (157, '\\tlog:')
    (158, "\\t\\tadapterStats = \\'Logs/{sample}_adapterStats\\',")
    (159, "\\t\\ttrimStats = \\'Logs/{sample}_trimStats\\'")
    (160, '\\tenvmodules:')
    (161, "\\t\\tmodules[\\'bbmapVer\\']")
    (162, '\\tshell:')
    (163, '\\t\\t"""')
    (164, '\\t\\tbbduk.sh in1={input.r1} in2={input.r2} out1={output.r1} out2={output.r2} ktrim=r ref=adapters rcomp=t tpe=t tbo=t hdist=1 mink=11 stats={log.adapterStats} > {log.trimStats}')
    (165, '\\t\\t"""')
    (166, '\\t\\t#bbduk.sh in1={input.r1} in2={input.r2} out1={output.r1} out2={output.r2} ktrim=r ref=adapters rcomp=t tpe=t tbo=t hdist=1 mink=11')
    (167, '')
    (168, 'rule fastQC_trim:')
    (169, '\\tinput:')
    (170, "\\t\\t\\'Fastq/{sample}_R1_trim.fastq.gz\\',")
    (171, '\\toutput:')
    (172, "\\t\\t\\'FastQC/{sample}_R1_trim_fastqc.html\\'")
    (173, '\\tenvmodules:')
    (174, "\\t\\tmodules[\\'fastqcVer\\']")
    (175, '\\tshell:')
    (176, '\\t\\t"""')
    (177, '\\t\\tfastqc -o ./FastQC/ -f fastq {input}')
    (178, '\\t\\t"""')
    (179, '')
    (180, 'rule fastqScreen:')
    (181, '\\tinput:')
    (182, "\\t\\t\\'Fastq/{sample}_R1_trim.fastq.gz\\'")
    (183, '\\toutput:')
    (184, "\\t\\ttxt = \\'FQscreen/{sample}_R1_trim_screen.txt\\',")
    (185, "\\t\\thtml = \\'FQscreen/{sample}_R1_trim_screen.html\\'")
    (186, '\\tparams:')
    (187, "\\t\\tfqscreenPath = modules[\\'fqscreenPath\\'],")
    (188, "\\t\\tfqscreenConf = modules[\\'fqscreenConf\\']")
    (189, '\\tthreads: 4')
    (190, '\\tshell:')
    (191, '\\t\\t"""')
    (192, '\\t\\t{params.fqscreenPath} --threads {threads} --force --aligner bowtie2 -conf {params.fqscreenConf} {input} --outdir ./FQscreen/')
    (193, '\\t\\t"""')
    (194, '')
    (195, 'def get_genome_fastas(config, speciesList):')
    (196, '\\t"""')
    (197, '\\tTakes config info & list of species as input, returns dict where keys = species name, values = genome fasta paths')
    (198, '\\t"""')
    (199, "\\tget_fasta = lambda species : (species, config[\\'genome\\'][species][\\'fasta\\'])")
    (200, '\\tfastas = {genome:fasta for (genome, fasta) in [get_fasta(s) for s in speciesList]}')
    (201, '\\treturn(fastas)')
    (202, '')
    (203, 'genome_fastas = get_genome_fastas(config, speciesList)')
    (204, '')
    (205, 'rule bowtie2index:')
    (206, '    \\tinput:')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=metagenome-atlas/genome-annotator, file=Snakefile
context_key: ["if not \\'input\\' in config"]
    (8, "if not \\'input\\' in config:")
    (9, '    raise IOError("need a \\\'input\\\' argument in the config file")')
    (10, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=metagenome-atlas/genome-annotator, file=Snakefile
context_key: ["if \\',\\' in config[\\'input\\']"]
    (11, "if \\',\\' in config[\\'input\\']:")
    (12, "    input_files= config[\\'input\\'].strip().split(\\',\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=metagenome-atlas/genome-annotator, file=Snakefile
context_key: ["elif os.path.isdir(config[\\'input\\'])"]
    (13, "elif os.path.isdir(config[\\'input\\']):")
    (14, '    input_files = glob(config[\\\'input\\\'] +"/*.*")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=metagenome-atlas/genome-annotator, file=Snakefile
context_key: ["if not extension in [\\'.fna\\',\\'.fasta\\',\\'.fa_nt\\']"]
    (26, "if not extension in [\\'.fna\\',\\'.fasta\\',\\'.fa_nt\\']:")
    (27, '    raise IOError(f"Files have {extension} extension which is not a correct fasta nucleotide extension")')
    (28, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['params', 'run', 'if not os.path.exists(sub_dir)']
    (684, '            if not os.path.exists(sub_dir):')
    (685, '                os.mkdir(sub_dir)')
    (686, '        # list viridian assemblies')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['run', 'if not os.path.exists(sub_dir)']
    (721, '            if not os.path.exists(sub_dir):')
    (722, '                os.mkdir(sub_dir)')
    (723, '        # list artic assemblies')
    (724, '        art_assemblies = sorted([f for f in input if "artic_ART" in f and len(f.split("/")) == 2])')
    (725, '        badread_assemblies = sorted([f for f in input if "epi2me_Badread" in f and len(f.split("/")) == 2])')
    (726, '        truth_vcfs = sorted([f for f in input if "truth_vcfs" in f])')
    (727, '        # run covid truth eval')
    (728, '        run_cte(params.primer_scheme,')
    (729, '                art_assemblies,')
    (730, '                os.path.join(output[0], "ART_assemblies"),')
    (731, '                os.path.dirname(truth_vcfs[0]),')
    (732, '                params.container_dir,')
    (733, '                "artic")')
    (734, '        run_cte(params.primer_scheme,')
    (735, '                badread_assemblies,')
    (736, '                os.path.join(output[0], "Badread_assemblies"),')
    (737, '                os.path.dirname(truth_vcfs[0]),')
    (738, '                params.container_dir,')
    (739, '                "artic")')
    (740, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['run', 'if not os.path.exists("usher_phylogenies")']
    (752, '        if not os.path.exists("usher_phylogenies"):')
    (753, '            os.mkdir("usher_phylogenies")')
    (754, '        # list assemblies')
    (755, '        aa_assemblies = [v for v in input if len(v.split("/")) == 2]')
    (756, '        # write tsv of assemblies for ushonium input')
    (757, '        samples_rows = []')
    (758, '        for a in aa_assemblies:')
    (759, '            name = os.path.basename(a)')
    (760, '            filename = os.path.join(a, "consensus_trimmed.fa")')
    (761, '            samples_rows.append(name + "\\\\t" + filename)')
    (762, '        tsv_path = os.path.join("usher_phylogenies", "artic_ART.tsv")')
    (763, '        with open(tsv_path, "w") as o:')
    (764, '            o.write("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['run', 'if not os.path.exists("usher_phylogenies")']
    (782, '        if not os.path.exists("usher_phylogenies"):')
    (783, '            os.mkdir("usher_phylogenies")')
    (784, '        # list assemblies')
    (785, '        eb_assemblies = [v for v in input if len(v.split("/")) == 2]')
    (786, '        # write tsv of assemblies for ushonium input')
    (787, '        samples_rows = []')
    (788, '        for a in eb_assemblies:')
    (789, '            name = os.path.basename(a)')
    (790, '            filename = os.path.join(a, "consensus_trimmed.fa")')
    (791, '            samples_rows.append(name + "\\\\t" + filename)')
    (792, '        tsv_path = os.path.join("usher_phylogenies", "epi2me_Badread.tsv")')
    (793, '        with open(tsv_path, "w") as o:')
    (794, '            o.write("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['run', 'if not os.path.exists("usher_phylogenies")']
    (812, '        if not os.path.exists("usher_phylogenies"):')
    (813, '            os.mkdir("usher_phylogenies")')
    (814, '        # list assemblies')
    (815, '        va_assemblies = [v for v in input if len(v.split("/")) == 2]')
    (816, '        # write tsv of assemblies for ushonium input')
    (817, '        samples_rows = []')
    (818, '        for a in va_assemblies:')
    (819, '            name = os.path.basename(a)')
    (820, '            filename = os.path.join(a, "consensus.fa")')
    (821, '            samples_rows.append(name + "\\\\t" + filename)')
    (822, '        tsv_path = os.path.join("usher_phylogenies", "viridian_ART.tsv")')
    (823, '        with open(tsv_path, "w") as o:')
    (824, '            o.write("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/viridian_simulations_workflow, file=Snakefile
context_key: ['run', 'if not os.path.exists("usher_phylogenies")']
    (842, '        if not os.path.exists("usher_phylogenies"):')
    (843, '            os.mkdir("usher_phylogenies")')
    (844, '        # list assemblies')
    (845, '        vb_assemblies = [v for v in input if len(v.split("/")) == 2]')
    (846, '        # write tsv of assemblies for ushonium input')
    (847, '        samples_rows = []')
    (848, '        for a in vb_assemblies:')
    (849, '            name = os.path.basename(a)')
    (850, '            filename = os.path.join(a, "consensus.fa")')
    (851, '            samples_rows.append(name + "\\\\t" + filename)')
    (852, '        tsv_path = os.path.join("usher_phylogenies", "viridian_Badread.tsv")')
    (853, '        with open(tsv_path, "w") as o:')
    (854, '            o.write("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=apoire27/BodennmillerPipeline, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'if len(fol_path_spillover_slide_acs) > 0']
    (117, 'if do_compensation:')
    (118, '    if len(file_path_orig_sm) == 0:')
    (119, '        if len(fol_path_spillover_slide_acs) > 0:')
    (120, "            spillover_mode = \\'estimate_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=apoire27/BodennmillerPipeline, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'else']
    (121, '        else:')
    (122, '            raise ValueError(\\\'Either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (123, "                             \\'needs to be provided.\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=apoire27/BodennmillerPipeline, file=workflow/Snakefile
context_key: ['if do_compensation', 'elif len(fol_path_spillover_slide_acs) == 0']
    (124, '    elif len(fol_path_spillover_slide_acs) == 0:')
    (125, "            spillover_mode = \\'copy_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=apoire27/BodennmillerPipeline, file=workflow/Snakefile
context_key: ['if do_compensation', 'else']
    (126, '    else:')
    (127, '        raise ValueError(\\\'Only provide either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (128, "                         \\'but not both.\\')")
    (129, '')
    (130, '# Produce a list of all cellprofiler output files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=apoire27/BodennmillerPipeline, file=workflow/Snakefile
context_key: ['if not do_compensation']
    (267, 'if not do_compensation:')
    (268, "    config_dict_cp[\\'measuremasks\\'] = {")
    (269, '        \\\'message\\\':  """\\\\')
    (270, '                    Measure masks ')
    (271, '                    ')
    (272, '                    Measure segmentation masks')
    (273, '                    """,')
    (274, "        \\'run_size\\': 1,")
    (275, "        \\'plugins\\': cp_plugins,")
    (276, "        \\'pipeline\\': \\'resources/cp4_pipelines/adapted/3_measure_mask_basic.cppipe\\',")
    (277, "        \\'input_files\\': [")
    (278, '            # Folder containing segmentation masks')
    (279, '            fol_path_cellmasks,')
    (280, '            # Folder containing image stacks to measure')
    (281, '            fol_path_full,')
    (282, '            # Folder containing cell probabilities')
    (283, '            fol_path_probabilities,')
    (284, '            # Acquisition metadata')
    (285, '            file_path_acmeta,')
    (286, '            # Channel metadata')
    (287, '            file_path_channelmeta_full,')
    (288, '            file_path_channelmeta_cellprobab')
    (289, '        ],')
    (290, "        \\'output_patterns\\': {\\'.\\':")
    (291, '                            # Folder containing cellprofiler measurement')
    (292, '                                cp_measurements_output_files,')
    (293, '                            # Subfolder containing segmentation masks used')
    (294, '                            # for the measurements')
    (295, "                            \\'masks\\': directory(fol_path_cp_masks),")
    (296, '                            # Subfolder containing the images used for the')
    (297, '                            # measurements.')
    (298, "                            \\'images\\': directory(fol_path_cp_images)},")
    (299, "        \\'input_folder\\': fol_path_cp_input")
    (300, '    }')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=apoire27/BodennmillerPipeline, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'params']
    (454, 'if do_compensation:')
    (455, '    if spillover_mode == "estimate_sm":')
    (456, '        rule get_ss_panel_from_panel:')
    (457, '            input: csv_panel')
    (458, '            output: file_path_ss_panel')
    (459, '            params:')
    (460, '                col_metal=csv_panel_metal,')
    (461, '                col_spillover=csv_panel_spillover')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=apoire27/BodennmillerPipeline, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'run']
    (462, '            run:')
    (463, '                dat_csv = pd.read_csv(csv_panel)')
    (464, '                assert params.col_spillover in dat_csv.columns, ValueError(')
    (465, '                    f"The panel csv {csv_panel} should contain a boolean column"')
    (466, '                    f"{params.col_spillover} indicating the metals used for the"')
    (467, '                    f"spillover acquisitions.")')
    (468, '                metals = dat_csv.query(')
    (469, "                    f\\'{params.col_spillover} == 1\\')[params.col_metal].values")
    (470, "                with open(output[0], \\'w\\') as f:")
    (471, '                    for m in metals:')
    (472, "                        f.write(m+\\'\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bricoletc/single-cell-rna-seq-4predocs, file=Snakefile
context_key: ['if "markers" in config.get("celltype", {})']
    (17, 'if "markers" in config.get("celltype", {}):')
    (18, '    markers = pd.read_csv(config["celltype"]["markers"], sep="\\\\t").set_index("name", drop=False)')
    (19, '    markers.loc[:, "parent"].fillna("root", inplace=True)')
    (20, '')
    (21, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bioinfo-dirty-jobs/rna-seq-star-deseq2, file=rules/align.smk
context_key: ['if not is_single_end(**wildcards)']
    (1, '    if not is_single_end(**wildcards):')
    (2, '        # paired-end sample')
    (3, '        return expand("trimmed/{sample}-{unit}.{group}.fastq.gz",')
    (4, '                      group=[1, 2], **wildcards)')
    (5, '    # single end sample')
    (6, '    return "trimmed/{sample}-{unit}.fastq.gz".format(**wildcards)')
    (7, '')
    (8, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skchronicles/RNA-seek, file=workflow/Snakefile
context_key: ['if paired_end', 'rule all', 'params']
    (51, 'if paired_end:')
    (52, '    # Paired-end Target files')
    (53, '    rule all:')
    (54, '        params:')
    (55, "            batch=\\'--time=168:00:00\\',")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skchronicles/RNA-seek, file=workflow/Snakefile
context_key: ['if paired_end', 'rule all', 'input']
    (56, '        input:')
    (57, '            # FastQValidator')
    (58, '            expand(join(workpath,"rawQC","{name}.validated.R1.fastq.log"), name=provided(samples, wf_api.use_singularity)),')
    (59, '            expand(join(workpath,"rawQC","{name}.validated.R2.fastq.log"), name=provided(samples, wf_api.use_singularity)),')
    (60, '')
    (61, '            # Flowcell Lane information')
    (62, '            expand(join(workpath,"rawQC","{name}.fastq.info.txt"),name=samples),')
    (63, '')
    (64, '            # FastQC (before and after trimming)')
    (65, '            expand(join(workpath,"rawQC","{name}.R1_fastqc.zip"), name=samples),')
    (66, '            expand(join(workpath,"rawQC","{name}.R2_fastqc.zip"), name=samples),')
    (67, '            expand(join(workpath,"QC","{name}.R1.trim_fastqc.zip"), name=samples),')
    (68, '            expand(join(workpath,"QC","{name}.R2.trim_fastqc.zip"), name=samples),')
    (69, '')
    (70, '            # BBtools Insert Size')
    (71, '            expand(join(workpath,"QC","{name}_insert_sizes.txt"), name=samples),')
    (72, '')
    (73, '            # FastScreen (Using two sets of reference databases)')
    (74, '            expand(join(workpath,"FQscreen","{name}.R1.trim_screen.txt"),name=samples),')
    (75, '            expand(join(workpath,"FQscreen","{name}.R1.trim_screen.png"),name=samples),')
    (76, '            expand(join(workpath,"FQscreen","{name}.R2.trim_screen.txt"),name=samples),')
    (77, '            expand(join(workpath,"FQscreen","{name}.R2.trim_screen.png"),name=samples),')
    (78, '            expand(join(workpath,"FQscreen2","{name}.R1.trim_screen.txt"),name=samples),')
    (79, '            expand(join(workpath,"FQscreen2","{name}.R1.trim_screen.png"),name=samples),')
    (80, '            expand(join(workpath,"FQscreen2","{name}.R2.trim_screen.txt"),name=samples),')
    (81, '            expand(join(workpath,"FQscreen2","{name}.R2.trim_screen.png"),name=samples),')
    (82, '')
    (83, '            # Kraken + Krona')
    (84, '            expand(join(workpath,kraken_dir,"{name}.trim.kraken_bacteria.taxa.txt"),name=samples),')
    (85, '            expand(join(workpath,kraken_dir,"{name}.trim.kraken_bacteria.krona.html"),name=samples),')
    (86, '')
    (87, '            # STAR')
    (88, '            expand(join(workpath,bams_dir,"{name}.p2.Aligned.toTranscriptome.out.bam"),name=samples),')
    (89, '')
    (90, '            # Arriba')
    (91, '            expand(join(workpath,"fusions","{name}_fusions.tsv"),name=provided(samples, references(config, pfamily, [\\\'FUSIONBLACKLIST\\\', \\\'FUSIONCYTOBAND\\\', \\\'FUSIONPROTDOMAIN\\\']))),')
    (92, '            expand(join(workpath,"fusions","{name}_fusions.arriba.pdf"),name=provided(samples, references(config, pfamily, [\\\'FUSIONBLACKLIST\\\', \\\'FUSIONCYTOBAND\\\', \\\'FUSIONPROTDOMAIN\\\']))),')
    (93, '')
    (94, '            # Bam to stranded bigwigs')
    (95, '            expand(join(workpath,bams_dir,"{name}.star_rg_added.sorted.dmark.bam"),name=samples),')
    (96, '            expand(join(workpath,bams_dir,"{name}.fwd.bw"),name=samples),')
    (97, '            expand(join(workpath,bams_dir,"{name}.rev.bw"),name=samples),')
    (98, '')
    (99, '            # Picard')
    (100, '            expand(join(workpath,log_dir,"{name}.RnaSeqMetrics.txt"),name=samples),')
    (101, '            expand(join(workpath,log_dir,"{name}.star.duplic"),name=samples),')
    (102, '')
    (103, '            # Preseq')
    (104, '            expand(join(workpath,preseq_dir,"{name}.ccurve"),name=samples),')
    (105, '')
    (106, '            # QualiMap (bamQC and counts)')
    (107, '            expand(join(workpath,"QualiMap","{name}","qualimapReport.html"),name=samples),')
    (108, '')
    (109, '            # RSeQC')
    (110, '            expand(join(workpath,rseqc_dir,"{name}.strand.info"),name=samples),')
    (111, '            expand(join(workpath,rseqc_dir,"{name}.Rdist.info"),name=samples),')
    (112, '            expand(join(workpath,rseqc_dir,"{name}.inner_distance_freq.txt"),name=samples),')
    (113, '            expand(join(workpath,rseqc_dir,"{name}.star_rg_added.sorted.dmark.summary.txt"),name=samples),')
    (114, '            join(workpath,degall_dir,"combined_TIN.tsv"),')
    (115, '')
    (116, '            # RSEM merge and counts')
    (117, '            expand(join(workpath,degall_dir,"{name}.RSEM.genes.results"),name=samples),')
    (118, '            expand(join(workpath,degall_dir,"{name}.RSEM.isoforms.results"),name=samples),')
    (119, '            join(workpath,degall_dir,"RSEM.genes.FPKM.all_samples.txt"),')
    (120, '            join(workpath,degall_dir,"RSEM.isoforms.FPKM.all_samples.txt"),')
    (121, '            #join(workpath,degall_dir,"RawCountFile_RSEM_genes_filtered.txt"),')
    (122, '            #join(workpath,star_dir,"sampletable.txt"),')
    (123, '')
    (124, '            # PCA Reports')
    (125, '            #expand(join(workpath,degall_dir,"PcaReport_{dtype}.html"),dtype=dtypes),')
    (126, '')
    (127, '            # MultiQC')
    (128, '            join(workpath,"Reports","multiqc_report.html"),')
    (129, '')
    (130, '            # RNA QC Report')
    (131, '            join(workpath,"Reports","RNA_Report.html"),')
    (132, '')
    (133, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skchronicles/RNA-seek, file=workflow/Snakefile
context_key: ['elif not paired_end', 'rule all', 'input']
    (134, 'elif not paired_end:')
    (135, '    # Single-end Target files')
    (136, '    rule all:')
    (137, "        params: batch=\\'--time=168:00:00\\'")
    (138, '        input:')
    (139, '            # FastQValidator')
    (140, '            expand(join(workpath,"rawQC","{name}.validated.R1.fastq.log"), name=provided(samples, wf_api.use_singularity)),')
    (141, '')
    (142, '            # Flowcell Lane information')
    (143, '            expand(join(workpath,"rawQC","{name}.fastq.info.txt"),name=samples),')
    (144, '')
    (145, '            # FastQC (before and after trimming)')
    (146, '            expand(join(workpath,"rawQC","{name}.R1_fastqc.zip"), name=samples),')
    (147, '            expand(join(workpath,"QC","{name}.R1.trim_fastqc.zip"), name=samples),')
    (148, '')
    (149, '            # FastQ Screen')
    (150, '            expand(join(workpath,"FQscreen","{name}.R1.trim_screen.txt"),name=samples),')
    (151, '            expand(join(workpath,"FQscreen","{name}.R1.trim_screen.png"),name=samples),')
    (152, '            expand(join(workpath,"FQscreen2","{name}.R1.trim_screen.txt"),name=samples),')
    (153, '            expand(join(workpath,"FQscreen2","{name}.R1.trim_screen.png"),name=samples),')
    (154, '')
    (155, '            # Kraken + Krona')
    (156, '            expand(join(workpath,kraken_dir,"{name}.trim.kraken_bacteria.taxa.txt"),name=samples),')
    (157, '            expand(join(workpath,kraken_dir,"{name}.trim.kraken_bacteria.krona.html"),name=samples),')
    (158, '')
    (159, '            # STAR')
    (160, '            expand(join(workpath,bams_dir,"{name}.p2.Aligned.toTranscriptome.out.bam"),name=samples),')
    (161, '')
    (162, '            # Picard')
    (163, '            expand(join(workpath,log_dir,"{name}.RnaSeqMetrics.txt"),name=samples),')
    (164, '')
    (165, '            # QualiMap')
    (166, '            expand(join(workpath,"QualiMap","{name}","qualimapReport.html"),name=samples),')
    (167, '')
    (168, '            # Bam to stranded bigwigs')
    (169, '            expand(join(workpath,bams_dir,"{name}.fwd.bw"),name=samples),')
    (170, '            expand(join(workpath,bams_dir,"{name}.rev.bw"),name=samples),')
    (171, '')
    (172, '            # RSeQC')
    (173, '            expand(join(workpath,rseqc_dir,"{name}.star_rg_added.sorted.dmark.summary.txt"),name=samples),')
    (174, '            join(workpath,degall_dir,"combined_TIN.tsv"),')
    (175, '')
    (176, '            # RSEM')
    (177, '            expand(join(workpath,degall_dir,"{name}.RSEM.genes.results"),name=samples),')
    (178, '            expand(join(workpath,degall_dir,"{name}.RSEM.isoforms.results"),name=samples),')
    (179, '            join(workpath,degall_dir,"RSEM.genes.FPKM.all_samples.txt"),')
    (180, '            join(workpath,degall_dir,"RSEM.isoforms.FPKM.all_samples.txt"),')
    (181, '            #join(workpath,degall_dir,"RawCountFile_RSEM_genes_filtered.txt"),')
    (182, '            #join(workpath,star_dir,"sampletable.txt"),')
    (183, '')
    (184, '            # PCA Report')
    (185, '            #expand(join(workpath,degall_dir,"PcaReport_{dtype}.html"),dtype=dtypes),')
    (186, '')
    (187, '            # MultiQC')
    (188, '            join(workpath,"Reports","multiqc_report.html"),')
    (189, '')
    (190, '')
    (191, '# Rules common to single-end or paired-end data')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skchronicles/RNA-seek, file=workflow/Snakefile
context_key: ['if paired_end']
    (194, 'if paired_end:')
    (195, '    # rules common to paired-end data')
    (196, '    include: "rules/paired-end.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skchronicles/RNA-seek, file=workflow/Snakefile
context_key: ['elif not paired_end']
    (197, 'elif not paired_end:')
    (198, '    # rules common to paired-end data')
    (199, '    include: "rules/single-end.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skchronicles/RNA-seek, file=workflow/rules/paired-end.smk
context_key: ["if references(config, pfamily, [\\'FUSIONBLACKLIST\\', \\'FUSIONCYTOBAND\\', \\'FUSIONPROTDOMAIN\\'])", 'rule arriba']
    (601, "if references(config, pfamily, [\\'FUSIONBLACKLIST\\', \\'FUSIONCYTOBAND\\', \\'FUSIONPROTDOMAIN\\']):")
    (602, '    rule arriba:')
    (603, '        """')
    (604, '        Optional data processing step to align reads against reference genome using STAR in')
    (605, '        two-pass basic mode and call gene fusions using arriba. This rule is only run if the')
    (606, '        config contains the required reference files (most important of which is the blacklist).')
    (607, '        If these files are not provided, Arriba is not run.')
    (608, '        detected in the first-pass of STAR.')
    (609, '        @Input:')
    (610, '            Trimmed FastQ files (scatter)')
    (611, '        @Output:')
    (612, '            Predicted gene fusions and figures')
    (613, '        """')
    (614, '        input:')
    (615, '            R1=join(workpath,trim_dir,"{name}.R1.trim.fastq.gz"),')
    (616, '            R2=join(workpath,trim_dir,"{name}.R2.trim.fastq.gz"),')
    (617, "            blacklist=abstract_location(config[\\'references\\'][pfamily][\\'FUSIONBLACKLIST\\']),")
    (618, "            cytoband=abstract_location(config[\\'references\\'][pfamily][\\'FUSIONCYTOBAND\\']),")
    (619, "            protdomain=abstract_location(config[\\'references\\'][pfamily][\\'FUSIONPROTDOMAIN\\']),")
    (620, '        output:')
    (621, '            fusions=join(workpath,"fusions","{name}_fusions.tsv"),')
    (622, '            discarded=join(workpath,"fusions","{name}_fusions.discarded.tsv"),')
    (623, '            figure=join(workpath,"fusions","{name}_fusions.arriba.pdf"),')
    (624, '            bam=join(workpath,"fusions","{name}.p2.arriba.Aligned.sortedByCoord.out.bam"),')
    (625, '            bai=join(workpath,"fusions","{name}.p2.arriba.Aligned.sortedByCoord.out.bam.bai"),')
    (626, '        params:')
    (627, "            rname=\\'pl:arriba\\',")
    (628, '            prefix=join(workpath, "fusions", "{name}.p2"),')
    (629, '            # Exposed Parameters: modify config/genomes/{genome}.json to change default')
    (630, '            chimericbam="{name}.p2.arriba.Aligned.out.bam",')
    (631, "            stardir=config[\\'references\\'][pfamily][\\'GENOME_STARDIR\\'],")
    (632, "            gtffile=config[\\'references\\'][pfamily][\\'GTFFILE\\'],")
    (633, "            reffa=config[\\'references\\'][pfamily][\\'GENOME\\'],")
    (634, '            tmpdir=tmpdir,')
    (635, '        threads: int(allocated("threads", "arriba", cluster)),')
    (636, "        envmodules: config[\\'bin\\'][pfamily][\\'tool_versions\\'][\\'ARRIBAVER\\']")
    (637, "        container: config[\\'images\\'][\\'arriba\\']")
    (638, '        shell: """')
    (639, '        # Setups temporary directory for')
    (640, '        # intermediate files with built-in ')
    (641, '        # mechanism for deletion on exit')
    (642, '        if [ ! -d "{params.tmpdir}" ]; then mkdir -p "{params.tmpdir}"; fi')
    (643, '        tmp=$(mktemp -d -p "{params.tmpdir}")')
    (644, '        trap \\\'rm -rf "${{tmp}}"\\\' EXIT')
    (645, '')
    (646, '        # Optimal readlength for sjdbOverhang = max(ReadLength) - 1 [Default: 100]')
    (647, '        readlength=$(')
    (648, '            zcat {input.R1} | \\\\')
    (649, "            awk -v maxlen=100 \\'NR%4==2 {{if (length($1) > maxlen+0) maxlen=length($1)}}; \\\\")
    (650, "            END {{print maxlen-1}}\\'")
    (651, '        )')
    (652, '')
    (653, '        # Avoids inheriting $R_LIBS_SITE')
    (654, '        # from local env variables')
    (655, '        R_LIBS_SITE=/usr/local/lib/R/site-library')
    (656, '')
    (657, '        STAR --runThreadN {threads} \\\\')
    (658, '            --sjdbGTFfile {params.gtffile} \\\\')
    (659, '            --sjdbOverhang ${{readlength}} \\\\')
    (660, '            --genomeDir {params.stardir} \\\\')
    (661, '            --genomeLoad NoSharedMemory \\\\')
    (662, '            --readFilesIn {input.R1} {input.R2} \\\\')
    (663, '            --readFilesCommand zcat \\\\')
    (664, '            --outStd BAM_Unsorted \\\\')
    (665, '            --outSAMtype BAM Unsorted \\\\')
    (666, '            --outSAMunmapped Within \\\\')
    (667, '            --outFilterMultimapNmax 50 \\\\')
    (668, '            --peOverlapNbasesMin 10 \\\\')
    (669, '            --alignSplicedMateMapLminOverLmate 0.5 \\\\')
    (670, '            --alignSJstitchMismatchNmax 5 -1 5 5 \\\\')
    (671, '            --chimSegmentMin 10 \\\\')
    (672, '            --chimOutType WithinBAM HardClip \\\\')
    (673, '            --chimJunctionOverhangMin 10 \\\\')
    (674, '            --chimScoreDropMax 30 \\\\')
    (675, '            --chimScoreJunctionNonGTAG 0 \\\\')
    (676, '            --chimScoreSeparation 1 \\\\')
    (677, '            --chimSegmentReadGapMax 3 \\\\')
    (678, '            --chimMultimapNmax 50 \\\\')
    (679, '            --twopassMode Basic \\\\')
    (680, '            --outTmpDir=${{tmp}}/STARtmp_{wildcards.name} \\\\')
    (681, '            --outFileNamePrefix {params.prefix}. \\\\')
    (682, '        | tee ${{tmp}}/{params.chimericbam} | \\\\')
    (683, '        arriba -x /dev/stdin \\\\')
    (684, '            -o {output.fusions} \\\\')
    (685, '            -O {output.discarded} \\\\')
    (686, '            -a {params.reffa} \\\\')
    (687, '            -g {params.gtffile} \\\\')
    (688, '            -b {input.blacklist} \\\\')
    (689, '')
    (690, "        # Sorting and Indexing BAM files is required for Arriba\\'s Visualization")
    (691, '        samtools sort -@ {threads} \\\\')
    (692, '            -m 2G -T ${{tmp}}/SORTtmp_{wildcards.name} \\\\')
    (693, '            -O bam ${{tmp}}/{params.chimericbam} \\\\')
    (694, '            > {output.bam}')
    (695, '')
    (696, '        samtools index {output.bam} {output.bai}')
    (697, '        rm ${{tmp}}/{params.chimericbam}')
    (698, '')
    (699, '        # Generate Gene Fusions Visualization')
    (700, '        draw_fusions.R \\\\')
    (701, '            --fusions={output.fusions} \\\\')
    (702, '            --alignments={output.bam} \\\\')
    (703, '            --output={output.figure} \\\\')
    (704, '            --annotation={params.gtffile} \\\\')
    (705, '            --cytobands={input.cytoband} \\\\')
    (706, '            --proteinDomains={input.protdomain}')
    (707, '        """')
    (708, '')
    (709, '')
    (710, '# Post Alignment Rules')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/kraken2_classification, file=Snakefile
context_key: ['if not "remove_chordata" in config']
    (10, 'if not "remove_chordata" in config:')
    (11, "    config[\\'remove_chordata\\'] = \\'FALSE\\'")
    (12, '')
    (13, '#perform a check on the Lathe git repo and warn if not up to date')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/kraken2_classification, file=Snakefile
context_key: ['onstart', 'if stat != ""']
    (22, '    if stat != "":')
    (23, '        print()')
    (24, '        print("#")')
    (25, '        print("##")')
    (26, '        print("###")')
    (27, '        print("####")')
    (28, '        print("#####")')
    (29, '        print("######")')
    (30, '        print()')
    (31, "        print(\\'WARNING: Differences to latest version detected. Please reset changes and/or pull repo.\\')")
    (32, '        print()')
    (33, '        print("######")')
    (34, '        print("#####")')
    (35, '        print("####")')
    (36, '        print("###")')
    (37, '        print("##")')
    (38, '        print("#")')
    (39, '        # time.sleep(5)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "imp" in STEPS', 'include']
    (39, 'if "imp" in STEPS:')
    (40, '    include:')
    (41, '        "rules/imp.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "imp" in STEPS']
    (42, '    TARGETS += [')
    (43, '        "status/imp.done"')
    (44, '    ]')
    (45, '')
    (46, '# Viruses')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "viruses" in STEPS', 'include']
    (47, 'if "viruses" in STEPS:')
    (48, '    include:')
    (49, '        "rules/viruses.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "viruses" in STEPS']
    (50, '    TARGETS += [')
    (51, '        "status/viruses.done"')
    (52, '    ]')
    (53, '')
    (54, '# Eukaryotes ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "eukaryotes" in STEPS', 'include']
    (55, 'if "eukaryotes" in STEPS:')
    (56, '    include:')
    (57, '        "rules/eukaryotes.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "eukaryotes" in STEPS']
    (58, '    TARGETS += [')
    (59, '        "status/eukaryotes.done"')
    (60, '    ]')
    (61, '')
    (62, '# Bins')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "taxonomy" or "functions" or "mags" in STEPS', 'include']
    (63, 'if "taxonomy" or "functions" or "mags" in STEPS:')
    (64, '    include:')
    (65, '        "rules/bins.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "taxonomy" or "functions" or "mags" in STEPS']
    (66, '    TARGETS += [')
    (67, '        "status/bins.done"')
    (68, '    ]')
    (69, '')
    (70, '# Eukaryote binning')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "euk_bin" in STEPS', 'include']
    (71, 'if "euk_bin" in STEPS:')
    (72, '    include:')
    (73, '        "rules/euk_bin.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "euk_bin" in STEPS']
    (74, '    TARGETS += [')
    (75, '        "status/euk_bin.done"')
    (76, '    ]')
    (77, '')
    (78, '# GTDBtk + Checkm')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "taxonomy" in STEPS', 'include']
    (79, 'if "taxonomy" in STEPS:')
    (80, '    include:')
    (81, '        "rules/taxonomy.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "taxonomy" in STEPS']
    (82, '    TARGETS += [')
    (83, '        "status/gtdbtk_checkm.done"')
    (84, '    ]')
    (85, '')
    (86, '# Functional annotation (METABOLIC, MAGICCAVE, FUNCS, antiSMASH)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "functions" in STEPS', 'include']
    (87, 'if "functions" in STEPS:')
    (88, '    include:')
    (89, '        "rules/functions.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "functions" in STEPS']
    (90, '    TARGETS += [')
    (91, '        "status/functions.done"')
    (92, '    ]')
    (93, '')
    (94, '# MANTIS annotation')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "mantis" in STEPS', 'include']
    (95, 'if "mantis" in STEPS:')
    (96, '    include:')
    (97, '        "rules/mantis.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "mantis" in STEPS']
    (98, '    TARGETS += [')
    (99, '        "status/mantis.done"')
    (100, '    ]')
    (101, '')
    (102, '# Coassembly and Binning')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "coassembly_binning" in STEPS', 'include']
    (103, 'if "coassembly_binning" in STEPS:')
    (104, '    include:')
    (105, '        "rules/coassembly_binning.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "coassembly_binning" in STEPS']
    (106, '    TARGETS += [')
    (107, '        "status/coassembly_binning.done"')
    (108, '    ]')
    (109, '')
    (110, '# gRodon + PopCOGent ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "misc" in STEPS', 'include']
    (111, 'if "misc" in STEPS:')
    (112, '    include:')
    (113, '        "rules/misc.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if "misc" in STEPS']
    (114, '    TARGETS += [')
    (115, '        "status/misc.done"')
    (116, '    ]')
    (117, '')
    (118, '# No targets')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=susheelbhanu/nomis_pipeline, file=workflow/Snakefile
context_key: ['if len(TARGETS) == 0']
    (119, 'if len(TARGETS) == 0:')
    (120, "    raise Exception(\\'You are not serious. Nothing to be done? Really?\\')")
    (121, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "mylib_myalg" in pattern_strings', 'rule summarise_mylib_myalg', 'input']
    (9, 'if "mylib_myalg" in pattern_strings:')
    (10, '')
    (11, '    rule summarise_mylib_myalg:')
    (12, '        input:')
    (13, '            "workflow/scripts/evaluation/run_summarise.R",')
    (14, '            data=summarise_alg_input_data_path(),')
    (15, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (16, '            adjmat_est=summarise_alg_input_adjmat_est_path("mylib_myalg"),')
    (17, '            time=summarise_alg_input_time_path("mylib_myalg"),')
    (18, '            ntests=summarise_alg_input_ntests_path("mylib_myalg"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "mylib_myalg" in pattern_strings', 'rule summarise_mylib_myalg', 'output']
    (19, '        output:')
    (20, '            res=summarise_alg_output_res_path("mylib_myalg"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "mylib_myalg" in pattern_strings', 'rule summarise_mylib_myalg', 'shell']
    (21, '        shell:')
    (22, '            summarise_alg_shell("mylib_myalg")')
    (23, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "mylib_myalg" in pattern_strings', 'rule join_summaries_mylib_myalg', 'input']
    (24, '    rule join_summaries_mylib_myalg:')
    (25, '        input:')
    (26, '            "workflow/scripts/evaluation/run_summarise.R",')
    (27, '            conf=configfilename,')
    (28, '            res=join_string_sampled_model("mylib_myalg"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "mylib_myalg" in pattern_strings', 'rule join_summaries_mylib_myalg', 'output']
    (29, '        output:')
    (30, '            join_summaries_output("mylib_myalg"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "mylib_myalg" in pattern_strings', 'rule join_summaries_mylib_myalg', 'script']
    (31, '        script:')
    (32, '            "../scripts/evaluation/join_csv_files.R"')
    (33, '')
    (34, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "parallelDG" in pattern_strings', 'rule parallelDG', 'input']
    (35, 'if "parallelDG" in pattern_strings:')
    (36, '')
    (37, '    rule parallelDG:')
    (38, '        input:')
    (39, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "parallelDG" in pattern_strings', 'rule parallelDG', 'output']
    (40, '        output:')
    (41, '            adjvecs=alg_output_seqgraph_path("parallelDG"),')
    (42, '            time=alg_output_time_path("parallelDG"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "parallelDG" in pattern_strings', 'rule parallelDG', 'container']
    (43, '        container:')
    (44, '            docker_image("parallelDG")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "parallelDG" in pattern_strings', 'rule parallelDG', 'shell']
    (45, '        shell:')
    (46, '            alg_shell("parallelDG")')
    (47, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "parallelDG" in pattern_strings', 'rule parallelDG_est', 'input']
    (48, '    rule parallelDG_est:')
    (49, '        input:')
    (50, '            "workflow/scripts/evaluation/graphtraj_est.py",')
    (51, '            traj=alg_output_seqgraph_path_nocomp("parallelDG"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "parallelDG" in pattern_strings', 'rule parallelDG_est', 'output']
    (52, '        output:')
    (53, '            adjmat=alg_output_adjmat_path("parallelDG"),  #here is the difference from order_mcmc. matching diffferently.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "parallelDG" in pattern_strings', 'rule parallelDG_est', 'params']
    (54, '        params:')
    (55, '            graph_type="chordal",')
    (56, '            estimator="map",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "parallelDG" in pattern_strings', 'rule parallelDG_est', 'container']
    (57, '        container:')
    (58, '            docker_image("networkx")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "parallelDG" in pattern_strings', 'rule parallelDG_est', 'script']
    (59, '        script:')
    (60, '            "../scripts/evaluation/graphtraj_est.py"')
    (61, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "parallelDG" in pattern_strings', 'rule join_summaries_parallelDG', 'input']
    (62, '    rule join_summaries_parallelDG:')
    (63, '        input:')
    (64, '            "workflow/scripts/evaluation/run_summarise.R",')
    (65, '            conf=configfilename,')
    (66, '            res=join_string_sampled_model("parallelDG"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "parallelDG" in pattern_strings', 'rule join_summaries_parallelDG', 'output']
    (67, '        output:')
    (68, '            join_summaries_output("parallelDG"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "parallelDG" in pattern_strings', 'rule join_summaries_parallelDG', 'script']
    (69, '        script:')
    (70, '            "../scripts/evaluation/join_csv_files.R"')
    (71, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "parallelDG" in pattern_strings', 'rule summarise_parallelDG', 'input']
    (72, '    rule summarise_parallelDG:')
    (73, '        input:')
    (74, '            "workflow/scripts/evaluation/run_summarise.R",')
    (75, '            data=summarise_alg_input_data_path(),')
    (76, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (77, '            adjmat_est=summarise_alg_input_adjmat_est_path("parallelDG"),')
    (78, '            time=summarise_alg_input_time_path("parallelDG"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "parallelDG" in pattern_strings', 'rule summarise_parallelDG', 'output']
    (79, '        output:')
    (80, '            res=summarise_alg_output_res_path("parallelDG"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "parallelDG" in pattern_strings', 'rule summarise_parallelDG', 'shell']
    (81, '        shell:')
    (82, '            summarise_alg_shell("parallelDG")')
    (83, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "causaldag_gsp" in pattern_strings', 'rule causaldag_gsp', 'input']
    (84, 'if "causaldag_gsp" in pattern_strings:')
    (85, '')
    (86, '    rule causaldag_gsp:')
    (87, '        input:')
    (88, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "causaldag_gsp" in pattern_strings', 'rule causaldag_gsp', 'output']
    (89, '        output:')
    (90, '            adjmat=alg_output_adjmat_path("causaldag_gsp"),')
    (91, '            time=alg_output_time_path("causaldag_gsp"),')
    (92, '            ntests=alg_output_ntests_path("causaldag_gsp"),')
    (93, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "causaldag_gsp" in pattern_strings', 'rule causaldag_gsp', 'container']
    (94, '        container:')
    (95, '            docker_image("causaldag")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "causaldag_gsp" in pattern_strings', 'rule causaldag_gsp', 'script']
    (96, '        script:')
    (97, '            "../scripts/structure_learning_algorithms/causaldag_gsp.py"')
    (98, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "causaldag_gsp" in pattern_strings', 'rule summarise_causaldag_gsp', 'input']
    (99, '    rule summarise_causaldag_gsp:')
    (100, '        input:')
    (101, '            "workflow/scripts/evaluation/run_summarise.R",')
    (102, '            data=summarise_alg_input_data_path(),')
    (103, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (104, '            adjmat_est=summarise_alg_input_adjmat_est_path("causaldag_gsp"),')
    (105, '            time=summarise_alg_input_time_path("causaldag_gsp"),')
    (106, '            ntests=summarise_alg_input_ntests_path("causaldag_gsp"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "causaldag_gsp" in pattern_strings', 'rule summarise_causaldag_gsp', 'output']
    (107, '        output:')
    (108, '            res=summarise_alg_output_res_path("causaldag_gsp"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "causaldag_gsp" in pattern_strings', 'rule summarise_causaldag_gsp', 'shell']
    (109, '        shell:')
    (110, '            summarise_alg_shell("causaldag_gsp")')
    (111, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "causaldag_gsp" in pattern_strings', 'rule join_summaries_causaldag_gsp', 'input']
    (112, '    rule join_summaries_causaldag_gsp:')
    (113, '        input:')
    (114, '            "workflow/scripts/evaluation/run_summarise.R",')
    (115, '            conf=configfilename,')
    (116, '            res=join_string_sampled_model("causaldag_gsp"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "causaldag_gsp" in pattern_strings', 'rule join_summaries_causaldag_gsp', 'output']
    (117, '        output:')
    (118, '            join_summaries_output("causaldag_gsp"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "causaldag_gsp" in pattern_strings', 'rule join_summaries_causaldag_gsp', 'script']
    (119, '        script:')
    (120, '            "../scripts/evaluation/join_csv_files.R"')
    (121, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears" in pattern_strings', 'rule gcastle_notears', 'input']
    (122, 'if "gcastle_notears" in pattern_strings:')
    (123, '')
    (124, '    rule gcastle_notears:')
    (125, '        input:')
    (126, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears" in pattern_strings', 'rule gcastle_notears', 'output']
    (127, '        output:')
    (128, '            adjmat=alg_output_adjmat_path("gcastle_notears"),')
    (129, '            time=alg_output_time_path("gcastle_notears"),')
    (130, '            ntests=alg_output_ntests_path("gcastle_notears"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears" in pattern_strings', 'rule gcastle_notears', 'params']
    (131, '        params:')
    (132, '            alg="notears",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears" in pattern_strings', 'rule gcastle_notears', 'container']
    (133, '        container:')
    (134, '            docker_image("gcastle")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears" in pattern_strings', 'rule gcastle_notears', 'script']
    (135, '        script:')
    (136, '            "../scripts/structure_learning_algorithms/gcastle.py"')
    (137, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears" in pattern_strings', 'rule summarise_gcastle_notears', 'input']
    (138, '    rule summarise_gcastle_notears:')
    (139, '        input:')
    (140, '            "workflow/scripts/evaluation/run_summarise.R",')
    (141, '            data=summarise_alg_input_data_path(),')
    (142, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (143, '            adjmat_est=summarise_alg_input_adjmat_est_path("gcastle_notears"),')
    (144, '            time=summarise_alg_input_time_path("gcastle_notears"),')
    (145, '            ntests=summarise_alg_input_ntests_path("gcastle_notears"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears" in pattern_strings', 'rule summarise_gcastle_notears', 'output']
    (146, '        output:')
    (147, '            res=summarise_alg_output_res_path("gcastle_notears"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears" in pattern_strings', 'rule summarise_gcastle_notears', 'shell']
    (148, '        shell:')
    (149, '            summarise_alg_shell("gcastle_notears")')
    (150, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears" in pattern_strings', 'rule join_summaries_gcastle_notears', 'input']
    (151, '    rule join_summaries_gcastle_notears:')
    (152, '        input:')
    (153, '            "workflow/scripts/evaluation/run_summarise.R",')
    (154, '            conf=configfilename,')
    (155, '            res=join_string_sampled_model("gcastle_notears"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears" in pattern_strings', 'rule join_summaries_gcastle_notears', 'output']
    (156, '        output:')
    (157, '            join_summaries_output("gcastle_notears"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears" in pattern_strings', 'rule join_summaries_gcastle_notears', 'script']
    (158, '        script:')
    (159, '            "../scripts/evaluation/join_csv_files.R"')
    (160, '')
    (161, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_pc" in pattern_strings', 'rule gcastle_pc', 'input']
    (162, 'if "gcastle_pc" in pattern_strings:')
    (163, '')
    (164, '    rule gcastle_pc:')
    (165, '        input:')
    (166, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_pc" in pattern_strings', 'rule gcastle_pc', 'output']
    (167, '        output:')
    (168, '            adjmat=alg_output_adjmat_path("gcastle_pc"),')
    (169, '            time=alg_output_time_path("gcastle_pc"),')
    (170, '            ntests=alg_output_ntests_path("gcastle_pc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_pc" in pattern_strings', 'rule gcastle_pc', 'params']
    (171, '        params:')
    (172, '            alg="pc",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_pc" in pattern_strings', 'rule gcastle_pc', 'container']
    (173, '        container:')
    (174, '            docker_image("gcastle")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_pc" in pattern_strings', 'rule gcastle_pc', 'script']
    (175, '        script:')
    (176, '            "../scripts/structure_learning_algorithms/gcastle.py"')
    (177, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_pc" in pattern_strings', 'rule summarise_gcastle_pc', 'input']
    (178, '    rule summarise_gcastle_pc:')
    (179, '        input:')
    (180, '            "workflow/scripts/evaluation/run_summarise.R",')
    (181, '            data=summarise_alg_input_data_path(),')
    (182, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (183, '            adjmat_est=summarise_alg_input_adjmat_est_path("gcastle_pc"),')
    (184, '            time=summarise_alg_input_time_path("gcastle_pc"),')
    (185, '            ntests=summarise_alg_input_ntests_path("gcastle_pc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_pc" in pattern_strings', 'rule summarise_gcastle_pc', 'output']
    (186, '        output:')
    (187, '            res=summarise_alg_output_res_path("gcastle_pc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_pc" in pattern_strings', 'rule summarise_gcastle_pc', 'shell']
    (188, '        shell:')
    (189, '            summarise_alg_shell("gcastle_pc")')
    (190, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_pc" in pattern_strings', 'rule join_summaries_gcastle_pc', 'input']
    (191, '    rule join_summaries_gcastle_pc:')
    (192, '        input:')
    (193, '            "workflow/scripts/evaluation/run_summarise.R",')
    (194, '            conf=configfilename,')
    (195, '            res=join_string_sampled_model("gcastle_pc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_pc" in pattern_strings', 'rule join_summaries_gcastle_pc', 'output']
    (196, '        output:')
    (197, '            join_summaries_output("gcastle_pc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_pc" in pattern_strings', 'rule join_summaries_gcastle_pc', 'script']
    (198, '        script:')
    (199, '            "../scripts/evaluation/join_csv_files.R"')
    (200, '')
    (201, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_anm" in pattern_strings', 'rule gcastle_anm', 'input']
    (202, 'if "gcastle_anm" in pattern_strings:')
    (203, '')
    (204, '    rule gcastle_anm:')
    (205, '        input:')
    (206, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_anm" in pattern_strings', 'rule gcastle_anm', 'output']
    (207, '        output:')
    (208, '            adjmat=alg_output_adjmat_path("gcastle_anm"),')
    (209, '            time=alg_output_time_path("gcastle_anm"),')
    (210, '            ntests=alg_output_ntests_path("gcastle_anm"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_anm" in pattern_strings', 'rule gcastle_anm', 'params']
    (211, '        params:')
    (212, '            alg="anm",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_anm" in pattern_strings', 'rule gcastle_anm', 'container']
    (213, '        container:')
    (214, '            docker_image("gcastle")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_anm" in pattern_strings', 'rule gcastle_anm', 'script']
    (215, '        script:')
    (216, '            "../scripts/structure_learning_algorithms/gcastle.py"')
    (217, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_anm" in pattern_strings', 'rule summarise_gcastle_anm', 'input']
    (218, '    rule summarise_gcastle_anm:')
    (219, '        input:')
    (220, '            "workflow/scripts/evaluation/run_summarise.R",')
    (221, '            data=summarise_alg_input_data_path(),')
    (222, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (223, '            adjmat_est=summarise_alg_input_adjmat_est_path("gcastle_anm"),')
    (224, '            time=summarise_alg_input_time_path("gcastle_anm"),')
    (225, '            ntests=summarise_alg_input_ntests_path("gcastle_anm"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_anm" in pattern_strings', 'rule summarise_gcastle_anm', 'output']
    (226, '        output:')
    (227, '            res=summarise_alg_output_res_path("gcastle_anm"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_anm" in pattern_strings', 'rule summarise_gcastle_anm', 'shell']
    (228, '        shell:')
    (229, '            summarise_alg_shell("gcastle_anm")')
    (230, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_anm" in pattern_strings', 'rule join_summaries_gcastle_anm', 'input']
    (231, '    rule join_summaries_gcastle_anm:')
    (232, '        input:')
    (233, '            "workflow/scripts/evaluation/run_summarise.R",')
    (234, '            conf=configfilename,')
    (235, '            res=join_string_sampled_model("gcastle_anm"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_anm" in pattern_strings', 'rule join_summaries_gcastle_anm', 'output']
    (236, '        output:')
    (237, '            join_summaries_output("gcastle_anm"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_anm" in pattern_strings', 'rule join_summaries_gcastle_anm', 'script']
    (238, '        script:')
    (239, '            "../scripts/evaluation/join_csv_files.R"')
    (240, '')
    (241, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_direct_lingam" in pattern_strings', 'rule gcastle_direct_lingam', 'input']
    (242, 'if "gcastle_direct_lingam" in pattern_strings:')
    (243, '')
    (244, '    rule gcastle_direct_lingam:')
    (245, '        input:')
    (246, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_direct_lingam" in pattern_strings', 'rule gcastle_direct_lingam', 'output']
    (247, '        output:')
    (248, '            adjmat=alg_output_adjmat_path("gcastle_direct_lingam"),')
    (249, '            time=alg_output_time_path("gcastle_direct_lingam"),')
    (250, '            ntests=alg_output_ntests_path("gcastle_direct_lingam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_direct_lingam" in pattern_strings', 'rule gcastle_direct_lingam', 'params']
    (251, '        params:')
    (252, '            alg="direct_lingam",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_direct_lingam" in pattern_strings', 'rule gcastle_direct_lingam', 'container']
    (253, '        container:')
    (254, '            docker_image("gcastle")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_direct_lingam" in pattern_strings', 'rule gcastle_direct_lingam', 'script']
    (255, '        script:')
    (256, '            "../scripts/structure_learning_algorithms/gcastle.py"')
    (257, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_direct_lingam" in pattern_strings', 'rule summarise_gcastle_direct_lingam', 'input']
    (258, '    rule summarise_gcastle_direct_lingam:')
    (259, '        input:')
    (260, '            "workflow/scripts/evaluation/run_summarise.R",')
    (261, '            data=summarise_alg_input_data_path(),')
    (262, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (263, '            adjmat_est=summarise_alg_input_adjmat_est_path("gcastle_direct_lingam"),')
    (264, '            time=summarise_alg_input_time_path("gcastle_direct_lingam"),')
    (265, '            ntests=summarise_alg_input_ntests_path("gcastle_direct_lingam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_direct_lingam" in pattern_strings', 'rule summarise_gcastle_direct_lingam', 'output']
    (266, '        output:')
    (267, '            res=summarise_alg_output_res_path("gcastle_direct_lingam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_direct_lingam" in pattern_strings', 'rule summarise_gcastle_direct_lingam', 'shell']
    (268, '        shell:')
    (269, '            summarise_alg_shell("gcastle_direct_lingam")')
    (270, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_direct_lingam" in pattern_strings', 'rule join_summaries_gcastle_direct_lingam', 'input']
    (271, '    rule join_summaries_gcastle_direct_lingam:')
    (272, '        input:')
    (273, '            "workflow/scripts/evaluation/run_summarise.R",')
    (274, '            conf=configfilename,')
    (275, '            res=join_string_sampled_model("gcastle_direct_lingam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_direct_lingam" in pattern_strings', 'rule join_summaries_gcastle_direct_lingam', 'output']
    (276, '        output:')
    (277, '            join_summaries_output("gcastle_direct_lingam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_direct_lingam" in pattern_strings', 'rule join_summaries_gcastle_direct_lingam', 'script']
    (278, '        script:')
    (279, '            "../scripts/evaluation/join_csv_files.R"')
    (280, '')
    (281, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_ica_lingam" in pattern_strings', 'rule gcastle_ica_lingam', 'input']
    (282, 'if "gcastle_ica_lingam" in pattern_strings:')
    (283, '')
    (284, '    rule gcastle_ica_lingam:')
    (285, '        input:')
    (286, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_ica_lingam" in pattern_strings', 'rule gcastle_ica_lingam', 'output']
    (287, '        output:')
    (288, '            adjmat=alg_output_adjmat_path("gcastle_ica_lingam"),')
    (289, '            time=alg_output_time_path("gcastle_ica_lingam"),')
    (290, '            ntests=alg_output_ntests_path("gcastle_ica_lingam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_ica_lingam" in pattern_strings', 'rule gcastle_ica_lingam', 'params']
    (291, '        params:')
    (292, '            alg="ica_lingam",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_ica_lingam" in pattern_strings', 'rule gcastle_ica_lingam', 'container']
    (293, '        container:')
    (294, '            docker_image("gcastle")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_ica_lingam" in pattern_strings', 'rule gcastle_ica_lingam', 'script']
    (295, '        script:')
    (296, '            "../scripts/structure_learning_algorithms/gcastle.py"')
    (297, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_ica_lingam" in pattern_strings', 'rule summarise_gcastle_ica_lingam', 'input']
    (298, '    rule summarise_gcastle_ica_lingam:')
    (299, '        input:')
    (300, '            "workflow/scripts/evaluation/run_summarise.R",')
    (301, '            data=summarise_alg_input_data_path(),')
    (302, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (303, '            adjmat_est=summarise_alg_input_adjmat_est_path("gcastle_ica_lingam"),')
    (304, '            time=summarise_alg_input_time_path("gcastle_ica_lingam"),')
    (305, '            ntests=summarise_alg_input_ntests_path("gcastle_ica_lingam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_ica_lingam" in pattern_strings', 'rule summarise_gcastle_ica_lingam', 'output']
    (306, '        output:')
    (307, '            res=summarise_alg_output_res_path("gcastle_ica_lingam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_ica_lingam" in pattern_strings', 'rule summarise_gcastle_ica_lingam', 'shell']
    (308, '        shell:')
    (309, '            summarise_alg_shell("gcastle_ica_lingam")')
    (310, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_ica_lingam" in pattern_strings', 'rule join_summaries_gcastle_ica_lingam', 'input']
    (311, '    rule join_summaries_gcastle_ica_lingam:')
    (312, '        input:')
    (313, '            "workflow/scripts/evaluation/run_summarise.R",')
    (314, '            conf=configfilename,')
    (315, '            res=join_string_sampled_model("gcastle_ica_lingam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_ica_lingam" in pattern_strings', 'rule join_summaries_gcastle_ica_lingam', 'output']
    (316, '        output:')
    (317, '            join_summaries_output("gcastle_ica_lingam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_ica_lingam" in pattern_strings', 'rule join_summaries_gcastle_ica_lingam', 'script']
    (318, '        script:')
    (319, '            "../scripts/evaluation/join_csv_files.R"')
    (320, '')
    (321, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_nonlinear" in pattern_strings', 'rule gcastle_notears_nonlinear', 'input']
    (322, 'if "gcastle_notears_nonlinear" in pattern_strings:')
    (323, '')
    (324, '    rule gcastle_notears_nonlinear:')
    (325, '        input:')
    (326, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_nonlinear" in pattern_strings', 'rule gcastle_notears_nonlinear', 'output']
    (327, '        output:')
    (328, '            adjmat=alg_output_adjmat_path("gcastle_notears_nonlinear"),')
    (329, '            time=alg_output_time_path("gcastle_notears_nonlinear"),')
    (330, '            ntests=alg_output_ntests_path("gcastle_notears_nonlinear"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_nonlinear" in pattern_strings', 'rule gcastle_notears_nonlinear', 'params']
    (331, '        params:')
    (332, '            alg="notears_nonlinear",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_nonlinear" in pattern_strings', 'rule gcastle_notears_nonlinear', 'container']
    (333, '        container:')
    (334, '            docker_image("gcastle")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_nonlinear" in pattern_strings', 'rule gcastle_notears_nonlinear', 'script']
    (335, '        script:')
    (336, '            "../scripts/structure_learning_algorithms/gcastle.py"')
    (337, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_nonlinear" in pattern_strings', 'rule summarise_gcastle_notears_nonlinear', 'input']
    (338, '    rule summarise_gcastle_notears_nonlinear:')
    (339, '        input:')
    (340, '            "workflow/scripts/evaluation/run_summarise.R",')
    (341, '            data=summarise_alg_input_data_path(),')
    (342, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (343, '            adjmat_est=summarise_alg_input_adjmat_est_path("gcastle_notears_nonlinear"),')
    (344, '            time=summarise_alg_input_time_path("gcastle_notears_nonlinear"),')
    (345, '            ntests=summarise_alg_input_ntests_path("gcastle_notears_nonlinear"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_nonlinear" in pattern_strings', 'rule summarise_gcastle_notears_nonlinear', 'output']
    (346, '        output:')
    (347, '            res=summarise_alg_output_res_path("gcastle_notears_nonlinear"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_nonlinear" in pattern_strings', 'rule summarise_gcastle_notears_nonlinear', 'shell']
    (348, '        shell:')
    (349, '            summarise_alg_shell("gcastle_notears_nonlinear")')
    (350, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_nonlinear" in pattern_strings', 'rule join_summaries_gcastle_notears_nonlinear', 'input']
    (351, '    rule join_summaries_gcastle_notears_nonlinear:')
    (352, '        input:')
    (353, '            "workflow/scripts/evaluation/run_summarise.R",')
    (354, '            conf=configfilename,')
    (355, '            res=join_string_sampled_model("gcastle_notears_nonlinear"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_nonlinear" in pattern_strings', 'rule join_summaries_gcastle_notears_nonlinear', 'output']
    (356, '        output:')
    (357, '            join_summaries_output("gcastle_notears_nonlinear"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_nonlinear" in pattern_strings', 'rule join_summaries_gcastle_notears_nonlinear', 'script']
    (358, '        script:')
    (359, '            "../scripts/evaluation/join_csv_files.R"')
    (360, '')
    (361, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_low_rank" in pattern_strings', 'rule gcastle_notears_low_rank', 'input']
    (362, 'if "gcastle_notears_low_rank" in pattern_strings:')
    (363, '')
    (364, '    rule gcastle_notears_low_rank:')
    (365, '        input:')
    (366, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_low_rank" in pattern_strings', 'rule gcastle_notears_low_rank', 'output']
    (367, '        output:')
    (368, '            adjmat=alg_output_adjmat_path("gcastle_notears_low_rank"),')
    (369, '            time=alg_output_time_path("gcastle_notears_low_rank"),')
    (370, '            ntests=alg_output_ntests_path("gcastle_notears_low_rank"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_low_rank" in pattern_strings', 'rule gcastle_notears_low_rank', 'params']
    (371, '        params:')
    (372, '            alg="notears_low_rank",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_low_rank" in pattern_strings', 'rule gcastle_notears_low_rank', 'container']
    (373, '        container:')
    (374, '            docker_image("gcastle")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_low_rank" in pattern_strings', 'rule gcastle_notears_low_rank', 'script']
    (375, '        script:')
    (376, '            "../scripts/structure_learning_algorithms/gcastle.py"')
    (377, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_low_rank" in pattern_strings', 'rule summarise_gcastle_notears_low_rank', 'input']
    (378, '    rule summarise_gcastle_notears_low_rank:')
    (379, '        input:')
    (380, '            "workflow/scripts/evaluation/run_summarise.R",')
    (381, '            data=summarise_alg_input_data_path(),')
    (382, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (383, '            adjmat_est=summarise_alg_input_adjmat_est_path("gcastle_notears_low_rank"),')
    (384, '            time=summarise_alg_input_time_path("gcastle_notears_low_rank"),')
    (385, '            ntests=summarise_alg_input_ntests_path("gcastle_notears_low_rank"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_low_rank" in pattern_strings', 'rule summarise_gcastle_notears_low_rank', 'output']
    (386, '        output:')
    (387, '            res=summarise_alg_output_res_path("gcastle_notears_low_rank"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_low_rank" in pattern_strings', 'rule summarise_gcastle_notears_low_rank', 'shell']
    (388, '        shell:')
    (389, '            summarise_alg_shell("gcastle_notears_low_rank")')
    (390, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_low_rank" in pattern_strings', 'rule join_summaries_gcastle_notears_low_rank', 'input']
    (391, '    rule join_summaries_gcastle_notears_low_rank:')
    (392, '        input:')
    (393, '            "workflow/scripts/evaluation/run_summarise.R",')
    (394, '            conf=configfilename,')
    (395, '            res=join_string_sampled_model("gcastle_notears_low_rank"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_low_rank" in pattern_strings', 'rule join_summaries_gcastle_notears_low_rank', 'output']
    (396, '        output:')
    (397, '            join_summaries_output("gcastle_notears_low_rank"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_notears_low_rank" in pattern_strings', 'rule join_summaries_gcastle_notears_low_rank', 'script']
    (398, '        script:')
    (399, '            "../scripts/evaluation/join_csv_files.R"')
    (400, '')
    (401, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_golem" in pattern_strings', 'rule gcastle_golem', 'input']
    (402, 'if "gcastle_golem" in pattern_strings:')
    (403, '')
    (404, '    rule gcastle_golem:')
    (405, '        input:')
    (406, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_golem" in pattern_strings', 'rule gcastle_golem', 'output']
    (407, '        output:')
    (408, '            adjmat=alg_output_adjmat_path("gcastle_golem"),')
    (409, '            time=alg_output_time_path("gcastle_golem"),')
    (410, '            ntests=alg_output_ntests_path("gcastle_golem"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_golem" in pattern_strings', 'rule gcastle_golem', 'params']
    (411, '        params:')
    (412, '            alg="golem",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_golem" in pattern_strings', 'rule gcastle_golem', 'container']
    (413, '        container:')
    (414, '            docker_image("gcastle")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_golem" in pattern_strings', 'rule gcastle_golem', 'script']
    (415, '        script:')
    (416, '            "../scripts/structure_learning_algorithms/gcastle.py"')
    (417, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_golem" in pattern_strings', 'rule summarise_gcastle_golem', 'input']
    (418, '    rule summarise_gcastle_golem:')
    (419, '        input:')
    (420, '            "workflow/scripts/evaluation/run_summarise.R",')
    (421, '            data=summarise_alg_input_data_path(),')
    (422, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (423, '            adjmat_est=summarise_alg_input_adjmat_est_path("gcastle_golem"),')
    (424, '            time=summarise_alg_input_time_path("gcastle_golem"),')
    (425, '            ntests=summarise_alg_input_ntests_path("gcastle_golem"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_golem" in pattern_strings', 'rule summarise_gcastle_golem', 'output']
    (426, '        output:')
    (427, '            res=summarise_alg_output_res_path("gcastle_golem"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_golem" in pattern_strings', 'rule summarise_gcastle_golem', 'shell']
    (428, '        shell:')
    (429, '            summarise_alg_shell("gcastle_golem")')
    (430, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_golem" in pattern_strings', 'rule join_summaries_gcastle_golem', 'input']
    (431, '    rule join_summaries_gcastle_golem:')
    (432, '        input:')
    (433, '            "workflow/scripts/evaluation/run_summarise.R",')
    (434, '            conf=configfilename,')
    (435, '            res=join_string_sampled_model("gcastle_golem"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_golem" in pattern_strings', 'rule join_summaries_gcastle_golem', 'output']
    (436, '        output:')
    (437, '            join_summaries_output("gcastle_golem"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_golem" in pattern_strings', 'rule join_summaries_gcastle_golem', 'script']
    (438, '        script:')
    (439, '            "../scripts/evaluation/join_csv_files.R"')
    (440, '')
    (441, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_grandag" in pattern_strings', 'rule gcastle_grandag', 'input']
    (442, 'if "gcastle_grandag" in pattern_strings:')
    (443, '')
    (444, '    rule gcastle_grandag:')
    (445, '        input:')
    (446, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_grandag" in pattern_strings', 'rule gcastle_grandag', 'output']
    (447, '        output:')
    (448, '            adjmat=alg_output_adjmat_path("gcastle_grandag"),')
    (449, '            time=alg_output_time_path("gcastle_grandag"),')
    (450, '            ntests=alg_output_ntests_path("gcastle_grandag"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_grandag" in pattern_strings', 'rule gcastle_grandag', 'params']
    (451, '        params:')
    (452, '            alg="grandag",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_grandag" in pattern_strings', 'rule gcastle_grandag', 'container']
    (453, '        container:')
    (454, '            docker_image("gcastle")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_grandag" in pattern_strings', 'rule gcastle_grandag', 'script']
    (455, '        script:')
    (456, '            "../scripts/structure_learning_algorithms/gcastle.py"')
    (457, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_grandag" in pattern_strings', 'rule summarise_gcastle_grandag', 'input']
    (458, '    rule summarise_gcastle_grandag:')
    (459, '        input:')
    (460, '            "workflow/scripts/evaluation/run_summarise.R",')
    (461, '            data=summarise_alg_input_data_path(),')
    (462, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (463, '            adjmat_est=summarise_alg_input_adjmat_est_path("gcastle_grandag"),')
    (464, '            time=summarise_alg_input_time_path("gcastle_grandag"),')
    (465, '            ntests=summarise_alg_input_ntests_path("gcastle_grandag"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_grandag" in pattern_strings', 'rule summarise_gcastle_grandag', 'output']
    (466, '        output:')
    (467, '            res=summarise_alg_output_res_path("gcastle_grandag"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_grandag" in pattern_strings', 'rule summarise_gcastle_grandag', 'shell']
    (468, '        shell:')
    (469, '            summarise_alg_shell("gcastle_grandag")')
    (470, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_grandag" in pattern_strings', 'rule join_summaries_gcastle_grandag', 'input']
    (471, '    rule join_summaries_gcastle_grandag:')
    (472, '        input:')
    (473, '            "workflow/scripts/evaluation/run_summarise.R",')
    (474, '            conf=configfilename,')
    (475, '            res=join_string_sampled_model("gcastle_grandag"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_grandag" in pattern_strings', 'rule join_summaries_gcastle_grandag', 'output']
    (476, '        output:')
    (477, '            join_summaries_output("gcastle_grandag"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_grandag" in pattern_strings', 'rule join_summaries_gcastle_grandag', 'script']
    (478, '        script:')
    (479, '            "../scripts/evaluation/join_csv_files.R"')
    (480, '')
    (481, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_mcsl" in pattern_strings', 'rule gcastle_mcsl', 'input']
    (482, 'if "gcastle_mcsl" in pattern_strings:')
    (483, '')
    (484, '    rule gcastle_mcsl:')
    (485, '        input:')
    (486, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_mcsl" in pattern_strings', 'rule gcastle_mcsl', 'output']
    (487, '        output:')
    (488, '            adjmat=alg_output_adjmat_path("gcastle_mcsl"),')
    (489, '            time=alg_output_time_path("gcastle_mcsl"),')
    (490, '            ntests=alg_output_ntests_path("gcastle_mcsl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_mcsl" in pattern_strings', 'rule gcastle_mcsl', 'params']
    (491, '        params:')
    (492, '            alg="mcsl",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_mcsl" in pattern_strings', 'rule gcastle_mcsl', 'container']
    (493, '        container:')
    (494, '            docker_image("gcastle")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_mcsl" in pattern_strings', 'rule gcastle_mcsl', 'script']
    (495, '        script:')
    (496, '            "../scripts/structure_learning_algorithms/gcastle.py"')
    (497, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_mcsl" in pattern_strings', 'rule summarise_gcastle_mcsl', 'input']
    (498, '    rule summarise_gcastle_mcsl:')
    (499, '        input:')
    (500, '            "workflow/scripts/evaluation/run_summarise.R",')
    (501, '            data=summarise_alg_input_data_path(),')
    (502, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (503, '            adjmat_est=summarise_alg_input_adjmat_est_path("gcastle_mcsl"),')
    (504, '            time=summarise_alg_input_time_path("gcastle_mcsl"),')
    (505, '            ntests=summarise_alg_input_ntests_path("gcastle_mcsl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_mcsl" in pattern_strings', 'rule summarise_gcastle_mcsl', 'output']
    (506, '        output:')
    (507, '            res=summarise_alg_output_res_path("gcastle_mcsl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_mcsl" in pattern_strings', 'rule summarise_gcastle_mcsl', 'shell']
    (508, '        shell:')
    (509, '            summarise_alg_shell("gcastle_mcsl")')
    (510, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_mcsl" in pattern_strings', 'rule join_summaries_gcastle_mcsl', 'input']
    (511, '    rule join_summaries_gcastle_mcsl:')
    (512, '        input:')
    (513, '            "workflow/scripts/evaluation/run_summarise.R",')
    (514, '            conf=configfilename,')
    (515, '            res=join_string_sampled_model("gcastle_mcsl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_mcsl" in pattern_strings', 'rule join_summaries_gcastle_mcsl', 'output']
    (516, '        output:')
    (517, '            join_summaries_output("gcastle_mcsl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_mcsl" in pattern_strings', 'rule join_summaries_gcastle_mcsl', 'script']
    (518, '        script:')
    (519, '            "../scripts/evaluation/join_csv_files.R"')
    (520, '###############')
    (521, '')
    (522, '')
    (523, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_gae" in pattern_strings', 'rule gcastle_gae', 'input']
    (524, 'if "gcastle_gae" in pattern_strings:')
    (525, '')
    (526, '    rule gcastle_gae:')
    (527, '        input:')
    (528, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_gae" in pattern_strings', 'rule gcastle_gae', 'output']
    (529, '        output:')
    (530, '            adjmat=alg_output_adjmat_path("gcastle_gae"),')
    (531, '            time=alg_output_time_path("gcastle_gae"),')
    (532, '            ntests=alg_output_ntests_path("gcastle_gae"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_gae" in pattern_strings', 'rule gcastle_gae', 'params']
    (533, '        params:')
    (534, '            alg="gae",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_gae" in pattern_strings', 'rule gcastle_gae', 'container']
    (535, '        container:')
    (536, '            docker_image("gcastle")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_gae" in pattern_strings', 'rule gcastle_gae', 'script']
    (537, '        script:')
    (538, '            "../scripts/structure_learning_algorithms/gcastle.py"')
    (539, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_gae" in pattern_strings', 'rule summarise_gcastle_gae', 'input']
    (540, '    rule summarise_gcastle_gae:')
    (541, '        input:')
    (542, '            "workflow/scripts/evaluation/run_summarise.R",')
    (543, '            data=summarise_alg_input_data_path(),')
    (544, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (545, '            adjmat_est=summarise_alg_input_adjmat_est_path("gcastle_gae"),')
    (546, '            time=summarise_alg_input_time_path("gcastle_gae"),')
    (547, '            ntests=summarise_alg_input_ntests_path("gcastle_gae"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_gae" in pattern_strings', 'rule summarise_gcastle_gae', 'output']
    (548, '        output:')
    (549, '            res=summarise_alg_output_res_path("gcastle_gae"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_gae" in pattern_strings', 'rule summarise_gcastle_gae', 'shell']
    (550, '        shell:')
    (551, '            summarise_alg_shell("gcastle_gae")')
    (552, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_gae" in pattern_strings', 'rule join_summaries_gcastle_gae', 'input']
    (553, '    rule join_summaries_gcastle_gae:')
    (554, '        input:')
    (555, '            "workflow/scripts/evaluation/run_summarise.R",')
    (556, '            conf=configfilename,')
    (557, '            res=join_string_sampled_model("gcastle_gae"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_gae" in pattern_strings', 'rule join_summaries_gcastle_gae', 'output']
    (558, '        output:')
    (559, '            join_summaries_output("gcastle_gae"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_gae" in pattern_strings', 'rule join_summaries_gcastle_gae', 'script']
    (560, '        script:')
    (561, '            "../scripts/evaluation/join_csv_files.R"')
    (562, '###############')
    (563, '')
    (564, '')
    (565, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_rl" in pattern_strings', 'rule gcastle_rl', 'input']
    (566, 'if "gcastle_rl" in pattern_strings:')
    (567, '')
    (568, '    rule gcastle_rl:')
    (569, '        input:')
    (570, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_rl" in pattern_strings', 'rule gcastle_rl', 'output']
    (571, '        output:')
    (572, '            adjmat=alg_output_adjmat_path("gcastle_rl"),')
    (573, '            time=alg_output_time_path("gcastle_rl"),')
    (574, '            ntests=alg_output_ntests_path("gcastle_rl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_rl" in pattern_strings', 'rule gcastle_rl', 'params']
    (575, '        params:')
    (576, '            alg="rl",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_rl" in pattern_strings', 'rule gcastle_rl', 'container']
    (577, '        container:')
    (578, '            docker_image("gcastle")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_rl" in pattern_strings', 'rule gcastle_rl', 'script']
    (579, '        script:')
    (580, '            "../scripts/structure_learning_algorithms/gcastle.py"')
    (581, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_rl" in pattern_strings', 'rule summarise_gcastle_rl', 'input']
    (582, '    rule summarise_gcastle_rl:')
    (583, '        input:')
    (584, '            "workflow/scripts/evaluation/run_summarise.R",')
    (585, '            data=summarise_alg_input_data_path(),')
    (586, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (587, '            adjmat_est=summarise_alg_input_adjmat_est_path("gcastle_rl"),')
    (588, '            time=summarise_alg_input_time_path("gcastle_rl"),')
    (589, '            ntests=summarise_alg_input_ntests_path("gcastle_rl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_rl" in pattern_strings', 'rule summarise_gcastle_rl', 'output']
    (590, '        output:')
    (591, '            res=summarise_alg_output_res_path("gcastle_rl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_rl" in pattern_strings', 'rule summarise_gcastle_rl', 'shell']
    (592, '        shell:')
    (593, '            summarise_alg_shell("gcastle_rl")')
    (594, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_rl" in pattern_strings', 'rule join_summaries_gcastle_rl', 'input']
    (595, '    rule join_summaries_gcastle_rl:')
    (596, '        input:')
    (597, '            "workflow/scripts/evaluation/run_summarise.R",')
    (598, '            conf=configfilename,')
    (599, '            res=join_string_sampled_model("gcastle_rl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_rl" in pattern_strings', 'rule join_summaries_gcastle_rl', 'output']
    (600, '        output:')
    (601, '            join_summaries_output("gcastle_rl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_rl" in pattern_strings', 'rule join_summaries_gcastle_rl', 'script']
    (602, '        script:')
    (603, '            "../scripts/evaluation/join_csv_files.R"')
    (604, '###############')
    (605, '')
    (606, '')
    (607, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_corl" in pattern_strings', 'rule gcastle_corl', 'input']
    (608, 'if "gcastle_corl" in pattern_strings:')
    (609, '')
    (610, '    rule gcastle_corl:')
    (611, '        input:')
    (612, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_corl" in pattern_strings', 'rule gcastle_corl', 'output']
    (613, '        output:')
    (614, '            adjmat=alg_output_adjmat_path("gcastle_corl"),')
    (615, '            time=alg_output_time_path("gcastle_corl"),')
    (616, '            ntests=alg_output_ntests_path("gcastle_corl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_corl" in pattern_strings', 'rule gcastle_corl', 'params']
    (617, '        params:')
    (618, '            alg="corl",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_corl" in pattern_strings', 'rule gcastle_corl', 'container']
    (619, '        container:')
    (620, '            docker_image("gcastle")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_corl" in pattern_strings', 'rule gcastle_corl', 'script']
    (621, '        script:')
    (622, '            "../scripts/structure_learning_algorithms/gcastle.py"')
    (623, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_corl" in pattern_strings', 'rule summarise_gcastle_corl', 'input']
    (624, '    rule summarise_gcastle_corl:')
    (625, '        input:')
    (626, '            "workflow/scripts/evaluation/run_summarise.R",')
    (627, '            data=summarise_alg_input_data_path(),')
    (628, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (629, '            adjmat_est=summarise_alg_input_adjmat_est_path("gcastle_corl"),')
    (630, '            time=summarise_alg_input_time_path("gcastle_corl"),')
    (631, '            ntests=summarise_alg_input_ntests_path("gcastle_corl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_corl" in pattern_strings', 'rule summarise_gcastle_corl', 'output']
    (632, '        output:')
    (633, '            res=summarise_alg_output_res_path("gcastle_corl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_corl" in pattern_strings', 'rule summarise_gcastle_corl', 'shell']
    (634, '        shell:')
    (635, '            summarise_alg_shell("gcastle_corl")')
    (636, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_corl" in pattern_strings', 'rule join_summaries_gcastle_corl', 'input']
    (637, '    rule join_summaries_gcastle_corl:')
    (638, '        input:')
    (639, '            "workflow/scripts/evaluation/run_summarise.R",')
    (640, '            conf=configfilename,')
    (641, '            res=join_string_sampled_model("gcastle_corl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_corl" in pattern_strings', 'rule join_summaries_gcastle_corl', 'output']
    (642, '        output:')
    (643, '            join_summaries_output("gcastle_corl"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gcastle_corl" in pattern_strings', 'rule join_summaries_gcastle_corl', 'script']
    (644, '        script:')
    (645, '            "../scripts/evaluation/join_csv_files.R"')
    (646, '')
    (647, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "sklearn_glasso" in pattern_strings', 'rule sklearn_glasso', 'input']
    (648, 'if "sklearn_glasso" in pattern_strings:')
    (649, '')
    (650, '    rule sklearn_glasso:')
    (651, '        input:')
    (652, '            data=alg_input_data(),')
    (653, '            sklearn_glasso="workflow/scripts/structure_learning_algorithms/sklearn_glasso.py",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "sklearn_glasso" in pattern_strings', 'rule sklearn_glasso', 'output']
    (654, '        output:')
    (655, '            adjmat=alg_output_adjmat_path("sklearn_glasso"),')
    (656, '            time=alg_output_time_path("sklearn_glasso"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "sklearn_glasso" in pattern_strings', 'rule sklearn_glasso', 'container']
    (657, '        container:')
    (658, '            docker_image("pydatascience")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "sklearn_glasso" in pattern_strings', 'rule sklearn_glasso', 'script']
    (659, '        script:')
    (660, '            "../scripts/structure_learning_algorithms/sklearn_glasso.py"')
    (661, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "sklearn_glasso" in pattern_strings', 'rule summarise_sklearn_glasso', 'input']
    (662, '    rule summarise_sklearn_glasso:')
    (663, '        input:')
    (664, '            "workflow/scripts/evaluation/run_summarise.R",')
    (665, '            data=summarise_alg_input_data_path(),')
    (666, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (667, '            adjmat_est=summarise_alg_input_adjmat_est_path("sklearn_glasso"),')
    (668, '            time=summarise_alg_input_time_path("sklearn_glasso"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "sklearn_glasso" in pattern_strings', 'rule summarise_sklearn_glasso', 'output']
    (669, '        output:')
    (670, '            res=summarise_alg_output_res_path("sklearn_glasso"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "sklearn_glasso" in pattern_strings', 'rule summarise_sklearn_glasso', 'shell']
    (671, '        shell:')
    (672, '            summarise_alg_shell("sklearn_glasso")')
    (673, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "sklearn_glasso" in pattern_strings', 'rule join_summaries_sklearn_glasso', 'input']
    (674, '    rule join_summaries_sklearn_glasso:')
    (675, '        input:')
    (676, '            "workflow/scripts/evaluation/run_summarise.R",')
    (677, '            conf=configfilename,')
    (678, '            res=join_string_sampled_model("sklearn_glasso"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "sklearn_glasso" in pattern_strings', 'rule join_summaries_sklearn_glasso', 'output']
    (679, '        output:')
    (680, '            join_summaries_output("sklearn_glasso"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "sklearn_glasso" in pattern_strings', 'rule join_summaries_sklearn_glasso', 'script']
    (681, '        script:')
    (682, '            "../scripts/evaluation/join_csv_files.R"')
    (683, '')
    (684, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_tabu" in pattern_strings', 'rule tabu', 'input']
    (685, 'if "bnlearn_tabu" in pattern_strings:')
    (686, '')
    (687, '    rule tabu:')
    (688, '        input:')
    (689, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_tabu" in pattern_strings', 'rule tabu', 'output']
    (690, '        output:')
    (691, '            adjmat=alg_output_adjmat_path("bnlearn_tabu"),')
    (692, '            time=alg_output_time_path("bnlearn_tabu"),')
    (693, '            ntests=alg_output_ntests_path("bnlearn_tabu"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_tabu" in pattern_strings', 'rule tabu', 'container']
    (694, '        container:')
    (695, '            docker_image("bnlearn")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_tabu" in pattern_strings', 'rule tabu', 'script']
    (696, '        script:')
    (697, '            "../scripts/structure_learning_algorithms/bnlearn_tabu.R"')
    (698, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_tabu" in pattern_strings', 'rule summarise_tabu', 'input']
    (699, '    rule summarise_tabu:')
    (700, '        input:')
    (701, '            "workflow/scripts/evaluation/run_summarise.R",')
    (702, '            data=summarise_alg_input_data_path(),')
    (703, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (704, '            adjmat_est=summarise_alg_input_adjmat_est_path("bnlearn_tabu"),')
    (705, '            time=summarise_alg_input_time_path("bnlearn_tabu"),')
    (706, '            ntests=summarise_alg_input_ntests_path("bnlearn_tabu"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_tabu" in pattern_strings', 'rule summarise_tabu', 'output']
    (707, '        output:')
    (708, '            res=summarise_alg_output_res_path("bnlearn_tabu"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_tabu" in pattern_strings', 'rule summarise_tabu', 'shell']
    (709, '        shell:')
    (710, '            summarise_alg_shell("bnlearn_tabu")')
    (711, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_tabu" in pattern_strings', 'rule join_summaries_tabu', 'input']
    (712, '    rule join_summaries_tabu:')
    (713, '        input:')
    (714, '            "workflow/scripts/evaluation/run_summarise.R",')
    (715, '            conf=configfilename,')
    (716, '            res=join_string_sampled_model("bnlearn_tabu"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_tabu" in pattern_strings', 'rule join_summaries_tabu', 'output']
    (717, '        output:')
    (718, '            join_summaries_output("bnlearn_tabu"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_tabu" in pattern_strings', 'rule join_summaries_tabu', 'script']
    (719, '        script:')
    (720, '            "../scripts/evaluation/join_csv_files.R"')
    (721, '')
    (722, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "notears" in pattern_strings', 'rule notears', 'input']
    (723, 'if "notears" in pattern_strings:')
    (724, '')
    (725, '    rule notears:')
    (726, '        input:')
    (727, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "notears" in pattern_strings', 'rule notears', 'output']
    (728, '        output:')
    (729, '            adjmat=alg_output_adjmat_path("notears"),')
    (730, '            time=alg_output_time_path("notears"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "notears" in pattern_strings', 'rule notears', 'container']
    (731, '        container:')
    (732, '            docker_image("notears")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "notears" in pattern_strings', 'rule notears', 'shell']
    (733, '        shell:')
    (734, '            alg_shell("notears")')
    (735, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "notears" in pattern_strings', 'rule summarise_notears', 'input']
    (736, '    rule summarise_notears:')
    (737, '        input:')
    (738, '            "workflow/scripts/evaluation/run_summarise.R",')
    (739, '            data=summarise_alg_input_data_path(),')
    (740, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (741, '            adjmat_est=summarise_alg_input_adjmat_est_path("notears"),')
    (742, '            time=summarise_alg_input_time_path("notears"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "notears" in pattern_strings', 'rule summarise_notears', 'output']
    (743, '        output:')
    (744, '            res=summarise_alg_output_res_path("notears"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "notears" in pattern_strings', 'rule summarise_notears', 'shell']
    (745, '        shell:')
    (746, '            summarise_alg_shell("notears")')
    (747, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "notears" in pattern_strings', 'rule join_summaries_notears', 'input']
    (748, '    rule join_summaries_notears:')
    (749, '        input:')
    (750, '            "workflow/scripts/evaluation/run_summarise.R",')
    (751, '            conf=configfilename,')
    (752, '            res=join_string_sampled_model("notears"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "notears" in pattern_strings', 'rule join_summaries_notears', 'output']
    (753, '        output:')
    (754, '            join_summaries_output("notears"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "notears" in pattern_strings', 'rule join_summaries_notears', 'script']
    (755, '        script:')
    (756, '            "../scripts/evaluation/join_csv_files.R"')
    (757, '')
    (758, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hc" in pattern_strings', 'rule hc', 'input']
    (759, 'if "bnlearn_hc" in pattern_strings:')
    (760, '')
    (761, '    rule hc:')
    (762, '        input:')
    (763, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hc" in pattern_strings', 'rule hc', 'output']
    (764, '        output:')
    (765, '            adjmat=alg_output_adjmat_path("bnlearn_hc"),')
    (766, '            time=alg_output_time_path("bnlearn_hc"),')
    (767, '            ntests=alg_output_ntests_path("bnlearn_hc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hc" in pattern_strings', 'rule hc', 'container']
    (768, '        container:')
    (769, '            docker_image("bnlearn")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hc" in pattern_strings', 'rule hc', 'script']
    (770, '        script:')
    (771, '            "../scripts/structure_learning_algorithms/bnlearn_hc.R"')
    (772, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hc" in pattern_strings', 'rule summarise_hc', 'input']
    (773, '    rule summarise_hc:')
    (774, '        input:')
    (775, '            "workflow/scripts/evaluation/run_summarise.R",')
    (776, '            data=summarise_alg_input_data_path(),')
    (777, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (778, '            adjmat_est=summarise_alg_input_adjmat_est_path("bnlearn_hc"),')
    (779, '            time=summarise_alg_input_time_path("bnlearn_hc"),')
    (780, '            ntests=summarise_alg_input_ntests_path("bnlearn_hc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hc" in pattern_strings', 'rule summarise_hc', 'output']
    (781, '        output:')
    (782, '            res=summarise_alg_output_res_path("bnlearn_hc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hc" in pattern_strings', 'rule summarise_hc', 'shell']
    (783, '        shell:')
    (784, '            summarise_alg_shell("bnlearn_hc")')
    (785, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hc" in pattern_strings', 'rule join_summaries_hc', 'input']
    (786, '    rule join_summaries_hc:')
    (787, '        input:')
    (788, '            "workflow/scripts/evaluation/run_summarise.R",')
    (789, '            conf=configfilename,')
    (790, '            res=join_string_sampled_model("bnlearn_hc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hc" in pattern_strings', 'rule join_summaries_hc', 'output']
    (791, '        output:')
    (792, '            join_summaries_output("bnlearn_hc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hc" in pattern_strings', 'rule join_summaries_hc', 'script']
    (793, '        script:')
    (794, '            "../scripts/evaluation/join_csv_files.R"')
    (795, '')
    (796, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_interiamb" in pattern_strings', 'rule bnlearn_interiamb', 'input']
    (797, 'if "bnlearn_interiamb" in pattern_strings:')
    (798, '')
    (799, '    rule bnlearn_interiamb:')
    (800, '        input:')
    (801, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_interiamb" in pattern_strings', 'rule bnlearn_interiamb', 'output']
    (802, '        output:')
    (803, '            adjmat=alg_output_adjmat_path("bnlearn_interiamb"),')
    (804, '            time=alg_output_time_path("bnlearn_interiamb"),')
    (805, '            ntests=alg_output_ntests_path("bnlearn_interiamb"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_interiamb" in pattern_strings', 'rule bnlearn_interiamb', 'container']
    (806, '        container:')
    (807, '            docker_image("bnlearn")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_interiamb" in pattern_strings', 'rule bnlearn_interiamb', 'script']
    (808, '        script:')
    (809, '            "../scripts/structure_learning_algorithms/bnlearn_inter-iamb.R"')
    (810, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_interiamb" in pattern_strings', 'rule summarise_bnlearn_interiamb', 'input']
    (811, '    rule summarise_bnlearn_interiamb:')
    (812, '        input:')
    (813, '            "workflow/scripts/evaluation/run_summarise.R",')
    (814, '            data=summarise_alg_input_data_path(),')
    (815, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (816, '            adjmat_est=summarise_alg_input_adjmat_est_path("bnlearn_interiamb"),')
    (817, '            time=summarise_alg_input_time_path("bnlearn_interiamb"),')
    (818, '            ntests=summarise_alg_input_ntests_path("bnlearn_interiamb"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_interiamb" in pattern_strings', 'rule summarise_bnlearn_interiamb', 'output']
    (819, '        output:')
    (820, '            res=summarise_alg_output_res_path("bnlearn_interiamb"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_interiamb" in pattern_strings', 'rule summarise_bnlearn_interiamb', 'shell']
    (821, '        shell:')
    (822, '            summarise_alg_shell("bnlearn_interiamb")')
    (823, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_interiamb" in pattern_strings', 'rule join_summaries_bnlearn_interiamb', 'input']
    (824, '    rule join_summaries_bnlearn_interiamb:')
    (825, '        input:')
    (826, '            "workflow/scripts/evaluation/run_summarise.R",')
    (827, '            conf=configfilename,')
    (828, '            res=join_string_sampled_model("bnlearn_interiamb"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_interiamb" in pattern_strings', 'rule join_summaries_bnlearn_interiamb', 'output']
    (829, '        output:')
    (830, '            join_summaries_output("bnlearn_interiamb"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_interiamb" in pattern_strings', 'rule join_summaries_bnlearn_interiamb', 'script']
    (831, '        script:')
    (832, '            "../scripts/evaluation/join_csv_files.R"')
    (833, '')
    (834, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_gs" in pattern_strings', 'rule gs', 'input']
    (835, 'if "bnlearn_gs" in pattern_strings:')
    (836, '')
    (837, '    rule gs:')
    (838, '        input:')
    (839, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_gs" in pattern_strings', 'rule gs', 'output']
    (840, '        output:')
    (841, '            adjmat=alg_output_adjmat_path("bnlearn_gs"),')
    (842, '            time=alg_output_time_path("bnlearn_gs"),')
    (843, '            ntests=alg_output_ntests_path("bnlearn_gs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_gs" in pattern_strings', 'rule gs', 'container']
    (844, '        container:')
    (845, '            docker_image("bnlearn")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_gs" in pattern_strings', 'rule gs', 'script']
    (846, '        script:')
    (847, '            "../scripts/structure_learning_algorithms/" + "bnlearn_gs" + ".R"')
    (848, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_gs" in pattern_strings', 'rule summarise_gs', 'input']
    (849, '    rule summarise_gs:')
    (850, '        input:')
    (851, '            "workflow/scripts/evaluation/run_summarise.R",')
    (852, '            data=summarise_alg_input_data_path(),')
    (853, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (854, '            adjmat_est=summarise_alg_input_adjmat_est_path("bnlearn_gs"),')
    (855, '            time=summarise_alg_input_time_path("bnlearn_gs"),')
    (856, '            ntests=summarise_alg_input_ntests_path("bnlearn_gs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_gs" in pattern_strings', 'rule summarise_gs', 'output']
    (857, '        output:')
    (858, '            res=summarise_alg_output_res_path("bnlearn_gs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_gs" in pattern_strings', 'rule summarise_gs', 'shell']
    (859, '        shell:')
    (860, '            summarise_alg_shell("bnlearn_gs")')
    (861, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_gs" in pattern_strings', 'rule join_summaries_gs', 'input']
    (862, '    rule join_summaries_gs:')
    (863, '        input:')
    (864, '            "workflow/scripts/evaluation/run_summarise.R",')
    (865, '            conf=configfilename,')
    (866, '            res=join_string_sampled_model("bnlearn_gs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_gs" in pattern_strings', 'rule join_summaries_gs', 'output']
    (867, '        output:')
    (868, '            join_summaries_output("bnlearn_gs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_gs" in pattern_strings', 'rule join_summaries_gs', 'script']
    (869, '        script:')
    (870, '            "../scripts/evaluation/join_csv_files.R"')
    (871, '')
    (872, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_pcstable" in pattern_strings', 'rule bnlearn_pcstable', 'input']
    (873, 'if "bnlearn_pcstable" in pattern_strings:')
    (874, '')
    (875, '    rule bnlearn_pcstable:')
    (876, '        input:')
    (877, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_pcstable" in pattern_strings', 'rule bnlearn_pcstable', 'output']
    (878, '        output:')
    (879, '            adjmat=alg_output_adjmat_path("bnlearn_pcstable"),')
    (880, '            time=alg_output_time_path("bnlearn_pcstable"),')
    (881, '            ntests=alg_output_ntests_path("bnlearn_pcstable"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_pcstable" in pattern_strings', 'rule bnlearn_pcstable', 'container']
    (882, '        container:')
    (883, '            docker_image("bnlearn")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_pcstable" in pattern_strings', 'rule bnlearn_pcstable', 'script']
    (884, '        script:')
    (885, '            "../scripts/structure_learning_algorithms/" + "bnlearn_pcstable" + ".R"')
    (886, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_pcstable" in pattern_strings', 'rule summarise_bnlearn_pcstable', 'input']
    (887, '    rule summarise_bnlearn_pcstable:')
    (888, '        input:')
    (889, '            "workflow/scripts/evaluation/run_summarise.R",')
    (890, '            data=summarise_alg_input_data_path(),')
    (891, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (892, '            adjmat_est=summarise_alg_input_adjmat_est_path("bnlearn_pcstable"),')
    (893, '            time=summarise_alg_input_time_path("bnlearn_pcstable"),')
    (894, '            ntests=summarise_alg_input_ntests_path("bnlearn_pcstable"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_pcstable" in pattern_strings', 'rule summarise_bnlearn_pcstable', 'output']
    (895, '        output:')
    (896, '            res=summarise_alg_output_res_path("bnlearn_pcstable"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_pcstable" in pattern_strings', 'rule summarise_bnlearn_pcstable', 'shell']
    (897, '        shell:')
    (898, '            summarise_alg_shell("bnlearn_pcstable")')
    (899, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_pcstable" in pattern_strings', 'rule join_summaries_bnlearn_pcstable', 'input']
    (900, '    rule join_summaries_bnlearn_pcstable:')
    (901, '        input:')
    (902, '            "workflow/scripts/evaluation/run_summarise.R",')
    (903, '            conf=configfilename,')
    (904, '            res=join_string_sampled_model("bnlearn_pcstable"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_pcstable" in pattern_strings', 'rule join_summaries_bnlearn_pcstable', 'output']
    (905, '        output:')
    (906, '            join_summaries_output("bnlearn_pcstable"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_pcstable" in pattern_strings', 'rule join_summaries_bnlearn_pcstable', 'script']
    (907, '        script:')
    (908, '            "../scripts/evaluation/join_csv_files.R"')
    (909, '')
    (910, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iamb" in pattern_strings', 'rule bnlearn_iamb', 'input']
    (911, 'if "bnlearn_iamb" in pattern_strings:')
    (912, '')
    (913, '    rule bnlearn_iamb:')
    (914, '        input:')
    (915, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iamb" in pattern_strings', 'rule bnlearn_iamb', 'output']
    (916, '        output:')
    (917, '            adjmat=alg_output_adjmat_path("bnlearn_iamb"),')
    (918, '            time=alg_output_time_path("bnlearn_iamb"),')
    (919, '            ntests=alg_output_ntests_path("bnlearn_iamb"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iamb" in pattern_strings', 'rule bnlearn_iamb', 'container']
    (920, '        container:')
    (921, '            docker_image("bnlearn")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iamb" in pattern_strings', 'rule bnlearn_iamb', 'script']
    (922, '        script:')
    (923, '            "../scripts/structure_learning_algorithms/" + "bnlearn_iamb" + ".R"')
    (924, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iamb" in pattern_strings', 'rule summarise_bnlearn_iamb', 'input']
    (925, '    rule summarise_bnlearn_iamb:')
    (926, '        input:')
    (927, '            "workflow/scripts/evaluation/run_summarise.R",')
    (928, '            data=summarise_alg_input_data_path(),')
    (929, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (930, '            adjmat_est=summarise_alg_input_adjmat_est_path("bnlearn_iamb"),')
    (931, '            time=summarise_alg_input_time_path("bnlearn_iamb"),')
    (932, '            ntests=summarise_alg_input_ntests_path("bnlearn_iamb"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iamb" in pattern_strings', 'rule summarise_bnlearn_iamb', 'output']
    (933, '        output:')
    (934, '            res=summarise_alg_output_res_path("bnlearn_iamb"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iamb" in pattern_strings', 'rule summarise_bnlearn_iamb', 'shell']
    (935, '        shell:')
    (936, '            summarise_alg_shell("bnlearn_iamb")')
    (937, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iamb" in pattern_strings', 'rule join_summaries_bnlearn_iamb', 'input']
    (938, '    rule join_summaries_bnlearn_iamb:')
    (939, '        input:')
    (940, '            "workflow/scripts/evaluation/run_summarise.R",')
    (941, '            conf=configfilename,')
    (942, '            res=join_string_sampled_model("bnlearn_iamb"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iamb" in pattern_strings', 'rule join_summaries_bnlearn_iamb', 'output']
    (943, '        output:')
    (944, '            join_summaries_output("bnlearn_iamb"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iamb" in pattern_strings', 'rule join_summaries_bnlearn_iamb', 'script']
    (945, '        script:')
    (946, '            "../scripts/evaluation/join_csv_files.R"')
    (947, '')
    (948, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_fastiamb" in pattern_strings', 'rule bnlearn_fastiamb', 'input']
    (949, 'if "bnlearn_fastiamb" in pattern_strings:')
    (950, '')
    (951, '    rule bnlearn_fastiamb:')
    (952, '        input:')
    (953, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_fastiamb" in pattern_strings', 'rule bnlearn_fastiamb', 'output']
    (954, '        output:')
    (955, '            adjmat=alg_output_adjmat_path("bnlearn_fastiamb"),')
    (956, '            time=alg_output_time_path("bnlearn_fastiamb"),')
    (957, '            ntests=alg_output_ntests_path("bnlearn_fastiamb"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_fastiamb" in pattern_strings', 'rule bnlearn_fastiamb', 'container']
    (958, '        container:')
    (959, '            docker_image("bnlearn")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_fastiamb" in pattern_strings', 'rule bnlearn_fastiamb', 'script']
    (960, '        script:')
    (961, '            "../scripts/structure_learning_algorithms/" + "bnlearn_fastiamb" + ".R"')
    (962, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_fastiamb" in pattern_strings', 'rule summarise_bnlearn_fastiamb', 'input']
    (963, '    rule summarise_bnlearn_fastiamb:')
    (964, '        input:')
    (965, '            "workflow/scripts/evaluation/run_summarise.R",')
    (966, '            data=summarise_alg_input_data_path(),')
    (967, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (968, '            adjmat_est=summarise_alg_input_adjmat_est_path("bnlearn_fastiamb"),')
    (969, '            time=summarise_alg_input_time_path("bnlearn_fastiamb"),')
    (970, '            ntests=summarise_alg_input_ntests_path("bnlearn_fastiamb"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_fastiamb" in pattern_strings', 'rule summarise_bnlearn_fastiamb', 'output']
    (971, '        output:')
    (972, '            res=summarise_alg_output_res_path("bnlearn_fastiamb"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_fastiamb" in pattern_strings', 'rule summarise_bnlearn_fastiamb', 'shell']
    (973, '        shell:')
    (974, '            summarise_alg_shell("bnlearn_fastiamb")')
    (975, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_fastiamb" in pattern_strings', 'rule join_summaries_bnlearn_fastiamb', 'input']
    (976, '    rule join_summaries_bnlearn_fastiamb:')
    (977, '        input:')
    (978, '            "workflow/scripts/evaluation/run_summarise.R",')
    (979, '            conf=configfilename,')
    (980, '            res=join_string_sampled_model("bnlearn_fastiamb"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_fastiamb" in pattern_strings', 'rule join_summaries_bnlearn_fastiamb', 'output']
    (981, '        output:')
    (982, '            join_summaries_output("bnlearn_fastiamb"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_fastiamb" in pattern_strings', 'rule join_summaries_bnlearn_fastiamb', 'script']
    (983, '        script:')
    (984, '            "../scripts/evaluation/join_csv_files.R"')
    (985, '')
    (986, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iambfdr" in pattern_strings', 'rule bnlearn_iambfdr', 'input']
    (987, 'if "bnlearn_iambfdr" in pattern_strings:')
    (988, '')
    (989, '    rule bnlearn_iambfdr:')
    (990, '        input:')
    (991, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iambfdr" in pattern_strings', 'rule bnlearn_iambfdr', 'output']
    (992, '        output:')
    (993, '            adjmat=alg_output_adjmat_path("bnlearn_iambfdr"),')
    (994, '            time=alg_output_time_path("bnlearn_iambfdr"),')
    (995, '            ntests=alg_output_ntests_path("bnlearn_iambfdr"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iambfdr" in pattern_strings', 'rule bnlearn_iambfdr', 'container']
    (996, '        container:')
    (997, '            docker_image("bnlearn")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iambfdr" in pattern_strings', 'rule bnlearn_iambfdr', 'script']
    (998, '        script:')
    (999, '            "../scripts/structure_learning_algorithms/" + "bnlearn_iambfdr" + ".R"')
    (1000, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iambfdr" in pattern_strings', 'rule summarise_bnlearn_iambfdr', 'input']
    (1001, '    rule summarise_bnlearn_iambfdr:')
    (1002, '        input:')
    (1003, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1004, '            data=summarise_alg_input_data_path(),')
    (1005, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1006, '            adjmat_est=summarise_alg_input_adjmat_est_path("bnlearn_iambfdr"),')
    (1007, '            time=summarise_alg_input_time_path("bnlearn_iambfdr"),')
    (1008, '            ntests=summarise_alg_input_ntests_path("bnlearn_iambfdr"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iambfdr" in pattern_strings', 'rule summarise_bnlearn_iambfdr', 'output']
    (1009, '        output:')
    (1010, '            res=summarise_alg_output_res_path("bnlearn_iambfdr"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iambfdr" in pattern_strings', 'rule summarise_bnlearn_iambfdr', 'shell']
    (1011, '        shell:')
    (1012, '            summarise_alg_shell("bnlearn_iambfdr")')
    (1013, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iambfdr" in pattern_strings', 'rule join_summaries_bnlearn_iambfdr', 'input']
    (1014, '    rule join_summaries_bnlearn_iambfdr:')
    (1015, '        input:')
    (1016, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1017, '            conf=configfilename,')
    (1018, '            res=join_string_sampled_model("bnlearn_iambfdr"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iambfdr" in pattern_strings', 'rule join_summaries_bnlearn_iambfdr', 'output']
    (1019, '        output:')
    (1020, '            join_summaries_output("bnlearn_iambfdr"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_iambfdr" in pattern_strings', 'rule join_summaries_bnlearn_iambfdr', 'script']
    (1021, '        script:')
    (1022, '            "../scripts/evaluation/join_csv_files.R"')
    (1023, '')
    (1024, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmpc" in pattern_strings', 'rule bnlearn_mmpc', 'input']
    (1025, 'if "bnlearn_mmpc" in pattern_strings:')
    (1026, '')
    (1027, '    rule bnlearn_mmpc:')
    (1028, '        input:')
    (1029, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmpc" in pattern_strings', 'rule bnlearn_mmpc', 'output']
    (1030, '        output:')
    (1031, '            adjmat=alg_output_adjmat_path("bnlearn_mmpc"),')
    (1032, '            time=alg_output_time_path("bnlearn_mmpc"),')
    (1033, '            ntests=alg_output_ntests_path("bnlearn_mmpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmpc" in pattern_strings', 'rule bnlearn_mmpc', 'container']
    (1034, '        container:')
    (1035, '            docker_image("bnlearn")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmpc" in pattern_strings', 'rule bnlearn_mmpc', 'script']
    (1036, '        script:')
    (1037, '            "../scripts/structure_learning_algorithms/" + "bnlearn_mmpc" + ".R"')
    (1038, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmpc" in pattern_strings', 'rule summarise_bnlearn_mmpc', 'input']
    (1039, '    rule summarise_bnlearn_mmpc:')
    (1040, '        input:')
    (1041, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1042, '            data=summarise_alg_input_data_path(),')
    (1043, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1044, '            adjmat_est=summarise_alg_input_adjmat_est_path("bnlearn_mmpc"),')
    (1045, '            time=summarise_alg_input_time_path("bnlearn_mmpc"),')
    (1046, '            ntests=summarise_alg_input_ntests_path("bnlearn_mmpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmpc" in pattern_strings', 'rule summarise_bnlearn_mmpc', 'output']
    (1047, '        output:')
    (1048, '            res=summarise_alg_output_res_path("bnlearn_mmpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmpc" in pattern_strings', 'rule summarise_bnlearn_mmpc', 'shell']
    (1049, '        shell:')
    (1050, '            summarise_alg_shell("bnlearn_mmpc")')
    (1051, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmpc" in pattern_strings', 'rule join_summaries_bnlearn_mmpc', 'input']
    (1052, '    rule join_summaries_bnlearn_mmpc:')
    (1053, '        input:')
    (1054, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1055, '            conf=configfilename,')
    (1056, '            res=join_string_sampled_model("bnlearn_mmpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmpc" in pattern_strings', 'rule join_summaries_bnlearn_mmpc', 'output']
    (1057, '        output:')
    (1058, '            join_summaries_output("bnlearn_mmpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmpc" in pattern_strings', 'rule join_summaries_bnlearn_mmpc', 'script']
    (1059, '        script:')
    (1060, '            "../scripts/evaluation/join_csv_files.R"')
    (1061, '')
    (1062, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_sihitonpc" in pattern_strings', 'rule bnlearn_sihitonpc', 'input']
    (1063, 'if "bnlearn_sihitonpc" in pattern_strings:')
    (1064, '')
    (1065, '    rule bnlearn_sihitonpc:')
    (1066, '        input:')
    (1067, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_sihitonpc" in pattern_strings', 'rule bnlearn_sihitonpc', 'output']
    (1068, '        output:')
    (1069, '            adjmat=alg_output_adjmat_path("bnlearn_sihitonpc"),')
    (1070, '            time=alg_output_time_path("bnlearn_sihitonpc"),')
    (1071, '            ntests=alg_output_ntests_path("bnlearn_sihitonpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_sihitonpc" in pattern_strings', 'rule bnlearn_sihitonpc', 'container']
    (1072, '        container:')
    (1073, '            docker_image("bnlearn")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_sihitonpc" in pattern_strings', 'rule bnlearn_sihitonpc', 'script']
    (1074, '        script:')
    (1075, '            "../scripts/structure_learning_algorithms/" + "bnlearn_sihitonpc" + ".R"')
    (1076, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_sihitonpc" in pattern_strings', 'rule summarise_bnlearn_sihitonpc', 'input']
    (1077, '    rule summarise_bnlearn_sihitonpc:')
    (1078, '        input:')
    (1079, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1080, '            data=summarise_alg_input_data_path(),')
    (1081, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1082, '            adjmat_est=summarise_alg_input_adjmat_est_path("bnlearn_sihitonpc"),')
    (1083, '            time=summarise_alg_input_time_path("bnlearn_sihitonpc"),')
    (1084, '            ntests=summarise_alg_input_ntests_path("bnlearn_sihitonpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_sihitonpc" in pattern_strings', 'rule summarise_bnlearn_sihitonpc', 'output']
    (1085, '        output:')
    (1086, '            res=summarise_alg_output_res_path("bnlearn_sihitonpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_sihitonpc" in pattern_strings', 'rule summarise_bnlearn_sihitonpc', 'shell']
    (1087, '        shell:')
    (1088, '            summarise_alg_shell("bnlearn_sihitonpc")')
    (1089, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_sihitonpc" in pattern_strings', 'rule join_summaries_bnlearn_sihitonpc', 'input']
    (1090, '    rule join_summaries_bnlearn_sihitonpc:')
    (1091, '        input:')
    (1092, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1093, '            conf=configfilename,')
    (1094, '            res=join_string_sampled_model("bnlearn_sihitonpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_sihitonpc" in pattern_strings', 'rule join_summaries_bnlearn_sihitonpc', 'output']
    (1095, '        output:')
    (1096, '            join_summaries_output("bnlearn_sihitonpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_sihitonpc" in pattern_strings', 'rule join_summaries_bnlearn_sihitonpc', 'script']
    (1097, '        script:')
    (1098, '            "../scripts/evaluation/join_csv_files.R"')
    (1099, '')
    (1100, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hpc" in pattern_strings', 'rule bnlearn_hpc', 'input']
    (1101, 'if "bnlearn_hpc" in pattern_strings:')
    (1102, '')
    (1103, '    rule bnlearn_hpc:')
    (1104, '        input:')
    (1105, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hpc" in pattern_strings', 'rule bnlearn_hpc', 'output']
    (1106, '        output:')
    (1107, '            adjmat=alg_output_adjmat_path("bnlearn_hpc"),')
    (1108, '            time=alg_output_time_path("bnlearn_hpc"),')
    (1109, '            ntests=alg_output_ntests_path("bnlearn_hpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hpc" in pattern_strings', 'rule bnlearn_hpc', 'container']
    (1110, '        container:')
    (1111, '            docker_image("bnlearn")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hpc" in pattern_strings', 'rule bnlearn_hpc', 'script']
    (1112, '        script:')
    (1113, '            "../scripts/structure_learning_algorithms/" + "bnlearn_hpc" + ".R"')
    (1114, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hpc" in pattern_strings', 'rule summarise_bnlearn_hpc', 'input']
    (1115, '    rule summarise_bnlearn_hpc:')
    (1116, '        input:')
    (1117, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1118, '            data=summarise_alg_input_data_path(),')
    (1119, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1120, '            adjmat_est=summarise_alg_input_adjmat_est_path("bnlearn_hpc"),')
    (1121, '            time=summarise_alg_input_time_path("bnlearn_hpc"),')
    (1122, '            ntests=summarise_alg_input_ntests_path("bnlearn_hpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hpc" in pattern_strings', 'rule summarise_bnlearn_hpc', 'output']
    (1123, '        output:')
    (1124, '            res=summarise_alg_output_res_path("bnlearn_hpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hpc" in pattern_strings', 'rule summarise_bnlearn_hpc', 'shell']
    (1125, '        shell:')
    (1126, '            summarise_alg_shell("bnlearn_hpc")')
    (1127, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hpc" in pattern_strings', 'rule join_summaries_bnlearn_hpc', 'input']
    (1128, '    rule join_summaries_bnlearn_hpc:')
    (1129, '        input:')
    (1130, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1131, '            conf=configfilename,')
    (1132, '            res=join_string_sampled_model("bnlearn_hpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hpc" in pattern_strings', 'rule join_summaries_bnlearn_hpc', 'output']
    (1133, '        output:')
    (1134, '            join_summaries_output("bnlearn_hpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_hpc" in pattern_strings', 'rule join_summaries_bnlearn_hpc', 'script']
    (1135, '        script:')
    (1136, '            "../scripts/evaluation/join_csv_files.R"')
    (1137, '')
    (1138, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_h2pc" in pattern_strings', 'rule bnlearn_h2pc', 'input']
    (1139, 'if "bnlearn_h2pc" in pattern_strings:')
    (1140, '')
    (1141, '    rule bnlearn_h2pc:')
    (1142, '        input:')
    (1143, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_h2pc" in pattern_strings', 'rule bnlearn_h2pc', 'output']
    (1144, '        output:')
    (1145, '            adjmat=alg_output_adjmat_path("bnlearn_h2pc"),')
    (1146, '            time=alg_output_time_path("bnlearn_h2pc"),')
    (1147, '            ntests=alg_output_ntests_path("bnlearn_h2pc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_h2pc" in pattern_strings', 'rule bnlearn_h2pc', 'container']
    (1148, '        container:')
    (1149, '            docker_image("bnlearn")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_h2pc" in pattern_strings', 'rule bnlearn_h2pc', 'script']
    (1150, '        script:')
    (1151, '            "../scripts/structure_learning_algorithms/" + "bnlearn_h2pc" + ".R"')
    (1152, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_h2pc" in pattern_strings', 'rule summarise_bnlearn_h2pc', 'input']
    (1153, '    rule summarise_bnlearn_h2pc:')
    (1154, '        input:')
    (1155, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1156, '            data=summarise_alg_input_data_path(),')
    (1157, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1158, '            adjmat_est=summarise_alg_input_adjmat_est_path("bnlearn_h2pc"),')
    (1159, '            time=summarise_alg_input_time_path("bnlearn_h2pc"),')
    (1160, '            ntests=summarise_alg_input_ntests_path("bnlearn_h2pc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_h2pc" in pattern_strings', 'rule summarise_bnlearn_h2pc', 'output']
    (1161, '        output:')
    (1162, '            res=summarise_alg_output_res_path("bnlearn_h2pc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_h2pc" in pattern_strings', 'rule summarise_bnlearn_h2pc', 'shell']
    (1163, '        shell:')
    (1164, '            summarise_alg_shell("bnlearn_h2pc")')
    (1165, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_h2pc" in pattern_strings', 'rule join_summaries_bnlearn_h2pc', 'input']
    (1166, '    rule join_summaries_bnlearn_h2pc:')
    (1167, '        input:')
    (1168, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1169, '            conf=configfilename,')
    (1170, '            res=join_string_sampled_model("bnlearn_h2pc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_h2pc" in pattern_strings', 'rule join_summaries_bnlearn_h2pc', 'output']
    (1171, '        output:')
    (1172, '            join_summaries_output("bnlearn_h2pc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_h2pc" in pattern_strings', 'rule join_summaries_bnlearn_h2pc', 'script']
    (1173, '        script:')
    (1174, '            "../scripts/evaluation/join_csv_files.R"')
    (1175, '')
    (1176, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_rsmax2" in pattern_strings', 'rule bnlearn_rsmax2', 'input']
    (1177, 'if "bnlearn_rsmax2" in pattern_strings:')
    (1178, '')
    (1179, '    rule bnlearn_rsmax2:')
    (1180, '        input:')
    (1181, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_rsmax2" in pattern_strings', 'rule bnlearn_rsmax2', 'output']
    (1182, '        output:')
    (1183, '            adjmat=alg_output_adjmat_path("bnlearn_rsmax2"),')
    (1184, '            time=alg_output_time_path("bnlearn_rsmax2"),')
    (1185, '            ntests=alg_output_ntests_path("bnlearn_rsmax2"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_rsmax2" in pattern_strings', 'rule bnlearn_rsmax2', 'container']
    (1186, '        container:')
    (1187, '            docker_image("bnlearn")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_rsmax2" in pattern_strings', 'rule bnlearn_rsmax2', 'script']
    (1188, '        script:')
    (1189, '            "../scripts/structure_learning_algorithms/" + "bnlearn_rsmax2" + ".R"')
    (1190, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_rsmax2" in pattern_strings', 'rule summarise_bnlearn_rsmax2', 'input']
    (1191, '    rule summarise_bnlearn_rsmax2:')
    (1192, '        input:')
    (1193, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1194, '            data=summarise_alg_input_data_path(),')
    (1195, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1196, '            adjmat_est=summarise_alg_input_adjmat_est_path("bnlearn_rsmax2"),')
    (1197, '            time=summarise_alg_input_time_path("bnlearn_rsmax2"),')
    (1198, '            ntests=summarise_alg_input_ntests_path("bnlearn_rsmax2"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_rsmax2" in pattern_strings', 'rule summarise_bnlearn_rsmax2', 'output']
    (1199, '        output:')
    (1200, '            res=summarise_alg_output_res_path("bnlearn_rsmax2"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_rsmax2" in pattern_strings', 'rule summarise_bnlearn_rsmax2', 'shell']
    (1201, '        shell:')
    (1202, '            summarise_alg_shell("bnlearn_rsmax2")')
    (1203, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_rsmax2" in pattern_strings', 'rule join_summaries_bnlearn_rsmax2', 'input']
    (1204, '    rule join_summaries_bnlearn_rsmax2:')
    (1205, '        input:')
    (1206, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1207, '            conf=configfilename,')
    (1208, '            res=join_string_sampled_model("bnlearn_rsmax2"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_rsmax2" in pattern_strings', 'rule join_summaries_bnlearn_rsmax2', 'output']
    (1209, '        output:')
    (1210, '            join_summaries_output("bnlearn_rsmax2"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_rsmax2" in pattern_strings', 'rule join_summaries_bnlearn_rsmax2', 'script']
    (1211, '        script:')
    (1212, '            "../scripts/evaluation/join_csv_files.R"')
    (1213, '')
    (1214, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "rblip_asobs" in pattern_strings', 'rule blip', 'input']
    (1215, 'if "rblip_asobs" in pattern_strings:')
    (1216, '')
    (1217, '    rule blip:')
    (1218, '        input:')
    (1219, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "rblip_asobs" in pattern_strings', 'rule blip', 'output']
    (1220, '        output:')
    (1221, '            adjmat=alg_output_adjmat_path("rblip_asobs"),')
    (1222, '            time=alg_output_time_path("rblip_asobs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "rblip_asobs" in pattern_strings', 'rule blip', 'shell']
    (1223, '        shell:')
    (1224, '            alg_shell("rblip_asobs")')
    (1225, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "rblip_asobs" in pattern_strings', 'rule summarise_blip', 'input']
    (1226, '    rule summarise_blip:')
    (1227, '        input:')
    (1228, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1229, '            data=summarise_alg_input_data_path(),')
    (1230, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1231, '            adjmat_est=summarise_alg_input_adjmat_est_path("rblip_asobs"),')
    (1232, '            time=summarise_alg_input_time_path("rblip_asobs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "rblip_asobs" in pattern_strings', 'rule summarise_blip', 'output']
    (1233, '        output:')
    (1234, '            res=summarise_alg_output_res_path("rblip_asobs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "rblip_asobs" in pattern_strings', 'rule summarise_blip', 'shell']
    (1235, '        shell:')
    (1236, '            summarise_alg_shell("rblip_asobs")')
    (1237, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "rblip_asobs" in pattern_strings', 'rule join_summaries_blip', 'input']
    (1238, '    rule join_summaries_blip:')
    (1239, '        input:')
    (1240, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1241, '            conf=configfilename,')
    (1242, '            res=join_string_sampled_model("rblip_asobs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "rblip_asobs" in pattern_strings', 'rule join_summaries_blip', 'output']
    (1243, '        output:')
    (1244, '            join_summaries_output("rblip_asobs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "rblip_asobs" in pattern_strings', 'rule join_summaries_blip', 'script']
    (1245, '        script:')
    (1246, '            "../scripts/evaluation/join_csv_files.R"')
    (1247, '')
    (1248, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_itsearch" in pattern_strings', 'rule itsearch', 'input']
    (1249, 'if "bidag_itsearch" in pattern_strings:')
    (1250, '')
    (1251, '    rule itsearch:')
    (1252, '        input:')
    (1253, '            "workflow/scripts/structure_learning_algorithms/bidag_iterative_search.R",')
    (1254, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_itsearch" in pattern_strings', 'rule itsearch', 'output']
    (1255, '        output:')
    (1256, '            adjmat=alg_output_adjmat_path("bidag_itsearch"),')
    (1257, '            time=alg_output_time_path("bidag_itsearch"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_itsearch" in pattern_strings', 'rule itsearch', 'container']
    (1258, '        container:')
    (1259, '            docker_image("bidag")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_itsearch" in pattern_strings', 'rule itsearch', 'script']
    (1260, '        script:')
    (1261, '            "../scripts/structure_learning_algorithms/bidag_iterative_search.R"')
    (1262, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_itsearch" in pattern_strings', 'rule summarise_itsearch', 'input']
    (1263, '    rule summarise_itsearch:')
    (1264, '        input:')
    (1265, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1266, '            data=summarise_alg_input_data_path(),')
    (1267, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1268, '            adjmat_est=summarise_alg_input_adjmat_est_path("bidag_itsearch"),')
    (1269, '            time=summarise_alg_input_time_path("bidag_itsearch"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_itsearch" in pattern_strings', 'rule summarise_itsearch', 'output']
    (1270, '        output:')
    (1271, '            res=summarise_alg_output_res_path("bidag_itsearch"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_itsearch" in pattern_strings', 'rule summarise_itsearch', 'shell']
    (1272, '        shell:')
    (1273, '            summarise_alg_shell("bidag_itsearch")')
    (1274, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_itsearch" in pattern_strings', 'rule join_summaries_itsearch', 'input']
    (1275, '    rule join_summaries_itsearch:')
    (1276, '        input:')
    (1277, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1278, '            conf=configfilename,')
    (1279, '            res=join_string_sampled_model("bidag_itsearch"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_itsearch" in pattern_strings', 'rule join_summaries_itsearch', 'output']
    (1280, '        output:')
    (1281, '            join_summaries_output("bidag_itsearch"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_itsearch" in pattern_strings', 'rule join_summaries_itsearch', 'script']
    (1282, '        script:')
    (1283, '            "../scripts/evaluation/join_csv_files.R"')
    (1284, '')
    (1285, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "pcalg_pc" in pattern_strings', 'rule pcalg', 'input']
    (1286, 'if "pcalg_pc" in pattern_strings:')
    (1287, '')
    (1288, '    rule pcalg:')
    (1289, '        input:')
    (1290, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "pcalg_pc" in pattern_strings', 'rule pcalg', 'output']
    (1291, '        output:')
    (1292, '            adjmat=alg_output_adjmat_path("pcalg_pc"),')
    (1293, '            time=alg_output_time_path("pcalg_pc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "pcalg_pc" in pattern_strings', 'rule pcalg', 'container']
    (1294, '        container:')
    (1295, '            docker_image("pcalg")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "pcalg_pc" in pattern_strings', 'rule pcalg', 'script']
    (1296, '        script:')
    (1297, '            "../scripts/structure_learning_algorithms/pcalg_pc.R"')
    (1298, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "pcalg_pc" in pattern_strings', 'rule summarise_pcalg', 'input']
    (1299, '    rule summarise_pcalg:')
    (1300, '        input:')
    (1301, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1302, '            data=summarise_alg_input_data_path(),')
    (1303, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1304, '            adjmat_est=summarise_alg_input_adjmat_est_path("pcalg_pc"),')
    (1305, '            time=summarise_alg_input_time_path("pcalg_pc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "pcalg_pc" in pattern_strings', 'rule summarise_pcalg', 'output']
    (1306, '        output:')
    (1307, '            res=summarise_alg_output_res_path("pcalg_pc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "pcalg_pc" in pattern_strings', 'rule summarise_pcalg', 'shell']
    (1308, '        shell:')
    (1309, '            summarise_alg_shell("pcalg_pc")')
    (1310, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "pcalg_pc" in pattern_strings', 'rule join_summaries_pcalg', 'input']
    (1311, '    rule join_summaries_pcalg:')
    (1312, '        input:')
    (1313, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1314, '            conf=configfilename,')
    (1315, '            res=join_string_sampled_model("pcalg_pc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "pcalg_pc" in pattern_strings', 'rule join_summaries_pcalg', 'output']
    (1316, '        output:')
    (1317, '            join_summaries_output("pcalg_pc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "pcalg_pc" in pattern_strings', 'rule join_summaries_pcalg', 'script']
    (1318, '        script:')
    (1319, '            "../scripts/evaluation/join_csv_files.R"')
    (1320, '')
    (1321, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "dualpc" in pattern_strings', 'rule dualpc', 'input']
    (1322, 'if "dualpc" in pattern_strings:')
    (1323, '')
    (1324, '    rule dualpc:')
    (1325, '        input:')
    (1326, '            "workflow/scripts/structure_learning_algorithms/dualpc.R",')
    (1327, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "dualpc" in pattern_strings', 'rule dualpc', 'output']
    (1328, '        output:')
    (1329, '            adjmat=alg_output_adjmat_path("dualpc"),')
    (1330, '            time=alg_output_time_path("dualpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "dualpc" in pattern_strings', 'rule dualpc', 'container']
    (1331, '        container:')
    (1332, '            docker_image("dualpc")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "dualpc" in pattern_strings', 'rule dualpc', 'script']
    (1333, '        script:')
    (1334, '            "../scripts/structure_learning_algorithms/dualpc.R"')
    (1335, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "dualpc" in pattern_strings', 'rule summarise_dualpc', 'input']
    (1336, '    rule summarise_dualpc:')
    (1337, '        input:')
    (1338, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1339, '            data=summarise_alg_input_data_path(),')
    (1340, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1341, '            adjmat_est=summarise_alg_input_adjmat_est_path("dualpc"),')
    (1342, '            time=summarise_alg_input_time_path("dualpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "dualpc" in pattern_strings', 'rule summarise_dualpc', 'output']
    (1343, '        output:')
    (1344, '            res=summarise_alg_output_res_path("dualpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "dualpc" in pattern_strings', 'rule summarise_dualpc', 'shell']
    (1345, '        shell:')
    (1346, '            summarise_alg_shell("dualpc")')
    (1347, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "dualpc" in pattern_strings', 'rule join_summaries_dualpc', 'input']
    (1348, '    rule join_summaries_dualpc:')
    (1349, '        input:')
    (1350, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1351, '            conf=configfilename,')
    (1352, '            res=join_string_sampled_model("dualpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "dualpc" in pattern_strings', 'rule join_summaries_dualpc', 'output']
    (1353, '        output:')
    (1354, '            join_summaries_output("dualpc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "dualpc" in pattern_strings', 'rule join_summaries_dualpc', 'script']
    (1355, '        script:')
    (1356, '            "../scripts/evaluation/join_csv_files.R"')
    (1357, '')
    (1358, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmhc" in pattern_strings', 'rule mmhc', 'input']
    (1359, 'if "bnlearn_mmhc" in pattern_strings:')
    (1360, '')
    (1361, '    rule mmhc:')
    (1362, '        input:')
    (1363, '            "workflow/scripts/structure_learning_algorithms/bnlearn_mmhc.R",')
    (1364, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmhc" in pattern_strings', 'rule mmhc', 'output']
    (1365, '        output:')
    (1366, '            adjmat=alg_output_adjmat_path("bnlearn_mmhc"),')
    (1367, '            time=alg_output_time_path("bnlearn_mmhc"),')
    (1368, '            ntests=alg_output_ntests_path("bnlearn_mmhc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmhc" in pattern_strings', 'rule mmhc', 'script']
    (1369, '        script:')
    (1370, '            "../scripts/structure_learning_algorithms/bnlearn_mmhc.R"')
    (1371, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmhc" in pattern_strings', 'rule summarise_mmhc', 'input']
    (1372, '    rule summarise_mmhc:')
    (1373, '        input:')
    (1374, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1375, '            data=summarise_alg_input_data_path(),')
    (1376, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1377, '            adjmat_est=summarise_alg_input_adjmat_est_path("bnlearn_mmhc"),')
    (1378, '            time=summarise_alg_input_time_path("bnlearn_mmhc"),')
    (1379, '            ntests=summarise_alg_input_ntests_path("bnlearn_mmhc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmhc" in pattern_strings', 'rule summarise_mmhc', 'output']
    (1380, '        output:')
    (1381, '            res=summarise_alg_output_res_path("bnlearn_mmhc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmhc" in pattern_strings', 'rule summarise_mmhc', 'shell']
    (1382, '        shell:')
    (1383, '            summarise_alg_shell("bnlearn_mmhc")')
    (1384, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmhc" in pattern_strings', 'rule join_summaries_mmhc', 'input']
    (1385, '    rule join_summaries_mmhc:')
    (1386, '        input:')
    (1387, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1388, '            conf=configfilename,')
    (1389, '            res=join_string_sampled_model("bnlearn_mmhc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmhc" in pattern_strings', 'rule join_summaries_mmhc', 'output']
    (1390, '        output:')
    (1391, '            join_summaries_output("bnlearn_mmhc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bnlearn_mmhc" in pattern_strings', 'rule join_summaries_mmhc', 'script']
    (1392, '        script:')
    (1393, '            "../scripts/evaluation/join_csv_files.R"')
    (1394, '')
    (1395, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gobnilp" in pattern_strings', 'rule gobnilp', 'input']
    (1396, 'if "gobnilp" in pattern_strings:')
    (1397, '')
    (1398, '    rule gobnilp:')
    (1399, '        input:')
    (1400, '            data=alg_input_data(),')
    (1401, '            extra_args="resources/extra_args/{extra_args}",')
    (1402, '            constraints="resources/constraints/{constraints}",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gobnilp" in pattern_strings', 'rule gobnilp', 'output']
    (1403, '        output:')
    (1404, '            adjmat=alg_output_adjmat_path("gobnilp"),')
    (1405, '            time=alg_output_time_path("gobnilp"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gobnilp" in pattern_strings', 'rule gobnilp', 'container']
    (1406, '        container:')
    (1407, '            docker_image("gobnilp")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gobnilp" in pattern_strings', 'rule gobnilp', 'shell']
    (1408, '        shell:')
    (1409, '            alg_shell("gobnilp")')
    (1410, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gobnilp" in pattern_strings', 'rule summarise_gobnilp', 'input']
    (1411, '    rule summarise_gobnilp:')
    (1412, '        input:')
    (1413, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1414, '            data=summarise_alg_input_data_path(),')
    (1415, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1416, '            adjmat_est=summarise_alg_input_adjmat_est_path("gobnilp"),')
    (1417, '            time=summarise_alg_input_time_path("gobnilp"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gobnilp" in pattern_strings', 'rule summarise_gobnilp', 'output']
    (1418, '        output:')
    (1419, '            res=summarise_alg_output_res_path("gobnilp"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gobnilp" in pattern_strings', 'rule summarise_gobnilp', 'shell']
    (1420, '        shell:')
    (1421, '            summarise_alg_shell("gobnilp")')
    (1422, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gobnilp" in pattern_strings', 'rule join_summaries_gobnilp', 'input']
    (1423, '    rule join_summaries_gobnilp:')
    (1424, '        input:')
    (1425, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1426, '            conf=configfilename,')
    (1427, '            res=join_string_sampled_model("gobnilp"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gobnilp" in pattern_strings', 'rule join_summaries_gobnilp', 'output']
    (1428, '        output:')
    (1429, '            join_summaries_output("gobnilp"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gobnilp" in pattern_strings', 'rule join_summaries_gobnilp', 'script']
    (1430, '        script:')
    (1431, '            "../scripts/evaluation/join_csv_files.R"')
    (1432, '')
    (1433, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fges" in pattern_strings', 'rule tetrad_fges', 'input']
    (1434, 'if "tetrad_fges" in pattern_strings:')
    (1435, '')
    (1436, '    rule tetrad_fges:')
    (1437, '        input:')
    (1438, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fges" in pattern_strings', 'rule tetrad_fges', 'output']
    (1439, '        output:')
    (1440, '            adjmat=alg_output_adjmat_path("tetrad_fges"),')
    (1441, '            time=alg_output_time_path("tetrad_fges"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fges" in pattern_strings', 'rule tetrad_fges', 'container']
    (1442, '        container:')
    (1443, '            docker_image("tetrad")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fges" in pattern_strings', 'rule tetrad_fges', 'script']
    (1444, '        script:')
    (1445, '            "../scripts/structure_learning_algorithms/tetrad_fges.py"')
    (1446, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fges" in pattern_strings', 'rule summarise_tetrad_fges', 'input']
    (1447, '    rule summarise_tetrad_fges:')
    (1448, '        input:')
    (1449, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1450, '            data=summarise_alg_input_data_path(),')
    (1451, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1452, '            adjmat_est=summarise_alg_input_adjmat_est_path("tetrad_fges"),')
    (1453, '            time=summarise_alg_input_time_path("tetrad_fges"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fges" in pattern_strings', 'rule summarise_tetrad_fges', 'output']
    (1454, '        output:')
    (1455, '            res=summarise_alg_output_res_path("tetrad_fges"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fges" in pattern_strings', 'rule summarise_tetrad_fges', 'shell']
    (1456, '        shell:')
    (1457, '            summarise_alg_shell("tetrad_fges")')
    (1458, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fges" in pattern_strings', 'rule join_summaries_tetrad_fges', 'input']
    (1459, '    rule join_summaries_tetrad_fges:')
    (1460, '        input:')
    (1461, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1462, '            conf=configfilename,')
    (1463, '            res=join_string_sampled_model("tetrad_fges"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fges" in pattern_strings', 'rule join_summaries_tetrad_fges', 'output']
    (1464, '        output:')
    (1465, '            join_summaries_output("tetrad_fges"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fges" in pattern_strings', 'rule join_summaries_tetrad_fges', 'script']
    (1466, '        script:')
    (1467, '            "../scripts/evaluation/join_csv_files.R"')
    (1468, '')
    (1469, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fci" in pattern_strings', 'rule tetrad_fci', 'input']
    (1470, 'if "tetrad_fci" in pattern_strings:')
    (1471, '')
    (1472, '    rule tetrad_fci:')
    (1473, '        input:')
    (1474, '            "workflow/scripts/structure_learning_algorithms/tetrad_fci.py",')
    (1475, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fci" in pattern_strings', 'rule tetrad_fci', 'output']
    (1476, '        output:')
    (1477, '            adjmat=alg_output_adjmat_path("tetrad_fci"),')
    (1478, '            time=alg_output_time_path("tetrad_fci"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fci" in pattern_strings', 'rule tetrad_fci', 'container']
    (1479, '        container:')
    (1480, '            docker_image("tetrad")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fci" in pattern_strings', 'rule tetrad_fci', 'script']
    (1481, '        script:')
    (1482, '            "../scripts/structure_learning_algorithms/tetrad_fci.py"')
    (1483, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fci" in pattern_strings', 'rule summarise_tetrad_fci', 'input']
    (1484, '    rule summarise_tetrad_fci:')
    (1485, '        input:')
    (1486, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1487, '            data=summarise_alg_input_data_path(),')
    (1488, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1489, '            adjmat_est=summarise_alg_input_adjmat_est_path("tetrad_fci"),')
    (1490, '            time=summarise_alg_input_time_path("tetrad_fci"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fci" in pattern_strings', 'rule summarise_tetrad_fci', 'output']
    (1491, '        output:')
    (1492, '            res=summarise_alg_output_res_path("tetrad_fci"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fci" in pattern_strings', 'rule summarise_tetrad_fci', 'shell']
    (1493, '        shell:')
    (1494, '            summarise_alg_shell("tetrad_fci")')
    (1495, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fci" in pattern_strings', 'rule join_summaries_tetrad_fci', 'input']
    (1496, '    rule join_summaries_tetrad_fci:')
    (1497, '        input:')
    (1498, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1499, '            conf=configfilename,')
    (1500, '            res=join_string_sampled_model("tetrad_fci"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fci" in pattern_strings', 'rule join_summaries_tetrad_fci', 'output']
    (1501, '        output:')
    (1502, '            join_summaries_output("tetrad_fci"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fci" in pattern_strings', 'rule join_summaries_tetrad_fci', 'script']
    (1503, '        script:')
    (1504, '            "../scripts/evaluation/join_csv_files.R"')
    (1505, '')
    (1506, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_gfci" in pattern_strings', 'rule tetrad_gfci', 'input']
    (1507, 'if "tetrad_gfci" in pattern_strings:')
    (1508, '')
    (1509, '    rule tetrad_gfci:')
    (1510, '        input:')
    (1511, '            "workflow/scripts/structure_learning_algorithms/tetrad_gfci.py",')
    (1512, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_gfci" in pattern_strings', 'rule tetrad_gfci', 'output']
    (1513, '        output:')
    (1514, '            adjmat=alg_output_adjmat_path("tetrad_gfci"),')
    (1515, '            time=alg_output_time_path("tetrad_gfci"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_gfci" in pattern_strings', 'rule tetrad_gfci', 'container']
    (1516, '        container:')
    (1517, '            docker_image("tetrad")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_gfci" in pattern_strings', 'rule tetrad_gfci', 'script']
    (1518, '        script:')
    (1519, '            "../scripts/structure_learning_algorithms/tetrad_gfci.py"')
    (1520, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_gfci" in pattern_strings', 'rule summarise_tetrad_gfci', 'input']
    (1521, '    rule summarise_tetrad_gfci:')
    (1522, '        input:')
    (1523, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1524, '            data=summarise_alg_input_data_path(),')
    (1525, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1526, '            adjmat_est=summarise_alg_input_adjmat_est_path("tetrad_gfci"),')
    (1527, '            time=summarise_alg_input_time_path("tetrad_gfci"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_gfci" in pattern_strings', 'rule summarise_tetrad_gfci', 'output']
    (1528, '        output:')
    (1529, '            res=summarise_alg_output_res_path("tetrad_gfci"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_gfci" in pattern_strings', 'rule summarise_tetrad_gfci', 'shell']
    (1530, '        shell:')
    (1531, '            summarise_alg_shell("tetrad_gfci")')
    (1532, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_gfci" in pattern_strings', 'rule join_summaries_tetrad_gfci', 'input']
    (1533, '    rule join_summaries_tetrad_gfci:')
    (1534, '        input:')
    (1535, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1536, '            conf=configfilename,')
    (1537, '            res=join_string_sampled_model("tetrad_gfci"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_gfci" in pattern_strings', 'rule join_summaries_tetrad_gfci', 'output']
    (1538, '        output:')
    (1539, '            join_summaries_output("tetrad_gfci"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_gfci" in pattern_strings', 'rule join_summaries_tetrad_gfci', 'script']
    (1540, '        script:')
    (1541, '            "../scripts/evaluation/join_csv_files.R"')
    (1542, '')
    (1543, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_rfci" in pattern_strings', 'rule tetrad_rfci', 'input']
    (1544, 'if "tetrad_rfci" in pattern_strings:')
    (1545, '')
    (1546, '    rule tetrad_rfci:')
    (1547, '        input:')
    (1548, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_rfci" in pattern_strings', 'rule tetrad_rfci', 'output']
    (1549, '        output:')
    (1550, '            adjmat=alg_output_adjmat_path("tetrad_rfci"),')
    (1551, '            time=alg_output_time_path("tetrad_rfci"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_rfci" in pattern_strings', 'rule tetrad_rfci', 'container']
    (1552, '        container:')
    (1553, '            docker_image("tetrad")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_rfci" in pattern_strings', 'rule tetrad_rfci', 'script']
    (1554, '        script:')
    (1555, '            "../scripts/structure_learning_algorithms/tetrad_rfci.py"')
    (1556, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_rfci" in pattern_strings', 'rule summarise_tetrad_rfci', 'input']
    (1557, '    rule summarise_tetrad_rfci:')
    (1558, '        input:')
    (1559, '            data=summarise_alg_input_data_path(),')
    (1560, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1561, '            adjmat_est=summarise_alg_input_adjmat_est_path("tetrad_rfci"),')
    (1562, '            time=summarise_alg_input_time_path("tetrad_rfci"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_rfci" in pattern_strings', 'rule summarise_tetrad_rfci', 'output']
    (1563, '        output:')
    (1564, '            res=summarise_alg_output_res_path("tetrad_rfci"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_rfci" in pattern_strings', 'rule summarise_tetrad_rfci', 'shell']
    (1565, '        shell:')
    (1566, '            summarise_alg_shell("tetrad_rfci")')
    (1567, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_rfci" in pattern_strings', 'rule join_summaries_tetrad_rfci', 'input']
    (1568, '    rule join_summaries_tetrad_rfci:')
    (1569, '        input:')
    (1570, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1571, '            conf=configfilename,')
    (1572, '            res=join_string_sampled_model("tetrad_rfci"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_rfci" in pattern_strings', 'rule join_summaries_tetrad_rfci', 'output']
    (1573, '        output:')
    (1574, '            join_summaries_output("tetrad_rfci"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_rfci" in pattern_strings', 'rule join_summaries_tetrad_rfci', 'script']
    (1575, '        script:')
    (1576, '            "../scripts/evaluation/join_csv_files.R"')
    (1577, '')
    (1578, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_order_mcmc" in pattern_strings', 'rule order_mcmc', 'input']
    (1579, 'if "bidag_order_mcmc" in pattern_strings:')
    (1580, '')
    (1581, '    rule order_mcmc:')
    (1582, '        input:')
    (1583, '            "workflow/scripts/structure_learning_algorithms/bidag_order_mcmc.R",')
    (1584, '            data=alg_input_data(),')
    (1585, '            startspace="{output_dir}/adjmat_estimate/{data}/algorithm=/{startspace_algorithm}/seed={replicate}/adjmat.csv",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_order_mcmc" in pattern_strings', 'rule order_mcmc', 'output']
    (1586, '        output:  # data seems to be matched wrongly')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_order_mcmc" in pattern_strings', 'rule order_mcmc']
    (1587, '            seqgraph=alg_output_seqgraph_path("bidag_order_mcmc"),')
    (1588, '            time=alg_output_time_path("bidag_order_mcmc"),')
    (1589, '        container:')
    (1590, '            docker_image("bidag")')
    (1591, '        script:')
    (1592, '            "../scripts/structure_learning_algorithms/bidag_order_mcmc.R"')
    (1593, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_order_mcmc" in pattern_strings', 'rule order_mcmc_est', 'input']
    (1594, '    rule order_mcmc_est:')
    (1595, '        input:')
    (1596, '            "workflow/scripts/evaluation/graphtraj_est.py",')
    (1597, '            traj=alg_output_seqgraph_path_fine_match("bidag_order_mcmc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_order_mcmc" in pattern_strings', 'rule order_mcmc_est', 'output']
    (1598, '        output:')
    (1599, '            adjmat=adjmat_estimate_path_mcmc("bidag_order_mcmc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_order_mcmc" in pattern_strings', 'rule order_mcmc_est', 'params']
    (1600, '        params:')
    (1601, '            graph_type="dag",')
    (1602, '            estimator="threshold",')
    (1603, '            threshold="{threshold}",')
    (1604, '            burnin="{burnin}",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_order_mcmc" in pattern_strings', 'rule order_mcmc_est', 'container']
    (1605, '        container:')
    (1606, '            docker_image("networkx")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_order_mcmc" in pattern_strings', 'rule order_mcmc_est', 'script']
    (1607, '        script:')
    (1608, '            "../scripts/evaluation/graphtraj_est.py"')
    (1609, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_order_mcmc" in pattern_strings', 'rule summarise_order_mcmc', 'input']
    (1610, '    rule summarise_order_mcmc:')
    (1611, '        input:')
    (1612, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1613, '            data=data_path(),')
    (1614, '            adjmat_true=adjmat_true_path(),')
    (1615, '            adjmat_est=adjmat_estimate_path_mcmc("bidag_order_mcmc"),')
    (1616, '            time=time_path("bidag_order_mcmc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_order_mcmc" in pattern_strings', 'rule summarise_order_mcmc', 'output']
    (1617, '        output:')
    (1618, '            res=result_path_mcmc("bidag_order_mcmc"),  # {data} is used for the data module here. not as the whole datamodel')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_order_mcmc" in pattern_strings', 'rule summarise_order_mcmc', 'shell']
    (1619, '        shell:')
    (1620, '            summarise_alg_shell("bidag_order_mcmc")')
    (1621, '')
    (1622, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_order_mcmc" in pattern_strings']
    (1623, '    """ This should be OK')
    (1624, '                """')
    (1625, '')
    (1626, '    rule join_summaries_order_mcmc:')
    (1627, '        input:')
    (1628, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1629, '            conf=configfilename,')
    (1630, '            res=join_string_sampled_model("bidag_order_mcmc"),')
    (1631, '        output:')
    (1632, '            join_summaries_output("bidag_order_mcmc"),')
    (1633, '        script:')
    (1634, '            "../scripts/evaluation/join_csv_files.R"')
    (1635, '')
    (1636, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings', 'rule bidag_partition_mcmc', 'input']
    (1637, 'if "bidag_partition_mcmc" in pattern_strings:')
    (1638, '')
    (1639, '    rule bidag_partition_mcmc:')
    (1640, '        input:')
    (1641, '            "workflow/scripts/structure_learning_algorithms/bidag_partition_mcmc.R",')
    (1642, '            data=alg_input_data(),')
    (1643, '            startspace="{output_dir}/adjmat_estimate/{data}/algorithm=/{startspace_algorithm}/seed={replicate}/adjmat.csv",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings', 'rule bidag_partition_mcmc', 'output']
    (1644, '        output:  # data seems to be matched wrongly')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings', 'rule bidag_partition_mcmc']
    (1645, '            seqgraph=alg_output_seqgraph_path("bidag_partition_mcmc"),')
    (1646, '            time=alg_output_time_path("bidag_partition_mcmc"),')
    (1647, '        container:')
    (1648, '            docker_image("bidag")')
    (1649, '        script:')
    (1650, '            "../scripts/structure_learning_algorithms/bidag_partition_mcmc.R"')
    (1651, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings', 'rule bidag_partition_mcmc_est', 'input']
    (1652, '    rule bidag_partition_mcmc_est:')
    (1653, '        input:')
    (1654, '            "workflow/scripts/evaluation/graphtraj_est.py",')
    (1655, '            traj=alg_output_seqgraph_path_nocomp("bidag_partition_mcmc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings', 'rule bidag_partition_mcmc_est', 'output']
    (1656, '        output:')
    (1657, '            adjmat=alg_output_adjmat_path("bidag_partition_mcmc"),  #here is the difference from order_mcmc. matching diffferently.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings', 'rule bidag_partition_mcmc_est', 'params']
    (1658, '        params:')
    (1659, '            graph_type="dag",')
    (1660, '            estimator="map",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings', 'rule bidag_partition_mcmc_est', 'container']
    (1661, '        container:')
    (1662, '            docker_image("networkx")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings', 'rule bidag_partition_mcmc_est', 'script']
    (1663, '        script:')
    (1664, '            "../scripts/evaluation/graphtraj_est.py"')
    (1665, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings', 'rule summarise_bidag_partition_mcmc', 'input']
    (1666, '    rule summarise_bidag_partition_mcmc:')
    (1667, '        input:')
    (1668, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1669, '            data=summarise_alg_input_data_path(),')
    (1670, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1671, '            adjmat_est=summarise_alg_input_adjmat_est_path("bidag_partition_mcmc"),')
    (1672, '            time=summarise_alg_input_time_path("bidag_partition_mcmc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings', 'rule summarise_bidag_partition_mcmc', 'output']
    (1673, '        output:')
    (1674, '            res=summarise_alg_output_res_path("bidag_partition_mcmc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings', 'rule summarise_bidag_partition_mcmc', 'shell']
    (1675, '        shell:')
    (1676, '            summarise_alg_shell("bidag_partition_mcmc")')
    (1677, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings', 'rule join_summaries_bidag_partition_mcmc', 'input']
    (1678, '    rule join_summaries_bidag_partition_mcmc:')
    (1679, '        input:')
    (1680, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1681, '            conf=configfilename,')
    (1682, '            res=join_string_sampled_model("bidag_partition_mcmc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings', 'rule join_summaries_bidag_partition_mcmc', 'output']
    (1683, '        output:')
    (1684, '            join_summaries_output("bidag_partition_mcmc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "bidag_partition_mcmc" in pattern_strings', 'rule join_summaries_bidag_partition_mcmc', 'script']
    (1685, '        script:')
    (1686, '            "../scripts/evaluation/join_csv_files.R"')
    (1687, '')
    (1688, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings', 'rule trilearn', 'input']
    (1689, 'if "trilearn_pgibbs" in pattern_strings:')
    (1690, '')
    (1691, '    rule trilearn:')
    (1692, '        input:')
    (1693, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings', 'rule trilearn', 'output']
    (1694, '        output:')
    (1695, '            adjvecs=alg_output_seqgraph_path("trilearn_pgibbs"),')
    (1696, '            time=alg_output_time_path("trilearn_pgibbs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings', 'rule trilearn', 'container']
    (1697, '        container:')
    (1698, '            docker_image("trilearn")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings', 'rule trilearn', 'shell']
    (1699, '        shell:')
    (1700, '            alg_shell("trilearn_pgibbs")')
    (1701, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings', 'rule trilearn_est', 'input']
    (1702, '    rule trilearn_est:')
    (1703, '        input:')
    (1704, '            "workflow/scripts/evaluation/graphtraj_est.py",')
    (1705, '            traj=alg_output_seqgraph_path_nocomp("trilearn_pgibbs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings', 'rule trilearn_est', 'output']
    (1706, '        output:')
    (1707, '            adjmat=alg_output_adjmat_path("trilearn_pgibbs"),  #here is the difference from order_mcmc. matching diffferently.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings', 'rule trilearn_est', 'params']
    (1708, '        params:')
    (1709, '            graph_type="chordal",')
    (1710, '            estimator="map",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings', 'rule trilearn_est', 'container']
    (1711, '        container:')
    (1712, '            docker_image("networkx")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings', 'rule trilearn_est', 'script']
    (1713, '        script:')
    (1714, '            "../scripts/evaluation/graphtraj_est.py"')
    (1715, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings', 'rule summarise_trilearn', 'input']
    (1716, '    rule summarise_trilearn:')
    (1717, '        input:')
    (1718, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1719, '            data=summarise_alg_input_data_path(),')
    (1720, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1721, '            adjmat_est=summarise_alg_input_adjmat_est_path("trilearn_pgibbs"),')
    (1722, '            time=summarise_alg_input_time_path("trilearn_pgibbs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings', 'rule summarise_trilearn', 'output']
    (1723, '        output:')
    (1724, '            res=summarise_alg_output_res_path("trilearn_pgibbs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings', 'rule summarise_trilearn', 'shell']
    (1725, '        shell:')
    (1726, '            summarise_alg_shell("trilearn_pgibbs")')
    (1727, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings', 'rule join_summaries_trilearn', 'input']
    (1728, '    rule join_summaries_trilearn:')
    (1729, '        input:')
    (1730, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1731, '            conf=configfilename,')
    (1732, '            res=join_string_sampled_model("trilearn_pgibbs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings', 'rule join_summaries_trilearn', 'output']
    (1733, '        output:')
    (1734, '            join_summaries_output("trilearn_pgibbs"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "trilearn_pgibbs" in pattern_strings', 'rule join_summaries_trilearn', 'script']
    (1735, '        script:')
    (1736, '            "../scripts/evaluation/join_csv_files.R"')
    (1737, '')
    (1738, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gt13_multipair" in pattern_strings', 'rule gt13_multipair', 'input']
    (1739, 'if "gt13_multipair" in pattern_strings:')
    (1740, '')
    (1741, '    rule gt13_multipair:')
    (1742, '        input:')
    (1743, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gt13_multipair" in pattern_strings', 'rule gt13_multipair', 'output']
    (1744, '        output:')
    (1745, '            seqgraph=alg_output_seqgraph_path("gt13_multipair"),')
    (1746, '            time=alg_output_time_path("gt13_multipair"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gt13_multipair" in pattern_strings', 'rule gt13_multipair', 'container']
    (1747, '        container:')
    (1748, '            docker_image("thomasjava")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gt13_multipair" in pattern_strings', 'rule gt13_multipair', 'shell']
    (1749, '        shell:')
    (1750, '            alg_shell("gt13_multipair")')
    (1751, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gt13_multipair" in pattern_strings', 'rule gt13_multipair_est', 'input']
    (1752, '    rule gt13_multipair_est:')
    (1753, '        input:')
    (1754, '            "workflow/scripts/evaluation/graphtraj_est.py",')
    (1755, '            traj=alg_output_seqgraph_path_nocomp("gt13_multipair"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gt13_multipair" in pattern_strings', 'rule gt13_multipair_est', 'output']
    (1756, '        output:')
    (1757, '            adjmat=alg_output_adjmat_path("gt13_multipair"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gt13_multipair" in pattern_strings', 'rule gt13_multipair_est', 'params']
    (1758, '        params:')
    (1759, '            graph_type="chordal",')
    (1760, '            estimator="map",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gt13_multipair" in pattern_strings', 'rule gt13_multipair_est', 'container']
    (1761, '        container:')
    (1762, '            docker_image("networkx")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gt13_multipair" in pattern_strings', 'rule gt13_multipair_est', 'script']
    (1763, '        script:')
    (1764, '            "../scripts/evaluation/graphtraj_est.py"')
    (1765, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gt13_multipair" in pattern_strings', 'rule summarise_gt13_multipair', 'input']
    (1766, '    rule summarise_gt13_multipair:')
    (1767, '        input:')
    (1768, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1769, '            data=summarise_alg_input_data_path(),')
    (1770, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1771, '            adjmat_est=summarise_alg_input_adjmat_est_path("gt13_multipair"),')
    (1772, '            time=summarise_alg_input_time_path("gt13_multipair"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gt13_multipair" in pattern_strings', 'rule summarise_gt13_multipair', 'output']
    (1773, '        output:')
    (1774, '            res=summarise_alg_output_res_path("gt13_multipair"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gt13_multipair" in pattern_strings', 'rule summarise_gt13_multipair', 'shell']
    (1775, '        shell:')
    (1776, '            summarise_alg_shell("gt13_multipair")')
    (1777, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gt13_multipair" in pattern_strings', 'rule join_summaries_gt13_multipair', 'input']
    (1778, '    rule join_summaries_gt13_multipair:')
    (1779, '        input:')
    (1780, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1781, '            conf=configfilename,')
    (1782, '            res=join_string_sampled_model("gt13_multipair"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gt13_multipair" in pattern_strings', 'rule join_summaries_gt13_multipair', 'output']
    (1783, '        output:')
    (1784, '            join_summaries_output("gt13_multipair"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gt13_multipair" in pattern_strings', 'rule join_summaries_gt13_multipair', 'script']
    (1785, '        script:')
    (1786, '            "../scripts/evaluation/join_csv_files.R"')
    (1787, '')
    (1788, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gg99_singlepair" in pattern_strings', 'rule gg99_singlepair', 'input']
    (1789, 'if "gg99_singlepair" in pattern_strings:')
    (1790, '')
    (1791, '    rule gg99_singlepair:')
    (1792, '        input:')
    (1793, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gg99_singlepair" in pattern_strings', 'rule gg99_singlepair', 'output']
    (1794, '        output:')
    (1795, '            seqgraph=alg_output_seqgraph_path("gg99_singlepair"),')
    (1796, '            time=alg_output_time_path("gg99_singlepair"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gg99_singlepair" in pattern_strings', 'rule gg99_singlepair', 'container']
    (1797, '        container:')
    (1798, '            docker_image("thomasjava")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gg99_singlepair" in pattern_strings', 'rule gg99_singlepair', 'shell']
    (1799, '        shell:')
    (1800, '            alg_shell("gg99_singlepair")')
    (1801, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gg99_singlepair" in pattern_strings', 'rule gg99_singlepair_est', 'input']
    (1802, '    rule gg99_singlepair_est:')
    (1803, '        input:')
    (1804, '            "workflow/scripts/evaluation/graphtraj_est.py",')
    (1805, '            traj=alg_output_seqgraph_path_nocomp("gg99_singlepair"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gg99_singlepair" in pattern_strings', 'rule gg99_singlepair_est', 'output']
    (1806, '        output:')
    (1807, '            adjmat=alg_output_adjmat_path("gg99_singlepair"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gg99_singlepair" in pattern_strings', 'rule gg99_singlepair_est', 'params']
    (1808, '        params:')
    (1809, '            graph_type="chordal",')
    (1810, '            estimator="map",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gg99_singlepair" in pattern_strings', 'rule gg99_singlepair_est', 'container']
    (1811, '        container:')
    (1812, '            docker_image("networkx")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gg99_singlepair" in pattern_strings', 'rule gg99_singlepair_est', 'script']
    (1813, '        script:')
    (1814, '            "../scripts/evaluation/graphtraj_est.py"')
    (1815, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gg99_singlepair" in pattern_strings', 'rule summarise_gg99_singlepair', 'input']
    (1816, '    rule summarise_gg99_singlepair:')
    (1817, '        input:')
    (1818, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1819, '            data=summarise_alg_input_data_path(),')
    (1820, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1821, '            adjmat_est=summarise_alg_input_adjmat_est_path("gg99_singlepair"),')
    (1822, '            time=summarise_alg_input_time_path("gg99_singlepair"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gg99_singlepair" in pattern_strings', 'rule summarise_gg99_singlepair', 'output']
    (1823, '        output:')
    (1824, '            res=summarise_alg_output_res_path("gg99_singlepair"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gg99_singlepair" in pattern_strings', 'rule summarise_gg99_singlepair', 'shell']
    (1825, '        shell:')
    (1826, '            summarise_alg_shell("gg99_singlepair")')
    (1827, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gg99_singlepair" in pattern_strings', 'rule join_summaries_gg99_singlepair', 'input']
    (1828, '    rule join_summaries_gg99_singlepair:')
    (1829, '        input:')
    (1830, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1831, '            conf=configfilename,')
    (1832, '            res=join_string_sampled_model("gg99_singlepair"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gg99_singlepair" in pattern_strings', 'rule join_summaries_gg99_singlepair', 'output']
    (1833, '        output:')
    (1834, '            join_summaries_output("gg99_singlepair"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "gg99_singlepair" in pattern_strings', 'rule join_summaries_gg99_singlepair', 'script']
    (1835, '        script:')
    (1836, '            "../scripts/evaluation/join_csv_files.R"')
    (1837, '')
    (1838, '')
    (1839, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fofc" in pattern_strings', 'rule tetrad_fofc', 'input']
    (1840, 'if "tetrad_fofc" in pattern_strings:')
    (1841, '')
    (1842, '    rule tetrad_fofc:')
    (1843, '        input:')
    (1844, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fofc" in pattern_strings', 'rule tetrad_fofc', 'output']
    (1845, '        output:')
    (1846, '            adjmat=alg_output_adjmat_path("tetrad_fofc"),')
    (1847, '            time=alg_output_time_path("tetrad_fofc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fofc" in pattern_strings', 'rule tetrad_fofc', 'container']
    (1848, '        container:')
    (1849, '            docker_image("tetrad")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fofc" in pattern_strings', 'rule tetrad_fofc', 'script']
    (1850, '        script:')
    (1851, '            "../scripts/structure_learning_algorithms/tetrad_fofc.py"')
    (1852, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fofc" in pattern_strings', 'rule summarise_tetrad_fofc', 'input']
    (1853, '    rule summarise_tetrad_fofc:')
    (1854, '        input:')
    (1855, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1856, '            data=summarise_alg_input_data_path(),')
    (1857, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1858, '            adjmat_est=summarise_alg_input_adjmat_est_path("tetrad_fofc"),')
    (1859, '            time=summarise_alg_input_time_path("tetrad_fofc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fofc" in pattern_strings', 'rule summarise_tetrad_fofc', 'output']
    (1860, '        output:')
    (1861, '            res=summarise_alg_output_res_path("tetrad_fofc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fofc" in pattern_strings', 'rule summarise_tetrad_fofc', 'shell']
    (1862, '        shell:')
    (1863, '            summarise_alg_shell("tetrad_fofc")')
    (1864, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fofc" in pattern_strings', 'rule join_summaries_tetrad_fofc', 'input']
    (1865, '    rule join_summaries_tetrad_fofc:')
    (1866, '        input:')
    (1867, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1868, '            conf=configfilename,')
    (1869, '            res=join_string_sampled_model("tetrad_fofc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fofc" in pattern_strings', 'rule join_summaries_tetrad_fofc', 'output']
    (1870, '        output:')
    (1871, '            join_summaries_output("tetrad_fofc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fofc" in pattern_strings', 'rule join_summaries_tetrad_fofc', 'script']
    (1872, '        script:')
    (1873, '            "../scripts/evaluation/join_csv_files.R"')
    (1874, '')
    (1875, '')
    (1876, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fas" in pattern_strings', 'rule tetrad_fas', 'input']
    (1877, 'if "tetrad_fas" in pattern_strings:')
    (1878, '')
    (1879, '    rule tetrad_fas:')
    (1880, '        input:')
    (1881, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fas" in pattern_strings', 'rule tetrad_fas', 'output']
    (1882, '        output:')
    (1883, '            adjmat=alg_output_adjmat_path("tetrad_fas"),')
    (1884, '            time=alg_output_time_path("tetrad_fas"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fas" in pattern_strings', 'rule tetrad_fas', 'container']
    (1885, '        container:')
    (1886, '            docker_image("tetrad")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fas" in pattern_strings', 'rule tetrad_fas', 'script']
    (1887, '        script:')
    (1888, '            "../scripts/structure_learning_algorithms/tetrad_fas.py"')
    (1889, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fas" in pattern_strings', 'rule summarise_tetrad_fas', 'input']
    (1890, '    rule summarise_tetrad_fas:')
    (1891, '        input:')
    (1892, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1893, '            data=summarise_alg_input_data_path(),')
    (1894, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1895, '            adjmat_est=summarise_alg_input_adjmat_est_path("tetrad_fas"),')
    (1896, '            time=summarise_alg_input_time_path("tetrad_fas"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fas" in pattern_strings', 'rule summarise_tetrad_fas', 'output']
    (1897, '        output:')
    (1898, '            res=summarise_alg_output_res_path("tetrad_fas"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fas" in pattern_strings', 'rule summarise_tetrad_fas', 'shell']
    (1899, '        shell:')
    (1900, '            summarise_alg_shell("tetrad_fas")')
    (1901, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fas" in pattern_strings', 'rule join_summaries_tetrad_fas', 'input']
    (1902, '    rule join_summaries_tetrad_fas:')
    (1903, '        input:')
    (1904, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1905, '            conf=configfilename,')
    (1906, '            res=join_string_sampled_model("tetrad_fas"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fas" in pattern_strings', 'rule join_summaries_tetrad_fas', 'output']
    (1907, '        output:')
    (1908, '            join_summaries_output("tetrad_fas"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fas" in pattern_strings', 'rule join_summaries_tetrad_fas', 'script']
    (1909, '        script:')
    (1910, '            "../scripts/evaluation/join_csv_files.R"')
    (1911, '')
    (1912, '')
    (1913, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fask" in pattern_strings', 'rule tetrad_fask', 'input']
    (1914, 'if "tetrad_fask" in pattern_strings:')
    (1915, '')
    (1916, '    rule tetrad_fask:')
    (1917, '        input:')
    (1918, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fask" in pattern_strings', 'rule tetrad_fask', 'output']
    (1919, '        output:')
    (1920, '            adjmat=alg_output_adjmat_path("tetrad_fask"),')
    (1921, '            time=alg_output_time_path("tetrad_fask"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fask" in pattern_strings', 'rule tetrad_fask', 'container']
    (1922, '        container:')
    (1923, '            docker_image("tetrad")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fask" in pattern_strings', 'rule tetrad_fask', 'script']
    (1924, '        script:')
    (1925, '            "../scripts/structure_learning_algorithms/tetrad_fask.py"')
    (1926, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fask" in pattern_strings', 'rule summarise_tetrad_fask', 'input']
    (1927, '    rule summarise_tetrad_fask:')
    (1928, '        input:')
    (1929, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1930, '            data=summarise_alg_input_data_path(),')
    (1931, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1932, '            adjmat_est=summarise_alg_input_adjmat_est_path("tetrad_fask"),')
    (1933, '            time=summarise_alg_input_time_path("tetrad_fask"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fask" in pattern_strings', 'rule summarise_tetrad_fask', 'output']
    (1934, '        output:')
    (1935, '            res=summarise_alg_output_res_path("tetrad_fask"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fask" in pattern_strings', 'rule summarise_tetrad_fask', 'shell']
    (1936, '        shell:')
    (1937, '            summarise_alg_shell("tetrad_fask")')
    (1938, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fask" in pattern_strings', 'rule join_summaries_tetrad_fask', 'input']
    (1939, '    rule join_summaries_tetrad_fask:')
    (1940, '        input:')
    (1941, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1942, '            conf=configfilename,')
    (1943, '            res=join_string_sampled_model("tetrad_fask"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fask" in pattern_strings', 'rule join_summaries_tetrad_fask', 'output']
    (1944, '        output:')
    (1945, '            join_summaries_output("tetrad_fask"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_fask" in pattern_strings', 'rule join_summaries_tetrad_fask', 'script']
    (1946, '        script:')
    (1947, '            "../scripts/evaluation/join_csv_files.R"')
    (1948, '')
    (1949, '')
    (1950, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_pc-all" in pattern_strings', 'rule tetrad_pc_all', 'input']
    (1951, 'if "tetrad_pc-all" in pattern_strings:')
    (1952, '')
    (1953, '    rule tetrad_pc_all:')
    (1954, '        input:')
    (1955, '            "workflow/scripts/structure_learning_algorithms/tetrad_pc.py",')
    (1956, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_pc-all" in pattern_strings', 'rule tetrad_pc_all', 'output']
    (1957, '        output:')
    (1958, '            adjmat=alg_output_adjmat_path("tetrad_pc-all"),')
    (1959, '            time=alg_output_time_path("tetrad_pc-all"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_pc-all" in pattern_strings', 'rule tetrad_pc_all', 'container']
    (1960, '        container:')
    (1961, '            docker_image("tetrad")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_pc-all" in pattern_strings', 'rule tetrad_pc_all', 'script']
    (1962, '        script:')
    (1963, '            "../scripts/structure_learning_algorithms/tetrad_pc.py"')
    (1964, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_pc-all" in pattern_strings', 'rule summarise_tetrad_pc_all', 'input']
    (1965, '    rule summarise_tetrad_pc_all:')
    (1966, '        input:')
    (1967, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1968, '            data=summarise_alg_input_data_path(),')
    (1969, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (1970, '            adjmat_est=summarise_alg_input_adjmat_est_path("tetrad_pc-all"),')
    (1971, '            time=summarise_alg_input_time_path("tetrad_pc-all"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_pc-all" in pattern_strings', 'rule summarise_tetrad_pc_all', 'output']
    (1972, '        output:')
    (1973, '            res=summarise_alg_output_res_path("tetrad_pc-all"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_pc-all" in pattern_strings', 'rule summarise_tetrad_pc_all', 'shell']
    (1974, '        shell:')
    (1975, '            summarise_alg_shell("tetrad_pc-all")')
    (1976, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_pc-all" in pattern_strings', 'rule join_summaries_tetrad_pc_all', 'input']
    (1977, '    rule join_summaries_tetrad_pc_all:')
    (1978, '        input:')
    (1979, '            "workflow/scripts/evaluation/run_summarise.R",')
    (1980, '            conf=configfilename,')
    (1981, '            res=join_string_sampled_model("tetrad_pc-all"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_pc-all" in pattern_strings', 'rule join_summaries_tetrad_pc_all', 'output']
    (1982, '        output:')
    (1983, '            join_summaries_output("tetrad_pc-all"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_pc-all" in pattern_strings', 'rule join_summaries_tetrad_pc_all', 'script']
    (1984, '        script:')
    (1985, '            "../scripts/evaluation/join_csv_files.R"')
    (1986, '')
    (1987, '')
    (1988, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_lingam" in pattern_strings', 'rule tetrad_lingam', 'input']
    (1989, 'if "tetrad_lingam" in pattern_strings:')
    (1990, '')
    (1991, '    rule tetrad_lingam:')
    (1992, '        input:')
    (1993, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_lingam" in pattern_strings', 'rule tetrad_lingam', 'output']
    (1994, '        output:')
    (1995, '            adjmat=alg_output_adjmat_path("tetrad_lingam"),')
    (1996, '            time=alg_output_time_path("tetrad_lingam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_lingam" in pattern_strings', 'rule tetrad_lingam', 'container']
    (1997, '        container:')
    (1998, '            docker_image("tetrad")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_lingam" in pattern_strings', 'rule tetrad_lingam', 'script']
    (1999, '        script:')
    (2000, '            "../scripts/structure_learning_algorithms/tetrad_lingam.py"')
    (2001, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_lingam" in pattern_strings', 'rule summarise_tetrad_lingam', 'input']
    (2002, '    rule summarise_tetrad_lingam:')
    (2003, '        input:')
    (2004, '            "workflow/scripts/evaluation/run_summarise.R",')
    (2005, '            data=summarise_alg_input_data_path(),')
    (2006, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (2007, '            adjmat_est=summarise_alg_input_adjmat_est_path("tetrad_lingam"),')
    (2008, '            time=summarise_alg_input_time_path("tetrad_lingam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_lingam" in pattern_strings', 'rule summarise_tetrad_lingam', 'output']
    (2009, '        output:')
    (2010, '            res=summarise_alg_output_res_path("tetrad_lingam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_lingam" in pattern_strings', 'rule summarise_tetrad_lingam', 'shell']
    (2011, '        shell:')
    (2012, '            summarise_alg_shell("tetrad_lingam")')
    (2013, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_lingam" in pattern_strings', 'rule join_summaries_tetrad_lingam', 'input']
    (2014, '    rule join_summaries_tetrad_lingam:')
    (2015, '        input:')
    (2016, '            "workflow/scripts/evaluation/run_summarise.R",')
    (2017, '            conf=configfilename,')
    (2018, '            res=join_string_sampled_model("tetrad_lingam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_lingam" in pattern_strings', 'rule join_summaries_tetrad_lingam', 'output']
    (2019, '        output:')
    (2020, '            join_summaries_output("tetrad_lingam"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_lingam" in pattern_strings', 'rule join_summaries_tetrad_lingam', 'script']
    (2021, '        script:')
    (2022, '            "../scripts/evaluation/join_csv_files.R"')
    (2023, '')
    (2024, '')
    (2025, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_imgscont" in pattern_strings', 'rule tetrad_imgscont', 'input']
    (2026, 'if "tetrad_imgscont" in pattern_strings:')
    (2027, '')
    (2028, '    rule tetrad_imgscont:')
    (2029, '        input:')
    (2030, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_imgscont" in pattern_strings', 'rule tetrad_imgscont', 'output']
    (2031, '        output:')
    (2032, '            adjmat=alg_output_adjmat_path("tetrad_imgscont"),')
    (2033, '            time=alg_output_time_path("tetrad_imgscont"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_imgscont" in pattern_strings', 'rule tetrad_imgscont', 'container']
    (2034, '        container:')
    (2035, '            docker_image("tetrad")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_imgscont" in pattern_strings', 'rule tetrad_imgscont', 'script']
    (2036, '        script:')
    (2037, '            "../scripts/structure_learning_algorithms/tetrad_imgscont.py"')
    (2038, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_imgscont" in pattern_strings', 'rule summarise_tetrad_imgscont', 'input']
    (2039, '    rule summarise_tetrad_imgscont:')
    (2040, '        input:')
    (2041, '            "workflow/scripts/evaluation/run_summarise.R",')
    (2042, '            data=summarise_alg_input_data_path(),')
    (2043, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (2044, '            adjmat_est=summarise_alg_input_adjmat_est_path("tetrad_imgscont"),')
    (2045, '            time=summarise_alg_input_time_path("tetrad_imgscont"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_imgscont" in pattern_strings', 'rule summarise_tetrad_imgscont', 'output']
    (2046, '        output:')
    (2047, '            res=summarise_alg_output_res_path("tetrad_imgscont"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_imgscont" in pattern_strings', 'rule summarise_tetrad_imgscont', 'shell']
    (2048, '        shell:')
    (2049, '            summarise_alg_shell("tetrad_imgscont")')
    (2050, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_imgscont" in pattern_strings', 'rule join_summaries_tetrad_imgscont', 'input']
    (2051, '    rule join_summaries_tetrad_imgscont:')
    (2052, '        input:')
    (2053, '            "workflow/scripts/evaluation/run_summarise.R",')
    (2054, '            conf=configfilename,')
    (2055, '            res=join_string_sampled_model("tetrad_imgscont"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_imgscont" in pattern_strings', 'rule join_summaries_tetrad_imgscont', 'output']
    (2056, '        output:')
    (2057, '            join_summaries_output("tetrad_imgscont"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_imgscont" in pattern_strings', 'rule join_summaries_tetrad_imgscont', 'script']
    (2058, '        script:')
    (2059, '            "../scripts/evaluation/join_csv_files.R"')
    (2060, '')
    (2061, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_ftfc" in pattern_strings', 'rule tetrad_ftfc', 'input']
    (2062, 'if "tetrad_ftfc" in pattern_strings:')
    (2063, '')
    (2064, '    rule tetrad_ftfc:')
    (2065, '        input:')
    (2066, '            data=alg_input_data(),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_ftfc" in pattern_strings', 'rule tetrad_ftfc', 'output']
    (2067, '        output:')
    (2068, '            adjmat=alg_output_adjmat_path("tetrad_ftfc"),')
    (2069, '            time=alg_output_time_path("tetrad_ftfc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_ftfc" in pattern_strings', 'rule tetrad_ftfc', 'container']
    (2070, '        container:')
    (2071, '            docker_image("tetrad")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_ftfc" in pattern_strings', 'rule tetrad_ftfc', 'script']
    (2072, '        script:')
    (2073, '            "../scripts/structure_learning_algorithms/tetrad_ftfc.py"')
    (2074, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_ftfc" in pattern_strings', 'rule summarise_tetrad_ftfc', 'input']
    (2075, '    rule summarise_tetrad_ftfc:')
    (2076, '        input:')
    (2077, '            "workflow/scripts/evaluation/run_summarise.R",')
    (2078, '            data=summarise_alg_input_data_path(),')
    (2079, '            adjmat_true=summarise_alg_input_adjmat_true_path(),')
    (2080, '            adjmat_est=summarise_alg_input_adjmat_est_path("tetrad_ftfc"),')
    (2081, '            time=summarise_alg_input_time_path("tetrad_ftfc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_ftfc" in pattern_strings', 'rule summarise_tetrad_ftfc', 'output']
    (2082, '        output:')
    (2083, '            res=summarise_alg_output_res_path("tetrad_ftfc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_ftfc" in pattern_strings', 'rule summarise_tetrad_ftfc', 'shell']
    (2084, '        shell:')
    (2085, '            summarise_alg_shell("tetrad_ftfc")')
    (2086, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_ftfc" in pattern_strings', 'rule join_summaries_tetrad_ftfc', 'input']
    (2087, '    rule join_summaries_tetrad_ftfc:')
    (2088, '        input:')
    (2089, '            "workflow/scripts/evaluation/run_summarise.R",')
    (2090, '            conf=configfilename,')
    (2091, '            res=join_string_sampled_model("tetrad_ftfc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_ftfc" in pattern_strings', 'rule join_summaries_tetrad_ftfc', 'output']
    (2092, '        output:')
    (2093, '            join_summaries_output("tetrad_ftfc"),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_rules.smk
context_key: ['if "tetrad_ftfc" in pattern_strings', 'rule join_summaries_tetrad_ftfc', 'script']
    (2094, '        script:')
    (2095, '            "../scripts/evaluation/join_csv_files.R"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/rules/algorithm_summary_shell_command.smk
context_key: ['elif algorithm == "tetrad_imgscont"']
    (1113, '    elif algorithm == "tetrad_imgscont":')
    (1114, '        return  "Rscript workflow/scripts/evaluation/run_summarise.R " \\\\')
    (1115, '                "--adjmat_true {input.adjmat_true} " \\\\')
    (1116, '                "--adjmat_est {input.adjmat_est} " \\\\')
    (1117, '                "--filename {output.res} " \\\\')
    (1118, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname id              --colval {wildcards.id} " \\\\')
    (1119, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname replicate   --colval {wildcards.replicate} "\\\\')
    (1120, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname algorithm   --colval tetrad_imgscont "\\\\')
    (1121, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname adjmat           --colval {wildcards.adjmat} " \\\\')
    (1122, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname parameters              --colval {wildcards.bn} " \\\\')
    (1123, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname data           --colval {wildcards.data} " \\\\')
    (1124, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname timeout       --colval {wildcards.timeout} "\\\\')
    (1125, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname time        --colval `cat {input.time}` " \\\\')
    (1126, '                " && python workflow/scripts/utils/add_column.py --filename {output} --colname ntests             --colval None " \\\\')
    (1127, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/Snakefile
context_key: ['if ecode != 0 and apptainer_ecode != 0']
    (20, 'if ecode != 0 and apptainer_ecode != 0:')
    (21, '    raise Exception("Benchpress requires Singularity >= 3.2 or Apptainer.")')
    (22, '')
    (23, '# If Singularity and not apptainer is installer, check the version of Singularity.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/Snakefile
context_key: ['if ecode == 0 and apptainer_ecode != 0', 'if float(v1) < 3 or (float(v1)==3 and float(v2) < 2)']
    (24, 'if ecode == 0 and apptainer_ecode != 0:')
    (25, '    v1 = outp.split()[2].split(".")[0]')
    (26, '    v2 = outp.split()[2].split(".")[1]')
    (27, '    smkver = float(v1 + "." + v2)')
    (28, '    if float(v1) < 3 or (float(v1)==3 and float(v2) < 2):')
    (29, '        raise Exception("You have " +outp+ ". Benchpress requires Singularity >= 3.2.")')
    (30, '')
    (31, '# This is a workaround. Needed a variable for the configfilename')
    (32, '# and this seemed to be the only way. But there are probably better ways.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=felixleopoldo/benchpress, file=workflow/Snakefile
context_key: ['if arg=="--configfile" or arg=="--configfiles"']
    (37, '    if arg=="--configfile" or arg=="--configfiles": # This is strange')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khayer/rna_seq_standard_pipeline, file=workflow/rules/mapping.smk
context_key: ['if not config["stranded"]', 'rule bamCoverage_CPM_single', 'resources']
    (142, 'if not config["stranded"]:')
    (143, '    rule bamCoverage_CPM_single:')
    (144, '        input: "results/mapped/{sample_name}_Aligned.sortedByCoord.out.bam", "results/mapped/{sample_name}_Aligned.sortedByCoord.out.bam.bai"')
    (145, '        output: "results/coverage/{sample_name}_CPM.bw"')
    (146, '        log:  "00log/{sample_name}.bamCoverage_CPM_single"')
    (147, '        conda: "../envs/deeptools.yaml"')
    (148, '        resources:')
    (149, '            cpu = 4,')
    (150, '            mem = "10",')
    (151, '            time = "12:00:00"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khayer/rna_seq_standard_pipeline, file=workflow/rules/mapping.smk
context_key: ['if not config["stranded"]', 'rule bamCoverage_CPM_single', 'params']
    (152, '        params:')
    (153, '            blacklist = "--blackListFileName " + config["blacklist"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khayer/rna_seq_standard_pipeline, file=workflow/rules/mapping.smk
context_key: ['if not config["stranded"]', 'rule bamCoverage_CPM_single', 'message']
    (154, '        message: "bamCoverage_CPM_single {input}: {resources.cpu} threads" #"/ {params.mem}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khayer/rna_seq_standard_pipeline, file=workflow/rules/mapping.smk
context_key: ['if not config["stranded"]', 'rule bamCoverage_CPM_single', 'shell']
    (155, '        shell: ')
    (156, '            """')
    (157, '            bamCoverage -bs 1 -b {input[0]} -o {output[0]} -p 4 --normalizeUsing CPM {params.blacklist} --exactScaling --ignoreDuplicates --minMappingQuality 255')
    (158, '            """')
    (159, '')
    (160, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khayer/rna_seq_standard_pipeline, file=workflow/Snakefile
context_key: ['if not os.path.exists("00log")']
    (15, 'if not os.path.exists("00log"):')
    (16, '    os.makedirs("00log")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khayer/rna_seq_standard_pipeline, file=workflow/Snakefile
context_key: ['if not os.path.exists("logs_slurm")']
    (17, 'if not os.path.exists("logs_slurm"):')
    (18, '    os.makedirs("logs_slurm")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khayer/rna_seq_standard_pipeline, file=workflow/Snakefile
context_key: ['if not os.path.exists("results")']
    (19, 'if not os.path.exists("results"):')
    (20, '    os.makedirs("results")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khayer/rna_seq_standard_pipeline, file=workflow/Snakefile
context_key: ['if not os.path.exists("results/coverage")']
    (21, 'if not os.path.exists("results/coverage"):')
    (22, '    os.makedirs("results/coverage")')
    (23, '')
    (24, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2022_autoimmune_review, file=Snakefile
context_key: ['if (not \\\',\\\' in mapped_trait and mapped_trait.split("/")[-1] in efo_ids) or line[']
    (59, '                 if (not \\\',\\\' in mapped_trait and mapped_trait.split("/")[-1] in efo_ids) or line[:10] == "DATE ADDED":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2022_autoimmune_review, file=Snakefile
context_key: ['if (not \\\',\\\' in mapped_trait and mapped_trait.split("/")[-1] in efo_ids) or line[']
    (133, '                 if (not \\\',\\\' in mapped_trait and mapped_trait.split("/")[-1] in efo_ids) or line[:10] == "DATE ADDED":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2022_autoimmune_review, file=Snakefile
context_key: ['if (not "|" in pgs_efo and pgs_efo in efo_ids) or line[']
    (173, '                 if (not "|" in pgs_efo and pgs_efo in efo_ids) or line[:15] == "Polygenic Score":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2022_autoimmune_review, file=Snakefile
context_key: ['if (pgs_id in pgs_ids) or line[']
    (229, '                 if (pgs_id in pgs_ids) or line[:15] == "Polygenic Score":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2022_autoimmune_review, file=Snakefile
context_key: ['if (pgs_id in pgs_ids) or line[']
    (246, '                 if (pgs_id in pgs_ids) or line[:22] == "PGS Performance Metric":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2022_autoimmune_review, file=Snakefile
context_key: ['if pgs_id in pgs_ids or line[']
    (263, '                 if pgs_id in pgs_ids or line[:14] == "PGS Sample Set":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2022_autoimmune_review, file=Snakefile
context_key: ['if pgs_id in pgs_ids or line[']
    (303, '                 if pgs_id in pgs_ids or line[:15] == "Polygenic Score":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2022_autoimmune_review, file=Snakefile
context_key: ['if pgs_id in pgs_ids or line[']
    (321, '                 if pgs_id in pgs_ids or line[:15] == "PGS Performance":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2022_autoimmune_review, file=Snakefile
context_key: ['if pgs_id in pgs_ids or line[']
    (339, '                 if pgs_id in pgs_ids or line[:14] == "PGS Sample Set":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/2022_autoimmune_review, file=Snakefile
context_key: ['if (not "|" in ukb_efo and (ukb_efo in efo_ids) or line[']
    (359, '                 if (not "|" in ukb_efo and (ukb_efo in efo_ids) or line[:5] == "ZOOMA"):')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=franciscozorrilla/metaGEM, file=Snakefile
context_key: ['if [ $check -eq 0']
    (378, '            if [ $check -eq 0 ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=franciscozorrilla/metaGEM, file=Snakefile
context_key: ['if [[ $bin == *.strict.fa ]] || [[ $bin == *.permissive.fa ]] || [[ $bin == *.s.fa ]] || [[ $bin == *.p.fa ]];the']
    (1361, '            if [[ $bin == *.strict.fa ]] || [[ $bin == *.permissive.fa ]] || [[ $bin == *.s.fa ]] || [[ $bin == *.p.fa ]];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=franciscozorrilla/metaGEM, file=extra/Snakefile_single_end.py
context_key: ['if [[ $bin == *.strict.fa ]] || [[ $bin == *.permissive.fa ]];the']
    (947, '            if [[ $bin == *.strict.fa ]] || [[ $bin == *.permissive.fa ]];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=stracquadaniolab/workflow-pygna, file=Snakefile_paper_old
context_key: ['if len(config["parameters"]["sp_matrix"])>0']
    (25, 'if len(config["parameters"]["sp_matrix"])>0:')
    (26, '    SP_MATRIX=config["parameters"]["sp_matrix"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=stracquadaniolab/workflow-pygna, file=Snakefile_paper_old
context_key: ['if len(config["parameters"]["rwr_matrix"])>0']
    (29, 'if len(config["parameters"]["rwr_matrix"])>0:')
    (30, '    RWR_MATRIX=config["parameters"]["rwr_matrix"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=stracquadaniolab/workflow-pygna, file=Snakefile
context_key: ['if not os.path.exists(config["parameters"]["diagnostic_folder"])']
    (18, 'if not os.path.exists(config["parameters"]["diagnostic_folder"]):')
    (19, '    os.mkdir(config["parameters"]["diagnostic_folder"])')
    (20, '')
    (21, '# If the matrices for the full analysis are not specified, they are generated by the rule')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=stracquadaniolab/workflow-pygna, file=Snakefile
context_key: ['if len(config["parameters"]["sp_matrix"])>0']
    (22, 'if len(config["parameters"]["sp_matrix"])>0:')
    (23, '    SP_MATRIX=config["parameters"]["sp_matrix"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=stracquadaniolab/workflow-pygna, file=Snakefile
context_key: ['if len(config["parameters"]["rwr_matrix"])>0']
    (27, 'if len(config["parameters"]["rwr_matrix"])>0:')
    (28, '    RWR_MATRIX=config["parameters"]["rwr_matrix"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=stracquadaniolab/workflow-pygna, file=Snakefile_paper
context_key: ['if not os.path.exists(config["parameters"]["diagnostic_folder"])']
    (16, 'if not os.path.exists(config["parameters"]["diagnostic_folder"]):')
    (17, '    os.mkdir(config["parameters"]["diagnostic_folder"])')
    (18, '')
    (19, '# If the matrices for the full analysis are not specified, they are generated by the rule')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=stracquadaniolab/workflow-pygna, file=Snakefile_paper
context_key: ['if len(config["parameters"]["sp_matrix"])>0']
    (20, 'if len(config["parameters"]["sp_matrix"])>0:')
    (21, '    SP_MATRIX=config["parameters"]["sp_matrix"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=stracquadaniolab/workflow-pygna, file=Snakefile_paper
context_key: ['if len(config["parameters"]["rwr_matrix"])>0']
    (25, 'if len(config["parameters"]["rwr_matrix"])>0:')
    (26, '    RWR_MATRIX=config["parameters"]["rwr_matrix"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=TriassicSalamander/nimagen_snakemake, file=Snakefile
context_key: ['if is_sample_id == False', 'if "Sample_ID" in row']
    (17, '    if is_sample_id == False:')
    (18, '        if "Sample_ID" in row:')
    (19, '            is_sample_id = True')
    (20, '            continue')
    (21, '        else:')
    (22, '            continue')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=TriassicSalamander/nimagen_snakemake, file=Snakefile
context_key: ['if is_sample_id == False', 'elif is_sample_id == True']
    (23, '    elif is_sample_id == True:')
    (24, '        sample_count += 1')
    (25, "        SAMPLES.append(row.split(\\',\\')[0] + \\'_S\\' + str(sample_count)) ")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=laruzzante/biocentis-task, file=workflow/rules/utils.smk
context_key: ['if "sra-accessions" in config.keys()']
    (9, '    if "sra-accessions" in config.keys():\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=laruzzante/biocentis-task, file=workflow/rules/utils.smk
context_key: ['if "sequencing-technology" in config.keys()']
    (15, '    if "sequencing-technology" in config.keys():\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=laruzzante/biocentis-task, file=workflow/rules/utils.smk
context_key: ['if "reference-genome-url" in config.keys()']
    (18, '    if "reference-genome-url" in config.keys():\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=laruzzante/biocentis-task, file=workflow/rules/utils.smk
context_key: ['if seq in config.keys()']
    (27, '        if seq in config.keys():\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/Snakefile
context_key: ["if not config[\\'data\\']"]
    (10, "if not config[\\'data\\']:")
    (11, "    sys.exit(\\'No fastq file directory provided. Exiting program...\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/Snakefile
context_key: ["if not config[\\'reference\\']"]
    (19, "if not config[\\'reference\\']:")
    (20, "    sys.exit(\\'No reference directory provided. Exiting program...\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/Snakefile
context_key: ["if not config[\\'mode\\']"]
    (26, "if not config[\\'mode\\']:")
    (27, '    sys.exit("Sample processing mode not specified. Provide either \\\'RNA\\\' or \\\'DNA\\\' at command line or in \\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/Snakefile
context_key: ["if mode == \\'RNA\\' or config[\\'apply_snpeff\\']"]
    (33, "if mode == \\'RNA\\' or config[\\'apply_snpeff\\']:")
    (34, "    annotation = get_resource_file(ref_dir,type=\\'gtf\\')")
    (35, '')
    (36, "# extract sample names for all samples being run in analysis; used for wildcard in rule \\'all\\' below")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/Snakefile
context_key: ["if mode == \\'RNA\\'"]
    (46, "if mode == \\'RNA\\':")
    (47, "    aligner = [\\'star\\',\\'hisat2\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/Snakefile
context_key: ["if mode == \\'RNA\\'"]
    (52, "if mode == \\'RNA\\':")
    (53, '    include: "rules/RNA_indexing.smk"')
    (54, '    include: "rules/RNA_mapping.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/Snakefile
context_key: ["elif mode == \\'DNA\\'"]
    (55, "elif mode == \\'DNA\\':")
    (56, '    include: "rules/DNA_mapping.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=calliope-project/euro-calliope, file=Snakefile
context_key: ['onsuccess', 'if "email" in config.keys()']
    (42, '     if "email" in config.keys():')
    (43, '         shell("echo "" | mail -s \\\'euro-calliope succeeded\\\' {config[email]}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=calliope-project/euro-calliope, file=Snakefile
context_key: ['onerror', 'if "email" in config.keys()']
    (45, '     if "email" in config.keys():')
    (46, '         shell("echo "" | mail -s \\\'euro-calliope failed\\\' {config[email]}")')
    (47, '')
    (48, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/references, file=workflow/rules/common.smk
context_key: ['if not workflow.overwrite_configfiles']
    (20, 'if not workflow.overwrite_configfiles:')
    (21, '    sys.exit("At least one config file must be passed using --configfile/--configfiles, by command line or a profile!")')
    (22, '')
    (23, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/CoreGenomePrimers, file=bin/cgp/nontarget/cds/Snakefile
context_key: ['if cnt > 1']
    (33, '                        if cnt > 1:')
    (34, '                            break')
    (35, '                        outF.write(line)')
    (36, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/CoreGenomePrimers, file=bin/cgp/nontarget/cds/Snakefile
context_key: ["if str(config[\\'params\\'][\\'cgp\\'][\\'blast_nontarget\\'][\\'run_locally\\']).lower() == \\'true\\'"]
    (37, "if str(config[\\'params\\'][\\'cgp\\'][\\'blast_nontarget\\'][\\'run_locally\\']).lower() == \\'true\\':")
    (38, '    localrules: clusters_reps_blastx_nontarget')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/CoreGenomePrimers, file=Snakefile
context_key: ["if not os.path.isfile(config[\\'samples_file\\'])"]
    (21, "if not os.path.isfile(config[\\'samples_file\\']):")
    (22, "    raise IOError(\\'Cannot find file: {}\\'.format(config[\\'samples_file\\']))")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/CoreGenomePrimers, file=Snakefile
context_key: ["if x not in config[\\'samples\\'].columns"]
    (25, "    if x not in config[\\'samples\\'].columns:")
    (26, '        msg =\\\'Cannot find "{}" column in the samples table\\\'')
    (27, '        raise ValueError(msg.format(x))')
    (28, '### Fasta files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/CoreGenomePrimers, file=Snakefile
context_key: ['if not os.path.isfile(F)']
    (30, '    if not os.path.isfile(F):')
    (31, "        msg = \\'Cannot find file: {}\\'")
    (32, '        raise IOError(msg.format(F))')
    (33, '### samples')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/CoreGenomePrimers, file=Snakefile
context_key: ["if \\'Filters\\' in config[\\'samples\\'].columns"]
    (40, "if \\'Filters\\' in config[\\'samples\\'].columns:")
    (41, "    config[\\'filters\\'] = \\'--filter-list \\' + \\',\\'.join(config[\\'samples\\'].Filters.unique().tolist())")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/CoreGenomePrimers, file=Snakefile
context_key: ["if not os.path.isdir(config[\\'tmp_dir\\'])", "if not os.path.isdir(config[\\'tmp_dir\\'])"]
    (52, "if not os.path.isdir(config[\\'tmp_dir\\']):")
    (53, "    os.makedirs(config[\\'tmp_dir\\'])")
    (54, "    if not os.path.isdir(config[\\'tmp_dir\\']):")
    (55, "        raise IOError(\\'Cannot find tmp dir: {}\\'.format(config[\\'tmp_dir\\']))")
    (56, '')
    (57, '## including modular snakefiles')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akhanf/nnunet-fetalbrain, file=Snakefile
context_key: ["if model in config[\\'download_model\\'].keys()", 'rule download_model', 'params']
    (60, "if model in config[\\'download_model\\'].keys():")
    (61, '    ')
    (62, '    rule download_model:')
    (63, '        params: ')
    (64, "            url = config[\\'download_model\\'][model][\\'url\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akhanf/nnunet-fetalbrain, file=Snakefile
context_key: ["if model in config[\\'download_model\\'].keys()", 'rule download_model', 'output']
    (65, "        output: config[\\'download_model\\'][model][\\'tar\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akhanf/nnunet-fetalbrain, file=Snakefile
context_key: ["if model in config[\\'download_model\\'].keys()", 'rule download_model', 'shell']
    (66, "        shell: \\'wget {params.url}\\'")
    (67, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akhanf/nnunet-fetalbrain, file=Snakefile
context_key: ["if model in config[\\'download_model\\'].keys()", 'rule extract_model', 'output']
    (68, '    rule extract_model:')
    (69, "        input: config[\\'download_model\\'][model][\\'tar\\']")
    (70, '        output: ')
    (71, "            models = expand(os.path.join(\\'trained_models\\',config[\\'download_model\\'][model][\\'out\\']),fold=range(5)),")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akhanf/nnunet-fetalbrain, file=Snakefile
context_key: ["if model in config[\\'download_model\\'].keys()", 'rule extract_model', 'shell']
    (72, "        shell: \\'mkdir -p trained_model && tar -C trained_models -xvf {input}\\'")
    (73, '')
    (74, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vgteam/vg_snakemake, file=Snakefile
context_key: ["if MAPPER == \\'gaffe\\'"]
    (18, "if MAPPER == \\'gaffe\\':")
    (19, "    MAPPER = \\'gaffe{}k{}w{}N\\'.format(config[\\'mink\\'], config[\\'minw\\'], config[\\'covern\\'])")
    (20, '')
    (21, '##')
    (22, '## Main rules')
    (23, '##')
    (24, '')
    (25, '# indexes used by the default mapper and variant caller')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=troy-layouni/CellSegTest, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'if len(fol_path_spillover_slide_acs) > 0']
    (117, 'if do_compensation:')
    (118, '    if len(file_path_orig_sm) == 0:')
    (119, '        if len(fol_path_spillover_slide_acs) > 0:')
    (120, "            spillover_mode = \\'estimate_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=troy-layouni/CellSegTest, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'else']
    (121, '        else:')
    (122, '            raise ValueError(\\\'Either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (123, "                             \\'needs to be provided.\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=troy-layouni/CellSegTest, file=workflow/Snakefile
context_key: ['if do_compensation', 'elif len(fol_path_spillover_slide_acs) == 0']
    (124, '    elif len(fol_path_spillover_slide_acs) == 0:')
    (125, "            spillover_mode = \\'copy_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=troy-layouni/CellSegTest, file=workflow/Snakefile
context_key: ['if do_compensation', 'else']
    (126, '    else:')
    (127, '        raise ValueError(\\\'Only provide either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (128, "                         \\'but not both.\\')")
    (129, '')
    (130, '# Produce a list of all cellprofiler output files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=troy-layouni/CellSegTest, file=workflow/Snakefile
context_key: ['if not do_compensation']
    (267, 'if not do_compensation:')
    (268, "    config_dict_cp[\\'measuremasks\\'] = {")
    (269, '        \\\'message\\\':  """\\\\')
    (270, '                    Measure masks ')
    (271, '                    ')
    (272, '                    Measure segmentation masks')
    (273, '                    """,')
    (274, "        \\'run_size\\': 1,")
    (275, "        \\'plugins\\': cp_plugins,")
    (276, "        \\'pipeline\\': \\'resources/cp4_pipelines/adapted/3_measure_mask_basic.cppipe\\',")
    (277, "        \\'input_files\\': [")
    (278, '            # Folder containing segmentation masks')
    (279, '            fol_path_cellmasks,')
    (280, '            # Folder containing image stacks to measure')
    (281, '            fol_path_full,')
    (282, '            # Folder containing cell probabilities')
    (283, '            fol_path_probabilities,')
    (284, '            # Acquisition metadata')
    (285, '            file_path_acmeta,')
    (286, '            # Channel metadata')
    (287, '            file_path_channelmeta_full,')
    (288, '            file_path_channelmeta_cellprobab')
    (289, '        ],')
    (290, "        \\'output_patterns\\': {\\'.\\':")
    (291, '                            # Folder containing cellprofiler measurement')
    (292, '                                cp_measurements_output_files,')
    (293, '                            # Subfolder containing segmentation masks used')
    (294, '                            # for the measurements')
    (295, "                            \\'masks\\': directory(fol_path_cp_masks),")
    (296, '                            # Subfolder containing the images used for the')
    (297, '                            # measurements.')
    (298, "                            \\'images\\': directory(fol_path_cp_images)},")
    (299, "        \\'input_folder\\': fol_path_cp_input")
    (300, '    }')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=troy-layouni/CellSegTest, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'params']
    (454, 'if do_compensation:')
    (455, '    if spillover_mode == "estimate_sm":')
    (456, '        rule get_ss_panel_from_panel:')
    (457, '            input: csv_panel')
    (458, '            output: file_path_ss_panel')
    (459, '            params:')
    (460, '                col_metal=csv_panel_metal,')
    (461, '                col_spillover=csv_panel_spillover')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=troy-layouni/CellSegTest, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'run']
    (462, '            run:')
    (463, '                dat_csv = pd.read_csv(csv_panel)')
    (464, '                assert params.col_spillover in dat_csv.columns, ValueError(')
    (465, '                    f"The panel csv {csv_panel} should contain a boolean column"')
    (466, '                    f"{params.col_spillover} indicating the metals used for the"')
    (467, '                    f"spillover acquisitions.")')
    (468, '                metals = dat_csv.query(')
    (469, "                    f\\'{params.col_spillover} == 1\\')[params.col_metal].values")
    (470, "                with open(output[0], \\'w\\') as f:")
    (471, '                    for m in metals:')
    (472, "                        f.write(m+\\'\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['if not os.path.exists(directory)']
    (30, '    if not os.path.exists(directory): os.mkdir(directory)')
    (31, '')
    (32, '#Configure the singularity command.  All necessary paths that will be used by singularity must be provided in the --bind statement')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ["elif os.path.exists(config[\\'samples\\'])"]
    (54, "elif os.path.exists(config[\\'samples\\']):")
    (55, '    ind_num = 0')
    (56, "    with open(config[\\'samples\\'], \\'r\\') as samp_file:")
    (57, '        for ind in samp_file:')
    (58, '            if ind.strip().split()[1] in samps: ind_num += 1')
    (59, '')
    (60, '# Determine number of pairwise comparisons of individuals necessary for IBD/IBS calculation in PLINK and convert this to the number of necessary jobs')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['if num_jobs <= 1']
    (64, 'if num_jobs <= 1: # Attempt to catch an error that arises in instances where there are not that many jobs to run')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['run', "if os.path.exists(config[\\'samples\\'])"]
    (164, '        if os.path.exists(config[\\\'samples\\\']): cmd += f" --keep {config[\\\'samples\\\']}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['run', "if os.path.exists(config[\\'sites\\'])"]
    (165, '        if os.path.exists(config[\\\'sites\\\']): cmd += f" --exclude {config[\\\'sites\\\']}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['run', "if num_tested < int(config[\\'sHWE\\'][\\'test_threshold\\'])"]
    (258, "        if num_tested < int(config[\\'sHWE\\'][\\'test_threshold\\']): #If not enough SNPs will be tested, print error message and DO NOT create output file.  The documentatino of the pipeline encourages the user to adjust the parameters of the downsampling (more rounds, fewer individuals, lower threshold, etc.)")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['params', 'run', 'if not os.stat(file).st_size == 0']
    (283, '                if not os.stat(file).st_size == 0: #Make sure file is not empty')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['if entropy > max_entropy']
    (290, '                    if entropy > max_entropy: #Store current value if better than prior best result.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['if file_list']
    (319, '        if file_list: shell(f"find plink/DS/ -name \\\\\\\'{BASE}_LDprune_ctrls_DS*.bim\\\\\\\' | grep -v -w accessory/.tested_file_list.txt | xargs cat | cut -d \\\\" \\\\" -f 2 | sort -T {os.getcwd()} | uniq > accessory/sHWE_tested_snps.txt") #This command prints markers from the input .bim files IFF they were successfully tested (in our file list)')
    (320, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['run', "if num_tested < int(config[\\'sHWE\\'][\\'test_threshold\\'])"]
    (327, "        if num_tested < int(config[\\'sHWE\\'][\\'test_threshold\\']):")
    (328, '            print(f"Failed Checkpoint\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['run', "if os.path.exists(config[\\'covariate_file\\'])"]
    (415, "        if os.path.exists(config[\\'covariate_file\\']): #Not tested")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['run', 'if os.path.exists(VCF)', "if os.path.exists(config[\\'samples\\'])"]
    (431, '        if os.path.exists(VCF):')
    (432, '            cmd = f"{CMD_PREFIX} plink2 --vcf {VCF} dosage=HDS --extract plink/{BASE}_sub_mflt.bim --const-fid 0 "')
    (433, "            if os.path.exists(config[\\'samples\\']):")
    (434, '                cmd += f" --keep {config[\\\'samples\\\']} "')
    (435, '            cmd += f" --make-pgen --out input/{BASE}_sub {plnk_rsrc(rule, plink_run_specs)}; " \\\\')
    (436, '                   f"{CMD_PREFIX} plink2 --pfile input/{BASE}_sub --export vcf vcf-dosage=HDS-force --out input/{BASE}_sub;" \\\\')
    (437, '                   f"{CMD_PREFIX} bgzip input/{BASE}_sub.vcf; {CMD_PREFIX} tabix input/{BASE}_sub.vcf.gz"')
    (438, '            print(cmd)')
    (439, '            shell(cmd)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['run', 'if os.path.exists(VCF)', 'else']
    (440, '        else: print(f"Did not find VCF file at provided path: {VCF}")')
    (441, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['run', 'if f"{BASE}.admixmap.txt" in input']
    (476, '            if f"{BASE}.admixmap.txt" in input:')
    (477, '                report_line += f"admixMap_rslt=\\\\"{BASE}.admixmap.txt\\\\", global_ancestry=\\\\"{BASE}.globalancestry.txt\\\\","')
    (478, '            else:')
    (479, '                report_line += f"admixMap_rslt=\\\\"-9\\\\", global_ancestry=\\\\"-9\\\\","')
    (480, '                shell(f"touch {BASE}.admixmap.sig.txt")')
    (481, '            if f"{BASE}.dos.genesis.txt" in input: report_line += f"dosage_rslt=\\\\"{BASE}.dos.genesis.txt\\\\","')
    (482, '            else:')
    (483, '                report_line += f"dosage_rslt=\\\\"-9\\\\","')
    (484, '                shell(f"touch {BASE}.dos.genesis.sig.txt")')
    (485, '            report_line += f"config_file=\\\\"workflow/config.yml\\\\"))\\\\\\\' | R --vanilla"')
    (486, '            report_cmds.write(report_line)')
    (487, '        shell(f"{CMD_PREFIX} sh scripts/gather_report_data.sh; mv scripts/{BASE}-Mapping-report.pdf {BASE}-Mapping-report.pdf; mv input/*kinplot.png figures")')
    (488, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['run', "if os.path.exists(config[\\'annotation\\'][\\'typed_key\\'])"]
    (500, '            if os.path.exists(config[\\\'annotation\\\'][\\\'typed_key\\\']): cmd += f" -t {config[\\\'annotation\\\'][\\\'typed_key\\\']}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ['run', "if os.path.exists(config[\\'conditional_analysis\\'][\\'snp_list\\'])", 'if os.path.exists(f"conditional-analysis/{wildcards.marker}.raw")']
    (531, "        if os.path.exists(config[\\'conditional_analysis\\'][\\'snp_list\\']):")
    (532, "            if not os.path.exists(\\'conditional-analysis\\'): os.mkdir(\\'conditional-analysis\\')")
    (533, '            chrom, pos = f"{wildcards.marker}".split("-")')
    (534, '            shell(f"{CMD_PREFIX} plink --bfile plink/{BASE}_sub_mflt --snp {wildcards.marker.replace(\\\'-\\\',\\\':\\\')} --recodeAD --out conditional-analysis/{wildcards.marker} {plnk_rsrc(rule, plink_run_specs)} || true")')
    (535, '            if os.path.exists(f"conditional-analysis/{wildcards.marker}.raw"):')
    (536, '                shell(f"{CMD_PREFIX} Rscript {CODE}/genesis_conditional_analysis.R -a {chrom} -b {pos} -d {config[\\\'conditional_analysis\\\'][\\\'distance\\\']} "')
    (537, '                      f"-m conditional-analysis/{wildcards.marker}.raw "')
    (538, '                      f"-g {{input[0]}} -p {{input[1]}} -k {{input[2]}} -n {config[\\\'gwas\\\'][\\\'pc_num\\\']} -C {config[\\\'gwas\\\'][\\\'other_predictors\\\']} "')
    (539, '                      f"-o conditional-analysis/{BASE}-{wildcards.marker}.genesis.txt")')
    (540, '            else:')
    (541, '                print(f"Did not find {wildcards.marker} in plink/{BASE}_sub_mflt")')
    (542, '                shell(f"touch conditional-analysis/{BASE}-{wildcards.marker}.genesis.txt")')
    (543, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/fbmn_integration.smk
context_key: ['if GNPS_library', 'rule GNPS_annotations', 'input']
    (44, 'if GNPS_library:')
    (45, '    rule GNPS_annotations:')
    (46, '        input:')
    (47, '            lib= glob.glob(join("resources", "*.tsv")),')
    (48, '            featurematrix= join("results", "annotations", "FeatureTable_MSMS.tsv"),')
    (49, '            mgf_path= join("results", "GNPSexport", "MSMS.mgf")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/fbmn_integration.smk
context_key: ['if GNPS_library', 'rule GNPS_annotations', 'output']
    (50, '        output:')
    (51, '            gnps= join("results", "annotations", "FeatureTable_MSMS_GNPS.tsv")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/fbmn_integration.smk
context_key: ['if GNPS_library', 'rule GNPS_annotations', 'log']
    (52, '        log: join("workflow", "report", "logs", "annotate", "GNPS_annotations.log")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/fbmn_integration.smk
context_key: ['if GNPS_library', 'rule GNPS_annotations', 'threads']
    (53, '        threads: 4')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/fbmn_integration.smk
context_key: ['if GNPS_library', 'rule GNPS_annotations', 'conda']
    (54, '        conda:')
    (55, '            join("..", "envs", "openms.yaml")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/fbmn_integration.smk
context_key: ['if GNPS_library', 'rule GNPS_annotations', 'shell']
    (56, '        shell:')
    (57, '            """')
    (58, '            python workflow/scripts/GNPS.py {input.lib} {input.featurematrix} {input.mgf_path} {output.gnps} 2>> {log}')
    (59, '            """')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'rule spectral_matcher', 'input']
    (19, 'if MGF_library:')
    (20, '    print("computing MSMS matches with library:", MGF_library)')
    (21, '    rule spectral_matcher:')
    (22, '        input:')
    (23, '            mzml= join("results", "Interim", "annotations", "MSMS.mzML"),')
    (24, '            database= glob.glob(join("resources", "*.mgf"))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'rule spectral_matcher', 'output']
    (25, '        output:')
    (26, '            join("results", "Interim", "annotations", "MSMSMatcher.mzTab")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'rule spectral_matcher', 'log']
    (27, '        log: join("workflow", "report", "logs", "annotate", "spectral_matcher.log")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'rule spectral_matcher', 'threads']
    (28, '        threads: 4 ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'rule spectral_matcher', 'conda']
    (29, '        conda:')
    (30, '            join("..", "envs", "openms.yaml")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'rule spectral_matcher', 'shell']
    (31, '        shell:')
    (32, '            """')
    (33, '            MetaboliteSpectralMatcher -algorithm:merge_spectra "false" -in {input.mzml} -database {input.database} -out {output} 2>> {log}')
    (34, '            """  ')
    (35, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'if config["rules"]["sirius_csi"]==True', 'rule MSMS_annotations', 'input']
    (36, '    if config["rules"]["sirius_csi"]==True:')
    (37, '        rule MSMS_annotations:')
    (38, '            input:')
    (39, '                MZTAB = join("results", "Interim", "annotations", "MSMSMatcher.mzTab"),')
    (40, '                MGF = join("results", "GNPSexport", "MSMS.mgf"),')
    (41, '                MZML = join("results", "Interim", "annotations", "MSMS.mzML"),')
    (42, '                MATRIX= join("results", "annotations", "FeatureTable_siriuscsi.tsv")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'if config["rules"]["sirius_csi"]==True', 'rule MSMS_annotations', 'output']
    (43, '            output:')
    (44, '                MSMS_MATRIX= join("results", "annotations", "FeatureTable_MSMS.tsv")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'if config["rules"]["sirius_csi"]==True', 'rule MSMS_annotations', 'log']
    (45, '            log: join("workflow", "report", "logs", "annotate", "MSMS_annotations.log")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'if config["rules"]["sirius_csi"]==True', 'rule MSMS_annotations', 'threads']
    (46, '            threads: 4')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'if config["rules"]["sirius_csi"]==True', 'rule MSMS_annotations', 'conda']
    (47, '            conda:')
    (48, '                join("..", "envs", "pyopenms.yaml")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'if config["rules"]["sirius_csi"]==True', 'rule MSMS_annotations', 'shell']
    (49, '            shell:')
    (50, '                """')
    (51, '                python workflow/scripts/MSMS_annotations.py {input.MZTAB} {input.MGF} {input.MZML} {input.MATRIX} {output.MSMS_MATRIX} 2>> {log}')
    (52, '                """')
    (53, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'elif config["rules"]["sirius"]==True', 'rule MSMS_annotations', 'input']
    (54, '    elif config["rules"]["sirius"]==True:')
    (55, '            rule MSMS_annotations:')
    (56, '                input:')
    (57, '                    MSMS = join("results", "Interim", "annotations", "MSMSMatcher.mzTab"),')
    (58, '                    MGF = join("results", "GNPSexport", "MSMS.mgf"),')
    (59, '                    MZML = join("results", "Interim", "annotations", "MSMS.mzML"),')
    (60, '                    MATRIX= join("results", "annotations", "FeatureTable_sirius.tsv")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'elif config["rules"]["sirius"]==True', 'rule MSMS_annotations', 'output']
    (61, '                output:')
    (62, '                    MSMS_MATRIX= join("results", "annotations", "FeatureTable_MSMS.tsv")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'elif config["rules"]["sirius"]==True', 'rule MSMS_annotations', 'log']
    (63, '                log: join("workflow", "report", "logs", "annotate", "MSMS_annotations.log")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'elif config["rules"]["sirius"]==True', 'rule MSMS_annotations', 'threads']
    (64, '                threads: 4')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'elif config["rules"]["sirius"]==True', 'rule MSMS_annotations', 'conda']
    (65, '                conda:')
    (66, '                    join("..", "envs", "pyopenms.yaml")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'elif config["rules"]["sirius"]==True', 'rule MSMS_annotations', 'shell']
    (67, '                shell:')
    (68, '                    """')
    (69, '                    python workflow/scripts/MSMS_annotations.py {input.MSMS} {input.MGF} {input.MZML} {input.MATRIX} {output.MSMS_MATRIX} 2>> {log}')
    (70, '                    """')
    (71, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'else', 'rule MSMS_annotations', 'input']
    (72, '    else:')
    (73, '        rule MSMS_annotations:')
    (74, '            input:')
    (75, '                MSMS = join("results", "Interim", "annotations", "MSMSMatcher.mzTab"),')
    (76, '                MGF = join("results", "GNPSexport", "MSMS.mgf"),')
    (77, '                MZML = join("results", "Interim", "annotations", "MSMS.mzML"),')
    (78, '                MATRIX= join("results", "Preprocessed", "FeatureMatrix.tsv")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'else', 'rule MSMS_annotations', 'output']
    (79, '            output:')
    (80, '                MSMS_MATRIX= join("results", "annotations", "FeatureTable_MSMS.tsv")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'else', 'rule MSMS_annotations', 'log']
    (81, '            log: join("workflow", "report", "logs", "annotate", "MSMS_annotations.log")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'else', 'rule MSMS_annotations', 'threads']
    (82, '            threads: 4')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'else', 'rule MSMS_annotations', 'conda']
    (83, '            conda:')
    (84, '                join("..", "envs", "pyopenms.yaml")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBChub/snakemake-UmetaFlow, file=workflow/rules/spectralmatcher.smk
context_key: ['if MGF_library', 'else', 'rule MSMS_annotations', 'shell']
    (85, '            shell:')
    (86, '                """')
    (87, '                python workflow/scripts/MSMS_annotations.py {input.MSMS} {input.MGF} {input.MZML} {input.MATRIX} {output.MSMS_MATRIX} 2>> {log}')
    (88, '                """')
    (89, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhklab/MERIDA_snakemake_pipeline, file=Snakefile
context_key: ['if preselected_features is not None']
    (37, 'if preselected_features is not None:')
    (38, '    preselected_features = os.path.join(procdata, preselected_features)')
    (39, '')
    (40, '# -- 0.1 Make MERIDA input files for to grid search hyperparameters')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nasiegel88/tagseq-qiime2-snakemake-1, file=rules/02-dada2.smk
context_key: ['if [ "${{arr[@]}}" == yes ]; then\\']
    (123, '    if [ "${{arr[@]}}" == yes ]; then\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nasiegel88/tagseq-qiime2-snakemake-1, file=rules/02-dada2.smk
context_key: ['elif [ "${{arr[@]}}" == \\\'no\\\' ]; then\\']
    (132, '    elif [ "${{arr[@]}}" == \\\'no\\\' ]; then\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nasiegel88/tagseq-qiime2-snakemake-1, file=Snakefile
context_key: ["if DROP_BLANKS == \\'yes\\'"]
    (284, "if DROP_BLANKS == \\'yes\\':\\r")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nasiegel88/tagseq-qiime2-snakemake-1, file=Snakefile
context_key: ["if LONGITUDINAL == \\'yes\\'"]
    (292, "if LONGITUDINAL == \\'yes\\':\\r")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=metamaden/recountmethylation_instance, file=Snakefile
context_key: ['if os.path.isdir(server_repo_path)', 'if os.path.isdir(srcpath)']
    (48, 'if os.path.isdir(server_repo_path):')
    (49, '    print("Found server repo path.")')
    (50, '    srcpath = os.path.join(server_repo_path, "src")')
    (51, '    if os.path.isdir(srcpath):')
    (52, '        print("Found server src path.")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=metamaden/recountmethylation_instance, file=Snakefile
context_key: ['if os.path.isdir(server_repo_path)', 'else']
    (53, '    else:')
    (54, '        print("Error, couldn\\\'t find server src dir at path "+srcpath)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=metamaden/recountmethylation_instance, file=Snakefile
context_key: ['if not os.path.isdir(logsfn)']
    (69, 'if not os.path.isdir(logsfn):')
    (70, '    os.mkdir(logsfn)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=metamaden/recountmethylation_instance, file=Snakefile
context_key: ['if len(os.listdir(logspath)) > 0']
    (71, 'if len(os.listdir(logspath)) > 0:')
    (72, '    print("Found "+str(len(os.listdir(logspath)))+" files in logs dir "+')
    (73, '        logspath)')
    (74, '    accopt = str(input("(Y/N) Do you want to clear existing logs from the "+')
    (75, '        "logs dir?\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=metamaden/recountmethylation_instance, file=Snakefile
context_key: ['if accopt in ["y", "Y", "yes", "Yes", "YES"]']
    (77, '    if accopt in ["y", "Y", "yes", "Yes", "YES"]:')
    (78, '        print("Removing old log files...")')
    (79, '        for file in os.listdir(logspath):')
    (80, '            os.remove(os.path.join(logspath, file))')
    (81, '    elif accopt in ["n", "N", "no", "No", "NO"]:')
    (82, '        print("Skipping logs dir cleanup...")')
    (83, '    else:')
    (84, '        print("Error, invalid input. Skipping logs dir cleanup...")')
    (85, '')
    (86, '#---------------')
    (87, '# Get timestamps')
    (88, '#---------------')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=EmilySNichols/nnunet-fetalbrain, file=Snakefile
context_key: ["if model in config[\\'download_model\\'].keys()", 'rule download_model', 'params']
    (60, "if model in config[\\'download_model\\'].keys():")
    (61, '    ')
    (62, '    rule download_model:')
    (63, '        params: ')
    (64, "            url = config[\\'download_model\\'][model][\\'url\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=EmilySNichols/nnunet-fetalbrain, file=Snakefile
context_key: ["if model in config[\\'download_model\\'].keys()", 'rule download_model', 'output']
    (65, "        output: config[\\'download_model\\'][model][\\'tar\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=EmilySNichols/nnunet-fetalbrain, file=Snakefile
context_key: ["if model in config[\\'download_model\\'].keys()", 'rule download_model', 'shell']
    (66, "        shell: \\'wget {params.url}\\'")
    (67, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=EmilySNichols/nnunet-fetalbrain, file=Snakefile
context_key: ["if model in config[\\'download_model\\'].keys()", 'rule extract_model', 'output']
    (68, '    rule extract_model:')
    (69, "        input: config[\\'download_model\\'][model][\\'tar\\']")
    (70, '        output: ')
    (71, "            models = expand(os.path.join(\\'trained_models\\',config[\\'download_model\\'][model][\\'out\\']),fold=range(5)),")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=EmilySNichols/nnunet-fetalbrain, file=Snakefile
context_key: ["if model in config[\\'download_model\\'].keys()", 'rule extract_model', 'shell']
    (72, "        shell: \\'mkdir -p trained_model && tar -C trained_models -xvf {input}\\'")
    (73, '')
    (74, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=arabidopsisca/snakemake-rna-seq-star-deseq2, file=rules/diffexp.smk
context_key: ['if "strandedness" in units.columns']
    (1, '    if "strandedness" in units.columns:')
    (2, '        return units["strandedness"].tolist()')
    (3, '    else:')
    (4, '        strand_list=["none"]')
    (5, '        return strand_list*units.shape[0]')
    (6, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/nanopore-assembly, file=Snakefile
context_key: ['if "OUTDIR" in config']
    (10, 'if "OUTDIR" in config:')
    (11, '    workdir: config["OUTDIR"]')
    (12, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/nanopore-assembly, file=Snakefile
context_key: ['if "COMPARISON_GENOME" in config']
    (27, 'if "COMPARISON_GENOME" in config:')
    (28, '    comp_genome_results = expand("results/genome_alignment/{prefix}_{species}.png", prefix=PREFIX, species = config["COMPARISON_GENOME"].keys())')
    (29, '    MIN_ALIGNMENT_LENGTH = config["MIN_ALIGNMENT_LENGTH"]')
    (30, '    MIN_QUERY_LENGTH = config["MIN_QUERY_LENGTH"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/population-structural-var-calling-smoove, file=Snakefile
context_key: ['if "OUTDIR" in config']
    (11, 'if "OUTDIR" in config:')
    (12, '    # print("\\')
    (13, 'Saving to " + config["OUTDIR"] + "\\')
    (14, '")')
    (15, '    workdir: config["OUTDIR"]')
    (16, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-sorting-hat
context_key: ['if grep -q "$prot_exp_type_all" {input.config_file} ; the']
    (223, '        if grep -q "$prot_exp_type_all" {input.config_file} ; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-sorting-hat
context_key: ['if [ -z "$gff_file" ] || [ -z "$idf" ] || [ -z "$sdrf" ]; the']
    (233, '        if [ -z "$gff_file" ] || [ -z "$idf" ] || [ -z "$sdrf" ]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ['elif [ "$qcExitCode" -ne 0 ]; the']
    (1565, '        elif [ "$qcExitCode" -ne 0 ]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile
context_key: ['if [ -d "./qc/{wildcards.accession}_${{p}}_QM" ]; the']
    (1573, '            if [ -d "./qc/{wildcards.accession}_${{p}}_QM" ]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-recalculations
context_key: ["if experiment_type == \\'proteomics_baseline\\' or experiment_type ==\\'proteomics_baseline_dia\\' or experiment_type ==\\'proteomics_differential\\'"]
    (7, "if experiment_type == \\'proteomics_baseline\\' or experiment_type ==\\'proteomics_baseline_dia\\' or experiment_type ==\\'proteomics_differential\\':")
    (8, '    print(f"Recalculations not currently implemented for proteomics experiment: {experiment_type}")')
    (9, '    print(f"set goal=\\\'reprocess\\\' in the main script to reprocess")')
    (10, '    sys.exit(1)')
    (11, '')
    (12, '')
    (13, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/bulk-recalculations, file=Snakefile-reprocess
context_key: ["if get_from_config_or_metadata_summary(\\'experiment_type\\') == \\'rnaseq_mrna_baseline\\'"]
    (30, "    if get_from_config_or_metadata_summary(\\'experiment_type\\') == \\'rnaseq_mrna_baseline\\':")
    (31, '        metrics = get_metrics_reprocess()')
    (32, '')
    (33, '    # Read this now so that it is available for all other needs')
    (34, '    read_metadata_summary()')
    (35, '    global skip_accession')
    (36, '    skip_accession = read_skip_steps_file()')
    (37, "    required_config=[\\'tool\\']")
    (38, '    check_config_required(fields=required_config)')
    (39, '')
    (40, '    global experiment_type')
    (41, "    experiment_type=get_from_config_or_metadata_summary(\\'experiment_type\\')")
    (42, '')
    (43, '    print(experiment_type)')
    (44, '    # collect output for reprocessing')
    (45, "    if experiment_type == \\'rnaseq_mrna_baseline\\':")
    (46, '    #if config[\\\'tool\\\']=="all-baseline" or \\\'baseline-tracks\\\' in config[\\\'tool\\\'] or \\\'baseline-heatmap\\\' in config[\\\'tool\\\'] or \\\'baseline-coexpression\\\' in config[\\\'tool\\\']:')
    (47, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-add_runs_to_db.done" ))')
    (48, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-get_irap_versions_file.done" ))')
    (49, '        outputs.append(f"{config[\\\'accession\\\']}-raw-counts.tsv.undecorated")')
    (50, '        outputs.extend(expand(config[\\\'accession\\\']+"-{metric}.tsv.undecorated", metric=metrics))')
    (51, '        outputs.extend(expand(config[\\\'accession\\\']+"-transcripts-{metric}.tsv.undecorated", metric=["tpms"] ))')
    (52, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}-copy_transcript_relative_isoforms.done" ))')
    (53, "        if skip(config[\\'accession\\'],\\'rnaseq_qc\\'):")
    (54, '            outputs.extend(expand("qc/"+f"{config[\\\'accession\\\']}-irap-single-lib-report.tsv" )) ')
    (55, '        outputs.extend(expand(config[\\\'accession\\\']+"-{metric}.tsv.undecorated.quantile_normalized", metric=metrics))')
    (56, '        outputs.extend(expand(config[\\\'accession\\\']+"-{metric}.tsv.undecorated.aggregated", metric=metrics))')
    (57, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-rule-transcripts_na_check_{metric}.done", metric=["tpms"] ))')
    (58, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-quantile_normalise_transcripts_{metric}.done", metric=["tpms"] )) ')
    (59, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-summarize_transcripts_{metric}.done", metric=["tpms"] ))')
    (60, '        outputs.append(f"{config[\\\'accession\\\']}-analysis-methods.tsv_baseline_rnaseq")')
    (61, "        if skip(config[\\'accession\\'],\\'atlas_experiment_summary\\'):")
    (62, '            outputs.append(f"{config[\\\'accession\\\']}-atlasExperimentSummary.Rdata")')
    (63, '        outputs.extend(expand(config[\\\'accession\\\']+"-{metric}.tsv", metric=metrics))')
    (64, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-transcripts-{metric}.tsv.done", metric=["tpms"] ))')
    (65, "        if skip(config[\\'accession\\'],\\'baseline-heatmap\\'):")
    (66, '            outputs.extend(expand(f"{config[\\\'accession\\\']}"+"-heatmap-{metric}.pdf", metric=metrics ))')
    (67, '            outputs.append(f"{config[\\\'accession\\\']}-heatmap.pdf")')
    (68, '        # baseline tracks')
    (69, "        if skip(config[\\'accession\\'],\\'baseline-tracks\\'):")
    (70, "            check_config_required(fields=[\\'metadata_summary\\'], method=\\'baseline-tracks\\')")
    (71, '            outputs.extend(expand(config[\\\'accession\\\']+".{a_id}.genes.expressions_{metric}.bedGraph",')
    (72, '                            a_id=get_assay_ids(),')
    (73, '                            metric=metrics))')
    (74, '            outputs.extend(expand(  f"{config[\\\'accession\\\']}"+".{a_id}.genes.expressions.bedGraph", a_id=get_assay_ids() ) ) #symlink only for tpms')
    (75, '')
    (76, '        # generating coexpressions matrix for baseline experiment')
    (77, "        if skip(config[\\'accession\\'],\\'baseline-coexpression\\'):")
    (78, '            outputs.extend(expand(f"{config[\\\'accession\\\']}"+"-{metric}-coexpressions.tsv.gz", metric=metrics ))')
    (79, '            outputs.append(f"{config[\\\'accession\\\']}-coexpressions.tsv.gz")')
    (80, '')
    (81, '')
    (82, '    # collect output for differential rna-seq')
    (83, "    if experiment_type == \\'rnaseq_mrna_differential\\':")
    (84, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-add_runs_to_db.done" ))')
    (85, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-get_irap_versions_file.done" ))')
    (86, '        outputs.append(f"{config[\\\'accession\\\']}-raw-counts.tsv.undecorated")')
    (87, "        if skip(config[\\'accession\\'],\\'rnaseq_qc\\'):")
    (88, '            outputs.extend(expand("qc/"+f"{config[\\\'accession\\\']}-irap-single-lib-report.tsv" )) ')
    (89, '        outputs.append(f"{config[\\\'accession\\\']}-analytics.tsv.undecorated")')
    (90, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-{c_id}-mvaPlot.png.done", c_id=get_contrast_ids() ))')
    (91, '        outputs.append(f"{config[\\\'accession\\\']}-analytics.tsv.undecorated.unrounded")')
    (92, '        outputs.append(f"{config[\\\'accession\\\']}-analysis-methods.tsv_differential_rnaseq")')
    (93, "        if skip(config[\\'accession\\'],\\'atlas_experiment_summary\\'):")
    (94, '            outputs.append(f"{config[\\\'accession\\\']}-atlasExperimentSummary.Rdata")')
    (95, '        outputs.append(f"{config[\\\'accession\\\']}-analytics.tsv")')
    (96, '        outputs.append(f"{config[\\\'accession\\\']}-analytics.tsv.unrounded")')
    (97, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+".differential_statistics_rnaseq.done" ))')
    (98, '        outputs.append(f"{config[\\\'accession\\\']}-raw-counts.tsv")')
    (99, "        if skip(config[\\'accession\\'],\\'percentile_ranks\\'):")
    (100, '            outputs.append(f"{config[\\\'accession\\\']}-percentile-ranks.tsv")')
    (101, '        # differential gsea')
    (102, "        if skip(config[\\'accession\\'],\\'differential-gsea\\'):")
    (103, "            check_config_required(fields=[\\'bioentities_properties\\'], method=\\'differential-gsea\\')")
    (104, '            outputs.extend( expand( config[\\\'accession\\\']+".{c_id}.{ext_db}.{type}",')
    (105, '                        c_id=get_contrast_ids(),')
    (106, '                        ext_db=get_ext_db(),')
    (107, '                        type=["gsea.tsv", "gsea_list.tsv"]))')
    (108, '            outputs.extend( expand("logs/"+config[\\\'accession\\\']+".{c_id}.{ext_db}.{type}",')
    (109, '                        c_id=get_contrast_ids(),')
    (110, '                        ext_db=get_ext_db(),')
    (111, '                        type=["check_differential_gsea.done", "check_differential_gsea_list.done"]))')
    (112, '        # generate Genome browser tracks (bedGraph) for the experiment')
    (113, "        if skip(config[\\'accession\\'],\\'differential-tracks\\'):")
    (114, '            outputs.extend(expand(config[\\\'accession\\\']+".{id}.{type}", id=get_contrast_ids(), type=["genes.pval.bedGraph", "genes.log2foldchange.bedGraph"]))')
    (115, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-decorate_differential_rnaseq.done" ))')
    (116, '')
    (117, '    # collect output for differential microarrays')
    (118, "    if experiment_type == \\'microarray_1colour_mrna_differential\\' or experiment_type ==\\'microarray_2colour_mrna_differential\\' or experiment_type ==\\'microarray_1colour_microrna_differential\\':")
    (119, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-get_normalized_expressions_microarray.done" ))')
    (120, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-check_normalized_expressions_microarray.done" ))')
    (121, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-microarray_qc.done" ))')
    (122, '        outputs.append(f"{config[\\\'accession\\\']}-analysis-methods.tsv")')
    (123, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-decorate_temp_norm_expr_microarray.done" ))')
    (124, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-merge_probe_ids_microarray.done" ))')
    (125, '        #outputs.extend(expand(config[\\\'accession\\\']+"_{ad}-normalized-expressions.tsv.undecorated", ad=get_array_design_from_xml()  ))')
    (126, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-differential_statistics_microarray.done" ))')
    (127, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-check_nas_microarray.done" ))')
    (128, '        outputs.extend(expand(config[\\\'accession\\\']+"_{ad}-analytics.tsv.undecorated.unrounded", ad=get_array_design_from_xml()  ))')
    (129, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"_{ad}-round_log2_fold_changes_microarray.done" ,ad=get_array_design_from_xml()    ))')
    (130, "        if skip(config[\\'accession\\'],\\'atlas_experiment_summary\\'):")
    (131, '            outputs.append(f"{config[\\\'accession\\\']}-atlasExperimentSummary.Rdata")')
    (132, '        # decorate')
    (133, '        outputs.extend(expand(config[\\\'accession\\\']+"_{ad}-analytics.tsv", ad=get_array_design_from_xml()  ))')
    (134, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"_{ad}-decorate_differential_microarray.done" ,ad=get_array_design_from_xml()   ))')
    (135, "        if skip(config[\\'accession\\'],\\'percentile_ranks\\'):")
    (136, '            outputs.append(f"{config[\\\'accession\\\']}-percentile-ranks.tsv")')
    (137, '        # differential gsea')
    (138, "        if skip(config[\\'accession\\'],\\'differential-gsea\\'):")
    (139, "            check_config_required(fields=[\\'bioentities_properties\\'], method=\\'differential-gsea\\')")
    (140, '            outputs.extend( expand( config[\\\'accession\\\']+".{c_id}.{ext_db}.{type}",')
    (141, '                        c_id=get_contrast_ids(),')
    (142, '                        ext_db=get_ext_db(),')
    (143, '                        type=["gsea.tsv", "gsea_list.tsv"]))')
    (144, '            outputs.extend( expand("logs/"+config[\\\'accession\\\']+".{c_id}.{ext_db}.{type}",')
    (145, '                        c_id=get_contrast_ids(),')
    (146, '                        ext_db=get_ext_db(),')
    (147, '                        type=["check_differential_gsea.done", "check_differential_gsea_list.done"]))')
    (148, '        # generate Genome browser tracks (bedGraph) for the experiment')
    (149, "        if skip(config[\\'accession\\'],\\'differential-tracks\\'):")
    (150, '            outputs.extend(expand(config[\\\'accession\\\']+".{id}.{type}", id=get_contrast_ids(), type=["genes.pval.bedGraph", "genes.log2foldchange.bedGraph"]))')
    (151, '        # delete intermediate files')
    (152, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-delete_intermediate_files_microarray.done" ))')
    (153, '')
    (154, '')
    (155, '    # collect output for baseline proteomics')
    (156, "    if experiment_type == \\'proteomics_baseline\\' or experiment_type ==\\'proteomics_baseline_dia\\':")
    (157, '        outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-final_check_tsv_data_file_not_empty_of_data" ))')
    (158, '')
    (159, '    # collect output for differential proteomics')
    (160, "    if experiment_type == \\'proteomics_differential\\':")
    (161, "        if skip(config[\\'accession\\'],\\'percentile_ranks\\'):")
    (162, '            outputs.append(f"{config[\\\'accession\\\']}-percentile-ranks.tsv")')
    (163, '        # commenting the tracks as E-PROT-* appear not to be having any')
    (164, '        # outputs.extend(expand(config[\\\'accession\\\']+".{id}.{type}", id=get_contrast_ids(), type=["genes.pval.bedGraph", "genes.log2foldchange.bedGraph"]))')
    (165, "        if skip(config[\\'accession\\'],\\'differential-gsea\\'):")
    (166, '            outputs.extend( expand( config[\\\'accession\\\']+".{c_id}.{ext_db}.{type}",')
    (167, '                        c_id=get_contrast_ids(),')
    (168, '                        ext_db=get_ext_db(),')
    (169, '                        type=["gsea.tsv", "gsea_list.tsv"]))')
    (170, '            outputs.extend( expand("logs/"+config[\\\'accession\\\']+".{c_id}.{ext_db}.{type}",')
    (171, '                        c_id=get_contrast_ids(),')
    (172, '                        ext_db=get_ext_db(),')
    (173, '                        type=["check_differential_gsea.done", "check_differential_gsea_list.done"]))')
    (174, '')
    (175, '')
    (176, '    # For all experiment types, move data to atlas_exps and create condensed SDRF')
    (177, '    outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-copy_experiment_from_analysis_to_atlas_exps.done" ))')
    (178, '    outputs.extend(expand("logs/"+f"{config[\\\'accession\\\']}"+"-get_magetab_for_experiment.done" ))')
    (179, '')
    (180, '    print(outputs)')
    (181, "    print(\\'Getting list of outputs.. done\\')")
    (182, '    print(datetime.datetime.now())')
    (183, '')
    (184, '    return outputs')
    (185, '')
    (186, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nixonlab/DLBCL_HERV_atlas_GDC, file=workflow/rules/references.smk
context_key: ["if l[2] == \\'gene\\'", "if d[\\'gene_id\\'] in g_sym"]
    (77, "            if l[2] == \\'gene\\':")
    (78, "                if d[\\'gene_id\\'] in g_sym:")
    (79, '                    assert g_sym[d[\\\'gene_id\\\']] == d[\\\'gene_name\\\'], "Gene name mismatch: %s %s" % (d[\\\'gene_name\\\'], g_sym[d[\\\'gene_id\\\']])')
    (80, "                g_sym[d[\\'gene_id\\']] = d[\\'gene_name\\']")
    (81, '')
    (82, "            if l[2] == \\'transcript\\':")
    (83, "                if d[\\'transcript_id\\'] in tx_g:")
    (84, '                    assert tx_g[d[\\\'transcript_id\\\']] == d[\\\'gene_id\\\'], "Gene ID mismatch: %s %s" % (d[\\\'gene_id\\\'], tx_g[d[\\\'transcript_id\\\']])')
    (85, "                tx_g[d[\\'transcript_id\\']] = d[\\'gene_id\\']")
    (86, '')
    (87, '        # Generate ttg')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nixonlab/DLBCL_HERV_atlas_GDC, file=workflow/rules/references.smk
context_key: ["if l[2] == \\'gene\\'"]
    (88, "        with open(output[0], \\'w\\') as outh:")
    (89, "            print(\\'TXNAME\\\\tGENEID\\', file=outh)")
    (90, "            txlist = (l.strip(\\'\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tiramisutes/Snakemake_Gene_Family, file=Snakefile
context_key: ['if logs.is_dir()']
    (16, 'if logs.is_dir():')
    (17, '    pass')
    (18, '    #print("#"*40 + "\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nextstrain/monkeypox, file=ingest/workflow/snakemake_rules/slack_notifications.smk
context_key: ['if not slack_envvars_defined']
    (13, 'if not slack_envvars_defined:')
    (14, '    print(')
    (15, '        "ERROR: Slack notifications require two environment variables: \\\'SLACK_CHANNELS\\\' and \\\'SLACK_TOKEN\\\'.",')
    (16, '        file=sys.stderr,')
    (17, '    )')
    (18, '    sys.exit(1)')
    (19, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nextstrain/monkeypox, file=ingest/Snakefile
context_key: ['if send_slack_notifications']
    (74, 'if send_slack_notifications:')
    (75, '')
    (76, '    include: "workflow/snakemake_rules/slack_notifications.smk"')
    (77, '')
    (78, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nextstrain/monkeypox, file=Snakefile
context_key: ['if version.parse(augur_version) < version.parse(min_version)']
    (5, 'if version.parse(augur_version) < version.parse(min_version):')
    (6, '    print("This pipeline needs a newer version of augur than you currently have...")')
    (7, '    print(f"Current augur version: {augur_version}. Minimum required: {min_version}")')
    (8, '    sys.exit(1)')
    (9, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nextstrain/monkeypox, file=Snakefile
context_key: ['if not config']
    (10, 'if not config:')
    (11, '')
    (12, '    configfile: "config/config_hmpxv1.yaml"')
    (13, '')
    (14, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yoshihikosuzuki/purge_tips_smk, file=workflow/winnowmap/Snakefile
context_key: ["if stand_alone([\\'asm_fasta\\', \\'hifi_fastq\\'])", 'rule all']
    (9, "if stand_alone([\\'asm_fasta\\', \\'hifi_fastq\\']):")
    (10, "    _, ASM_BASE, _ = parse_path(config[\\'asm_fasta\\'])")
    (11, "    _, HIFI_BASE, _ = parse_path(config[\\'hifi_fastq\\'])")
    (12, '')
    (13, '    localrules: all')
    (14, '    rule all:')
    (15, '        input: f"{OUT_DIR}/{ASM_BASE}.{HIFI_BASE}.winnowmap.bam"')
    (16, '')
    (17, '################################################################################')
    (18, '#                               Module rules')
    (19, '################################################################################')
    (20, '')
    (21, '# TODO: Better way of handling inputs')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yoshihikosuzuki/purge_tips_smk, file=workflow/hifiasm/Snakefile
context_key: ["if stand_alone([\\'hifi_fastq\\'])", 'rule all']
    (9, "if stand_alone([\\'hifi_fastq\\']):")
    (10, "    _, HIFI_BASE, _ = parse_path(config[\\'hifi_fastq\\'])")
    (11, '')
    (12, '    localrules: all')
    (13, '    rule all:')
    (14, '        input: f"{OUT_DIR}/{HIFI_BASE}.hifiasm.bp.p_utg.fasta"')
    (15, '')
    (16, '################################################################################')
    (17, '#                               Module rules')
    (18, '################################################################################')
    (19, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab, file=workflow/rules/common.smk
context_key: ['if samples["Sample"].duplicated().any()']
    (26, 'if samples["Sample"].duplicated().any():')
    (27, '    raise ValueError("Duplicated values found! The Sample column in samples_table.csv mustn\\\'t contain duplicated values.")')
    (28, '')
    (29, '## Initialize project structure')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab, file=workflow/rules/common.smk
context_key: ['else', "if Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'fastqs\\', row[\\'Sample\\']).exists()"]
    (163, "        if Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'fastqs\\', row[\\'Sample\\']).exists():")
    (164, "            os.remove(str(Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'fastqs\\', row[\\'Sample\\'])))")
    (165, "        Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'fastqs\\', row[\\'Sample\\']).symlink_to(row[\\'path_to_fq\\'])")
    (166, "        samples.at[index,\\'path_to_R1\\'] = rget_fq_by_ext(row[\\'path_to_fq\\'], fq_r1_extensions)")
    (167, "        samples.at[index,\\'path_to_R2\\'] = rget_fq_by_ext(row[\\'path_to_fq\\'], fq_r2_extensions)")
    (168, "        samples.at[index,\\'File_R1\\'] = os.path.basename(samples.at[index,\\'path_to_R1\\'])")
    (169, "        samples.at[index,\\'File_R2\\'] = os.path.basename(samples.at[index,\\'path_to_R2\\'])")
    (170, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cmkobel/MS-pipeline1, file=snakefile
context_key: ['if len(config_database_glob) == 1']
    (59, 'if len(config_database_glob) == 1:')
    (60, '    raise Exception("Raised exception: no glob targets in config_database_glob") # Not tested yet.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cmkobel/MS-pipeline1, file=snakefile
context_key: ['if i==29']
    (64, '    if i==29:')
    (65, '        print(f"and {len(config_database_glob_read)-29} more..")')
    (66, '        break')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bnelsj/bwa_mem_mapping, file=Snakefile
context_key: ['if not os.path.exists("log")']
    (50, 'if not os.path.exists("log"):')
    (51, '    os.makedirs("log")')
    (52, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=amoffitt/MASQ, file=Snakefile
context_key: ['if isinstance(config["samples"],str)']
    (3, 'if isinstance(config["samples"],str):')
    (4, '    config["samples"] = [config["samples"]]')
    (5, '')
    (6, '# one fastq directory')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=amoffitt/MASQ, file=Snakefile
context_key: ['if "fastq_dirs" in config', 'if isinstance(config["fastq_dirs"],str)']
    (7, 'if "fastq_dirs" in config:')
    (8, '    if isinstance(config["fastq_dirs"],str):')
    (9, '        newdict={}')
    (10, '        for s in config["samples"]:')
    (11, '            newdict[s] = config["fastq_dirs"]')
    (12, '        config["fastq_dirs"] = newdict')
    (13, '')
    (14, '# Allow for locus table or filtered primer design table')
    (15, '# Check header of SNV_table')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=amoffitt/MASQ, file=Snakefile
context_key: ['if "SNP_ID" in header']
    (18, '    if "SNP_ID" in header:')
    (19, '        # primer design table, needs transforming')
    (20, '        config["convert_SNV_table"]=True')
    (21, '    else:')
    (22, '        # already transformed')
    (23, '        config["convert_SNV_table"]=False')
    (24, '')
    (25, '# get region number from snp input file ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=amoffitt/MASQ, file=Snakefile
context_key: ['if "regions" not in config']
    (26, 'if "regions" not in config:')
    (27, '    with open(config["SNV_table"],\\\'r\\\') as f:')
    (28, '        num_lines=0')
    (29, '        for line in f:')
    (30, '            num_lines += 1')
    (31, '        newlist = [str(x).zfill(3) for x in range(num_lines-1)]')
    (32, '        config["regions"] = newlist')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=amoffitt/MASQ, file=Snakefile
context_key: ['else', 'if isinstance(config["regions"],int)']
    (35, '    if isinstance(config["regions"],int):')
    (36, '        regnum = config["regions"]')
    (37, '        newlist = [str(x).zfill(3) for x in range(regnum)]')
    (38, '        config["regions"] = newlist')
    (39, '')
    (40, '')
    (41, '# Allow for locus table or filtered primer design table')
    (42, '# Check header of SNV_table')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=amoffitt/MASQ, file=Snakefile
context_key: ['if "SNP_ID" in header']
    (45, '    if "SNP_ID" in header:')
    (46, '        # primer design table, needs transforming')
    (47, '        config["convert_SNV_table"]=True')
    (48, '    else:')
    (49, '        # already transformed')
    (50, '        config["convert_SNV_table"]=False')
    (51, '')
    (52, '')
    (53, '# When barcode splitting is needed')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=amoffitt/MASQ, file=Snakefile
context_key: ['if "barcodes" in config']
    (54, 'if "barcodes" in config:')
    (55, '    # Load barcode table')
    (56, '    barcode_dict=dict()')
    (57, '    with open(config["barcode_file"],\\\'r\\\') as f:')
    (58, '        for line in f:')
    (59, '            x=line.strip().split(":")')
    (60, '            barcode_dict[int(x[0])]=x[1]')
    (61, '    # get sample barcode list string')
    (62, '    sample_bc_string="\\\\""')
    (63, '    for S,bc in config["barcodes"].items():')
    (64, '        sample_bc_string=sample_bc_string+S+":"+barcode_dict[bc]+" "')
    (65, '    sample_bc_string=sample_bc_string+"\\\\""')
    (66, '    # Only works for one barcode group per sample')
    (67, '    config["fastq_dirs"]=dict()')
    (68, '    for S in config["samples"]:')
    (69, '        config["fastq_dirs"][S]="bc_split_fastqs/"+S')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=amoffitt/MASQ, file=Snakefile
context_key: ['if "dna_input_ng" not in config']
    (76, 'if "dna_input_ng" not in config:')
    (77, '    config["dna_input_ng"]=10000000')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=amoffitt/MASQ, file=Snakefile
context_key: ['if "groupname" not in config']
    (78, 'if "groupname" not in config:')
    (79, '    config["groupname"]=config["samples"][0]')
    (80, '')
    (81, '######################################################################')
    (82, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/cluster_rnaseq, file=Snakefile
context_key: ['if y != ref_interest']
    (156, '              if y != ref_interest]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/cluster_rnaseq, file=Snakefile
context_key: ['if len(rest_levels) > 1', 'else', 'if len(covariates) > 1', 'else', 'rule all', 'rule index', 'rule files_qc', 'rule trimming', 'rule alignment', 'rule quantification', 'rule diffexp', 'rule plots', 'rule multiqc_all']
    (161, 'if len(rest_levels) > 1:')
    (162, '\\tallSamples = {"allSamples": list(set([ref_interest] + rest_levels))}')
    (163, '\\tallSamples.update(contrasts)')
    (164, 'else:')
    (165, '\\tallSamples = contrasts')
    (166, '')
    (167, '### Batch correction (for plotting PCAs and correlations)')
    (168, 'filesuffix = [""]')
    (169, 'if len(covariates) > 1:')
    (170, '\\tfilesuffix += ["_batchCorrected"]')
    (171, '\\tbatch = [x for x in covariates if x != var_interest]')
    (172, 'else:')
    (173, '\\tbatch = None')
    (174, '')
    (175, '')
    (176, '#### Final Step #### Global variable for the last step in the run')
    (177, 'final_step = get_final_step()')
    (178, '')
    (179, '')
    (180, '#### Load rules ####')
    (181, "include: \\'rules/common.smk\\'")
    (182, "include: \\'rules/qc.smk\\'")
    (183, "include: \\'rules/preprocess.smk\\'")
    (184, "include: \\'rules/index.smk\\'")
    (185, "include: \\'rules/align.smk\\'")
    (186, "include: \\'rules/quantification.smk\\'")
    (187, "include: \\'rules/deseq2.smk\\'")
    (188, "include: \\'rules/plots.smk\\'")
    (189, '')
    (190, '')
    (191, 'def get_index_input():')
    (192, '')
    (193, '\\tindex_input = config["ref"][chosen_aligner][f"{chosen_aligner}_index"]')
    (194, '\\treturn index_input')
    (195, '')
    (196, '')
    (197, 'def get_trimming_input():')
    (198, '')
    (199, '\\ttrimming_input = [f"{OUTDIR}/multiqc/multiqc_files_report.html"]')
    (200, '\\t')
    (201, "\\tfor sample in samples[\\'sample\\']:")
    (202, '\\t\\tif single_end:')
    (203, '\\t\\t\\ttrimming_input += [f"{OUTDIR}/qc/fastqc_concat/{sample}_R1_fastqc.html"]')
    (204, '\\t\\telse:')
    (205, '\\t\\t\\ttrimming_input += expand(f"{OUTDIR}/qc/fastqc_concat/{sample}_R{{strand}}_fastqc.html", strand=[1,2])')
    (206, '\\t')
    (207, "\\tfor sample in samples[\\'sample\\']:")
    (208, '\\t\\tif single_end:')
    (209, '\\t\\t\\ttrimming_input += [f"{OUTDIR}/trimmed/{sample}/{sample}_R1.fastq.gz"]')
    (210, '\\t\\telse:')
    (211, '\\t\\t\\ttrimming_input += expand(f"{OUTDIR}/trimmed/{sample}/{sample}_R{{strand}}.fastq.gz", strand=[1,2])')
    (212, '\\t')
    (213, '\\t#trimming_input += [f"{OUTDIR}/multiqc/multiqc_run_report.html"]')
    (214, '\\treturn trimming_input')
    (215, '')
    (216, '')
    (217, 'def get_alignment_input():')
    (218, '')
    (219, '\\talignment_input = [f"{OUTDIR}/multiqc/multiqc_files_report.html"]')
    (220, '\\t')
    (221, "\\tfor sample in samples[\\'sample\\']:")
    (222, '\\t\\tif single_end:')
    (223, '\\t\\t\\talignment_input += [f"{OUTDIR}/qc/fastqc_concat/{sample}_R1_fastqc.html"]')
    (224, '\\t\\telse:')
    (225, '\\t\\t\\talignment_input += expand(f"{OUTDIR}/qc/fastqc_concat/{sample}_R{{strand}}_fastqc.html", strand=[1,2])')
    (226, '\\t')
    (227, '\\tif chosen_aligner == "salmon":')
    (228, '\\t\\talignment_input += expand(f"{OUTDIR}/quant/salmon/{{sample}}/quant.sf", sample=samples[\\\'sample\\\'])')
    (229, '\\telse:')
    (230, '\\t\\talignment_input += expand(f"{OUTDIR}/mapped/{chosen_aligner}/{{sample}}/Aligned.sortedByCoord.out.bam", sample=samples[\\\'sample\\\'])')
    (231, '\\t')
    (232, '\\t#alignment_input += [f"{OUTDIR}/multiqc/multiqc_run_report.html"]')
    (233, '\\treturn alignment_input')
    (234, '')
    (235, '')
    (236, 'def get_quantification_input():')
    (237, '')
    (238, '\\tquantification_input= [f"{OUTDIR}/multiqc/multiqc_files_report.html"]')
    (239, '\\t')
    (240, '\\tquantification_input += [f"{OUTDIR}/deseq2/{deseq_path}/counts.tsv"]')
    (241, '\\t')
    (242, '\\tquantification_input += [f"{OUTDIR}/multiqc/multiqc_run_report.html"]')
    (243, '\\treturn quantification_input')
    (244, '')
    (245, '')
    (246, 'def get_diffexp_input():')
    (247, '')
    (248, '\\tdiffexp_input = [f"{OUTDIR}/multiqc/multiqc_files_report.html"]')
    (249, '')
    (250, '\\tdiffexp_input += expand(f"{OUTDIR}/deseq2/{deseq_path}/{{contrast}}/{{contrast}}_diffexp.xlsx", contrast=contrasts.keys())')
    (251, '\\tdiffexp_input += expand(f"{OUTDIR}/deseq2/{deseq_path}/{{contrast}}/{{contrast}}_diffexp.tsv", contrast=contrasts.keys())')
    (252, '\\t')
    (253, '\\tdiffexp_input += [f"{OUTDIR}/multiqc/multiqc_run_report.html"]')
    (254, '\\treturn diffexp_input')
    (255, '')
    (256, '')
    (257, 'def get_plots_input():')
    (258, '')
    (259, '\\tplots_input= [f"{OUTDIR}/multiqc/multiqc_files_report.html"]')
    (260, '\\t')
    (261, '\\tplots_input += expand(f"{OUTDIR}/deseq2/{deseq_path}/{{contrast}}/{{contrast}}_diffexp.xlsx", contrast=contrasts.keys())')
    (262, '\\tplots_input += expand(f"{OUTDIR}/deseq2/{deseq_path}/{{contrast}}/{{contrast}}_diffexp.tsv", contrast=contrasts.keys())')
    (263, '\\tplots_input += expand(f"{OUTDIR}/deseq2/{deseq_path}/{{contrast}}/plots/{{contrast}}_topbottomDEgenes.{{pext}}", \\\\')
    (264, '\\t\\t\\t\\t\\t\\tcontrast=contrasts.keys(), pext = ["pdf", "png"])')
    (265, '\\tplots_input += expand(f"{OUTDIR}/deseq2/{deseq_path}/{{ALLcontrast}}/plots/{{ALLcontrast}}_{{plot}}{{fsuffix}}.{{pext}}", \\\\')
    (266, '\\t\\t\\t\\t\\t\\tALLcontrast=allSamples.keys(), fsuffix=filesuffix, plot = ["pca", "dist"], pext = ["pdf", "png"])')
    (267, '\\tplots_input += expand(f"{OUTDIR}/deseq2/{deseq_path}/{{contrast}}/plots/{{contrast}}_MAplot.{{pext}}", \\\\')
    (268, '\\t\\t\\t\\t\\t\\tcontrast=contrasts.keys(), pext = ["pdf", "png"])')
    (269, '')
    (270, '\\tplots_input += [f"{OUTDIR}/multiqc/multiqc_run_report.html"]')
    (271, '\\treturn plots_input')
    (272, '')
    (273, '')
    (274, '')
    (275, '# TARGET RULES')
    (276, 'rule all:')
    (277, '\\tinput:')
    (278, '\\t\\tget_plots_input()')
    (279, '')
    (280, '')
    (281, 'rule index:')
    (282, '\\tinput:')
    (283, '\\t\\tget_index_input()')
    (284, '')
    (285, '')
    (286, "# Check files\\' MD5 and creates a MultiQC report using the fastqc\\'s reports for original files")
    (287, 'rule files_qc:')
    (288, '\\tinput:')
    (289, '\\t\\tf"{OUTDIR}/multiqc/multiqc_files_report.html"')
    (290, '')
    (291, '')
    (292, 'rule trimming:')
    (293, '\\tinput:')
    (294, '\\t\\tget_trimming_input()')
    (295, '')
    (296, '')
    (297, 'rule alignment:')
    (298, '\\tinput:')
    (299, '\\t\\tget_alignment_input()')
    (300, '')
    (301, '')
    (302, 'rule quantification:')
    (303, '\\tinput:')
    (304, '\\t\\tget_quantification_input()')
    (305, '')
    (306, '')
    (307, 'rule diffexp:')
    (308, '\\tinput:')
    (309, '\\t\\tget_diffexp_input()')
    (310, '')
    (311, '')
    (312, 'rule plots:')
    (313, '\\tinput:')
    (314, '\\t\\tget_plots_input()')
    (315, '')
    (316, '')
    (317, '# Creates a MultiQC report for all files that it founds, mixing all aligners or quantifiers that has been ran')
    (318, 'rule multiqc_all:')
    (319, '\\tinput:')
    (320, '\\t\\tf"{OUTDIR}/multiqc/multiqc_all_report.html"')
    (321, '')
    (322, '')
    (323, "'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=TasnubaS/Snakemake-WorkFlow, file=rules/diffexp.smk
context_key: ['if "strandedness" in units.columns']
    (1, '    if "strandedness" in units.columns:')
    (2, '        return units["strandedness"].tolist()')
    (3, '    else:')
    (4, '        strand_list=["none"]')
    (5, '        return strand_list*units.shape[0]')
    (6, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snakerTest, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'if len(fol_path_spillover_slide_acs) > 0']
    (117, 'if do_compensation:')
    (118, '    if len(file_path_orig_sm) == 0:')
    (119, '        if len(fol_path_spillover_slide_acs) > 0:')
    (120, "            spillover_mode = \\'estimate_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snakerTest, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'else']
    (121, '        else:')
    (122, '            raise ValueError(\\\'Either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (123, "                             \\'needs to be provided.\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snakerTest, file=workflow/Snakefile
context_key: ['if do_compensation', 'elif len(fol_path_spillover_slide_acs) == 0']
    (124, '    elif len(fol_path_spillover_slide_acs) == 0:')
    (125, "            spillover_mode = \\'copy_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snakerTest, file=workflow/Snakefile
context_key: ['if do_compensation', 'else']
    (126, '    else:')
    (127, '        raise ValueError(\\\'Only provide either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (128, "                         \\'but not both.\\')")
    (129, '')
    (130, '# Produce a list of all cellprofiler output files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snakerTest, file=workflow/Snakefile
context_key: ['if not do_compensation']
    (267, 'if not do_compensation:')
    (268, "    config_dict_cp[\\'measuremasks\\'] = {")
    (269, '        \\\'message\\\':  """\\\\')
    (270, '                    Measure masks ')
    (271, '                    ')
    (272, '                    Measure segmentation masks')
    (273, '                    """,')
    (274, "        \\'run_size\\': 1,")
    (275, "        \\'plugins\\': cp_plugins,")
    (276, "        \\'pipeline\\': \\'resources/cp4_pipelines/adapted/3_measure_mask_basic.cppipe\\',")
    (277, "        \\'input_files\\': [")
    (278, '            # Folder containing segmentation masks')
    (279, '            fol_path_cellmasks,')
    (280, '            # Folder containing image stacks to measure')
    (281, '            fol_path_full,')
    (282, '            # Folder containing cell probabilities')
    (283, '            fol_path_probabilities,')
    (284, '            # Acquisition metadata')
    (285, '            file_path_acmeta,')
    (286, '            # Channel metadata')
    (287, '            file_path_channelmeta_full,')
    (288, '            file_path_channelmeta_cellprobab')
    (289, '        ],')
    (290, "        \\'output_patterns\\': {\\'.\\':")
    (291, '                            # Folder containing cellprofiler measurement')
    (292, '                                cp_measurements_output_files,')
    (293, '                            # Subfolder containing segmentation masks used')
    (294, '                            # for the measurements')
    (295, "                            \\'masks\\': directory(fol_path_cp_masks),")
    (296, '                            # Subfolder containing the images used for the')
    (297, '                            # measurements.')
    (298, "                            \\'images\\': directory(fol_path_cp_images)},")
    (299, "        \\'input_folder\\': fol_path_cp_input")
    (300, '    }')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snakerTest, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'params']
    (454, 'if do_compensation:')
    (455, '    if spillover_mode == "estimate_sm":')
    (456, '        rule get_ss_panel_from_panel:')
    (457, '            input: csv_panel')
    (458, '            output: file_path_ss_panel')
    (459, '            params:')
    (460, '                col_metal=csv_panel_metal,')
    (461, '                col_spillover=csv_panel_spillover')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Masteryuanli/snakerTest, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'run']
    (462, '            run:')
    (463, '                dat_csv = pd.read_csv(csv_panel)')
    (464, '                assert params.col_spillover in dat_csv.columns, ValueError(')
    (465, '                    f"The panel csv {csv_panel} should contain a boolean column"')
    (466, '                    f"{params.col_spillover} indicating the metals used for the"')
    (467, '                    f"spillover acquisitions.")')
    (468, '                metals = dat_csv.query(')
    (469, "                    f\\'{params.col_spillover} == 1\\')[params.col_metal].values")
    (470, "                with open(output[0], \\'w\\') as f:")
    (471, '                    for m in metals:')
    (472, "                        f.write(m+\\'\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_AmpliconSeq, file=workflow/rules/quant.smk
context_key: ['if [ -d {params.plotsdir} ];then rm -rf {params.plotsdir};fi && mkdir {params.plotsdir} && cd {params.plotsdir} && cp {output.filteredmergedquant} . && \\']
    (47, 'if [ -d {params.plotsdir} ];then rm -rf {params.plotsdir};fi && mkdir {params.plotsdir} && cd {params.plotsdir} && cp {output.filteredmergedquant} . && \\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_AmpliconSeq, file=workflow/rules/trim.smk
context_key: ['if [ ! -d {params.outdir} ];then mkdir {params.outdir};f']
    (24, 'if [ ! -d {params.outdir} ];then mkdir {params.outdir};fi')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_AmpliconSeq, file=workflow/rules/trim.smk
context_key: ['if [ "{params.peorse}" == "PE" ];the']
    (25, 'if [ "{params.peorse}" == "PE" ];then')
    (26, '    ## Paired-end')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CCBR/CCBR_circRNA_AmpliconSeq, file=workflow/rules/align.smk
context_key: ['if [ "{params.peorse}" == "PE" ];the']
    (73, 'if [ "{params.peorse}" == "PE" ];then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MariaOsmala/preprint, file=workflow/Snakefile
context_key: ["if cwd.name == \\'workflow\\'", 'else', 'rule all']
    (5, "if cwd.name == \\'workflow\\':")
    (6, '\\tproject_dir = str(cwd.parent)')
    (7, 'else:')
    (8, '\\tproject_dir = str(cwd)')
    (9, '')
    (10, '# Load the user configuration')
    (11, 'configfile: f"{project_dir}/workflow/config.yaml"')
    (12, '')
    (13, '# Various paths we will be using in this analysis pipeline')
    (14, "data_dir = config[\\'data_dir\\']")
    (15, "workflow_dir = f\\'{project_dir}/workflow\\'")
    (16, "scripts_dir = f\\'{project_dir}/scripts\\'")
    (17, '')
    (18, '# Big list of all the samples')
    (19, "all_samples = pd.read_csv(f\\'{workflow_dir}/samples.tsv\\', sep=\\'\\\\t\\').sort_index()")
    (20, '')
    (21, 'def all_data_types(cell_line, exclude=None):')
    (22, '\\t"""')
    (23, '\\tGet all data types (chromatin features?) for a given cell line.')
    (24, '\\tOptionally exclude some data types.')
    (25, '\\t"""')
    (26, '\\tquery = f"cell_line==\\\'{cell_line}\\\'"')
    (27, '\\tif exclude is not None:')
    (28, '\\t\\tfor ex in exclude:')
    (29, '\\t\\t\\tquery += f\\\' and data_type != "{ex}"\\\'')
    (30, '\\treturn all_samples.dropna().query(query).data_type.unique()')
    (31, '')
    (32, '# Default rule to run the entire analysis pipeline')
    (33, 'rule all:')
    (34, "\\tinput: f\\'{data_dir}/K562/data_R/predictions.RData\\'")
    (35, '')
    (36, '# These include files contain the rules for the individual steps')
    (37, "include: \\'preprocessing.smk\\'")
    (38, "include: \\'train_predict.smk\\'")
    (39, "'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=shukwong/dna-seq-varlociraptor-test, file=workflow/Snakefile
context_key: ['if is_activated("report/stratify")']
    (32, 'if is_activated("report/stratify"):')
    (33, '    batches = samples[config["report"]["stratify"]["by-column"]].unique()')
    (34, '')
    (35, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/chipseq, file=workflow/rules/peak_analysis.smk
context_key: ['input', 'else "orph_rm_pe"']
    (7, '            else "orph_rm_pe")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=TheJacksonLaboratory/toymake, file=Snakefile
context_key: ['if smk_home != config["smk_home"]']
    (28, 'if smk_home != config["smk_home"]:')
    (29, '    sys.exit(')
    (30, '        "snakemake homedir, smk_home in config.yaml: {} is different than current "')
    (31, '        "workdir: {}".format(config["smk_home"], smk_home)')
    (32, '    )')
    (33, '')
    (34, '')
    (35, '## set workdir outside of snakemake home')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/biomarker, file=workflow/rules/common.smk
context_key: ['if not workflow.overwrite_configfiles']
    (17, 'if not workflow.overwrite_configfiles:')
    (18, '    sys.exit("At least one config file must be passed using --configfile/--configfiles, by command line or a profile!")')
    (19, '')
    (20, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=watronfire/monterey, file=workflow/rules/phylosor.smk
context_key: ['input', 'run', 'if "Unnamed']
    (12, '        if "Unnamed: 0" in md.columns:')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=watronfire/monterey, file=workflow/rules/support.smk
context_key: ['if wildcards.status == "actual"']
    (1, '    if wildcards.status == "actual":')
    (2, '        return config["input_locations"]["tree"]')
    (3, '    else:')
    (4, '        return f"results/trees/null_{wildcards.num}.tree"')
    (5, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khembach/Tollervey-iCLIP, file=Snakefile
context_key: ['if len(config) == 0', 'if not os.path.isfile(config["metatxt"])', "if not set([\\'names\\',\\'type\\']).issubset(samples.columns)", 'if config["useCondaR"] == True', 'else', 'rule all', 'rule setup', 'rule pkginstall', 'rule runfastqc', 'rule rundemultiplex', 'rule runtrimming', 'rule runstar', 'rule runxlsites', 'rule runannotate', 'rule run_removedup', 'rule run_filter_second_read', 'rule run_norm_bigwig', 'rule run_bigwig_second_read', 'rule listpackages', 'rule softwareversions', 'rule starindex', 'rule fastqc', 'rule fastqcdemultiplexed', 'rule fastqctrimmed', 'rule multiqc']
    (2, 'if len(config) == 0:')
    (3, '\\tif os.path.isfile("./config.yaml"):')
    (4, '\\t\\tconfigfile: "./config.yaml"')
    (5, '\\telse:')
    (6, '\\t\\tsys.exit("".join(["Make sure there is a config.yaml file in ", os.getcwd(), ')
    (7, '\\t\\t\\t" or specify one with the --configfile commandline parameter."]))')
    (8, '')
    (9, '## Make sure that all expected variables from the config file are in the config dictionary')
    (10, "configvars = [\\'annotation\\', \\'organism\\', \\'build\\', \\'release\\', \\'txome\\', \\'genome\\', \\'gtf\\', \\'STARindex\\', \\'readlength\\', \\'fldMean\\', \\'fldSD\\', \\'metatxt\\', \\'ncores\\', \\'FASTQ\\', \\'fqext1\\', \\'fqext2\\', \\'fqsuffix\\', \\'output\\', \\'useCondaR\\', \\'Rbin\\', \\'run_trimming\\', \\'run_STAR\\']")
    (11, 'for k in configvars:')
    (12, '\\tif k not in config:')
    (13, '\\t\\tconfig[k] = None')
    (14, '')
    (15, '## If any of the file paths is missing, replace it with ""')
    (16, 'def sanitizefile(str):')
    (17, '\\tif str is None:')
    (18, "\\t\\tstr = \\'\\'")
    (19, '\\treturn str')
    (20, '')
    (21, "config[\\'gtf\\'] = sanitizefile(config[\\'gtf\\'])")
    (22, "config[\\'genome\\'] = sanitizefile(config[\\'genome\\'])")
    (23, "config[\\'STARindex\\'] = sanitizefile(config[\\'STARindex\\'])")
    (24, "config[\\'metatxt\\'] = sanitizefile(config[\\'metatxt\\'])")
    (25, '')
    (26, '## Read metadata')
    (27, 'if not os.path.isfile(config["metatxt"]):')
    (28, '\\tsys.exit("".join(["Metadata file ", config["metatxt"], " does not exist."]))')
    (29, '')
    (30, 'import pandas as pd')
    (31, 'samples = pd.read_csv(config["metatxt"], sep=\\\'\\\\t\\\')')
    (32, '')
    (33, "if not set([\\'names\\',\\'type\\']).issubset(samples.columns):")
    (34, '\\tsys.exit("".join(["Make sure \\\'names\\\' and \\\'type\\\' are columns in ", config["metatxt"]]))')
    (35, '')
    (36, '')
    (37, '## Sanitize provided input and output directories')
    (38, 'import re')
    (39, 'def getpath(str):')
    (40, "\\tif str in [\\'\\', \\'.\\', \\'./\\']:")
    (41, "\\t\\treturn \\'\\'")
    (42, "\\tif str.startswith(\\'./\\'):")
    (43, "\\t\\tregex = re.compile(\\'^\\\\./?\\')")
    (44, "\\t\\tstr = regex.sub(\\'\\', str)")
    (45, "\\tif not str.endswith(\\'/\\'):")
    (46, "\\t\\tstr += \\'/\\'")
    (47, '\\treturn str')
    (48, '')
    (49, 'outputdir = getpath(config["output"])')
    (50, 'FASTQdir = getpath(config["FASTQ"])')
    (51, '')
    (52, '## Define the conda environment for all rules using R')
    (53, 'if config["useCondaR"] == True:')
    (54, '\\tRenv = "envs/environment_R.yaml"')
    (55, 'else:')
    (56, '\\tRenv = "envs/environment.yaml"')
    (57, '')
    (58, '## Define the R binary')
    (59, 'Rbin = config["Rbin"]')
    (60, '')
    (61, '## ------------------------------------------------------------------------------------ ##')
    (62, '## Target definitions')
    (63, '## ------------------------------------------------------------------------------------ ##')
    (64, '## Run all analyses')
    (65, 'rule all:')
    (66, '\\tinput:')
    (67, '\\t\\tos.path.join(outputdir, "MultiQC", "multiqc_report.html")')
    (68, '')
    (69, 'rule setup:')
    (70, '\\tinput:')
    (71, '\\t\\tos.path.join(outputdir, "Rout", "pkginstall_state.txt"),')
    (72, '\\t\\tos.path.join(outputdir, "Rout", "softwareversions.done")')
    (73, '')
    (74, '## Install R packages')
    (75, 'rule pkginstall:')
    (76, '\\tinput:')
    (77, '\\t\\tscript = "scripts/install_pkgs.R"')
    (78, '\\toutput:')
    (79, '\\t  \\tos.path.join(outputdir, "Rout", "pkginstall_state.txt")')
    (80, '\\tparams:')
    (81, '\\t\\tflag = config["annotation"],')
    (82, '\\t\\tncores = config["ncores"],')
    (83, '\\t\\torganism = config["organism"],')
    (84, '\\t\\tRbin = Rbin')
    (85, '\\tpriority:')
    (86, '\\t\\t50')
    (87, '\\tconda:')
    (88, '\\t\\tRenv')
    (89, '\\tlog:')
    (90, '\\t\\tos.path.join(outputdir, "Rout", "install_pkgs.Rout")')
    (91, '\\tbenchmark:')
    (92, '\\t  \\tos.path.join(outputdir, "benchmarks", "install_pkgs.txt")')
    (93, '\\tshell:')
    (94, '\\t\\t\\\'\\\'\\\'{params.Rbin} CMD BATCH --no-restore --no-save "--args outtxt=\\\'{output}\\\' ncores=\\\'{params.ncores}\\\' annotation=\\\'{params.flag}\\\' organism=\\\'{params.organism}\\\'" {input.script} {log}\\\'\\\'\\\'')
    (95, '')
    (96, '## FastQC on original (untrimmed) files')
    (97, 'rule runfastqc:')
    (98, '\\tinput:')
    (99, '\\t\\texpand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext1"]), "_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (100, '\\t\\texpand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext2"]), "_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (101, '\\t\\texpand(os.path.join(outputdir, "FastQC", "{sample}_fastqc.zip"), sample = samples.names[samples.type == \\\'SE\\\'].values.tolist())')
    (102, '')
    (103, 'rule rundemultiplex:')
    (104, '\\tinput:')
    (105, '\\t\\texpand(os.path.join(outputdir, "demultiplexed", "{sample}_demux.fastq.gz"), sample = samples.names[samples.type == \\\'SE\\\'].values.tolist())')
    (106, '')
    (107, '## Trimming and FastQC on trimmed files')
    (108, 'rule runtrimming:')
    (109, '\\tinput:')
    (110, '\\t\\texpand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext1"]), "_val_1_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (111, '\\t\\texpand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext2"]), "_val_2_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (112, '\\t\\texpand(os.path.join(outputdir, "FastQC", "{sample}_umi_removed_trimmed_fastqc.zip"), sample = samples.names[samples.type == \\\'SE\\\'].values.tolist())')
    (113, '')
    (114, '## STAR alignment')
    (115, 'rule runstar:')
    (116, '\\tinput:')
    (117, '\\t\\texpand(os.path.join(outputdir, "STAR", "{sample}", "{sample}_Aligned.sortedByCoord.out.bam.bai"), sample = samples.names.values.tolist()),')
    (118, '\\t\\texpand(os.path.join(outputdir, "STARbigwig", "{sample}_Aligned.sortedByCoord.out.bw"), sample = samples.names.values.tolist())')
    (119, '')
    (120, 'rule runxlsites:')
    (121, '\\tinput:')
    (122, '\\t\\texpand(os.path.join(outputdir, "xlsites", "{sample}_cDNA_unique.bed"), sample = samples.names.values.tolist())')
    (123, '')
    (124, 'rule runannotate:')
    (125, '\\tinput:')
    (126, '\\t\\texpand(os.path.join(outputdir, "annotate", "{sample}_biotype.tab"), sample = samples.names.values.tolist())')
    (127, '')
    (128, 'rule run_removedup:')
    (129, '\\tinput:')
    (130, '\\t\\texpand(os.path.join(outputdir, "BAM_deduplicated", "{sample}_deduplicated.bam.bai"), sample = samples.names.values.tolist())')
    (131, '')
    (132, 'rule run_filter_second_read:')
    (133, '\\tinput:')
    (134, '\\t\\texpand(os.path.join(outputdir, "BAM_deduplicated/{sample}_deduplicated.r2.bam.bai"), sample = samples.names.values.tolist())')
    (135, '')
    (136, 'rule run_norm_bigwig:')
    (137, '\\tinput:')
    (138, '\\t\\texpand(os.path.join(outputdir, "bigwig", "{sample}_deduplicated.r2.ucsc.replicatesNorm.bw"), sample = samples.names.values.tolist())')
    (139, '')
    (140, 'rule run_bigwig_second_read:')
    (141, '\\tinput:')
    (142, '\\t\\texpand(os.path.join(outputdir, "bigwig", "{sample}_deduplicated.r2.bw"), sample = samples.names.values.tolist())')
    (143, '')
    (144, '')
    (145, '## List all the packages that were used by the R analyses')
    (146, 'rule listpackages:')
    (147, '\\tlog:')
    (148, '\\t\\tos.path.join(outputdir, "Rout", "list_packages.Rout")')
    (149, '\\tparams:')
    (150, '\\t\\tRoutdir = os.path.join(outputdir, "Rout"),')
    (151, '\\t\\touttxt = os.path.join(outputdir, "R_package_versions.txt"),')
    (152, '\\t\\tscript = "scripts/list_packages.R",')
    (153, '\\t\\tRbin = Rbin')
    (154, '\\tconda:')
    (155, '\\t\\tRenv')
    (156, '\\tshell:')
    (157, '\\t\\t\\\'\\\'\\\'{params.Rbin} CMD BATCH --no-restore --no-save "--args Routdir=\\\'{params.Routdir}\\\' outtxt=\\\'{params.outtxt}\\\'" {params.script} {log}\\\'\\\'\\\'')
    (158, '')
    (159, '## Print the versions of all software packages')
    (160, 'rule softwareversions:')
    (161, '\\toutput:')
    (162, '\\t\\ttouch(os.path.join(outputdir, "Rout", "softwareversions.done"))')
    (163, '\\tlog:')
    (164, '\\t\\tos.path.join(outputdir, "logs", "softversions.log")')
    (165, '\\tconda:')
    (166, '\\t\\t"envs/environment.yaml"')
    (167, '\\tshell:')
    (168, '\\t\\t"echo -n \\\'ARMOR version \\\' && cat version; "')
    (169, '\\t\\t"salmon --version; trim_galore --version; "')
    (170, '\\t\\t"echo -n \\\'cutadapt \\\' && cutadapt --version; "')
    (171, '\\t\\t"fastqc --version; STAR --version; samtools --version; multiqc --version; "')
    (172, '\\t\\t"bedtools --version"')
    (173, '')
    (174, '## ------------------------------------------------------------------------------------ ##')
    (175, '## Reference preparation')
    (176, '## ------------------------------------------------------------------------------------ ##')
    (177, '## Generate STAR index')
    (178, 'rule starindex:')
    (179, '\\tinput:')
    (180, '\\t\\tgenome = config["genome"],')
    (181, '\\t\\tgtf = config["gtf"]')
    (182, '\\toutput:')
    (183, '\\t\\tos.path.join(config["STARindex"], "SA"),')
    (184, '\\t\\tos.path.join(config["STARindex"], "chrNameLength.txt")')
    (185, '\\tlog:')
    (186, '\\t\\tos.path.join(outputdir, "logs", "STAR_index.log")')
    (187, '\\tbenchmark:')
    (188, '\\t\\tos.path.join(outputdir, "benchmarks", "STAR_index.txt")')
    (189, '\\tparams:')
    (190, '\\t\\tSTARindex = lambda wildcards, output: os.path.dirname(output[0]),   ## dirname of first output')
    (191, '\\t\\treadlength = config["readlength"],')
    (192, '\\t\\tstarextraparams = config["additional_star_index"]')
    (193, '\\tconda:')
    (194, '\\t\\t"envs/environment.yaml"')
    (195, '\\tthreads:')
    (196, '\\t\\tconfig["ncores"]')
    (197, '\\tshell:')
    (198, '\\t\\t"echo \\\'STAR version:\\')
    (199, '\\\' > {log}; STAR --version >> {log}; "')
    (200, '\\t\\t"STAR --runMode genomeGenerate --runThreadN {threads} --genomeDir {params.STARindex} "')
    (201, '\\t\\t"--genomeFastaFiles {input.genome} --sjdbGTFfile {input.gtf} --sjdbOverhang {params.readlength} "')
    (202, '\\t\\t"{params.starextraparams}"')
    (203, '')
    (204, '')
    (205, '## ------------------------------------------------------------------------------------ ##')
    (206, '## Quality control')
    (207, '## ------------------------------------------------------------------------------------ ##')
    (208, '## FastQC, original reads')
    (209, 'rule fastqc:')
    (210, '\\tinput:')
    (211, '\\t\\tfastq = os.path.join(FASTQdir, "".join(["{sample}.", str(config["fqsuffix"]), ".gz"]))')
    (212, '\\toutput:')
    (213, '\\t\\tos.path.join(outputdir, "FastQC", "{sample}_fastqc.zip")')
    (214, '\\tparams:')
    (215, '\\t\\tFastQC = lambda wildcards, output: os.path.dirname(output[0])   ## dirname of first output')
    (216, '\\tlog:')
    (217, '\\t\\tos.path.join(outputdir, "logs", "fastqc_{sample}.log")')
    (218, '\\tbenchmark:')
    (219, '\\t\\tos.path.join(outputdir, "benchmarks", "fastqc_{sample}.txt")')
    (220, '\\tconda:')
    (221, '\\t\\t"envs/environment.yaml"')
    (222, '\\tthreads:')
    (223, '\\t\\tconfig["ncores"]')
    (224, '\\tshell:')
    (225, '\\t\\t"echo \\\'FastQC version:\\')
    (226, '\\\' > {log}; fastqc --version >> {log}; "')
    (227, '\\t\\t"fastqc -o {params.FastQC} -t {threads} {input.fastq}"')
    (228, '')
    (229, '# demultiplexed reads')
    (230, 'rule fastqcdemultiplexed:')
    (231, '\\tinput:')
    (232, '\\t\\tfastq = os.path.join(outputdir, "demultiplexed", "{sample}.fastq.gz")')
    (233, '\\toutput:')
    (234, '\\t\\tos.path.join(outputdir, "FastQC", "{sample}_fastqc.zip")')
    (235, '\\tparams:')
    (236, '\\t\\tFastQC = lambda wildcards, output: os.path.dirname(output[0])   ## dirname of first output')
    (237, '\\tlog:')
    (238, '\\t\\tos.path.join(outputdir, "logs", "fastqc_demultiplexed_{sample}.log")')
    (239, '\\tbenchmark:')
    (240, '\\t\\tos.path.join(outputdir, "benchmarks", "fastqc_demultiplexed_{sample}.txt")')
    (241, '\\tconda:')
    (242, '\\t\\t"envs/environment.yaml"')
    (243, '\\tthreads:')
    (244, '\\t\\tconfig["ncores"]')
    (245, '\\tshell:')
    (246, '\\t\\t"echo \\\'FastQC version:\\')
    (247, '\\\' > {log}; fastqc --version >> {log}; "')
    (248, '\\t\\t"fastqc -o {params.FastQC} -t {threads} {input.fastq}"')
    (249, '')
    (250, '')
    (251, '')
    (252, '## FastQC, trimmed reads')
    (253, 'rule fastqctrimmed:')
    (254, '\\tinput:')
    (255, '\\t\\tfastq = os.path.join(outputdir, "FASTQtrimmed", "{sample}.fq.gz")')
    (256, '\\toutput:')
    (257, '\\t\\tos.path.join(outputdir, "FastQC", "{sample}_fastqc.zip")')
    (258, '\\tparams:')
    (259, '\\t\\tFastQC = lambda wildcards, output: os.path.dirname(output[0])   ## dirname of first output')
    (260, '\\tlog:')
    (261, '\\t\\tos.path.join(outputdir, "logs", "fastqc_trimmed_{sample}.log")')
    (262, '\\tbenchmark:')
    (263, '\\t\\tos.path.join(outputdir, "benchmarks", "fastqc_trimmed_{sample}.txt")')
    (264, '\\tconda:')
    (265, '\\t\\t"envs/environment.yaml"')
    (266, '\\tthreads:')
    (267, '\\t\\tconfig["ncores"]')
    (268, '\\tshell:')
    (269, '\\t\\t"echo \\\'FastQC version:\\')
    (270, '\\\' > {log}; fastqc --version >> {log}; "')
    (271, '\\t\\t"fastqc -o {params.FastQC} -t {threads} {input.fastq}"')
    (272, '')
    (273, '')
    (274, '')
    (275, '# The config.yaml files determines which steps should be performed')
    (276, 'def multiqc_input(wildcards):')
    (277, '\\tinput = []')
    (278, '\\tinput.extend(expand(os.path.join(outputdir, "FastQC", "{sample}_fastqc.zip"), sample = samples.names[samples.type == \\\'SE\\\'].values.tolist()))')
    (279, '\\tinput.extend(expand(os.path.join(outputdir, "FastQC", "{sample}_demux_fastqc.zip"), sample = samples.names[samples.type == \\\'SE\\\'].values.tolist()))')
    (280, '\\tif config["run_trimming"]:')
    (281, '\\t\\tinput.extend(expand(os.path.join(outputdir, "FASTQtrimmed", "{sample}_umi_removed_trimmed.fq.gz"), sample = samples.names[samples.type == \\\'SE\\\'].values.tolist()))')
    (282, '\\t\\tinput.extend(expand(os.path.join(outputdir, "FastQC", "{sample}_umi_removed_trimmed_fastqc.zip"), sample = samples.names[samples.type == \\\'SE\\\'].values.tolist()))')
    (283, '\\tif config["run_STAR"]:')
    (284, '\\t\\tinput.extend(expand(os.path.join(outputdir, "STAR", "{sample}", "{sample}_Aligned.sortedByCoord.out.bam.bai"), sample = samples.names.values.tolist()))')
    (285, '\\treturn input')
    (286, '')
    (287, '## Determine the input directories for MultiQC depending on the config file')
    (288, 'def multiqc_params(wildcards):')
    (289, '\\tparam = [os.path.join(outputdir, "FastQC")]')
    (290, '\\tif config["run_trimming"]:')
    (291, '\\t\\tparam.append(os.path.join(outputdir, "FASTQtrimmed"))')
    (292, '\\tif config["run_STAR"]:')
    (293, '\\t\\tparam.append(os.path.join(outputdir, "STAR"))')
    (294, '\\treturn param')
    (295, '')
    (296, '## MultiQC')
    (297, 'rule multiqc:')
    (298, '\\tinput:')
    (299, '\\t\\tmultiqc_input')
    (300, '\\toutput:')
    (301, '\\t\\tos.path.join(outputdir, "MultiQC", "multiqc_report.html")')
    (302, '\\tparams:')
    (303, '\\t\\tinputdirs = multiqc_params,')
    (304, '\\t\\tMultiQCdir = lambda wildcards, output: os.path.dirname(output[0])   ## dirname of first output')
    (305, '\\tlog:')
    (306, '\\t\\tos.path.join(outputdir, "logs", "multiqc.log")')
    (307, '\\tbenchmark:')
    (308, '\\t\\tos.path.join(outputdir, "benchmarks", "multiqc.txt")')
    (309, '\\tconda:')
    (310, '\\t\\t"envs/environment.yaml"')
    (311, '\\tshell:')
    (312, '\\t\\t"echo \\\'MultiQC version:\\')
    (313, '\\\' > {log}; multiqc --version >> {log}; "')
    (314, '\\t\\t"multiqc {params.inputdirs} -f -o {params.MultiQCdir}"')
    (315, '')
    (316, '')
    (317, '## ------------------------------------------------------------------------------------ ##')
    (318, '## Demultiplex and extract random barcode')
    (319, '## ------------------------------------------------------------------------------------ ##')
    (320, '# the iCount files were sequenced with Illumina using the L3 linker')
    (321, '# adapter recognized by cuatdap: AGATCGGAAGAGC')
    (322, '')
    (323, 'def get_barcode(wildcards):')
    (324, '    return samples.barcode[samples.names == wildcards.sample].values.tolist()')
    (325, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=csoneson/ARMOR, file=Snakefile
context_key: ['if len(config) == 0', 'if not os.path.isfile(config["metatxt"])', "if not set([\\'names\\',\\'type\\']).issubset(samples.columns)", 'if config["useCondaR"] == True', 'else', 'rule all', 'rule setup', 'rule pkginstall', 'rule runfastqc', 'rule runtrimming', 'rule runsalmonquant', 'rule runstar', 'rule listpackages', 'rule softwareversions', 'rule salmonindex']
    (2, 'if len(config) == 0:')
    (3, '\\tif os.path.isfile("./config.yaml"):')
    (4, '\\t\\tconfigfile: "./config.yaml"')
    (5, '\\telse:')
    (6, '\\t\\tsys.exit("".join(["Make sure there is a config.yaml file in ", os.getcwd(), ')
    (7, '\\t\\t\\t" or specify one with the --configfile commandline parameter."]))')
    (8, '')
    (9, '## Make sure that all expected variables from the config file are in the config dictionary')
    (10, "configvars = [\\'annotation\\', \\'organism\\', \\'build\\', \\'release\\', \\'txome\\', \\'genome\\', \\'gtf\\', \\'salmonindex\\', \\'salmonk\\', \\'STARindex\\', \\'readlength\\', \\'fldMean\\', \\'fldSD\\', \\'metatxt\\', \\'design\\', \\'contrast\\', \\'genesets\\', \\'ncores\\', \\'FASTQ\\', \\'fqext1\\', \\'fqext2\\', \\'fqsuffix\\', \\'output\\', \\'useCondaR\\', \\'Rbin\\', \\'run_trimming\\', \\'run_STAR\\', \\'run_DRIMSeq\\', \\'run_camera\\']")
    (11, 'for k in configvars:')
    (12, '\\tif k not in config:')
    (13, '\\t\\tconfig[k] = None')
    (14, '')
    (15, '## If any of the file paths is missing, replace it with ""')
    (16, 'def sanitizefile(str):')
    (17, '\\tif str is None:')
    (18, "\\t\\tstr = \\'\\'")
    (19, '\\treturn str')
    (20, '')
    (21, "config[\\'txome\\'] = sanitizefile(config[\\'txome\\'])")
    (22, "config[\\'gtf\\'] = sanitizefile(config[\\'gtf\\'])")
    (23, "config[\\'genome\\'] = sanitizefile(config[\\'genome\\'])")
    (24, "config[\\'STARindex\\'] = sanitizefile(config[\\'STARindex\\'])")
    (25, "config[\\'salmonindex\\'] = sanitizefile(config[\\'salmonindex\\'])")
    (26, "config[\\'metatxt\\'] = sanitizefile(config[\\'metatxt\\'])")
    (27, '')
    (28, '## Read metadata')
    (29, 'if not os.path.isfile(config["metatxt"]):')
    (30, '\\tsys.exit("".join(["Metadata file ", config["metatxt"], " does not exist."]))')
    (31, '')
    (32, 'import pandas as pd')
    (33, 'samples = pd.read_csv(config["metatxt"], sep=\\\'\\\\t\\\')')
    (34, '')
    (35, "if not set([\\'names\\',\\'type\\']).issubset(samples.columns):")
    (36, '\\tsys.exit("".join(["Make sure \\\'names\\\' and \\\'type\\\' are columns in ", config["metatxt"]]))')
    (37, '')
    (38, '')
    (39, '## Sanitize provided input and output directories')
    (40, 'import re')
    (41, 'def getpath(str):')
    (42, "\\tif str in [\\'\\', \\'.\\', \\'./\\']:")
    (43, "\\t\\treturn \\'\\'")
    (44, "\\tif str.startswith(\\'./\\'):")
    (45, "\\t\\tregex = re.compile(\\'^\\\\./?\\')")
    (46, "\\t\\tstr = regex.sub(\\'\\', str)")
    (47, "\\tif not str.endswith(\\'/\\'):")
    (48, "\\t\\tstr += \\'/\\'")
    (49, '\\treturn str')
    (50, '')
    (51, 'outputdir = getpath(config["output"])')
    (52, 'FASTQdir = getpath(config["FASTQ"])')
    (53, '')
    (54, '## Define the conda environment for all rules using R')
    (55, 'if config["useCondaR"] == True:')
    (56, '\\tRenv = "envs/environment_R.yaml"')
    (57, 'else:')
    (58, '\\tRenv = "envs/environment.yaml"')
    (59, '')
    (60, '## Define the R binary')
    (61, 'Rbin = config["Rbin"]')
    (62, '')
    (63, '## ------------------------------------------------------------------------------------ ##')
    (64, '## Target definitions')
    (65, '## ------------------------------------------------------------------------------------ ##')
    (66, '## Run all analyses')
    (67, 'rule all:')
    (68, '\\tinput:')
    (69, '\\t\\tos.path.join(outputdir, "MultiQC", "multiqc_report.html"),')
    (70, '\\t\\tos.path.join(outputdir, "outputR", "shiny_sce.rds")')
    (71, '')
    (72, 'rule setup:')
    (73, '\\tinput:')
    (74, '\\t\\tos.path.join(outputdir, "Rout", "pkginstall_state.txt"),')
    (75, '\\t\\tos.path.join(outputdir, "Rout", "softwareversions.done")')
    (76, '')
    (77, '## Install R packages')
    (78, 'rule pkginstall:')
    (79, '\\tinput:')
    (80, '\\t\\tscript = "scripts/install_pkgs.R"')
    (81, '\\toutput:')
    (82, '\\t  \\tos.path.join(outputdir, "Rout", "pkginstall_state.txt")')
    (83, '\\tparams:')
    (84, '\\t\\tflag = config["annotation"],')
    (85, '\\t\\tncores = config["ncores"],')
    (86, '\\t\\torganism = config["organism"],')
    (87, '\\t\\tRbin = Rbin')
    (88, '\\tpriority:')
    (89, '\\t\\t50')
    (90, '\\tconda:')
    (91, '\\t\\tRenv')
    (92, '\\tlog:')
    (93, '\\t\\tos.path.join(outputdir, "Rout", "install_pkgs.Rout")')
    (94, '\\tbenchmark:')
    (95, '\\t  \\tos.path.join(outputdir, "benchmarks", "install_pkgs.txt")')
    (96, '\\tshell:')
    (97, '\\t\\t\\\'\\\'\\\'{params.Rbin} CMD BATCH --no-restore --no-save "--args outtxt=\\\'{output}\\\' ncores=\\\'{params.ncores}\\\' annotation=\\\'{params.flag}\\\' organism=\\\'{params.organism}\\\'" {input.script} {log}\\\'\\\'\\\'')
    (98, '')
    (99, '## FastQC on original (untrimmed) files')
    (100, 'rule runfastqc:')
    (101, '\\tinput:')
    (102, '\\t\\texpand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext1"]), "_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (103, '\\t\\texpand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext2"]), "_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (104, '\\t\\texpand(os.path.join(outputdir, "FastQC", "{sample}_fastqc.zip"), sample = samples.names[samples.type == \\\'SE\\\'].values.tolist())')
    (105, '')
    (106, '## Trimming and FastQC on trimmed files')
    (107, 'rule runtrimming:')
    (108, '\\tinput:')
    (109, '\\t\\texpand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext1"]), "_val_1_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (110, '\\t\\texpand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext2"]), "_val_2_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (111, '\\t\\texpand(os.path.join(outputdir, "FastQC", "{sample}_trimmed_fastqc.zip"), sample = samples.names[samples.type == \\\'SE\\\'].values.tolist())')
    (112, '')
    (113, '## Salmon quantification')
    (114, 'rule runsalmonquant:')
    (115, '\\tinput:')
    (116, '\\t\\texpand(os.path.join(outputdir, "salmon", "{sample}", "quant.sf"), sample = samples.names.values.tolist())')
    (117, '')
    (118, '## STAR alignment')
    (119, 'rule runstar:')
    (120, '\\tinput:')
    (121, '\\t\\texpand(os.path.join(outputdir, "STAR", "{sample}", "{sample}_Aligned.sortedByCoord.out.bam.bai"), sample = samples.names.values.tolist()),')
    (122, '\\t\\texpand(os.path.join(outputdir, "STARbigwig", "{sample}_Aligned.sortedByCoord.out.bw"), sample = samples.names.values.tolist())')
    (123, '')
    (124, '## List all the packages that were used by the R analyses')
    (125, 'rule listpackages:')
    (126, '\\tlog:')
    (127, '\\t\\tos.path.join(outputdir, "Rout", "list_packages.Rout")')
    (128, '\\tparams:')
    (129, '\\t\\tRoutdir = os.path.join(outputdir, "Rout"),')
    (130, '\\t\\touttxt = os.path.join(outputdir, "R_package_versions.txt"),')
    (131, '\\t\\tscript = "scripts/list_packages.R",')
    (132, '\\t\\tRbin = Rbin')
    (133, '\\tconda:')
    (134, '\\t\\tRenv')
    (135, '\\tshell:')
    (136, '\\t\\t\\\'\\\'\\\'{params.Rbin} CMD BATCH --no-restore --no-save "--args Routdir=\\\'{params.Routdir}\\\' outtxt=\\\'{params.outtxt}\\\'" {params.script} {log}\\\'\\\'\\\'')
    (137, '')
    (138, '## Print the versions of all software packages')
    (139, 'rule softwareversions:')
    (140, '\\toutput:')
    (141, '\\t\\ttouch(os.path.join(outputdir, "Rout", "softwareversions.done"))')
    (142, '\\tlog:')
    (143, '\\t\\tos.path.join(outputdir, "logs", "softversions.log")')
    (144, '\\tconda:')
    (145, '\\t\\t"envs/environment.yaml"')
    (146, '\\tshell:')
    (147, '\\t\\t"echo -n \\\'ARMOR version \\\' && cat version; "')
    (148, '\\t\\t"salmon --version; trim_galore --version; "')
    (149, '\\t\\t"echo -n \\\'cutadapt \\\' && cutadapt --version; "')
    (150, '\\t\\t"fastqc --version; STAR --version; samtools --version; multiqc --version; "')
    (151, '\\t\\t"bedtools --version"')
    (152, '')
    (153, '## ------------------------------------------------------------------------------------ ##')
    (154, '## Reference preparation')
    (155, '## ------------------------------------------------------------------------------------ ##')
    (156, '## Generate Salmon index from merged cDNA and ncRNA files')
    (157, 'rule salmonindex:')
    (158, '\\tinput:')
    (159, '\\t\\ttxome = config["txome"]')
    (160, '\\toutput:')
    (161, '\\t\\tos.path.join(config["salmonindex"], "versionInfo.json")')
    (162, '\\tlog:')
    (163, '\\t\\tos.path.join(outputdir, "logs", "salmon_index.log")')
    (164, '\\tbenchmark:')
    (165, '\\t\\tos.path.join(outputdir, "benchmarks", "salmon_index.txt")')
    (166, '\\tparams:')
    (167, '\\t\\tsalmonoutdir = lambda wildcards, output: os.path.dirname(output[0]),   ## dirname of first output')
    (168, '\\t\\tanno = config["annotation"],')
    (169, '\\t\\tsalmonextraparams = config["additional_salmon_index"]')
    (170, '\\tconda:')
    (171, '\\t\\t"envs/environment.yaml"')
    (172, '\\tshell:')
    (173, '\\t  """')
    (174, '\\t  if [ {params.anno} == "Gencode" ]; then')
    (175, "      echo \\'Salmon version:\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Teichlab/sc_snakemake, file=Snakefile
context_key: ["if FROM == \\'cram\\'"]
    (21, "if FROM == \\'cram\\':")
    (22, "    glob_pattern = os.path.join(CRAM_FOLDER, config[\\'pattern\\'] + \\'.cram\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Teichlab/sc_snakemake, file=Snakefile
context_key: ["elif FROM == \\'fastq\\'"]
    (23, "elif FROM == \\'fastq\\':")
    (24, "    glob_pattern = os.path.join(FASTQ_FOLDER, config[\\'pattern\\'] + \\'.fastq\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefile
context_key: ["if \\'imported\\' not in config"]
    (5, "if \\'imported\\' not in config:")
    (6, '    configfile: "config.yaml"')
    (7, '')
    (8, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefile
context_key: ['if not no_variant_calling']
    (66, 'if not no_variant_calling:')
    (67, "    SAMP1 = {samp: GLOBAL_SAMP[samp][0] for samp in config[\\'SAMP_NAMES\\']}")
    (68, '    include: "Snakefiles/Snakefile-variant_calling"')
    (69, "    config[\\'vcf_file\\'] = rules.filter_hets.output.vcf")
    (70, '')
    (71, '# WASP pipeline')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefile
context_key: ['if no_variant_calling']
    (72, 'if no_variant_calling:')
    (73, '    SAMP2 = {')
    (74, '        samp: GLOBAL_SAMP[samp][1] if len(GLOBAL_SAMP[samp]) == 2 else GLOBAL_SAMP[samp][0]')
    (75, "        for samp in config[\\'SAMP_NAMES\\']")
    (76, '    }')
    (77, '    def read_vcf_samples():')
    (78, '        f = open(config[\\\'sample_file\\\'], "r")')
    (79, '        samp_dict = {}')
    (80, '        for line in f:')
    (81, '            words = line.strip().split("\\\\t")')
    (82, "            if words[1] in config[\\'SAMP_NAMES\\']:")
    (83, '                samp_dict[words[1]] = words[0]')
    (84, '        return samp_dict')
    (85, '    SAMP_TO_VCF_ID = read_vcf_samples()')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/snp2h5_rules.smk
context_key: ['if (use_default_gq)']
    (12, 'if (use_default_gq) {')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/snp2h5_rules.smk
context_key: ['if (use_default_gq)']
    (33, 'if (use_default_gq) {')
    (34, '  # calculate the genotype error')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/snp2h5_rules.smk
context_key: ['if (use_default_gq)']
    (87, 'if (use_default_gq) {')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/Snakefile-WASP
context_key: ['if (use_default_gq)']
    (12, 'if (use_default_gq) {')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/Snakefile-WASP
context_key: ['if (use_default_gq)']
    (33, 'if (use_default_gq) {')
    (34, '  # calculate the genotype error')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/Snakefile-WASP
context_key: ['if (use_default_gq)']
    (87, 'if (use_default_gq) {')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/README.variant_calling.md
context_key: ['if (use_default_gq)']
    (12, 'if (use_default_gq) {')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/README.variant_calling.md
context_key: ['if (use_default_gq)']
    (33, 'if (use_default_gq) {')
    (34, '  # calculate the genotype error')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/README.variant_calling.md
context_key: ['if (use_default_gq)']
    (87, 'if (use_default_gq) {')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/Snakefile-variant_calling
context_key: ['if (use_default_gq)']
    (12, 'if (use_default_gq) {')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/Snakefile-variant_calling
context_key: ['if (use_default_gq)']
    (33, 'if (use_default_gq) {')
    (34, '  # calculate the genotype error')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/Snakefile-variant_calling
context_key: ['if (use_default_gq)']
    (87, 'if (use_default_gq) {')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/snp2h5_rules.smk
context_key: ['if (use_default_gq)']
    (12, 'if (use_default_gq) {')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/snp2h5_rules.smk
context_key: ['if (use_default_gq)']
    (33, 'if (use_default_gq) {')
    (34, '  # calculate the genotype error')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/snp2h5_rules.smk
context_key: ['if (use_default_gq)']
    (87, 'if (use_default_gq) {')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aryarm/as_analysis, file=Snakefiles/Snakefile-counts
context_key: ["if \\'SAMP3\\' not in globals()", "if config[\\'rna_only\\'] and len(words) == 4"]
    (10, "if \\'SAMP3\\' not in globals():")
    (11, '    def read_samples():')
    (12, '        """Function to get names and fastq paths from a sample file specified')
    (13, '        in the configuration. Input file is expected to have 4 columns:')
    (14, '        <vcf_sample_id> <unique_sample_id> <dna_bam_path> <rna_bam_path>. If')
    (15, '        <dna_bam_path> is left out, the pipeline will default to an rna-only')
    (16, "        analysis, which doesn\\'t require dna reads but is more conservative.")
    (17, '        Modify this function as needed to provide a dictionary of sample_id')
    (18, '        keys and (fastq1, fastq1) values."""')
    (19, '        f = open(config[\\\'sample_file\\\'], "r")')
    (20, '        samp_dict = {}')
    (21, '        for line in f:')
    (22, '            words = line.strip().split("\\\\t")')
    (23, "            if config[\\'rna_only\\'] and len(words) == 4:")
    (24, '                samp_dict[words[1]] = (words[3],)')
    (25, '            elif len(words) == 3:')
    (26, '                samp_dict[words[1]] = (words[2],)')
    (27, '                # sanity check to make sure that rna_only is set to true')
    (28, "                config[\\'rna_only\\'] = True")
    (29, '            else:')
    (30, '                samp_dict[words[1]] = (words[2], words[3])')
    (31, '        return samp_dict')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/population-variant-calling, file=Snakefile
context_key: ['if "OUTDIR" in config']
    (10, 'if "OUTDIR" in config:')
    (11, '    workdir: config["OUTDIR"]')
    (12, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=collaborativebioinformatics/RPG_Pikachu, file=Snakefile
context_key: ['if not os.path.exists("filtered_vcf")']
    (13, 'if not os.path.exists("filtered_vcf"):')
    (14, '    os.makedirs("filtered_vcf")')
    (15, '')
    (16, '##include rules (can be commented out)')
    (17, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=rimjhimroy/snakeGATK4, file=Snakefile
context_key: ['if SCAFFS=="ALL"', 'if line.startswith(">")']
    (13, 'if SCAFFS=="ALL":')
    (14, "    with open(REFERENCE,\\'rt\\') as fh:")
    (15, '        for line in fh:')
    (16, '            line = line.strip()')
    (17, '            if line.startswith(">"):')
    (18, '                line = line.split(" ")[0]')
    (19, '                scaffolds.append(line[1:])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=GeneMANIA/pipeline, file=snakefiles/merge.snakefile
context_key: ["if \\'orgs\\' in config"]
    (24, "if \\'orgs\\' in config:")
    (25, "    orgs = config[\\'orgs\\']")
    (26, "    MERGE_ROOT_FOLDERS = orgs.split(\\',\\')")
    (27, '')
    (28, '# or scan for peers')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=GeneMANIA/pipeline, file=snakefiles/merge.snakefile
context_key: ['if MERGE_ROOT_FOLDERS == []']
    (29, 'if MERGE_ROOT_FOLDERS == []:')
    (30, "    PEERS = glob_wildcards(\\'../{folder}/result/generic_db/SCHEMA.txt\\')")
    (31, "    MERGE_ROOT_FOLDERS = expand(\\'../{folder}\\', folder=PEERS.folder)")
    (32, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=calliope-project/sector-coupled-euro-calliope, file=Snakefile
context_key: ['onsuccess', 'if "email" in config.keys()']
    (21, '    if "email" in config.keys():')
    (22, '        shell("echo "" | mail -s \\\'sector-coupled euro-calliope succeeded\\\' {config[email]}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=calliope-project/sector-coupled-euro-calliope, file=Snakefile
context_key: ['onerror', 'if "email" in config.keys()']
    (24, '    if "email" in config.keys():')
    (25, '        shell("echo "" | mail -s \\\'sector-coupled euro-calliope crashed\\\' {config[email]}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/rules/ffpe.smk
context_key: ['if [ "{wildcards.vc_outdir}" == "{config[output_params][MERGED_SOMATIC_OUTDIR]}" ]; the']
    (156, '    if [ "{wildcards.vc_outdir}" == "{config[output_params][MERGED_SOMATIC_OUTDIR]}" ]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ["if \\'CNV_CALLING\\' in config[\\'input_params\\']", "if config[\\'input_params\\'][\\'CNV_CALLING\\'].lower() in [\\'true\\',\\'t\\',\\'yes\\']"]
    (382, "if \\'CNV_CALLING\\' in config[\\'input_params\\']:")
    (383, "    if config[\\'input_params\\'][\\'CNV_CALLING\\'].lower() in [\\'true\\',\\'t\\',\\'yes\\']:")
    (384, '        include: "rules/cnv.smk"')
    (385, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ["if \\'FFPE_FILTER\\' in config[\\'input_params\\']", "if config[\\'input_params\\'][\\'FFPE_FILTER\\'].lower() in [\\'true\\',\\'t\\',\\'yes\\']"]
    (386, "if \\'FFPE_FILTER\\' in config[\\'input_params\\']:")
    (387, "    if config[\\'input_params\\'][\\'FFPE_FILTER\\'].lower() in [\\'true\\',\\'t\\',\\'yes\\']:")
    (388, '        include: "rules/ffpe.smk"')
    (389, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['if tn_mode=="paired"']
    (390, 'if tn_mode=="paired":')
    (391, '    include: "rules/somatic_snps.paired.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mtandon09/CCBR_GATK4_Exome_Seq_Pipeline, file=workflow/Snakefile
context_key: ['elif tn_mode=="tumor_only"']
    (392, 'elif tn_mode=="tumor_only":')
    (393, '    include: "rules/somatic_snps.tumor_only.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/pipeline-nanopore-denovo-isoforms, file=Snakefile
context_key: ['if not workflow.overwrite_configfiles']
    (11, 'if not workflow.overwrite_configfiles:')
    (12, '    configfile: "config.yml"')
    (13, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nanoporetech/pipeline-nanopore-denovo-isoforms, file=Snakefile
context_key: ['if not path.isabs(in_fastq)']
    (21, 'if not path.isabs(in_fastq):')
    (22, '\\tin_fastq = path.join(SNAKEDIR, in_fastq)')
    (23, '')
    (24, '')
    (25, 'class Node:')
    (26, '    def __init__(self, Id, File, Left, Right, Parent, Level):')
    (27, '        self.Id = Id')
    (28, '        self.File = File')
    (29, '        self.Left = Left')
    (30, '        self.Right = Right')
    (31, '        self.Parent = Parent')
    (32, '        self.Level = Level')
    (33, '        self.Done = False')
    (34, '        self.RightSide = False')
    (35, '    def __repr__(self):')
    (36, '        return "Node:{} Level: {} File: {} Done: {} Left: {} Right: {} Parent: {}".format(self.Id, self.Level, self.File, self.Done, self.Left.Id if self.Left is not None else None, self.Right.Id if self.Right is not None else None, self.Parent.Id if self.Parent is not None else None)')
    (37, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/prealignment, file=workflow/rules/common.smk
context_key: ['if not workflow.overwrite_configfiles']
    (19, 'if not workflow.overwrite_configfiles:')
    (20, '    sys.exit("At least one config file must be passed using --configfile/--configfiles, by command line or a profile!")')
    (21, '')
    (22, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Mira0507/snakemake_hisat2, file=Snakefile
context_key: ['input', 'run', 'if len(input.fastq) == 2']
    (65, '        if len(input.fastq) == 2:    # if paired-end')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imc, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'if len(fol_path_spillover_slide_acs) > 0']
    (117, 'if do_compensation:')
    (118, '    if len(file_path_orig_sm) == 0:')
    (119, '        if len(fol_path_spillover_slide_acs) > 0:')
    (120, "            spillover_mode = \\'estimate_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imc, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'else']
    (121, '        else:')
    (122, '            raise ValueError(\\\'Either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (123, "                             \\'needs to be provided.\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imc, file=workflow/Snakefile
context_key: ['if do_compensation', 'elif len(fol_path_spillover_slide_acs) == 0']
    (124, '    elif len(fol_path_spillover_slide_acs) == 0:')
    (125, "            spillover_mode = \\'copy_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imc, file=workflow/Snakefile
context_key: ['if do_compensation', 'else']
    (126, '    else:')
    (127, '        raise ValueError(\\\'Only provide either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (128, "                         \\'but not both.\\')")
    (129, '')
    (130, '# Produce a list of all cellprofiler output files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imc, file=workflow/Snakefile
context_key: ['if not do_compensation']
    (267, 'if not do_compensation:')
    (268, "    config_dict_cp[\\'measuremasks\\'] = {")
    (269, '        \\\'message\\\':  """\\\\')
    (270, '                    Measure masks ')
    (271, '                    ')
    (272, '                    Measure segmentation masks')
    (273, '                    """,')
    (274, "        \\'run_size\\': 1,")
    (275, "        \\'plugins\\': cp_plugins,")
    (276, "        \\'pipeline\\': \\'resources/cp4_pipelines/adapted/3_measure_mask_basic.cppipe\\',")
    (277, "        \\'input_files\\': [")
    (278, '            # Folder containing segmentation masks')
    (279, '            fol_path_cellmasks,')
    (280, '            # Folder containing image stacks to measure')
    (281, '            fol_path_full,')
    (282, '            # Folder containing cell probabilities')
    (283, '            fol_path_probabilities,')
    (284, '            # Acquisition metadata')
    (285, '            file_path_acmeta,')
    (286, '            # Channel metadata')
    (287, '            file_path_channelmeta_full,')
    (288, '            file_path_channelmeta_cellprobab')
    (289, '        ],')
    (290, "        \\'output_patterns\\': {\\'.\\':")
    (291, '                            # Folder containing cellprofiler measurement')
    (292, '                                cp_measurements_output_files,')
    (293, '                            # Subfolder containing segmentation masks used')
    (294, '                            # for the measurements')
    (295, "                            \\'masks\\': directory(fol_path_cp_masks),")
    (296, '                            # Subfolder containing the images used for the')
    (297, '                            # measurements.')
    (298, "                            \\'images\\': directory(fol_path_cp_images)},")
    (299, "        \\'input_folder\\': fol_path_cp_input")
    (300, '    }')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imc, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'params']
    (454, 'if do_compensation:')
    (455, '    if spillover_mode == "estimate_sm":')
    (456, '        rule get_ss_panel_from_panel:')
    (457, '            input: csv_panel')
    (458, '            output: file_path_ss_panel')
    (459, '            params:')
    (460, '                col_metal=csv_panel_metal,')
    (461, '                col_spillover=csv_panel_spillover')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imc, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'run']
    (462, '            run:')
    (463, '                dat_csv = pd.read_csv(csv_panel)')
    (464, '                assert params.col_spillover in dat_csv.columns, ValueError(')
    (465, '                    f"The panel csv {csv_panel} should contain a boolean column"')
    (466, '                    f"{params.col_spillover} indicating the metals used for the"')
    (467, '                    f"spillover acquisitions.")')
    (468, '                metals = dat_csv.query(')
    (469, "                    f\\'{params.col_spillover} == 1\\')[params.col_metal].values")
    (470, "                with open(output[0], \\'w\\') as f:")
    (471, '                    for m in metals:')
    (472, "                        f.write(m+\\'\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/VariantCalling-snakemake, file=rules/joint_call.smk
context_key: ['if pipeline_mode == "JOINT_CALL"', 'include']
    (48, 'if pipeline_mode == "JOINT_CALL" :')
    (49, '    include:')
    (50, '        # include_prefix + "/gatk_genomicsDBI.smk"')
    (51, '        "gatk_genomicsDBI.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/VariantCalling-snakemake, file=Snakefile
context_key: ['if pipeline_mode == "BATCH"', 'rule all', 'input']
    (54, 'if pipeline_mode == "BATCH":')
    (55, '    print(pipeline_mode)')
    (56, '    ##### local rules #####')
    (57, '    localrules: all')
    (58, '')
    (59, '    ##### Target rules #####')
    (60, '    rule all:')
    (61, '        input:')
    (62, '            call_variants_by_sex(BASE_OUT + "/" + config["rules"]["gatk_hap_caller"]["out_dir"]),')
    (63, '            expand(BASE_OUT + "/"+ config["rules"]["stats"]["out_dir"] + "/{sample}_validate.txt", sample=sample_names),')
    (64, '            expand(BASE_OUT + "/"+ config["rules"]["stats"]["out_dir"] + "/{sample}_flagstat.txt", sample=sample_names),')
    (65, '            expand(BASE_OUT +"/" +config["rules"]["stats"]["out_dir"] + "/{sample}_ismetrics.txt", sample=sample_names),')
    (66, '            expand(BASE_OUT +"/" +config["rules"]["stats"]["out_dir"] + "/{sample}_stats.txt", sample=sample_names),')
    (67, '            expand(BASE_OUT +"/" +config["rules"]["stats"]["out_dir"] + "/{sample}_wgsmetrics.txt", sample=sample_names),')
    (68, '            call_variants_by_sex(config["files_path"]["base_joint_call_path"])')
    (69, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=macsx82/VariantCalling-snakemake, file=Snakefile
context_key: ['if pipeline_mode == "BATCH"', 'include']
    (96, 'if pipeline_mode == "BATCH" :')
    (97, '    include:')
    (98, '        include_prefix + "/alignment.smk"')
    (99, '    include:')
    (100, '        include_prefix + "/bqsr.smk"')
    (101, '    include:')
    (102, '        include_prefix + "/var_call.smk"')
    (103, '    include:')
    (104, '        include_prefix + "/stats.smk"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/rules/phylogeny.smk
context_key: ['if  [[ `grep -A 1 "outliers" {output.log}` ]]; the']
    (152, '        if  [[ `grep -A 1 "outliers" {output.log}` ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/rules/download.smk
context_key: ['if wildcards.sample in ftp][0']
    (42, '          if wildcards.sample in ftp][0]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/rules/download.smk
context_key: ['resources', 'if [[ {wildcards.reads_origin} == "reference" ]]; the']
    (48, '      if [[ {wildcards.reads_origin} == "reference" ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/rules/download.smk
context_key: ['resources', 'if [[ {wildcards.ext} == "fna" || {wildcards.ext} == "gff" ]]; the']
    (49, '        if [[ {wildcards.ext} == "fna" || {wildcards.ext} == "gff" ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/Snakefile
context_key: ['if not config']
    (24, 'if not config:')
    (25, '    #print("ERROR: Please specify --configfile")')
    (26, '    #quit(1)')
    (27, '    print("WARNING: Using default configfile results/config/snakemake.yaml")')
    (28, '    configfile: os.path.join("results", "config", "snakemake.yaml")')
    (29, '')
    (30, '# Pipeline directories')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/Snakefile
context_key: ['if workflow.singularity_prefix']
    (68, 'if workflow.singularity_prefix:')
    (69, '    os.environ["SINGULARITY_CACHEDIR"] =  workflow.singularity_prefix')
    (70, '    os.environ["NXF_SINGULARITY_CACHEDIR"] =  workflow.singularity_prefix')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/Snakefile
context_key: ['if workflow.conda_prefix']
    (75, 'if workflow.conda_prefix:')
    (76, '    os.environ["CONDA_CACHEDIR"] =  workflow.conda_prefix')
    (77, '    os.environ["NXF_CONDA_CACHEDIR"] = workflow.conda_prefix')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/Snakefile
context_key: ['if "LANGUAGE" not in os.environ']
    (82, 'if "LANGUAGE" not in os.environ:')
    (83, '    os.environ["LANGUAGE"] = "en_US.UTF-8"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/Snakefile
context_key: ['if "LANG" not in os.environ']
    (84, 'if "LANG" not in os.environ:')
    (85, '    os.environ["LANG"] = "en_US.UTF-8"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ktmeaton/plague-phylogeography, file=workflow/Snakefile
context_key: ['if "LC_ALL" not in os.environ']
    (86, 'if "LC_ALL" not in os.environ:')
    (87, '    os.environ["LC_ALL"] = "en_US.UTF-8"')
    (88, '')
    (89, '# -----------------------------------------------------------------------------#')
    (90, '# Local rules                                                                  #')
    (91, '# -----------------------------------------------------------------------------#')
    (92, '# Rule that need internet access will be localrules')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=csoneson/WagnerEMT2020, file=Snakefile
context_key: ['if len(config) == 0', 'if os.path.isfile("./config.yaml")']
    (2, 'if len(config) == 0:')
    (3, '  if os.path.isfile("./config.yaml"):')
    (4, '    configfile: "./config.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=csoneson/WagnerEMT2020, file=Snakefile
context_key: ['if len(config) == 0', 'else']
    (5, '  else:')
    (6, '    sys.exit("Make sure there is a config.yaml file in " + os.getcwd() + " or specify one with the --configfile commandline parameter.")')
    (7, '')
    (8, '## Make sure that all expected variables from the config file are in the config dictionary')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mitoNGS/MToolBox_snakemake, file=snakefiles/variant_calling.snakefile
context_key: ['if "snakefiles" in path']
    (17, '    if "snakefiles" in path:')
    (18, '        sys.path.append(path.replace("/snakefiles", ""))')
    (19, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mitoNGS/MToolBox_snakemake, file=snakefiles/variant_calling.snakefile
context_key: ['if species is None']
    (53, 'if species is None:')
    (54, '    # should be parsed with the reference_genome.tab')
    (55, '    pass')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mitoNGS/MToolBox_snakemake, file=snakefiles/variant_calling.snakefile
context_key: ['else', 'if species in list(reference_tab["species"])']
    (57, '    if species in list(reference_tab["species"]):')
    (58, '        analysis_tab = analysis_tab.assign(species=species)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khembach/TDP-CLIPseq, file=Snakefile
context_key: ['if len(config) == 0', 'if not os.path.isfile(config["metatxt"])', "if not set([\\'names\\',\\'type\\']).issubset(samples.columns)", 'if config["useCondaR"] == True', 'else', 'rule all', 'rule setup', 'rule pkginstall', 'rule runfastqc', 'rule runtrimming', 'rule run_clip5', 'rule runstar', 'rule runstar_clip5', 'rule runstar_clip5_separate', 'rule runstar_clip5_R2', 'rule run_removedup', 'rule run_removedup_clip5_R2', 'rule run_filter_second_read', 'rule run_clipper', 'rule run_clipper_clip5_R2', 'rule run_norm_bigwig', 'rule run_bigwig_second_read', 'rule runsalmonquant', 'rule listpackages', 'rule softwareversions', 'rule starindex', 'rule fastqc', 'rule fastqctrimmed', 'rule multiqc', 'rule trimgaloreSE', 'rule trimgalorePE', 'rule trimgalorePE_2nd_round', 'rule cutadapt_clip5', 'rule sort_fastq_clip5', 'rule starPE', 'rule starPE_2nd_round', 'rule starPE_clip5', 'rule star_clip5_separate', 'rule bamindex', 'rule bamindex_clip5', 'rule bamindex_clip5_R2', 'rule bigwig']
    (2, 'if len(config) == 0:')
    (3, '\\tif os.path.isfile("./config.yaml"):')
    (4, '\\t\\tconfigfile: "./config.yaml"')
    (5, '\\telse:')
    (6, '\\t\\tsys.exit("".join(["Make sure there is a config.yaml file in ", os.getcwd(), ')
    (7, '\\t\\t\\t" or specify one with the --configfile commandline parameter."]))')
    (8, '')
    (9, '## Make sure that all expected variables from the config file are in the config dictionary')
    (10, "configvars = [\\'annotation\\', \\'organism\\', \\'build\\', \\'release\\', \\'txome\\', \\'genome\\', \\'gtf\\', \\'STARindex\\', \\'readlength\\', \\'fldMean\\', \\'fldSD\\', \\'metatxt\\', \\'ncores\\', \\'FASTQ\\', \\'fqext1\\', \\'fqext2\\', \\'fqsuffix\\', \\'output\\', \\'useCondaR\\', \\'Rbin\\', \\'run_trimming\\', \\'run_STAR\\']")
    (11, 'for k in configvars:')
    (12, '\\tif k not in config:')
    (13, '\\t\\tconfig[k] = None')
    (14, '')
    (15, '## If any of the file paths is missing, replace it with ""')
    (16, 'def sanitizefile(str):')
    (17, '\\tif str is None:')
    (18, "\\t\\tstr = \\'\\'")
    (19, '\\treturn str')
    (20, '')
    (21, "config[\\'gtf\\'] = sanitizefile(config[\\'gtf\\'])")
    (22, "config[\\'genome\\'] = sanitizefile(config[\\'genome\\'])")
    (23, "config[\\'STARindex\\'] = sanitizefile(config[\\'STARindex\\'])")
    (24, "config[\\'metatxt\\'] = sanitizefile(config[\\'metatxt\\'])")
    (25, "#config[\\'salmonindex\\'] = sanitizefile(config[\\'salmonindex\\'])")
    (26, '')
    (27, '## Read metadata')
    (28, 'if not os.path.isfile(config["metatxt"]):')
    (29, '\\tsys.exit("".join(["Metadata file ", config["metatxt"], " does not exist."]))')
    (30, '')
    (31, 'import pandas as pd')
    (32, 'samples = pd.read_csv(config["metatxt"], sep=\\\'\\\\t\\\')')
    (33, '')
    (34, "if not set([\\'names\\',\\'type\\']).issubset(samples.columns):")
    (35, '\\tsys.exit("".join(["Make sure \\\'names\\\' and \\\'type\\\' are columns in ", config["metatxt"]]))')
    (36, '')
    (37, '')
    (38, '## Sanitize provided input and output directories')
    (39, 'import re')
    (40, 'def getpath(str):')
    (41, "\\tif str in [\\'\\', \\'.\\', \\'./\\']:")
    (42, "\\t\\treturn \\'\\'")
    (43, "\\tif str.startswith(\\'./\\'):")
    (44, "\\t\\tregex = re.compile(\\'^\\\\./?\\')")
    (45, "\\t\\tstr = regex.sub(\\'\\', str)")
    (46, "\\tif not str.endswith(\\'/\\'):")
    (47, "\\t\\tstr += \\'/\\'")
    (48, '\\treturn str')
    (49, '')
    (50, 'outputdir = getpath(config["output"])')
    (51, 'FASTQdir = getpath(config["FASTQ"])')
    (52, '')
    (53, '## Define the conda environment for all rules using R')
    (54, 'if config["useCondaR"] == True:')
    (55, '\\tRenv = "envs/environment_R.yaml"')
    (56, 'else:')
    (57, '\\tRenv = "envs/environment.yaml"')
    (58, '')
    (59, '## Define the R binary')
    (60, 'Rbin = config["Rbin"]')
    (61, '')
    (62, '## ------------------------------------------------------------------------------------ ##')
    (63, '## Target definitions')
    (64, '## ------------------------------------------------------------------------------------ ##')
    (65, '## Run all analyses')
    (66, 'rule all:')
    (67, '\\tinput:')
    (68, '\\t\\tos.path.join(outputdir, "MultiQC", "multiqc_report.html")')
    (69, '')
    (70, 'rule setup:')
    (71, '\\tinput:')
    (72, '\\t\\tos.path.join(outputdir, "Rout", "pkginstall_state.txt"),')
    (73, '\\t\\tos.path.join(outputdir, "Rout", "softwareversions.done")')
    (74, '')
    (75, '## Install R packages')
    (76, 'rule pkginstall:')
    (77, '\\tinput:')
    (78, '\\t\\tscript = "scripts/install_pkgs.R"')
    (79, '\\toutput:')
    (80, '\\t  \\tos.path.join(outputdir, "Rout", "pkginstall_state.txt")')
    (81, '\\tparams:')
    (82, '\\t\\tflag = config["annotation"],')
    (83, '\\t\\tncores = config["ncores"],')
    (84, '\\t\\torganism = config["organism"],')
    (85, '\\t\\tRbin = Rbin')
    (86, '\\tpriority:')
    (87, '\\t\\t50')
    (88, '\\tconda:')
    (89, '\\t\\tRenv')
    (90, '\\tlog:')
    (91, '\\t\\tos.path.join(outputdir, "Rout", "install_pkgs.Rout")')
    (92, '\\tbenchmark:')
    (93, '\\t  \\tos.path.join(outputdir, "benchmarks", "install_pkgs.txt")')
    (94, '\\tshell:')
    (95, '\\t\\t\\\'\\\'\\\'{params.Rbin} CMD BATCH --no-restore --no-save "--args outtxt=\\\'{output}\\\' ncores=\\\'{params.ncores}\\\' annotation=\\\'{params.flag}\\\' organism=\\\'{params.organism}\\\'" {input.script} {log}\\\'\\\'\\\'')
    (96, '')
    (97, '## FastQC on original (untrimmed) files')
    (98, 'rule runfastqc:')
    (99, '\\tinput:')
    (100, '\\t\\texpand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext1"]), "_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (101, '\\t\\texpand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext2"]), "_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (102, '\\t\\texpand(os.path.join(outputdir, "FastQC", "{sample}_fastqc.zip"), sample = samples.names[samples.type == \\\'SE\\\'].values.tolist())')
    (103, '')
    (104, '## Trimming and FastQC on trimmed files')
    (105, 'rule runtrimming:')
    (106, '\\tinput:')
    (107, '\\t\\texpand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext1"]), "_val_1_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (108, '\\t\\texpand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext2"]), "_val_2_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (109, '\\t\\texpand(os.path.join(outputdir, "FastQC", "{sample}_trimmed_fastqc.zip"), sample = samples.names[samples.type == \\\'SE\\\'].values.tolist())')
    (110, '')
    (111, '')
    (112, '## remove first 5 bases from each read')
    (113, 'rule run_clip5:')
    (114, '\\tinput:')
    (115, '\\t\\texpand(os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext1"]), "_val_1.fq_clip5_sorted.gz"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (116, '\\t\\tfastq2 = expand(os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext2"]), "_val_2.fq_clip5_sorted.gz"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist())')
    (117, '')
    (118, '## STAR alignment')
    (119, 'rule runstar:')
    (120, '\\tinput:')
    (121, '\\t\\texpand(os.path.join(outputdir, config["star_output"], "{sample}", "{sample}_Aligned.sortedByCoord.out.bam.bai"), sample = samples.names.values.tolist())')
    (122, '\\t\\t# ,expand(os.path.join(outputdir, "STARbigwig", "{sample}_Aligned.sortedByCoord.out.bw"), sample = samples.names.values.tolist())')
    (123, '')
    (124, 'rule runstar_clip5:')
    (125, '\\tinput:')
    (126, '\\t\\texpand(os.path.join(outputdir, "STAR_clip5", "{sample}", "{sample}_Aligned.sortedByCoord.out.bam.bai"), sample = samples.names.values.tolist())')
    (127, '')
    (128, '')
    (129, 'rule runstar_clip5_separate:')
    (130, '\\tinput:')
    (131, '\\t\\texpand(os.path.join(outputdir, "STAR_clip5", "{sample}", "{sample}_R1_Aligned.sortedByCoord.out.bam"), sample = samples.names.values.tolist()),')
    (132, '\\t\\texpand(os.path.join(outputdir, "STAR_clip5", "{sample}", "{sample}_R2_Aligned.sortedByCoord.out.bam"), sample = samples.names.values.tolist())')
    (133, '')
    (134, '')
    (135, 'rule runstar_clip5_R2:')
    (136, '\\tinput:')
    (137, '\\t\\texpand(os.path.join(outputdir, config["star_output"], "{sample}", "{sample}_R2_Aligned.sortedByCoord.out.bam.bai"), sample = samples.names.values.tolist())')
    (138, '')
    (139, '')
    (140, 'rule run_removedup:')
    (141, '\\tinput:')
    (142, '\\t\\texpand(os.path.join(outputdir, config["dedup_output"], "{sample}_deduplicated.bam.bai"), sample = samples.names.values.tolist())')
    (143, '')
    (144, 'rule run_removedup_clip5_R2:')
    (145, '\\tinput:')
    (146, '\\t\\texpand(os.path.join(outputdir, config["dedup_output"], "{sample}_R2_deduplicated.bam.bai"), sample = samples.names.values.tolist())')
    (147, '')
    (148, '')
    (149, '')
    (150, 'rule run_filter_second_read:')
    (151, '\\tinput:')
    (152, '\\t\\texpand(os.path.join(outputdir, "BAM_deduplicated/{sample}_deduplicated.r2.bam.bai"), sample = samples.names.values.tolist())')
    (153, '')
    (154, 'rule run_clipper:')
    (155, '\\tinput:')
    (156, '\\t\\texpand(os.path.join(outputdir, config["dedup_output"], "{sample}_deduplicated.r2.ucsc.bam.bai"), sample = samples.names.values.tolist()),')
    (157, '\\t\\texpand(os.path.join(outputdir, config["clipper"], "{sample}_deduplicated.r2.ucsc.clipper_peaks.bed"), sample = samples.names.values.tolist())')
    (158, '')
    (159, 'rule run_clipper_clip5_R2:')
    (160, '\\tinput:')
    (161, '\\t\\texpand(os.path.join(outputdir, config["dedup_output"], "{sample}_R2_deduplicated.ucsc.bam.bai"), sample = samples.names.values.tolist()),')
    (162, '\\t\\texpand(os.path.join(outputdir, config["clipper"], "{sample}_R2_deduplicated.ucsc.clipper_peaks.bed"), sample = samples.names.values.tolist())')
    (163, '')
    (164, '')
    (165, '')
    (166, 'rule run_norm_bigwig:')
    (167, '\\tinput:')
    (168, '\\t\\texpand(os.path.join(outputdir, "bigwig", "{sample}_deduplicated.r2.ucsc.replicatesNorm.bw"), sample = samples.names.values.tolist())')
    (169, '')
    (170, 'rule run_bigwig_second_read:')
    (171, '\\tinput:')
    (172, '\\t\\texpand(os.path.join(outputdir, "bigwig", "{sample}_deduplicated.r2.bw"), sample = samples.names.values.tolist())')
    (173, '')
    (174, '## Salmon quantification')
    (175, 'rule runsalmonquant:')
    (176, '\\tinput:')
    (177, '\\t\\texpand(os.path.join(outputdir, "salmon", "{sample}", "quant.sf"), sample = samples.names.values.tolist()),')
    (178, '\\t\\tos.path.join(outputdir, "outputR", "tximeta_se.rds")')
    (179, '')
    (180, '')
    (181, '## List all the packages that were used by the R analyses')
    (182, 'rule listpackages:')
    (183, '\\tlog:')
    (184, '\\t\\tos.path.join(outputdir, "Rout", "list_packages.Rout")')
    (185, '\\tparams:')
    (186, '\\t\\tRoutdir = os.path.join(outputdir, "Rout"),')
    (187, '\\t\\touttxt = os.path.join(outputdir, "R_package_versions.txt"),')
    (188, '\\t\\tscript = "scripts/list_packages.R",')
    (189, '\\t\\tRbin = Rbin')
    (190, '\\tconda:')
    (191, '\\t\\tRenv')
    (192, '\\tshell:')
    (193, '\\t\\t\\\'\\\'\\\'{params.Rbin} CMD BATCH --no-restore --no-save "--args Routdir=\\\'{params.Routdir}\\\' outtxt=\\\'{params.outtxt}\\\'" {params.script} {log}\\\'\\\'\\\'')
    (194, '')
    (195, '## Print the versions of all software packages')
    (196, 'rule softwareversions:')
    (197, '\\toutput:')
    (198, '\\t\\ttouch(os.path.join(outputdir, "Rout", "softwareversions.done"))')
    (199, '\\tlog:')
    (200, '\\t\\tos.path.join(outputdir, "logs", "softversions.log")')
    (201, '\\tconda:')
    (202, '\\t\\t"envs/environment.yaml"')
    (203, '\\tshell:')
    (204, '\\t\\t"echo -n \\\'ARMOR version \\\' && cat version; "')
    (205, '\\t\\t"salmon --version; trim_galore --version; "')
    (206, '\\t\\t"echo -n \\\'cutadapt \\\' && cutadapt --version; "')
    (207, '\\t\\t"fastqc --version; STAR --version; samtools --version; multiqc --version; "')
    (208, '\\t\\t"bedtools --version"')
    (209, '')
    (210, '## ------------------------------------------------------------------------------------ ##')
    (211, '## Reference preparation')
    (212, '## ------------------------------------------------------------------------------------ ##')
    (213, '## Generate STAR index')
    (214, 'rule starindex:')
    (215, '\\tinput:')
    (216, '\\t\\tgenome = config["genome"],')
    (217, '\\t\\tgtf = config["gtf"]')
    (218, '\\toutput:')
    (219, '\\t\\tos.path.join(config["STARindex"], "SA"),')
    (220, '\\t\\tos.path.join(config["STARindex"], "chrNameLength.txt")')
    (221, '\\tlog:')
    (222, '\\t\\tos.path.join(outputdir, "logs", "STAR_index.log")')
    (223, '\\tbenchmark:')
    (224, '\\t\\tos.path.join(outputdir, "benchmarks", "STAR_index.txt")')
    (225, '\\tparams:')
    (226, '\\t\\tSTARindex = lambda wildcards, output: os.path.dirname(output[0]),   ## dirname of first output')
    (227, '\\t\\treadlength = config["readlength"],')
    (228, '\\t\\tstarextraparams = config["additional_star_index"]')
    (229, '\\tconda:')
    (230, '\\t\\t"envs/environment.yaml"')
    (231, '\\tthreads:')
    (232, '\\t\\tconfig["ncores"]')
    (233, '\\tshell:')
    (234, '\\t\\t"echo \\\'STAR version:\\')
    (235, '\\\' > {log}; STAR --version >> {log}; "')
    (236, '\\t\\t"STAR --runMode genomeGenerate --runThreadN {threads} --genomeDir {params.STARindex} "')
    (237, '\\t\\t"--genomeFastaFiles {input.genome} --sjdbGTFfile {input.gtf} --sjdbOverhang {params.readlength} "')
    (238, '\\t\\t"{params.starextraparams}"')
    (239, '')
    (240, '# ## Generate Salmon index from merged cDNA and ncRNA files')
    (241, '# rule salmonindex:')
    (242, '# \\tinput:')
    (243, '# \\t\\ttxome = config["txome"]')
    (244, '# \\toutput:')
    (245, '# \\t\\tos.path.join(config["salmonindex"], "versionInfo.json")')
    (246, '# \\tlog:')
    (247, '# \\t\\tos.path.join(outputdir, "logs", "salmon_index.log")')
    (248, '# \\tbenchmark:')
    (249, '# \\t\\tos.path.join(outputdir, "benchmarks", "salmon_index.txt")')
    (250, '# \\tparams:')
    (251, '# \\t\\tsalmonoutdir = lambda wildcards, output: os.path.dirname(output[0]),   ## dirname of first output')
    (252, '# \\t\\tanno = config["annotation"],')
    (253, '# \\t\\tsalmonextraparams = config["additional_salmon_index"]')
    (254, '# \\tconda:')
    (255, '# \\t\\t"envs/environment.yaml"')
    (256, '# \\tshell:')
    (257, '# \\t  """')
    (258, '# \\t  if [ {params.anno} == "Gencode" ]; then')
    (259, "#       echo \\'Salmon version:\\")
    (260, "\\' > {log}; salmon --version >> {log};")
    (261, '#   \\t  salmon index -t {input.txome} -i {params.salmonoutdir} --gencode {params.salmonextraparams}')
    (262, '#     else')
    (263, "#   \\t  echo \\'Salmon version:\\")
    (264, "\\' > {log}; salmon --version >> {log};")
    (265, '#       salmon index -t {input.txome} -i {params.salmonoutdir} {params.salmonextraparams}')
    (266, '#     fi')
    (267, '#     """')
    (268, '')
    (269, '# ## Generate linkedtxome mapping')
    (270, '# rule linkedtxome:')
    (271, '# \\tinput:')
    (272, '# \\t\\ttxome = config["txome"],')
    (273, '# \\t\\tgtf = config["gtf"],')
    (274, '# \\t\\tsalmonidx = os.path.join(config["salmonindex"], "versionInfo.json"),')
    (275, '# \\t\\tscript = "scripts/generate_linkedtxome.R",')
    (276, '# \\t\\tinstall = os.path.join(outputdir, "Rout", "pkginstall_state.txt")')
    (277, '# \\tlog:')
    (278, '# \\t\\tos.path.join(outputdir, "Rout", "generate_linkedtxome.Rout")')
    (279, '# \\tbenchmark:')
    (280, '# \\t\\tos.path.join(outputdir, "benchmarks", "generate_linkedtxome.txt")')
    (281, '# \\toutput:')
    (282, '# \\t\\t"".join([config["salmonindex"], ".json"])')
    (283, '# \\tparams:')
    (284, '# \\t\\tflag = config["annotation"],')
    (285, '# \\t\\torganism = config["organism"],')
    (286, '# \\t\\trelease = str(config["release"]),')
    (287, '# \\t\\tbuild = config["build"],')
    (288, '# \\t\\tRbin = Rbin')
    (289, '# \\tconda:')
    (290, '# \\t\\tRenv')
    (291, '# \\tshell:')
    (292, '# \\t\\t\\\'\\\'\\\'{params.Rbin} CMD BATCH --no-restore --no-save "--args transcriptfasta=\\\'{input.txome}\\\' salmonidx=\\\'{input.salmonidx}\\\' gtf=\\\'{input.gtf}\\\' annotation=\\\'{params.flag}\\\' organism=\\\'{params.organism}\\\' release=\\\'{params.release}\\\' build=\\\'{params.build}\\\' output=\\\'{output}\\\'" {input.script} {log}\\\'\\\'\\\'')
    (293, '')
    (294, '')
    (295, '## ------------------------------------------------------------------------------------ ##')
    (296, '## Quality control')
    (297, '## ------------------------------------------------------------------------------------ ##')
    (298, '## FastQC, original reads')
    (299, 'rule fastqc:')
    (300, '\\tinput:')
    (301, '\\t\\tfastq = os.path.join(FASTQdir, "".join(["{sample}.", str(config["fqsuffix"]), ".gz"]))')
    (302, '\\toutput:')
    (303, '\\t\\tos.path.join(outputdir, "FastQC", "{sample}_fastqc.zip")')
    (304, '\\tparams:')
    (305, '\\t\\tFastQC = lambda wildcards, output: os.path.dirname(output[0])   ## dirname of first output')
    (306, '\\tlog:')
    (307, '\\t\\tos.path.join(outputdir, "logs", "fastqc_{sample}.log")')
    (308, '\\tbenchmark:')
    (309, '\\t\\tos.path.join(outputdir, "benchmarks", "fastqc_{sample}.txt")')
    (310, '\\tconda:')
    (311, '\\t\\t"envs/environment.yaml"')
    (312, '\\tthreads:')
    (313, '\\t\\tconfig["ncores"]')
    (314, '\\tshell:')
    (315, '\\t\\t"echo \\\'FastQC version:\\')
    (316, '\\\' > {log}; fastqc --version >> {log}; "')
    (317, '\\t\\t"fastqc -o {params.FastQC} -t {threads} {input.fastq}"')
    (318, '')
    (319, '## FastQC, trimmed reads')
    (320, 'rule fastqctrimmed:')
    (321, '\\tinput:')
    (322, '\\t\\tfastq = os.path.join(outputdir, "FASTQtrimmed", "{sample}.fq.gz")')
    (323, '\\toutput:')
    (324, '\\t\\tos.path.join(outputdir, "FastQC", "{sample}_fastqc.zip")')
    (325, '\\tparams:')
    (326, '\\t\\tFastQC = lambda wildcards, output: os.path.dirname(output[0])   ## dirname of first output')
    (327, '\\tlog:')
    (328, '\\t\\tos.path.join(outputdir, "logs", "fastqc_trimmed_{sample}.log")')
    (329, '\\tbenchmark:')
    (330, '\\t\\tos.path.join(outputdir, "benchmarks", "fastqc_trimmed_{sample}.txt")')
    (331, '\\tconda:')
    (332, '\\t\\t"envs/environment.yaml"')
    (333, '\\tthreads:')
    (334, '\\t\\tconfig["ncores"]')
    (335, '\\tshell:')
    (336, '\\t\\t"echo \\\'FastQC version:\\')
    (337, '\\\' > {log}; fastqc --version >> {log}; "')
    (338, '\\t\\t"fastqc -o {params.FastQC} -t {threads} {input.fastq}"')
    (339, '')
    (340, '')
    (341, '')
    (342, '# The config.yaml files determines which steps should be performed')
    (343, 'def multiqc_input(wildcards):')
    (344, '\\tinput = []')
    (345, '\\tinput.extend(expand(os.path.join(outputdir, "FastQC", "{sample}_fastqc.zip"), sample = samples.names[samples.type == \\\'SE\\\'].values.tolist()))')
    (346, '\\tinput.extend(expand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext1"]), "_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (347, '\\tinput.extend(expand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext2"]), "_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (348, '\\tif config["run_trimming"]:')
    (349, '\\t\\tinput.extend(expand(os.path.join(outputdir, "FASTQtrimmed", "{sample}_trimmed.fq.gz"), sample = samples.names[samples.type == \\\'SE\\\'].values.tolist()))')
    (350, '\\t\\tinput.extend(expand(os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext1"]), "_val_1.fq.gz"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (351, '\\t\\tinput.extend(expand(os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext2"]), "_val_2.fq.gz"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (352, '\\t\\t# input.extend(expand(os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext1"]), "_val_1_val_1.fq.gz"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (353, '\\t\\t# input.extend(expand(os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext2"]), "_val_2_val_2.fq.gz"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (354, '\\t\\tinput.extend(expand(os.path.join(outputdir, "FastQC", "{sample}_trimmed_fastqc.zip"), sample = samples.names[samples.type == \\\'SE\\\'].values.tolist()))')
    (355, '\\t\\tinput.extend(expand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext1"]), "_val_1_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (356, '\\t\\tinput.extend(expand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext2"]), "_val_2_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (357, '\\t\\t# input.extend(expand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext1"]), "_val_1_val_1_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (358, '\\t\\t# input.extend(expand(os.path.join(outputdir, "FastQC", "".join(["{sample}_", str(config["fqext2"]), "_val_2_val_2_fastqc.zip"])), sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (359, '\\tif config["run_STAR"]:')
    (360, '\\t\\tinput.extend(expand(os.path.join(outputdir, "STAR", "{sample}", "{sample}_Aligned.sortedByCoord.out.bam.bai"), sample = samples.names.values.tolist()))')
    (361, '\\treturn input')
    (362, '')
    (363, '## Determine the input directories for MultiQC depending on the config file')
    (364, 'def multiqc_params(wildcards):')
    (365, '\\tparam = [os.path.join(outputdir, "FastQC")]')
    (366, '\\tif config["run_trimming"]:')
    (367, '\\t\\tparam.append(os.path.join(outputdir, "FASTQtrimmed"))')
    (368, '\\tif config["run_STAR"]:')
    (369, '\\t\\tparam.append(os.path.join(outputdir, "STAR"))')
    (370, '\\treturn param')
    (371, '')
    (372, '## MultiQC')
    (373, 'rule multiqc:')
    (374, '\\tinput:')
    (375, '\\t\\tmultiqc_input')
    (376, '\\toutput:')
    (377, '\\t\\tos.path.join(outputdir, "MultiQC", "multiqc_report.html")')
    (378, '\\tparams:')
    (379, '\\t\\tinputdirs = multiqc_params,')
    (380, '\\t\\tMultiQCdir = lambda wildcards, output: os.path.dirname(output[0])   ## dirname of first output')
    (381, '\\tlog:')
    (382, '\\t\\tos.path.join(outputdir, "logs", "multiqc.log")')
    (383, '\\tbenchmark:')
    (384, '\\t\\tos.path.join(outputdir, "benchmarks", "multiqc.txt")')
    (385, '\\tconda:')
    (386, '\\t\\t"envs/environment.yaml"')
    (387, '\\tshell:')
    (388, '\\t\\t"echo \\\'MultiQC version:\\')
    (389, '\\\' > {log}; multiqc --version >> {log}; "')
    (390, '\\t\\t"multiqc {params.inputdirs} -f -o {params.MultiQCdir}"')
    (391, '')
    (392, '')
    (393, '## ------------------------------------------------------------------------------------ ##')
    (394, '## Adapter trimming')
    (395, '## ------------------------------------------------------------------------------------ ##')
    (396, '# TrimGalore!')
    (397, 'rule trimgaloreSE:')
    (398, '\\tinput:')
    (399, '\\t\\tfastq = os.path.join(FASTQdir, "".join(["{sample}.", str(config["fqsuffix"]), ".gz"]))')
    (400, '\\toutput:')
    (401, '\\t\\tos.path.join(outputdir, "FASTQtrimmed", "{sample}_trimmed.fq.gz")')
    (402, '\\tparams:')
    (403, '\\t\\tFASTQtrimmeddir = lambda wildcards, output: os.path.dirname(output[0])   ## dirname of first output')
    (404, '\\tlog:')
    (405, '\\t\\tos.path.join(outputdir, "logs", "trimgalore_{sample}.log")')
    (406, '\\tbenchmark:')
    (407, '\\t\\tos.path.join(outputdir, "benchmarks", "trimgalore_{sample}.txt")')
    (408, '\\tconda:')
    (409, '\\t\\t"envs/environment.yaml"')
    (410, '\\tshell:')
    (411, '\\t\\t"echo \\\'TrimGalore! version:\\')
    (412, '\\\' > {log}; trim_galore --version >> {log}; "')
    (413, '\\t\\t"trim_galore -q 20 --phred33 --length 20 -o {params.FASTQtrimmeddir} --path_to_cutadapt cutadapt {input.fastq}"')
    (414, '')
    (415, 'rule trimgalorePE:')
    (416, '\\tinput:')
    (417, '\\t\\tfastq1 = os.path.join(FASTQdir, "".join(["{sample}_", str(config["fqext1"]), ".", str(config["fqsuffix"]), ".gz"])),')
    (418, '\\t\\tfastq2 = os.path.join(FASTQdir, "".join(["{sample}_", str(config["fqext2"]), ".", str(config["fqsuffix"]), ".gz"]))')
    (419, '\\toutput:')
    (420, '\\t\\tos.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext1"]), "_val_1.fq.gz"])),')
    (421, '\\t\\tos.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext2"]), "_val_2.fq.gz"]))')
    (422, '\\tparams:')
    (423, '\\t\\tFASTQtrimmeddir = lambda wildcards, output: os.path.dirname(output[0])   ## dirname of first output')
    (424, '\\tlog:')
    (425, '\\t\\tos.path.join(outputdir, "logs", "trimgalore_{sample}.log")')
    (426, '\\tbenchmark:')
    (427, '\\t\\tos.path.join(outputdir, "benchmarks", "trimgalore_{sample}.txt")')
    (428, '\\tconda:')
    (429, '\\t\\t"envs/environment.yaml"')
    (430, '\\tshell:')
    (431, '\\t\\t"echo \\\'TrimGalore! version:\\')
    (432, '\\\' > {log}; trim_galore --version >> {log}; "')
    (433, '\\t\\t"trim_galore -q 20 --phred33 --length 20 -o {params.FASTQtrimmeddir} --path_to_cutadapt cutadapt "')
    (434, '\\t\\t"--paired {input.fastq1} {input.fastq2}"')
    (435, '')
    (436, '')
    (437, '')
    (438, '## second round of adapter trimming to remove duplicate adapter ligation events')
    (439, '## we specify the TruSeq adapters taken from https://emea.support.illumina.com/bulletins/2016/12/what-sequences-do-i-use-for-adapter-trimming.html')
    (440, '# --adapter AGATCGGAAGAGCACACGTCTGAACTCCAGTCA --adapter2 AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT')
    (441, 'rule trimgalorePE_2nd_round:')
    (442, '\\tinput:')
    (443, '\\t\\tfastq1 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext1"]), "_val_1.fq.gz"])),')
    (444, '\\t\\tfastq2 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext2"]), "_val_2.fq.gz"]))')
    (445, '\\toutput:')
    (446, '\\t\\tos.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext1"]), "_val_1_val_1.fq.gz"])),')
    (447, '\\t\\tos.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext2"]), "_val_2_val_2.fq.gz"]))')
    (448, '\\tparams:')
    (449, '\\t\\tFASTQtrimmeddir = lambda wildcards, output: os.path.dirname(output[0])   ## dirname of first output')
    (450, '\\tlog:')
    (451, '\\t\\tos.path.join(outputdir, "logs", "trimgalore_{sample}.log")')
    (452, '\\tbenchmark:')
    (453, '\\t\\tos.path.join(outputdir, "benchmarks", "trimgalore_{sample}.txt")')
    (454, '\\tconda:')
    (455, '\\t\\t"envs/environment.yaml"')
    (456, '\\tshell:')
    (457, '\\t\\t"echo \\\'TrimGalore! version:\\')
    (458, '\\\' > {log}; trim_galore --version >> {log}; "')
    (459, '\\t\\t"trim_galore -q 20 --phred33 --length 20 -o {params.FASTQtrimmeddir} --path_to_cutadapt cutadapt "')
    (460, '\\t\\t"--paired {input.fastq1} {input.fastq2} --illumina"')
    (461, '')
    (462, '')
    (463, '## we remove the first 5 bases from R1 and R2 because they are enriched for Gs and Cs')
    (464, 'rule cutadapt_clip5:')
    (465, '\\tinput:')
    (466, '\\t\\tfastq1 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext1"]), "_val_1.fq.gz"])),')
    (467, '\\t\\tfastq2 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext2"]), "_val_2.fq.gz"]))')
    (468, '\\toutput:')
    (469, '\\t\\tfastq1 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext1"]), "_val_1.fq_clip5.gz"])),')
    (470, '\\t\\tfastq2 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext2"]), "_val_2.fq_clip5.gz"]))')
    (471, '\\tparams:')
    (472, '\\t\\tFASTQtrimmeddir = lambda wildcards, output: os.path.dirname(output[0])   ## dirname of first output')
    (473, '\\tlog:')
    (474, '\\t\\tos.path.join(outputdir, "logs", "cutadapt_clip5_{sample}.log")')
    (475, '\\tbenchmark:')
    (476, '\\t\\tos.path.join(outputdir, "benchmarks", "cutadapt_clip5_{sample}.txt")')
    (477, '\\tconda:')
    (478, '\\t\\t"envs/environment.yaml"')
    (479, '\\tthreads:')
    (480, '\\t\\tconfig["ncores"]')
    (481, '\\tshell:')
    (482, '\\t\\t"echo \\\'cutadapt version:\\')
    (483, '\\\' > {log}; cutadapt --version >> {log}; "')
    (484, '\\t\\t"cutadapt -u 5 -U 5 -j {threads} -o {output.fastq1} -p {output.fastq2} {input.fastq1} {input.fastq2}"')
    (485, '')
    (486, '')
    (487, '')
    (488, 'rule sort_fastq_clip5:')
    (489, '\\tinput:')
    (490, '\\t\\tfastq1 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext1"]), "_val_1.fq_clip5.gz"])),')
    (491, '\\t\\tfastq2 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext2"]), "_val_2.fq_clip5.gz"]))')
    (492, '\\toutput:')
    (493, '\\t\\t\\tfastq1 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext1"]), "_val_1.fq_clip5_sorted.gz"])),')
    (494, '\\t\\t\\tfastq2 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext2"]), "_val_2.fq_clip5_sorted.gz"]))')
    (495, '\\tparams:')
    (496, '\\t\\tFASTQtrimmeddir = lambda wildcards, output: os.path.dirname(output[0])   ## dirname of first output')
    (497, '\\tbenchmark:')
    (498, '\\t\\tos.path.join(outputdir, "benchmarks", "sort_clip5_{sample}.txt")')
    (499, '\\tshell:')
    (500, '\\t\\t"zcat {input.fastq1} | paste - - - - | sort -k1,1 -S 3G | tr \\\'\\\\t\\\' \\\'\\')
    (501, '\\\' | gzip > {output.fastq1}; "')
    (502, '\\t\\t"zcat {input.fastq2} | paste - - - - | sort -k1,1 -S 3G | tr \\\'\\\\t\\\' \\\'\\')
    (503, '\\\' | gzip > {output.fastq2}"')
    (504, '')
    (505, '## -S is buffer size for main memory: increase to -S 20G')
    (506, '## --parallel=10')
    (507, '')
    (508, '')
    (509, '# ## ------------------------------------------------------------------------------------ ##')
    (510, '# ## Salmon abundance estimation')
    (511, '# ## ------------------------------------------------------------------------------------ ##')
    (512, '# # Estimate abundances with Salmon')
    (513, '# rule salmonSE:')
    (514, '# \\tinput:')
    (515, '# \\t\\tindex = os.path.join(config["salmonindex"], "versionInfo.json"),')
    (516, '# \\t\\tfastq = os.path.join(outputdir, "FASTQtrimmed", "{sample}_trimmed.fq.gz") if config["run_trimming"] else os.path.join(FASTQdir, "".join(["{sample}.", str(config["fqsuffix"]), ".gz"]))')
    (517, '# \\toutput:')
    (518, '# \\t\\tos.path.join(outputdir, "salmon", "{sample}", "quant.sf")')
    (519, '# \\tlog:')
    (520, '# \\t\\tos.path.join(outputdir, "logs", "salmon_{sample}.log")')
    (521, '# \\tbenchmark:')
    (522, '# \\t\\tos.path.join(outputdir, "benchmarks", "salmon_{sample}.txt")')
    (523, '# \\tthreads:')
    (524, '# \\t\\tconfig["ncores"]')
    (525, '# \\tparams:')
    (526, "# \\t\\tsalmonindex = lambda wildcards, input: os.path.dirname(input[\\'index\\']),   ## dirname of index input")
    (527, '# \\t\\tsalmondir = lambda wildcards, output: os.path.dirname(os.path.dirname(output[0])),   ## dirname of first output')
    (528, '# \\t\\tsalmonextraparams = config["additional_salmon_quant"]')
    (529, '# \\tconda:')
    (530, '# \\t\\t"envs/environment.yaml"')
    (531, '# \\tshell:')
    (532, '# \\t\\t"echo \\\'Salmon version:\\')
    (533, '\\\' > {log}; salmon --version >> {log}; "')
    (534, '# \\t\\t"salmon quant -i {params.salmonindex} -l A -r {input.fastq} "')
    (535, '# \\t\\t"-o {params.salmondir}/{wildcards.sample} -p {threads} {params.salmonextraparams}"')
    (536, '')
    (537, '# rule salmonPE:')
    (538, '# \\tinput:')
    (539, '# \\t\\tindex = os.path.join(config["salmonindex"], "versionInfo.json"),')
    (540, '# \\t\\tfastq1 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext1"]), "_val_1.fq.gz"])) if config["run_trimming"] else os.path.join(FASTQdir, "".join(["{sample}_", str(config["fqext1"]), ".", str(config["fqsuffix"]), ".gz"])),')
    (541, '# \\t\\tfastq2 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext2"]), "_val_2.fq.gz"])) if config["run_trimming"] else os.path.join(FASTQdir, "".join(["{sample}_", str(config["fqext2"]), ".", str(config["fqsuffix"]), ".gz"]))')
    (542, '# \\toutput:')
    (543, '# \\t\\tos.path.join(outputdir, "salmon", "{sample}", "quant.sf")')
    (544, '# \\tlog:')
    (545, '# \\t\\tos.path.join(outputdir, "logs", "salmon_{sample}.log")')
    (546, '# \\tbenchmark:')
    (547, '# \\t\\tos.path.join(outputdir, "benchmarks", "salmon_{sample}.txt")')
    (548, '# \\tthreads:')
    (549, '# \\t\\tconfig["ncores"]')
    (550, '# \\tparams:')
    (551, "# \\t\\tsalmonindex = lambda wildcards, input: os.path.dirname(input[\\'index\\']),   ## dirname of index input")
    (552, '# \\t\\tsalmondir = lambda wildcards, output: os.path.dirname(os.path.dirname(output[0])),   ## dirname of first output')
    (553, '# \\t\\tsalmonextraparams = config["additional_salmon_quant"]')
    (554, '# \\tconda:')
    (555, '# \\t\\t"envs/environment.yaml"')
    (556, '# \\tshell:')
    (557, '# \\t\\t"echo \\\'Salmon version:\\')
    (558, '\\\' > {log}; salmon --version >> {log}; "')
    (559, '# \\t\\t"salmon quant -i {params.salmonindex} -l A -1 {input.fastq1} -2 {input.fastq2} "')
    (560, '# \\t\\t"-o {params.salmondir}/{wildcards.sample} -p {threads} {params.salmonextraparams}"')
    (561, '')
    (562, '')
    (563, '# ## ------------------------------------------------------------------------------------ ##')
    (564, '# ## Transcript quantification')
    (565, '# ## ------------------------------------------------------------------------------------ ##')
    (566, '# ## tximeta')
    (567, '# rule tximeta:')
    (568, '# \\tinput:')
    (569, '# \\t    os.path.join(outputdir, "Rout", "pkginstall_state.txt"),')
    (570, '# \\t\\texpand(os.path.join(outputdir, "salmon", "{sample}", "quant.sf"), sample = samples.names.values.tolist()),')
    (571, '# \\t\\tmetatxt = config["metatxt"],')
    (572, '# \\t\\tsalmonidx = os.path.join(config["salmonindex"], "versionInfo.json"),')
    (573, '# \\t\\tjson = "".join([config["salmonindex"], ".json"]),')
    (574, '# \\t\\tscript = "scripts/run_tximeta.R"')
    (575, '# \\toutput:')
    (576, '# \\t\\tos.path.join(outputdir, "outputR", "tximeta_se.rds")')
    (577, '# \\tlog:')
    (578, '# \\t\\tos.path.join(outputdir, "Rout", "tximeta_se.Rout")')
    (579, '# \\tbenchmark:')
    (580, '# \\t\\tos.path.join(outputdir, "benchmarks", "tximeta_se.txt")')
    (581, '# \\tparams:')
    (582, '# \\t\\tsalmondir = lambda wildcards, input: os.path.dirname(os.path.dirname(input[1])),   ## dirname of second output')
    (583, '# \\t\\tflag = config["annotation"],')
    (584, '# \\t\\torganism = config["organism"],')
    (585, '# \\t\\tRbin = Rbin')
    (586, '# \\tconda:')
    (587, '# \\t\\tRenv')
    (588, '# \\tshell:')
    (589, '# \\t\\t\\\'\\\'\\\'{params.Rbin} CMD BATCH --no-restore --no-save "--args salmondir=\\\'{params.salmondir}\\\' json=\\\'{input.json}\\\' metafile=\\\'{input.metatxt}\\\' outrds=\\\'{output}\\\' annotation=\\\'{params.flag}\\\' organism=\\\'{params.organism}\\\'" {input.script} {log}\\\'\\\'\\\'')
    (590, '')
    (591, '')
    (592, '## ------------------------------------------------------------------------------------ ##')
    (593, '## STAR mapping')
    (594, '## ------------------------------------------------------------------------------------ ##')
    (595, '## Genome mapping with STAR')
    (596, 'rule starPE:')
    (597, '\\tinput:')
    (598, '\\t\\tindex = os.path.join(config["STARindex"], "SA"),')
    (599, '\\t\\tfastq1 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext1"]), "_val_1.fq.gz"])) if config["run_trimming"] else os.path.join(FASTQdir, "".join(["{sample}_", str(config["fqext1"]), ".", str(config["fqsuffix"]), ".gz"])),')
    (600, '\\t\\tfastq2 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext2"]), "_val_2.fq.gz"])) if config["run_trimming"] else os.path.join(FASTQdir, "".join(["{sample}_", str(config["fqext2"]), ".", str(config["fqsuffix"]), ".gz"]))')
    (601, '\\toutput:')
    (602, '\\t\\tos.path.join(outputdir, config["star_output"], "{sample}", "{sample}_Aligned.sortedByCoord.out.bam")')
    (603, '\\tthreads:')
    (604, '\\t\\tconfig["ncores"]')
    (605, '\\tlog:')
    (606, '\\t\\tos.path.join(outputdir, "logs", "STAR_{sample}.log")')
    (607, '\\tbenchmark:')
    (608, '\\t\\tos.path.join(outputdir, "benchmarks", "STAR_{sample}.txt")')
    (609, '\\tparams:')
    (610, "\\t\\tSTARindex = lambda wildcards, input: os.path.dirname(input[\\'index\\']),   ## dirname of index input")
    (611, '\\t\\tSTARdir = lambda wildcards, output: os.path.dirname(os.path.dirname(output[0])),   ## dirname of first output')
    (612, '\\t\\tstarextraparams = config["additional_star_align"]')
    (613, '\\tconda:')
    (614, '\\t\\t"envs/environment.yaml"')
    (615, '\\tshell:')
    (616, '\\t\\t"echo \\\'STAR version:\\')
    (617, '\\\' > {log}; STAR --version >> {log}; "')
    (618, '\\t\\t"STAR --genomeDir {params.STARindex} --readFilesIn {input.fastq1} {input.fastq2} "')
    (619, '\\t\\t"--runThreadN {threads} --outFileNamePrefix {params.STARdir}/{wildcards.sample}/{wildcards.sample}_ "')
    (620, '\\t\\t"--outSAMtype BAM SortedByCoordinate --readFilesCommand gunzip -c "')
    (621, '\\t\\t"{params.starextraparams}"')
    (622, '')
    (623, '')
    (624, '')
    (625, '')
    (626, '')
    (627, '')
    (628, 'rule starPE_2nd_round:')
    (629, '\\tinput:')
    (630, '\\t\\tindex = os.path.join(config["STARindex"], "SA"),')
    (631, '\\t\\tfastq1 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext1"]), "_val_1_val_1.fq.gz"])) if config["run_trimming"] else os.path.join(FASTQdir, "".join(["{sample}_", str(config["fqext1"]), ".", str(config["fqsuffix"]), ".gz"])),')
    (632, '\\t\\tfastq2 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext2"]), "_val_2_val_2.fq.gz"])) if config["run_trimming"] else os.path.join(FASTQdir, "".join(["{sample}_", str(config["fqext2"]), ".", str(config["fqsuffix"]), ".gz"]))')
    (633, '\\toutput:')
    (634, '\\t\\tos.path.join(outputdir, "STAR_round2", "{sample}", "{sample}_Aligned.sortedByCoord.out.bam")')
    (635, '\\tthreads:')
    (636, '\\t\\tconfig["ncores"]')
    (637, '\\tlog:')
    (638, '\\t\\tos.path.join(outputdir, "logs", "STAR_{sample}.log")')
    (639, '\\tbenchmark:')
    (640, '\\t\\tos.path.join(outputdir, "benchmarks", "STAR_{sample}.txt")')
    (641, '\\tparams:')
    (642, "\\t\\tSTARindex = lambda wildcards, input: os.path.dirname(input[\\'index\\']),   ## dirname of index input")
    (643, '\\t\\tSTARdir = lambda wildcards, output: os.path.dirname(os.path.dirname(output[0])),   ## dirname of first output')
    (644, '\\t\\tstarextraparams = config["additional_star_align_2nd_round"]')
    (645, '\\tconda:')
    (646, '\\t\\t"envs/environment.yaml"')
    (647, '\\tshell:')
    (648, '\\t\\t"echo \\\'STAR version:\\')
    (649, '\\\' > {log}; STAR --version >> {log}; "')
    (650, '\\t\\t"STAR --genomeDir {params.STARindex} --readFilesIn {input.fastq1} {input.fastq2} "')
    (651, '\\t\\t"--runThreadN {threads} --outFileNamePrefix {params.STARdir}/{wildcards.sample}/{wildcards.sample}_ "')
    (652, '\\t\\t"--outSAMtype BAM SortedByCoordinate --readFilesCommand gunzip -c "')
    (653, '\\t\\t"{params.starextraparams}"')
    (654, '')
    (655, '')
    (656, '')
    (657, 'rule starPE_clip5:')
    (658, '\\tinput:')
    (659, '\\t\\tindex = os.path.join(config["STARindex"], "SA"),')
    (660, '\\t\\tfastq1 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext1"]), "_val_1.fq_clip5.gz"])),')
    (661, '\\t\\tfastq2 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext2"]), "_val_2.fq_clip5.gz"]))')
    (662, '\\toutput:')
    (663, '\\t\\tos.path.join(outputdir, "STAR_clip5", "{sample}", "{sample}_Aligned.sortedByCoord.out.bam")')
    (664, '\\tthreads:')
    (665, '\\t\\tconfig["ncores"]')
    (666, '\\tlog:')
    (667, '\\t\\tos.path.join(outputdir, "logs", "STAR_clip5_{sample}.log")')
    (668, '\\tbenchmark:')
    (669, '\\t\\tos.path.join(outputdir, "benchmarks", "STAR_clip5_{sample}.txt")')
    (670, '\\tparams:')
    (671, "\\t\\tSTARindex = lambda wildcards, input: os.path.dirname(input[\\'index\\']),   ## dirname of index input")
    (672, '\\t\\tSTARdir = lambda wildcards, output: os.path.dirname(os.path.dirname(output[0])),   ## dirname of first output')
    (673, '\\t\\tstarextraparams = config["additional_star_align"]')
    (674, '\\tconda:')
    (675, '\\t\\t"envs/environment.yaml"')
    (676, '\\tshell:')
    (677, '\\t\\t"echo \\\'STAR version:\\')
    (678, '\\\' > {log}; STAR --version >> {log}; "')
    (679, '\\t\\t"STAR --genomeDir {params.STARindex} --readFilesIn {input.fastq1} {input.fastq2} "')
    (680, '\\t\\t"--runThreadN {threads} --outFileNamePrefix {params.STARdir}/{wildcards.sample}/{wildcards.sample}_ "')
    (681, '\\t\\t"--outSAMtype BAM SortedByCoordinate --readFilesCommand gunzip -c "')
    (682, '\\t\\t"{params.starextraparams}"')
    (683, '')
    (684, '')
    (685, 'rule star_clip5_separate:')
    (686, '\\tinput:')
    (687, '\\t\\tindex = os.path.join(config["STARindex"], "SA"),')
    (688, '\\t\\t# fastq1 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext1"]), "_val_1.fq_clip5.gz"])) ,')
    (689, '\\t\\tfastq2 = os.path.join(outputdir, "FASTQtrimmed", "".join(["{sample}_", str(config["fqext2"]), "_val_2.fq_clip5.gz"]))')
    (690, '\\toutput:')
    (691, '\\t\\t# out1 = os.path.join(outputdir, "STAR_clip5", "{sample}", "{sample}_R1_Aligned.sortedByCoord.out.bam"),')
    (692, '\\t\\tout2 = os.path.join(outputdir, config["star_output"], "{sample}", "{sample}_R2_Aligned.sortedByCoord.out.bam")')
    (693, '\\tthreads:')
    (694, '\\t\\tconfig["ncores"]')
    (695, '\\tlog:')
    (696, '\\t\\tos.path.join(outputdir, "logs", "STAR_clip5_{sample}.log")')
    (697, '\\tbenchmark:')
    (698, '\\t\\tos.path.join(outputdir, "benchmarks", "STAR_clip5_{sample}.txt")')
    (699, '\\tparams:')
    (700, "\\t\\tSTARindex = lambda wildcards, input: os.path.dirname(input[\\'index\\']),   ## dirname of index input")
    (701, '\\t\\tSTARdir = lambda wildcards, output: os.path.dirname(os.path.dirname(output[0])),   ## dirname of first output')
    (702, '\\t\\tstarextraparams = config["additional_star_align"]')
    (703, '\\tconda:')
    (704, '\\t\\t"envs/environment.yaml"')
    (705, '\\tshell:')
    (706, '\\t\\t"echo \\\'STAR version:\\')
    (707, '\\\' > {log}; STAR --version >> {log}; "')
    (708, '\\t\\t# "STAR --genomeDir {params.STARindex} --readFilesIn {input.fastq1} "')
    (709, '\\t\\t# "--runThreadN {threads} --outFileNamePrefix {params.STARdir}/{wildcards.sample}/{wildcards.sample}_R1_ "')
    (710, '\\t\\t# "--outSAMtype BAM SortedByCoordinate --readFilesCommand gunzip -c "')
    (711, '\\t\\t# "{params.starextraparams}; "')
    (712, '\\t\\t"STAR --genomeDir {params.STARindex} --readFilesIn {input.fastq2} "')
    (713, '\\t\\t"--runThreadN {threads} --outFileNamePrefix {params.STARdir}/{wildcards.sample}/{wildcards.sample}_R2_ "')
    (714, '\\t\\t"--outSAMtype BAM SortedByCoordinate --readFilesCommand gunzip -c "')
    (715, '\\t\\t"{params.starextraparams}"')
    (716, '')
    (717, '')
    (718, '')
    (719, '')
    (720, '## Index bam files')
    (721, 'rule bamindex:')
    (722, '\\tinput:')
    (723, '\\t\\tbam = os.path.join(outputdir, config["star_output"], "{sample}", "{sample}_Aligned.sortedByCoord.out.bam")')
    (724, '\\toutput:')
    (725, '\\t\\tos.path.join(outputdir, config["star_output"], "{sample}", "{sample}_Aligned.sortedByCoord.out.bam.bai")')
    (726, '\\tlog:')
    (727, '\\t\\tos.path.join(outputdir, "logs", "samtools_index_{sample}.log")')
    (728, '\\tbenchmark:')
    (729, '\\t\\tos.path.join(outputdir, "benchmarks", "samtools_index_{sample}.txt")')
    (730, '\\tconda:')
    (731, '\\t\\t"envs/environment.yaml"')
    (732, '\\tshell:')
    (733, '\\t\\t"echo \\\'samtools version:\\')
    (734, '\\\' > {log}; samtools --version >> {log}; "')
    (735, '\\t\\t"samtools index {input.bam}"')
    (736, '')
    (737, '## Index bam files')
    (738, 'rule bamindex_clip5:')
    (739, '\\tinput:')
    (740, '\\t\\tbam = os.path.join(outputdir, "STAR_clip5", "{sample}", "{sample}_Aligned.sortedByCoord.out.bam")')
    (741, '\\toutput:')
    (742, '\\t\\tos.path.join(outputdir, "STAR_clip5", "{sample}", "{sample}_Aligned.sortedByCoord.out.bam.bai")')
    (743, '\\tlog:')
    (744, '\\t\\tos.path.join(outputdir, "logs", "samtools_index_clip5_{sample}.log")')
    (745, '\\tbenchmark:')
    (746, '\\t\\tos.path.join(outputdir, "benchmarks", "samtools_index_clip5_{sample}.txt")')
    (747, '\\tconda:')
    (748, '\\t\\t"envs/environment.yaml"')
    (749, '\\tshell:')
    (750, '\\t\\t"echo \\\'samtools version:\\')
    (751, '\\\' > {log}; samtools --version >> {log}; "')
    (752, '\\t\\t"samtools index {input.bam}"')
    (753, '')
    (754, 'rule bamindex_clip5_R2:')
    (755, '\\tinput:')
    (756, '\\t\\tbam = os.path.join(outputdir, config["star_output"], "{sample}", "{sample}_R2_Aligned.sortedByCoord.out.bam")')
    (757, '\\toutput:')
    (758, '\\t\\tos.path.join(outputdir, config["star_output"], "{sample}", "{sample}_R2_Aligned.sortedByCoord.out.bam.bai")')
    (759, '\\tlog:')
    (760, '\\t\\tos.path.join(outputdir, "logs", "samtools_index_clip5_R2_{sample}.log")')
    (761, '\\tbenchmark:')
    (762, '\\t\\tos.path.join(outputdir, "benchmarks", "samtools_index_clip5_R2_{sample}.txt")')
    (763, '\\tconda:')
    (764, '\\t\\t"envs/environment.yaml"')
    (765, '\\tshell:')
    (766, '\\t\\t"echo \\\'samtools version:\\')
    (767, '\\\' > {log}; samtools --version >> {log}; "')
    (768, '\\t\\t"samtools index {input.bam}"')
    (769, '')
    (770, '## Convert BAM files to bigWig')
    (771, 'rule bigwig:')
    (772, '\\tinput:')
    (773, '\\t\\tbam = os.path.join(outputdir, "STAR", "{sample}", "{sample}_Aligned.sortedByCoord.out.bam"),')
    (774, '\\t\\tchrl = os.path.join(config["STARindex"], "chrNameLength.txt")')
    (775, '\\toutput:')
    (776, '\\t\\tos.path.join(outputdir, "STARbigwig", "{sample}_Aligned.sortedByCoord.out.bw")')
    (777, '\\tparams:')
    (778, '\\t\\tSTARbigwigdir = lambda wildcards, output: os.path.dirname(output[0])   ## dirname of first output')
    (779, '\\tlog:')
    (780, '\\t\\tos.path.join(outputdir, "logs", "bigwig_{sample}.log")')
    (781, '\\tbenchmark:')
    (782, '\\t\\tos.path.join(outputdir, "benchmarks", "bigwig_{sample}.txt")')
    (783, '\\tconda:')
    (784, '\\t\\t"envs/environment.yaml"')
    (785, '\\tshell:')
    (786, '\\t\\t"echo \\\'bedtools version:\\')
    (787, '\\\' > {log}; bedtools --version >> {log}; "')
    (788, '\\t\\t"bedtools genomecov -split -ibam {input.bam} -bg | LC_COLLATE=C sort -k1,1 -k2,2n > "')
    (789, '\\t\\t"{params.STARbigwigdir}/{wildcards.sample}_Aligned.sortedByCoord.out.bedGraph; "')
    (790, '\\t\\t"bedGraphToBigWig {params.STARbigwigdir}/{wildcards.sample}_Aligned.sortedByCoord.out.bedGraph "')
    (791, '\\t\\t"{input.chrl} {output}; rm -f {params.STARbigwigdir}/{wildcards.sample}_Aligned.sortedByCoord.out.bedGraph"')
    (792, '')
    (793, '## Normalize the two replicates to the mean library size ')
    (794, '## the scaling factors were computed with R (clipper_analysis.Rmd)')
    (795, 'def get_sf(wildcards):')
    (796, '    return config["sf"][wildcards.sample] ')
    (797, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/pyflow-ChIPseq, file=Snakefile
context_key: ['if control in CONTROLS']
    (46, '    if control in CONTROLS:')
    (47, '        ALL_PEAKS.append("08peak_macs1/{}_vs_{}_macs1_peaks.bed".format(case, control))')
    (48, '        ALL_PEAKS.append("08peak_macs1/{}_vs_{}_macs1_nomodel_peaks.bed".format(case, control))')
    (49, '        ALL_PEAKS.append("09peak_macs2/{}_vs_{}_macs2_peaks.xls".format(case, control))')
    (50, '        ALL_inputSubtract_BIGWIG.append("06bigwig_inputSubtract/{}_subtract_{}.bw".format(case, control))')
    (51, '        ALL_SUPER.append("11superEnhancer/{}_vs_{}-super/".format(case, control))')
    (52, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=david-a-parry/vase_family_filtering_workflow, file=workflow/Snakefile
context_key: ['if not filter_modes']
    (4, 'if not filter_modes:')
    (5, '    raise ValueError("No filtering modes found in config file. At least one " +')
    (6, '                     "of \\\'recessive\\\', \\\'de_novo\\\' or \\\'dominant\\\' must be " +')
    (7, '                     "specified in the \\\'filtering\\\' section.")')
    (8, '')
    (9, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=david-a-parry/vase_family_filtering_workflow, file=workflow/rules/vase.smk
context_key: ['if len(get_segregation_modes()) > 1', 'rule merge_filtered_variants', 'input']
    (96, 'if len(get_segregation_modes()) > 1:')
    (97, '    rule merge_filtered_variants:')
    (98, '        input:')
    (99, '            vcfs=lambda w: expand(')
    (100, '                  "results/vase_filtered/vase_filtered.{seg}.vcf.gz",')
    (101, '                seg=get_segregation_modes()),')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=david-a-parry/vase_family_filtering_workflow, file=workflow/rules/vase.smk
context_key: ['if len(get_segregation_modes()) > 1', 'rule merge_filtered_variants', 'output']
    (102, '        output:')
    (103, '            vcf="results/vase_filtered/vase_filtered.all.vcf.gz",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=david-a-parry/vase_family_filtering_workflow, file=workflow/rules/vase.smk
context_key: ['if len(get_segregation_modes()) > 1', 'rule merge_filtered_variants', 'log']
    (104, '        log:')
    (105, '            "logs/picard/merge_vase_filtered.log",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=david-a-parry/vase_family_filtering_workflow, file=workflow/rules/vase.smk
context_key: ['if len(get_segregation_modes()) > 1', 'rule merge_filtered_variants', 'resources']
    (106, '        resources:')
    (107, '            mem_gb=get_mem_gb')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=david-a-parry/vase_family_filtering_workflow, file=workflow/rules/vase.smk
context_key: ['if len(get_segregation_modes()) > 1', 'rule merge_filtered_variants', 'wrapper']
    (108, '        wrapper:')
    (109, '          "0.84.0/bio/picard/mergevcfs"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MW55/Natrix, file=rules/demultiplexing.smk
context_key: ['output', 'else temp(expand("demultiplexed/{unit.sample}_{unit.unit}_R{read}.fastq.gz", unit=units.reset_index().itertuples(), read=reads))']
    (4, '        else temp(expand("demultiplexed/{unit.sample}_{unit.unit}_R{read}.fastq.gz", unit=units.reset_index().itertuples(), read=reads)) )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=biolaboro/PSET, file=workflow/Snakefile
context_key: ['if "-num_alignments" not in cmd']
    (139, '        if "-num_alignments" not in cmd:')
    (140, '            cmd = (*cmd, "-num_alignments", info["sequences"].replace(",", ""))')
    (141, '        check_call(cmd)')
    (142, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/whole-genome-alignment, file=Snakefile
context_key: ['if "OUTDIR" in config']
    (6, 'if "OUTDIR" in config:')
    (7, '    workdir: config["OUTDIR"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CUMoellerLab/sn-mg-pipeline, file=resources/scripts/run_snakefile_bin_on_subset.R
context_key: ['if (length(contig_fps_from) == length(file_list))']
    (47, 'if (length(contig_fps_from) == length(file_list)) {')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=Snakefile
context_key: ['onstart', 'if not os.path.exists("logs")']
    (14, '   if not os.path.exists("logs"):')
    (15, '     os.makedirs("logs")')
    (16, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=Snakefile
context_key: ['if DIFFEXPRESS.lower() == "on" and len(samples["condition"].unique()) <= 1']
    (26, 'if DIFFEXPRESS.lower() == "on" and len(samples["condition"].unique()) <= 1:')
    (27, '    sys.exit("Differential Expression requested, but only one condition given.\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=Snakefile
context_key: ['if "RIBO" not in samples["method"].unique()']
    (32, 'if "RIBO" not in samples["method"].unique():')
    (33, '    hasRIBO=False')
    (34, '    print("No Ribo-seq libraries were detected. No prediction tools for this setup are currently implemented. If you have pure Ribo-seq libraries, please use the method tag RIBO. Continuing...")')
    (35, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=Snakefile
context_key: ['if DIFFEXPRESS.lower() == "on"', 'if CONTRASTS != ""']
    (40, 'if DIFFEXPRESS.lower() == "on":')
    (41, '    if CONTRASTS != "":')
    (42, '        CONTRASTS = CONTRASTS.split(",")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=Snakefile
context_key: ['if DIFFEXPRESS.lower() == "on"', 'else']
    (43, '    else:')
    (44, '        CONTRASTS=[item for sublist in [[(\\\'-\\\'.join(str(i) for i in x))] for x in list((iter.combinations(sorted(samples["condition"].unique(), key=lambda s: s.lower()),2)))]  for item in sublist]')
    (45, '')
    (46, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=Snakefile
context_key: ['if hasRIBO', 'elif DIFFEXPRESS.lower() == "off" and DEEPRIBO.lower() == "on"', 'rule all', 'input']
    (138, '    elif DIFFEXPRESS.lower() == "off" and DEEPRIBO.lower() == "on":')
    (139, '       rule all:')
    (140, '          input:')
    (141, '              expand("metageneprofiling/TIS/raw/{method}-{condition}-{replicate}", zip, method=samples_meta_start["method"], condition=samples_meta_start["condition"], replicate=samples_meta_start["replicate"]),')
    (142, '              expand("metageneprofiling/TIS/norm/{method}-{condition}-{replicate}", zip, method=samples_meta_start["method"], condition=samples_meta_start["condition"], replicate=samples_meta_start["replicate"]),')
    (143, '              expand("metageneprofiling/TTS/raw/{method}-{condition}-{replicate}", zip, method=samples_meta_stop["method"], condition=samples_meta_stop["condition"], replicate=samples_meta_stop["replicate"]),')
    (144, '              expand("metageneprofiling/TTS/norm/{method}-{condition}-{replicate}", zip, method=samples_meta_stop["method"], condition=samples_meta_stop["condition"], replicate=samples_meta_stop["replicate"]),')
    (145, '              get_wigfiles,')
    (146, '              "qc/multi/multiqc_report.html",')
    (147, '              "tracks/potentialStopCodons.gff",')
    (148, '              "tracks/potentialStartCodons.gff",')
    (149, '              "tracks/potentialAlternativeStartCodons.gff",')
    (150, '              "tracks/potentialRibosomeBindingSite.gff",')
    (151, '              "auxiliary/annotation_total.xlsx",')
    (152, '              "auxiliary/annotation_unique.xlsx",')
    (153, '              "auxiliary/total_read_counts.xlsx",')
    (154, '              "auxiliary/unique_read_counts.xlsx",')
    (155, '              "auxiliary/samples.xlsx",')
    (156, '              "auxiliary/predictions_reparation.xlsx",')
    (157, '              "figures/heatmap_SpearmanCorr_readCounts.pdf",')
    (158, '              "auxiliary/predictions_deepribo.xlsx",')
    (159, '              rules.createOverviewTablePredictions.output,')
    (160, '              "metageneprofiling/merged_offsets.json"')
    (161, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=Snakefile
context_key: ['if hasRIBO', 'elif DIFFEXPRESS.lower() == "on" and DEEPRIBO.lower() == "off"', 'rule all', 'input']
    (162, '    elif DIFFEXPRESS.lower() == "on" and DEEPRIBO.lower() == "off":')
    (163, '       rule all:')
    (164, '          input:')
    (165, '              expand("metageneprofiling/TIS/raw/{method}-{condition}-{replicate}", zip, method=samples_meta_start["method"], condition=samples_meta_start["condition"], replicate=samples_meta_start["replicate"]),')
    (166, '              expand("metageneprofiling/TIS/norm/{method}-{condition}-{replicate}", zip, method=samples_meta_start["method"], condition=samples_meta_start["condition"], replicate=samples_meta_start["replicate"]),')
    (167, '              expand("metageneprofiling/TTS/raw/{method}-{condition}-{replicate}", zip, method=samples_meta_stop["method"], condition=samples_meta_stop["condition"], replicate=samples_meta_stop["replicate"]),')
    (168, '              expand("metageneprofiling/TTS/norm/{method}-{condition}-{replicate}", zip, method=samples_meta_stop["method"], condition=samples_meta_stop["condition"], replicate=samples_meta_stop["replicate"]),')
    (169, '              get_wigfiles,')
    (170, '              "qc/multi/multiqc_report.html",')
    (171, '              "tracks/potentialStopCodons.gff",')
    (172, '              "tracks/potentialStartCodons.gff",')
    (173, '              "tracks/potentialAlternativeStartCodons.gff",')
    (174, '              "tracks/potentialRibosomeBindingSite.gff",')
    (175, '              "auxiliary/annotation_total.xlsx",')
    (176, '              "auxiliary/annotation_unique.xlsx",')
    (177, '              "auxiliary/total_read_counts.xlsx",')
    (178, '              "auxiliary/unique_read_counts.xlsx",')
    (179, '              "auxiliary/samples.xlsx",')
    (180, '              "auxiliary/predictions_reparation.xlsx",')
    (181, '              "figures/heatmap_SpearmanCorr_readCounts.pdf",')
    (182, '              rules.createOverviewTableDiffExpr.output,')
    (183, '              unpack(getContrast),')
    (184, '              unpack(getContrastXtail),')
    (185, '              unpack(getContrastRiborex),')
    (186, '              unpack(getContrastDeltaTE),')
    (187, '              "metageneprofiling/merged_offsets.json"')
    (188, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=Snakefile
context_key: ['if hasRIBO', 'else', 'rule all', 'input']
    (189, '    else:')
    (190, '       rule all:')
    (191, '          input:')
    (192, '              expand("metageneprofiling/TIS/raw/{method}-{condition}-{replicate}", zip, method=samples_meta_start["method"], condition=samples_meta_start["condition"], replicate=samples_meta_start["replicate"]),')
    (193, '              expand("metageneprofiling/TIS/norm/{method}-{condition}-{replicate}", zip, method=samples_meta_start["method"], condition=samples_meta_start["condition"], replicate=samples_meta_start["replicate"]),')
    (194, '              expand("metageneprofiling/TTS/raw/{method}-{condition}-{replicate}", zip, method=samples_meta_stop["method"], condition=samples_meta_stop["condition"], replicate=samples_meta_stop["replicate"]),')
    (195, '              expand("metageneprofiling/TTS/norm/{method}-{condition}-{replicate}", zip, method=samples_meta_stop["method"], condition=samples_meta_stop["condition"], replicate=samples_meta_stop["replicate"]),')
    (196, '              get_wigfiles,')
    (197, '              "qc/multi/multiqc_report.html",')
    (198, '              "tracks/potentialStopCodons.gff",')
    (199, '              "tracks/potentialStartCodons.gff",')
    (200, '              "tracks/potentialAlternativeStartCodons.gff",')
    (201, '              "tracks/potentialRibosomeBindingSite.gff",')
    (202, '              "auxiliary/annotation_total.xlsx",')
    (203, '              "auxiliary/annotation_unique.xlsx",')
    (204, '              "auxiliary/total_read_counts.xlsx",')
    (205, '              "auxiliary/unique_read_counts.xlsx",')
    (206, '              "auxiliary/samples.xlsx",')
    (207, '              "auxiliary/predictions_reparation.xlsx",')
    (208, '              "figures/heatmap_SpearmanCorr_readCounts.pdf",')
    (209, '              rules.createOverviewTableReparation.output,')
    (210, '              "metageneprofiling/merged_offsets.json"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=Snakefile
context_key: ['else', 'if DIFFEXPRESS.lower() == "on"', 'rule all', 'input']
    (212, '     if DIFFEXPRESS.lower() == "on":')
    (213, '       rule all:')
    (214, '          input:')
    (215, '              expand("metageneprofiling/TIS/raw/{method}-{condition}-{replicate}", zip, method=samples_meta_start["method"], condition=samples_meta_start["condition"], replicate=samples_meta_start["replicate"]),')
    (216, '              expand("metageneprofiling/TIS/norm/{method}-{condition}-{replicate}", zip, method=samples_meta_start["method"], condition=samples_meta_start["condition"], replicate=samples_meta_start["replicate"]),')
    (217, '              expand("metageneprofiling/TTS/raw/{method}-{condition}-{replicate}", zip, method=samples_meta_stop["method"], condition=samples_meta_stop["condition"], replicate=samples_meta_stop["replicate"]),')
    (218, '              expand("metageneprofiling/TTS/norm/{method}-{condition}-{replicate}", zip, method=samples_meta_stop["method"], condition=samples_meta_stop["condition"], replicate=samples_meta_stop["replicate"]),')
    (219, '              get_wigfiles,')
    (220, '              "tracks/potentialStopCodons.gff",')
    (221, '              "tracks/potentialStartCodons.gff",')
    (222, '              "tracks/potentialAlternativeStartCodons.gff",')
    (223, '              "tracks/potentialRibosomeBindingSite.gff",')
    (224, '              "auxiliary/annotation_total.xlsx",')
    (225, '              "auxiliary/annotation_unique.xlsx",')
    (226, '              "auxiliary/total_read_counts.xlsx",')
    (227, '              "auxiliary/unique_read_counts.xlsx",')
    (228, '              "auxiliary/samples.xlsx",')
    (229, '              "figures/heatmap_SpearmanCorr_readCounts.pdf",')
    (230, '              unpack(getContrast),')
    (231, '              unpack(getContrastXtail),')
    (232, '              unpack(getContrastRiborex),')
    (233, '              unpack(getContrastDeltaTE),')
    (234, '              "metageneprofiling/merged_offsets.json"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=rules/auxiliary.smk
context_key: ['try', 'if first_line == "##gff-version 3"']
    (5, '        if first_line == "##gff-version 3":')
    (6, '            return "GFF3"')
    (7, '        else:')
    (8, '            return "GTF2"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=rules/diffex_deltate.smk
context_key: ['try', 'if line == "False"']
    (6, '            if line == "False":')
    (7, '                line = False')
    (8, '            else:')
    (9, '                line = True')
    (10, '        return line')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/HRIBO, file=rules/diffex_contrast.smk
context_key: ['output', 'run', 'if not os.path.exists("contrasts")']
    (4, '        if not os.path.exists("contrasts"):')
    (5, '            os.makedirs("contrasts")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nixonlab/HERV_scGCB, file=workflow/rules/download_remote.smk
context_key: ["if l[2] == \\'gene\\'", "if d[\\'gene_id\\'] in g_sym"]
    (91, "            if l[2] == \\'gene\\':")
    (92, "                if d[\\'gene_id\\'] in g_sym:")
    (93, '                    assert g_sym[d[\\\'gene_id\\\']] == d[\\\'gene_name\\\'], "Gene name mismatch: %s %s" % (d[\\\'gene_name\\\'], g_sym[d[\\\'gene_id\\\']])')
    (94, "                g_sym[d[\\'gene_id\\']] = d[\\'gene_name\\']")
    (95, '')
    (96, "            if l[2] == \\'transcript\\':")
    (97, "                if d[\\'transcript_id\\'] in tx_g:")
    (98, '                    assert tx_g[d[\\\'transcript_id\\\']] == d[\\\'gene_id\\\'], "Gene ID mismatch: %s %s" % (d[\\\'gene_id\\\'], tx_g[d[\\\'transcript_id\\\']])')
    (99, "                tx_g[d[\\'transcript_id\\']] = d[\\'gene_id\\']")
    (100, '')
    (101, '        # Generate ttg')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nixonlab/HERV_scGCB, file=workflow/rules/download_remote.smk
context_key: ["if l[2] == \\'gene\\'"]
    (102, "        with open(output[0], \\'w\\') as outh:")
    (103, "            print(\\'TXNAME\\\\tGENEID\\', file=outh)")
    (104, "            txlist = (l.strip(\\'\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/trim.smk
context_key: ['if not skip_trimming', 'rule Trim_Adapters_PE', 'input']
    (2, 'if not skip_trimming:')
    (3, '')
    (4, '    ruleorder: Trim_Adapters_PE > Trim_Adapters_SE')
    (5, '')
    (6, '')
    (7, '    rule Trim_Adapters_PE:')
    (8, '        input:')
    (9, '            get_fastq')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/trim.smk
context_key: ['if not skip_trimming', 'rule Trim_Adapters_PE', 'output']
    (10, '        output:')
    (11, '            fastq1="results/trimmed/{sample}-{unit}.1.fastq.gz",')
    (12, '            fastq2="results/trimmed/{sample}-{unit}.2.fastq.gz",')
    (13, '            qc="results/trimmed/{sample}-{unit}.qc.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/trim.smk
context_key: ['if not skip_trimming', 'rule Trim_Adapters_PE', 'threads']
    (14, '        threads: 8')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/trim.smk
context_key: ['if not skip_trimming', 'rule Trim_Adapters_PE', 'params']
    (15, '        params:')
    (16, '            adapters="-a {0} -A {0} {1}".format(config["trimming"]["adapter"], config["params"]["cutadapt-pe"]),')
    (17, '            extra="--minimum-length=30 -a A\\\\{100\\\\} -A A\\\\{100\\\\}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/trim.smk
context_key: ['if not skip_trimming', 'rule Trim_Adapters_PE', 'resources']
    (18, '        resources:')
    (19, '            mem_mb=64000')
    (20, '        # This is so that it only keeps a set of reads if both pairs are at least 20bp long')
    (21, "        # And, it performs 3\\' poly-A trimming on the reads")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/trim.smk
context_key: ['if not skip_trimming', 'rule Trim_Adapters_PE', 'log']
    (22, '        log: "results/logs/cutadapt/{sample}-{unit}.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/trim.smk
context_key: ['if not skip_trimming', 'rule Trim_Adapters_PE', 'wrapper']
    (23, '        wrapper:')
    (24, '            "v1.5.0/bio/cutadapt/pe"')
    (25, '')
    (26, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/trim.smk
context_key: ['if not skip_trimming', 'rule Trim_Adapters_SE', 'input']
    (27, '    rule Trim_Adapters_SE:')
    (28, '        input:')
    (29, '            get_fastq')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/trim.smk
context_key: ['if not skip_trimming', 'rule Trim_Adapters_SE', 'output']
    (30, '        output:')
    (31, '            fastq="results/trimmed/{sample}-{unit}.fastq.gz",')
    (32, '            qc="results/trimmed/{sample}-{unit}.qc.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/trim.smk
context_key: ['if not skip_trimming', 'rule Trim_Adapters_SE', 'threads']
    (33, '        threads: 8')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/trim.smk
context_key: ['if not skip_trimming', 'rule Trim_Adapters_SE', 'params']
    (34, '        params:')
    (35, '            adapters="-a {0} {1}".format(config["trimming"]["adapter"], config["params"]["cutadapt-se"]),')
    (36, '            extra="--minimum-length=35 -a A\\\\{100\\\\}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/trim.smk
context_key: ['if not skip_trimming', 'rule Trim_Adapters_SE', 'resources']
    (37, '        resources:')
    (38, '            mem_mb=64000')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/trim.smk
context_key: ['if not skip_trimming', 'rule Trim_Adapters_SE', 'log']
    (39, '        log: "results/logs/cutadapt/{sample}-{unit}.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbp44/snakemake-workflow-rna-seq, file=workflow/rules/trim.smk
context_key: ['if not skip_trimming', 'rule Trim_Adapters_SE', 'wrapper']
    (40, '        wrapper:')
    (41, '            "v1.5.0/bio/cutadapt/se"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/population-mapping, file=Snakefile
context_key: ['if "OUTDIR" in config']
    (14, 'if "OUTDIR" in config:')
    (15, '    print("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mahajrod/BuscoPhylo, file=Snakefile
context_key: ['if "species_list" not in config']
    (30, 'if "species_list" not in config:')
    (31, '    config["species_list"] = [f.name[:-6] for f in genome_dir_path.iterdir() if f.is_file() and f.suffix == ".fasta"]')
    (32, '')
    (33, '#---- necessary functions ----')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexmsalmeida/ml-microbiome, file=Snakefile
context_key: ['if not os.path.exists(log_dir)']
    (13, 'if not os.path.exists(log_dir):')
    (14, '    os.makedirs(log_dir)')
    (15, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexmsalmeida/ml-microbiome, file=Snakefile
context_key: ['if not os.path.exists(model_dir)']
    (17, 'if not os.path.exists(model_dir):')
    (18, '    os.makedirs(model_dir)')
    (19, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Konjkov/QMC-snakemake-workflow, file=Snakefile
context_key: ["if backflow[2] != \\'00\\'"]
    (666, "        if backflow[2] != \\'00\\':")
    (667, "            terms.append(phi_term.format(number_of_phi_sets=nset + 1, phi_sets=\\'\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sorjuela/age_lesions_females, file=Snakefile
context_key: ['if len(config) == 0', 'if os.path.isfile("./config.yaml")']
    (2, 'if len(config) == 0:')
    (3, '  if os.path.isfile("./config.yaml"):')
    (4, '    configfile: "./config.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sorjuela/age_lesions_females, file=Snakefile
context_key: ['if len(config) == 0', 'else']
    (5, '  else:')
    (6, '    sys.exit("Make sure there is a config.yaml file in " + os.getcwd() + " or specify one with the --configfile commandline parameter.")')
    (7, '')
    (8, '## Make sure that all expected variables from the config file are in the config dictionary')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=elswob/neo4j-build-pipeline, file=workflow/Snakefile
context_key: ['if RELDIR in config']
    (48, '        if RELDIR in config:')
    (49, '            rels = config[RELDIR]')
    (50, '            for i in rels:')
    (51, '                o.write(f"integration rel {i}\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=elswob/neo4j-build-pipeline, file=workflow/Snakefile
context_key: ["if \\'meta_nodes\\' in db_schema"]
    (58, "            if \\'meta_nodes\\' in db_schema:")
    (59, "                nodes = db_schema[\\'meta_nodes\\']")
    (60, '                for i in nodes:')
    (61, '                    o.write(f"schema node {i}\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=elswob/neo4j-build-pipeline, file=workflow/Snakefile
context_key: ['else', "if \\'meta_rels\\' in db_schema"]
    (67, "            if \\'meta_rels\\' in db_schema:")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/cnv_sv, file=workflow/rules/common.smk
context_key: ['if not workflow.overwrite_configfiles']
    (20, 'if not workflow.overwrite_configfiles:')
    (21, '    sys.exit("At least one config file must be passed using --configfile/--configfiles, by command line or a profile!")')
    (22, '')
    (23, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zavolanlab/SCUREL, file=workflow/Snakefile
context_key: ['onstart', 'if not os.path.exists(os.path.dirname(config["cluster_log"]))']
    (50, '    if not os.path.exists(os.path.dirname(config["cluster_log"])):')
    (51, '        os.mkdir(os.path.dirname(config["cluster_log"]))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zavolanlab/SCUREL, file=workflow/Snakefile
context_key: ['onstart', 'if not os.path.exists(config["cluster_log"])']
    (52, '    if not os.path.exists(config["cluster_log"]):')
    (53, '        os.mkdir(config["cluster_log"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zavolanlab/SCUREL, file=workflow/Snakefile
context_key: ['onstart', "if not os.path.exists(\\'cellranger_count\\')"]
    (54, "    if not os.path.exists(\\'cellranger_count\\'):")
    (55, "        os.mkdir(\\'cellranger_count\\')")
    (56, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hevmarriott/DNAscanv2_snakemake, file=Snakefile
context_key: ['if reference == "hg19" or reference == "grch37"', 'if alsgenescanner == "true"']
    (64, 'if reference == "hg19" or reference == "grch37":')
    (65, '    path_expansionHunter_catalog = "resources/repeats/hg19_variant_catalog.json"')
    (66, '    path_delly_exclude_regions = "resources/delly_hg19.excl.tsv"')
    (67, '    melt_zipped_files = path_melt + "me_refs/1KGP_Hg19/*zip"')
    (68, '    melt_bed = path_melt + "add_bed_files/1KGP_Hg19/hg19.genes.bed"')
    (69, '    annovar_ref_version = "hg19"')
    (70, '    annotsv_ref_version = "GRCh37"')
    (71, '    if alsgenescanner == "true":')
    (72, '        alsgenescanner_bed = "resources/alsgenescanner/als_gene_scanner_hg19.bed"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hevmarriott/DNAscanv2_snakemake, file=Snakefile
context_key: ['if reference == "hg19" or reference == "grch37"', 'if reference == "grch37"']
    (73, '    if reference == "grch37":')
    (74, '        os.system("zcat resources/exome_hg19.bed.gz | sed \\\'s/chr//g\\\' | bgzip -c > resources/exome_grch37.bed.gz")')
    (75, '        os.system("zcat resources/hg19_gene_db.txt.gz | sed \\\'s/chr//g\\\' | bgzip -c > resources/grch37_gene_db.txt.gz")')
    (76, '        os.system("cp resources/hg19_gene_names.txt.gz resources/grch37_gene_names.txt.gz")')
    (77, '')
    (78, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hevmarriott/DNAscanv2_snakemake, file=Snakefile
context_key: ['if reference == "hg38" or reference == "grch38"', 'if alsgenescanner == "true"']
    (79, 'if reference == "hg38" or reference == "grch38":')
    (80, '    path_expansionHunter_catalog = "resources/repeats/hg38_variant_catalog.json"')
    (81, '    path_delly_exclude_regions = "resources/delly_hg38.excl.tsv"')
    (82, '    melt_zipped_files = path_melt + "me_refs/Hg38/*zip"')
    (83, '    melt_bed = path_melt + "add_bed_files/Hg38/Hg38.genes.bed"')
    (84, '    annovar_ref_version = "hg38"')
    (85, '    annotsv_ref_version = "GRCh38"')
    (86, '    if alsgenescanner == "true":')
    (87, '        alsgenescanner_bed = "resources/alsgenescanner/als_gene_scanner_hg38.bed"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hevmarriott/DNAscanv2_snakemake, file=Snakefile
context_key: ['if reference == "hg38" or reference == "grch38"', 'if reference == "grch38"']
    (88, '    if reference == "grch38":')
    (89, '        os.system("zcat resources/exome_hg38.bed.gz | sed \\\'s/chr//g\\\' | bgzip -c > resources/exome_grch38.bed.gz")')
    (90, '        os.system("zcat resources/hg38_gene_db.txt.gz | sed \\\'s/chr//g\\\' | bgzip -c > resources/grch38_gene_db.txt.gz")')
    (91, '        os.system("cp resources/hg38_gene_names.txt.gz resources/grch38_gene_names.txt.gz")')
    (92, '')
    (93, '#ADAPTING INPUT FILE FORMATS')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hevmarriott/DNAscanv2_snakemake, file=Snakefile
context_key: ['if format == "fastq" and paired == "single"']
    (94, 'if format == "fastq" and paired == "single":')
    (95, '    expand(input_dir + "{sample}.1.fq.gz", sample=sample_name)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hevmarriott/DNAscanv2_snakemake, file=Snakefile
context_key: ['if format == "fastq" and paired == "paired"']
    (96, 'if format == "fastq" and paired == "paired":')
    (97, '    expand(input_dir + "{sample}.1.fq.gz", sample=sample_name) + expand(input_dir + "{sample}.2.fq.gz", sample=sample_name)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hevmarriott/DNAscanv2_snakemake, file=Snakefile
context_key: ['if format == "sam"']
    (98, 'if format == "sam":')
    (99, '    expand(input_dir + "{sample}.sam", sample=sample_name)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hevmarriott/DNAscanv2_snakemake, file=Snakefile
context_key: ['if format == "bam"']
    (100, 'if format == "bam":')
    (101, '    expand(input_dir + "{sample}.bam", sample=sample_name)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hevmarriott/DNAscanv2_snakemake, file=Snakefile
context_key: ['if format == "cram"']
    (102, 'if format == "cram":')
    (103, '    expand(input_dir + "{sample}.cram", sample=sample_name)')
    (104, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hevmarriott/DNAscanv2_snakemake, file=Snakefile
context_key: ['if alsgenescanner == "true"']
    (105, 'if alsgenescanner == "true":')
    (106, '    alsgene_annovar_protocols = "refGene,dbnsfp33a,clinvar_20210501,intervar_20180118"')
    (107, '    alsgene_annovar_operations = "g,f,f,f"')
    (108, '    path_gene_list = ""')
    (109, '    filter_string = "false"')
    (110, '    BED = "true"')
    (111, '    annotation = "true"')
    (112, '    variantcalling = "true"')
    (113, '    SV = "true"')
    (114, '    expansion = "true"')
    (115, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hevmarriott/DNAscanv2_snakemake, file=Snakefile
context_key: ['if rm_dup == "true"', 'if exome == "true"']
    (116, 'if rm_dup == "true":')
    (117, '    if exome == "true":')
    (118, '        samblaster_cmq = "samblaster --ignoreUnmated |"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hevmarriott/DNAscanv2_snakemake, file=Snakefile
context_key: ['if rm_dup == "true"', 'else']
    (119, '    else:')
    (120, '        samblaster_cmq = "samblaster |"')
    (121, '        samblaster_bwa = "samblaster --ignoreUnmated |"')
    (122, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hevmarriott/DNAscanv2_snakemake, file=Snakefile
context_key: ['if RG']
    (128, 'if RG:')
    (129, '    rg_option_hisat2 = " --rg-id %s --rg LB:%s --rg PL:%s --rg PU:%s --rg SM:%s" % (RG_ID, RG_LB, RG_PL, RG_PU, RG_SM)')
    (130, '    rg_option_bwa = " -R \\\'@RG\\\\\\\\tID:%s\\\\\\\\tLB:%s\\\\\\\\tPL:%s\\\\\\\\tPU:%s\\\\\\\\tSM:%s\\\' " % (RG_ID, RG_LB, RG_PL, RG_PU, RG_SM)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=calliope-project/solar-and-wind-potentials, file=Snakefile
context_key: ['onsuccess', 'if "pushcut_secret" in config.keys()']
    (23, '    if "pushcut_secret" in config.keys():')
    (24, '        trigger_pushcut(event_name="snakemake_succeeded", secret=config["pushcut_secret"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=calliope-project/solar-and-wind-potentials, file=Snakefile
context_key: ['onerror', 'if "pushcut_secret" in config.keys()']
    (26, '    if "pushcut_secret" in config.keys():')
    (27, '        trigger_pushcut(event_name="snakemake_failed", secret=config["pushcut_secret"])')
    (28, '')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=robinmeyers/atac-encode-snakemake, file=Snakefile
context_key: ['if {params.use_tmpdir']
    (207, 'if {params.use_tmpdir}')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=robinmeyers/atac-encode-snakemake, file=Snakefile
context_key: ['if {params.use_tmpdir']
    (218, 'if {params.use_tmpdir}')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=robinmeyers/atac-encode-snakemake, file=Snakefile
context_key: ['if [ -e {output.metadata} ] && grep -q \\\'"status"']
    (223, 'if [ -e {output.metadata} ] && grep -q \\\'"status": "Succeeded"\\\' {output.metadata}')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PrincetonUniversity/gtex-gatk4, file=Snakefile
context_key: ["if \\'recal_bam\\' not in paths"]
    (10, "if \\'recal_bam\\' not in paths:")
    (11, "    configfile: paths[\\'base\\'] + \\'/subworkflows/GATK-bqsr.yaml\\'")
    (12, "    paths = join_config_paths(paths, config[\\'path\\'])")
    (13, '')
    (14, '# sample ids to run workflow with')
    (15, "#ids = glob_wildcards(paths[\\'fastq_R1\\'].replace(\\'{id}\\', \\'{id,[^_]+}\\')).id")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PrincetonUniversity/gtex-gatk4, file=Snakefile
context_key: ['if subworkflows is None']
    (36, 'if subworkflows is None:')
    (37, '    subworkflows = []')
    (38, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PrincetonUniversity/gtex-gatk4, file=Snakefile
context_key: ['if not os.path.exists(sub_path)']
    (45, '    if not os.path.exists(sub_path):')
    (46, '        continue')
    (47, '')
    (48, '    subw_outputs_dict[subw] = []')
    (49, '')
    (50, '    if os.path.exists(sub_config):')
    (51, '        configfile: sub_config')
    (52, "        paths = join_config_paths(paths, config[\\'path\\'])")
    (53, '')
    (54, '    include: sub_path')
    (55, '')
    (56, '    if len(subw_outputs_dict[subw]) == 0:')
    (57, '        continue')
    (58, '')
    (59, '    subw_outputs.extend(subw_outputs_dict[subw])')
    (60, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=szsctt/intvi_simulation, file=Snakefile
context_key: ["if \\'global\\' in config"]
    (9, "if \\'global\\' in config:\\t\\t")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/snv_indels, file=workflow/rules/common.smk
context_key: ['if not workflow.overwrite_configfiles']
    (19, 'if not workflow.overwrite_configfiles:')
    (20, '    sys.exit("At least one config file must be passed using --configfile/--configfiles, by command line or a profile!")')
    (21, '')
    (22, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['if not os.path.isdir(config["DATA_DIR"])']
    (10, 'if not os.path.isdir(config["DATA_DIR"]):')
    (11, '    raise Exception("Cannot find the dataset at %s" % config["DATA_DIR"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/spatial-frequency-preferences, file=Snakefile
context_key: ['if os.system("module list") == 0']
    (12, 'if os.system("module list") == 0:')
    (13, "    # then we\\'re on the cluster")
    (14, '    ON_CLUSTER = True')
    (15, '    shell.prefix("module load fsl/5.0.10; module load freesurfer/6.0.0; module load matlab/2020a; "')
    (16, '                 "export FSLOUTPUTTYPE=NIFTI_GZ; "')
    (17, '                 "export SUBJECTS_DIR=%s/derivatives/freesurfer; " % config["DATA_DIR"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if "OUTDIR" in config']
    (9, 'if "OUTDIR" in config:')
    (10, '    OUTDIR = config["OUTDIR"]')
    (11, '    workdir: config["OUTDIR"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if MKREF == "y"']
    (43, 'if MKREF == "y":')
    (44, '    GTF = config["GTF"]')
    (45, '    FASTA = config["FASTA"]')
    (46, '    REF_VERSION = config["REF_VERSION"]')
    (47, '    CR_MKREF_EXTRA = config["CR_MKREF_EXTRA"]')
    (48, '    ATTRIBUTES = config["ATTRIBUTES"]')
    (49, '    FILTER_GTF = config["FILTER_GTF"].lower()')
    (50, '')
    (51, '# Extra settings for cellranger count')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if RENAME =="n"']
    (67, 'if RENAME =="n":')
    (68, '    SAMPLES, REST, = glob_wildcards(os.path.join(DATA_DIR, "{samp}_S{rest}_R1_001.fastq.gz"))')
    (69, '# If RENAME = y, the fastq file names should be in the "{samp}_R1.fastq.gz" format')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['elif RENAME == "y"']
    (70, 'elif RENAME == "y":')
    (71, '    SAMPLES, = glob_wildcards(os.path.join(DATA_DIR, "{samp}_R1.fastq.gz"))')
    (72, '')
    (73, '# Create dictionary with sample names ')
    (74, '# If strings in SAMPLES are in a <sample>/<sample>.fastq format, the name of the directory ("sample") will be used as the sample name')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if "/" in el', 'if op[0] in samples_dict']
    (77, '    if "/" in el: ')
    (78, '    # if the string corresponds to directory/file.fastq.gz, split the string at the separator "/"')
    (79, '        op = el.split("/")')
    (80, '        # if the sample name op[0], the first element after the split, is already in the dictionary, add the ')
    (81, '        if op[0] in samples_dict:')
    (82, '            samples_dict[op[0]].append(op[1])')
    (83, '        else:')
    (84, '            samples_dict[op[0]] = [op[1]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if "/" in el', 'else']
    (85, '    else:')
    (86, '        samples_dict[el] = [el]')
    (87, '')
    (88, '')
    (89, '###### Create lists with cellranger count output ######')
    (90, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if RENAME.lower() == "y"']
    (123, '    if RENAME.lower() == "y":')
    (124, '        return(rename_files(os.path.join(os.path.abspath(OUTDIR),f"renamed_{wildcards.samples}.done")))')
    (125, '    elif RENAME.lower() == "n":')
    (126, '        return([])')
    (127, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if MKREF == "y"', 'rule edit_gtf', 'input']
    (163, 'if MKREF == "y":')
    (164, '    rule edit_gtf:')
    (165, "        \\'\\'\\'")
    (166, '        combine gene symbol and ensembl ID')
    (167, "        \\'\\'\\'")
    (168, '        input:')
    (169, '            GTF')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if MKREF == "y"', 'rule edit_gtf', 'output']
    (170, '        output:')
    (171, '            f"{Path(GTF).stem}.edited.gtf"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if MKREF == "y"', 'rule edit_gtf', 'message']
    (172, '        message:')
    (173, "            \\'Rule {rule} processing\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if MKREF == "y"', 'rule edit_gtf', 'shell']
    (174, '        shell:')
    (175, '            """')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if MKREF == "y"']
    (176, '    perl -p -e \\\'if (/gene_name/) {{s{{(gene_id\\\\s+"([^"]+).+?gene_name\\\\s+")([^"]+)}}{{$1$3_$2}}}} \\\\')
    (177, '    elsif (!/^#/ && /gene_id/) {{s/(gene_id\\\\s+"([^"]+)";\\\\s+)/$1gene_name "$2"; /}}\\\' {input} > {output}')
    (178, '            """')
    (179, '')
    (180, '    rule filter_GTF:')
    (181, '        input:')
    (182, '            rules.edit_gtf.output')
    (183, '        output:')
    (184, '            f"{Path(GTF).stem}.filtered.gtf"')
    (185, '        message:')
    (186, "            \\'Rule {rule} processing\\'")
    (187, '        params:')
    (188, '            attributes = ATTRIBUTES,')
    (189, '            cr_path = CELLRANGER_PATH')
    (190, '        shell:')
    (191, '            """')
    (192, '    {params.cr_path}/cellranger mkgtf \\\\')
    (193, '    {input} {output} {params.attributes}')
    (194, '            """')
    (195, '')
    (196, '    rule cellranger_mkref: # short run time. around 10 min')
    (197, '        input:')
    (198, '            fasta = FASTA,')
    (199, '            gtf = input_cellranger_mkref()')
    (200, '        output:')
    (201, '            outdir = directory(f"{PREFIX}_genome"),')
    (202, '            done = touch("mkref_done.txt")')
    (203, '        message:')
    (204, "            \\'Rule {rule} processing\\'")
    (205, '        params:')
    (206, '            ref_version = REF_VERSION,')
    (207, '            extra = CR_MKREF_EXTRA,')
    (208, '            cr_path = CELLRANGER_PATH')
    (209, '        shell:')
    (210, '            """')
    (211, '    {params.cr_path}/cellranger mkref \\\\')
    (212, '    --genome={output.outdir} \\\\')
    (213, '    --fasta={input.fasta} \\\\')
    (214, '    --genes={input.gtf} \\\\')
    (215, '    --nthreads=16 \\\\')
    (216, '    --ref-version={params.ref_version} \\\\')
    (217, '    {params.extra}')
    (218, '            """')
    (219, '')
    (220, '###### MAIN PIPELINE ######')
    (221, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if RENAME == "y"', 'snakefile']
    (231, 'if RENAME == "y":')
    (232, '    subworkflow rename_files:')
    (233, '        snakefile:')
    (234, '            os.path.join(workflow.basedir,"subworkflow/rename_fastqs/Snakefile")')
    (235, '        configfile:')
    (236, '            os.path.join(workflow.basedir, config_path) # use same config as used for the main snakemake file')
    (237, '        workdir:')
    (238, '            OUTDIR')
    (239, '')
    (240, '    # Path to renamed fastq files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if RENAME == "y"', 'if "OUTDIR" in config']
    (241, '    if "OUTDIR" in config:')
    (242, '        FASTQS_DIR = os.path.join(config["OUTDIR"], "1_renamed")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if RENAME == "y"', 'else']
    (243, '    else:')
    (244, '        FASTQS_DIR = "1_renamed"')
    (245, '')
    (246, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/single-cell-data-processing, file=Snakefile
context_key: ['if the cellranger count output directory was not created by cellranger count.']
    (251, 'if the cellranger count output directory was not created by cellranger count. ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JetBrains-Research/chipseq-smk-pipeline, file=rules/bam_quality_metrics.smk
context_key: ['if (mt!=0){{m0_t=m0/mt}} else {{m0_t=-1.0}}']
    (73, '        if (mt!=0){{m0_t=m0/mt}} else {{m0_t=-1.0}};')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JetBrains-Research/chipseq-smk-pipeline, file=rules/bam_quality_metrics.smk
context_key: ['if (m0!=0){{m1_0=m1/m0}} else {{m1_0=-1.0}}']
    (74, '        if (m0!=0){{m1_0=m1/m0}} else {{m1_0=-1.0}};')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JetBrains-Research/chipseq-smk-pipeline, file=rules/bam_quality_metrics.smk
context_key: ['if (m2!=0){{m1_2=m1/m2}} else {{m1_2=-1.0}}']
    (75, '        if (m2!=0){{m1_2=m1/m2}} else {{m1_2=-1.0}};')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JetBrains-Research/chipseq-smk-pipeline, file=Snakefile
context_key: ['onstart', 'if not os.path.exists(pipeline_dir)', 'if os.path.exists(src_dir)']
    (38, '        if not os.path.exists(pipeline_dir):')
    (39, '            src_dir = os.path.join(workflow.basedir , pipeline_dir)')
    (40, '            if os.path.exists(src_dir):')
    (41, '                print(f"Linking \\\'{pipeline_dir}\\\' directory:")')
    (42, '                shell(f"ln -sf {src_dir} {pipeline_dir}")')
    (43, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=JetBrains-Research/chipseq-smk-pipeline, file=Snakefile
context_key: ["if not os.path.exists(config[\\'fastq_dir\\'])"]
    (59, "if not os.path.exists(config[\\'fastq_dir\\']):")
    (60, '    raise ValueError(f"Reads directory not exists: {config[\\\'fastq_dir\\\']}")')
    (61, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=metagenome-atlas/genecatalog_atlas, file=Snakefile
context_key: ['if not "mem" in r.resources']
    (40, '    if not "mem" in r.resources:')
    (41, '        r.resources["mem"]=config["mem"]["default"]')
    (42, '    if not "time" in r.resources:')
    (43, '        r.resources["time"]=config["runtime"]["default"]')
    (44, '        ')
    (45, '        ')
    (46, '    # convert to new units')
    (47, '    r.resources["mem_mb"] = r.resources["mem"] * 1000')
    (48, '    r.resources["time_min"] = r.resources["time"] * 60')
    (49, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['if not op.isdir(config["DATA_DIR"])']
    (21, 'if not op.isdir(config["DATA_DIR"]):')
    (22, '    raise Exception("Cannot find the dataset at %s" % config["DATA_DIR"])')
    (23, "# for some reason, I can\\'t get os.system(\\'module list\\') to work")
    (24, '# properly on NYU Greene (it always returns a non-zero exit')
    (25, '# code). However, they do have the CLUSTER environmental variable')
    (26, '# defined, so we can use that')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ['if os.system("module list") == 0 or os.environ.get("CLUSTER", None)']
    (27, 'if os.system("module list") == 0 or os.environ.get("CLUSTER", None):')
    (28, "    # then we\\'re on the cluster")
    (29, '    ON_CLUSTER = True')
    (30, '    numpyro.set_host_device_count(multiprocessing.cpu_count())')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if not DATA_DIR.endswith(\\'/\\')"]
    (85, "if not DATA_DIR.endswith(\\'/\\'):")
    (86, "    DATA_DIR += \\'/\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if TEXTURE_DIR.endswith(os.sep) or TEXTURE_DIR.endswith(\\'/\\')"]
    (98, "if TEXTURE_DIR.endswith(os.sep) or TEXTURE_DIR.endswith(\\'/\\'):")
    (99, '    TEXTURE_DIR = TEXTURE_DIR[:-1]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=billbrod/foveated-metamers, file=Snakefile
context_key: ["if len(os.listdir(TEXTURE_DIR)) <= 800 and \\'textures-subset-for-testing\\' not in TEXTURE_DIR"]
    (100, "if len(os.listdir(TEXTURE_DIR)) <= 800 and \\'textures-subset-for-testing\\' not in TEXTURE_DIR:")
    (101, '    raise Exception(f"TEXTURE_DIR {TEXTURE_DIR} is incomplete!")')
    (102, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexmsalmeida/virsearch, file=Snakefile
context_key: ['if not os.path.exists(OUTPUT_DIR+"/"+sample+"/logs")']
    (16, '    if not os.path.exists(OUTPUT_DIR+"/"+sample+"/logs"):')
    (17, '        os.makedirs(OUTPUT_DIR+"/"+sample+"/logs")')
    (18, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dib-lab/sourmash-oddify, file=Snakefile
context_key: ['if name.endswith(genomes_extension)']
    (25, '        if name.endswith(genomes_extension):')
    (26, '            filename = os.path.join(root, name)')
    (27, "            if filename.startswith(\\'./\\'): filename = filename[2:]")
    (28, '            all_files.append(filename)')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['if not os.path.exists(directory)']
    (30, '    if not os.path.exists(directory): os.mkdir(directory)')
    (31, '')
    (32, '#Configure the singularity command.  All necessary paths that will be used by singularity must be provided in the --bind statement')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ["elif os.path.exists(config[\\'samples\\'])"]
    (54, "elif os.path.exists(config[\\'samples\\']):")
    (55, '    ind_num = 0')
    (56, "    with open(config[\\'samples\\'], \\'r\\') as samp_file:")
    (57, '        for ind in samp_file:')
    (58, '            if ind.strip().split()[1] in samps: ind_num += 1')
    (59, '')
    (60, '# Determine number of pairwise comparisons of individuals necessary for IBD/IBS calculation in PLINK and convert this to the number of necessary jobs')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['if num_jobs <= 1']
    (64, 'if num_jobs <= 1: # Attempt to catch an error that arises in instances where there are not that many jobs to run')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['run', "if os.path.exists(config[\\'samples\\'])"]
    (164, '        if os.path.exists(config[\\\'samples\\\']): cmd += f" --keep {config[\\\'samples\\\']}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['run', "if os.path.exists(config[\\'sites\\'])"]
    (165, '        if os.path.exists(config[\\\'sites\\\']): cmd += f" --exclude {config[\\\'sites\\\']}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['run', "if num_tested < int(config[\\'sHWE\\'][\\'test_threshold\\'])"]
    (258, "        if num_tested < int(config[\\'sHWE\\'][\\'test_threshold\\']): #If not enough SNPs will be tested, print error message and DO NOT create output file.  The documentatino of the pipeline encourages the user to adjust the parameters of the downsampling (more rounds, fewer individuals, lower threshold, etc.)")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['params', 'run', 'if not os.stat(file).st_size == 0']
    (283, '                if not os.stat(file).st_size == 0: #Make sure file is not empty')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['if entropy > max_entropy']
    (290, '                    if entropy > max_entropy: #Store current value if better than prior best result.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['if file_list']
    (319, '        if file_list: shell(f"find plink/DS/ -name \\\\\\\'{BASE}_LDprune_ctrls_DS*.bim\\\\\\\' | grep -v -w accessory/.tested_file_list.txt | xargs cat | cut -d \\\\" \\\\" -f 2 | sort -T {os.getcwd()} | uniq > accessory/sHWE_tested_snps.txt") #This command prints markers from the input .bim files IFF they were successfully tested (in our file list)')
    (320, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['run', "if num_tested < int(config[\\'sHWE\\'][\\'test_threshold\\'])"]
    (327, "        if num_tested < int(config[\\'sHWE\\'][\\'test_threshold\\']):")
    (328, '            print(f"Failed Checkpoint\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['run', "if os.path.exists(config[\\'covariate_file\\'])"]
    (415, "        if os.path.exists(config[\\'covariate_file\\']): #Not tested")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['run', 'if os.path.exists(VCF)', "if os.path.exists(config[\\'samples\\'])"]
    (431, '        if os.path.exists(VCF):')
    (432, '            cmd = f"{CMD_PREFIX} plink2 --vcf {VCF} dosage=HDS --extract plink/{BASE}_sub_mflt.bim --const-fid 0 "')
    (433, "            if os.path.exists(config[\\'samples\\']):")
    (434, '                cmd += f" --keep {config[\\\'samples\\\']} "')
    (435, '            cmd += f" --make-pgen --out input/{BASE}_sub {plnk_rsrc(rule, plink_run_specs)}; " \\\\')
    (436, '                   f"{CMD_PREFIX} plink2 --pfile input/{BASE}_sub --export vcf vcf-dosage=HDS-force --out input/{BASE}_sub;" \\\\')
    (437, '                   f"{CMD_PREFIX} bgzip input/{BASE}_sub.vcf; {CMD_PREFIX} tabix input/{BASE}_sub.vcf.gz"')
    (438, '            print(cmd)')
    (439, '            shell(cmd)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['run', 'if os.path.exists(VCF)', 'else']
    (440, '        else: print(f"Did not find VCF file at provided path: {VCF}")')
    (441, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['run', 'if f"{BASE}.admixmap.txt" in input']
    (476, '            if f"{BASE}.admixmap.txt" in input:')
    (477, '                report_line += f"admixMap_rslt=\\\\"{BASE}.admixmap.txt\\\\", global_ancestry=\\\\"{BASE}.globalancestry.txt\\\\","')
    (478, '            else:')
    (479, '                report_line += f"admixMap_rslt=\\\\"-9\\\\", global_ancestry=\\\\"-9\\\\","')
    (480, '                shell(f"touch {BASE}.admixmap.sig.txt")')
    (481, '            if f"{BASE}.dos.genesis.txt" in input: report_line += f"dosage_rslt=\\\\"{BASE}.dos.genesis.txt\\\\","')
    (482, '            else:')
    (483, '                report_line += f"dosage_rslt=\\\\"-9\\\\","')
    (484, '                shell(f"touch {BASE}.dos.genesis.sig.txt")')
    (485, '            report_line += f"config_file=\\\\"workflow/config.yml\\\\"))\\\\\\\' | R --vanilla"')
    (486, '            report_cmds.write(report_line)')
    (487, '        shell(f"{CMD_PREFIX} sh scripts/gather_report_data.sh; mv scripts/{BASE}-Mapping-report.pdf {BASE}-Mapping-report.pdf; mv input/*kinplot.png figures")')
    (488, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['run', "if os.path.exists(config[\\'annotation\\'][\\'typed_key\\'])"]
    (500, '            if os.path.exists(config[\\\'annotation\\\'][\\\'typed_key\\\']): cmd += f" -t {config[\\\'annotation\\\'][\\\'typed_key\\\']}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ['run', "if os.path.exists(config[\\'conditional_analysis\\'][\\'snp_list\\'])", 'if os.path.exists(f"conditional-analysis/{wildcards.marker}.raw")']
    (531, "        if os.path.exists(config[\\'conditional_analysis\\'][\\'snp_list\\']):")
    (532, "            if not os.path.exists(\\'conditional-analysis\\'): os.mkdir(\\'conditional-analysis\\')")
    (533, '            chrom, pos = f"{wildcards.marker}".split("-")')
    (534, '            shell(f"{CMD_PREFIX} plink --bfile plink/{BASE}_sub_mflt --snp {wildcards.marker.replace(\\\'-\\\',\\\':\\\')} --recodeAD --out conditional-analysis/{wildcards.marker} {plnk_rsrc(rule, plink_run_specs)} || true")')
    (535, '            if os.path.exists(f"conditional-analysis/{wildcards.marker}.raw"):')
    (536, '                shell(f"{CMD_PREFIX} Rscript {CODE}/genesis_conditional_analysis.R -a {chrom} -b {pos} -d {config[\\\'conditional_analysis\\\'][\\\'distance\\\']} "')
    (537, '                      f"-m conditional-analysis/{wildcards.marker}.raw "')
    (538, '                      f"-g {{input[0]}} -p {{input[1]}} -k {{input[2]}} -n {config[\\\'gwas\\\'][\\\'pc_num\\\']} -C {config[\\\'gwas\\\'][\\\'other_predictors\\\']} "')
    (539, '                      f"-o conditional-analysis/{BASE}-{wildcards.marker}.genesis.txt")')
    (540, '            else:')
    (541, '                print(f"Did not find {wildcards.marker} in plink/{BASE}_sub_mflt")')
    (542, '                shell(f"touch conditional-analysis/{BASE}-{wildcards.marker}.genesis.txt")')
    (543, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=steveped/snakemake_h3k27ac, file=workflow/rules/pairwise_comparisons.smk
context_key: ['if [[ "$TRIES" == 0 ]]; the']
    (41, '                if [[ "$TRIES" == 0 ]]; then')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=gustaveroussy/rna-count-kallisto, file=Snakefile
context_key: ['if sys.version_info < (3, 7)']
    (4, 'if sys.version_info < (3, 7):\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cshlwyang/RNAseq, file=rules/diffexp.smk
context_key: ['if "strandedness" in units.columns']
    (1, '    if "strandedness" in units.columns:')
    (2, '        return units["strandedness"].tolist()')
    (3, '    else:')
    (4, '        strand_list=["none"]')
    (5, '        return strand_list*units.shape[0]')
    (6, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=johapark/pacqproc, file=Snakefile
context_key: ['if "destripe" in config[\\\'processes\\\']']
    (20, 'if "destripe" in config[\\\'processes\\\']:')
    (21, '    targets.append(expand("{channel}_destriped/.done", channel=config[\\\'channels\\\']))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=johapark/pacqproc, file=Snakefile
context_key: ['if "stitch" in config[\\\'processes\\\']']
    (22, 'if "stitch" in config[\\\'processes\\\']:')
    (23, "    config[\\'master_channel\\'] = config[\\'channels\\'][config[\\'master_channel_index\\']]")
    (24, "    config[\\'non_master_channels\\'] = set(config[\\'channels\\']) - set([config[\\'master_channel\\']])")
    (25, '    targets.append(expand("{channel}" + destripe_flag + "/displacement.xml", channel=config[\\\'channels\\\']))')
    (26, '    targets.append(expand("{channel}" + destripe_flag + "/displproj.xml", channel=config[\\\'channels\\\']))')
    (27, '    targets.append(expand("{channel}" + stitch_flag + "/.done", channel=config[\\\'channels\\\']))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=johapark/pacqproc, file=Snakefile
context_key: ['if "precomputed" in config[\\\'processes\\\']']
    (28, 'if "precomputed" in config[\\\'processes\\\']:')
    (29, '    targets.append(expand("{label}.precomputed/.done", label=config[\\\'labels\\\']))')
    (30, '')
    (31, '')
    (32, '# Other settings')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PNNL-CompBio/perseq, file=Snakefile
context_key: ['if fasta_chunk']
    (515, '            if fasta_chunk:')
    (516, '                fasta_chunk.close()')
    (517, '')
    (518, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MerrimanLab/variant_calling_pipeline, file=vcf.snake
context_key: ['if wildcards.build == "b37"']
    (1, '    if wildcards.build == "b37":')
    (2, '        return ["-L {chr}".format(chr=chr) for chr in CHRS]')
    (3, '    else:')
    (4, '        return ["-L chr{chr}".format(chr=chr) for chr in CHRS]')
    (5, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=taylorreiter/2022-infant-mge, file=Snakefile
context_key: ['resources', 'if (substr($0, 1, 1)==">") {{filename=(substr($0,2) ".fa")}']
    (397, '        if (substr($0, 1, 1)==">") {{filename=(substr($0,2) ".fa")}}')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bayraktar1/RPCA, file=workflow/Snakefile
context_key: ['if len(READS) != len(set(READS))']
    (30, 'if len(READS) != len(set(READS)):')
    (31, '    raise RuntimeError("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jkrapohl/SnakeWRAP, file=Snakefile
context_key: ['if len(config) == 0', 'if os.path.isfile("./config.yaml")']
    (2, 'if len(config) == 0:')
    (3, '  if os.path.isfile("./config.yaml"):')
    (4, '    configfile: "./config.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jkrapohl/SnakeWRAP, file=Snakefile
context_key: ['if len(config) == 0', 'else']
    (5, '  else:')
    (6, '    sys.exit("Make sure there is a config.yaml file in " + os.getcwd() + " or specify one with the --configfile commandline parameter.")')
    (7, '')
    (8, '## Make sure that all expected variables from the config file are in the config dictionary')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexmsalmeida/metamap, file=Snakefile
context_key: ['if not os.path.exists(OUTPUT_DIR+"/mapping/"+sample+"/logs")']
    (23, '    if not os.path.exists(OUTPUT_DIR+"/mapping/"+sample+"/logs"):')
    (24, '        os.makedirs(OUTPUT_DIR+"/mapping/"+sample+"/logs")')
    (25, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=alexmsalmeida/metamap, file=Snakefile
context_key: ['if not os.path.exists(OUTPUT_DIR+"/summary/logs")']
    (26, 'if not os.path.exists(OUTPUT_DIR+"/summary/logs"):')
    (27, '    os.makedirs(OUTPUT_DIR+"/summary/logs")')
    (28, '')
    (29, '# rule that specifies the final expected output files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=qbicsoftware-archive/exomseq, file=Snakefile
context_key: ['if sha == orig_sha']
    (161, '            if sha == orig_sha:')
    (162, '                touch(str(output)+"OK")')
    (163, '            else:')
    (164, '                touch(str(output)+"FAILED")')
    (165, '        except Exception:')
    (166, '            pass')
    (167, '')
    (168, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/lathe, file=Snakefile
context_key: ["if len(items) == 1 or items[0] == \\'Sample\\' or items[0] == \\'#Sample\\' or items[0].startswith(\\'#\\')"]
    (27, "        if len(items) == 1 or items[0] == \\'Sample\\' or items[0] == \\'#Sample\\' or items[0].startswith(\\'#\\'):")
    (28, '            continue')
    (29, '        sample = items[0]')
    (30, '        # if this sample is already basecalled')
    (31, '        if items[1].endswith(".fq") or items[1].endswith(".fastq"):')
    (32, '            fq_files_dict[sample] = items[1]')
    (33, '            fast5_files_dict[sample] = ""')
    (34, '')
    (35, '        # Otherwise we assume its a fast5 directory')
    (36, '        else:')
    (37, '            # find fast5 files. These are expected to be below the directory provided in the config.')
    (38, "            # All fast5\\'s below this directory will be processed.")
    (39, '            # list provided directory recursively')
    (40, "            fast5_files_dict[sample] = glob.glob(os.path.join(items[1], \\'**\\', \\'*.fast5\\'), recursive=True)")
    (41, '            fq_files_dict[sample] = os.path.join(sample, "0.basecall", sample + ".fq")')
    (42, '            #create a dictionary which relates fast5 basenames to the full path of each file')
    (43, '            for f in fast5_files_dict[sample]:')
    (44, '                fast5_basename_to_path[os.path.splitext(os.path.basename(f))[0]] = f')
    (45, '')
    (46, '        # if items is length 3, then we have a SR dataset as well')
    (47, '        if len(items)==3:')
    (48, "            split_reads = items[2].split(\\',\\')")
    (49, "            # don\\'t think I need to check this as single end should also be supported")
    (50, '            # if len(split_reads) != 2:')
    (51, '                # sys.exit("Short reads specifiied must be length 2 and separated by a comma. You gave: " + items[2])')
    (52, '            sr_polish_dict[sample] = split_reads')
    (53, '        else:')
    (54, "            sr_polish_dict[sample] = \\'\\'")
    (55, '')
    (56, '')
    (57, '# print(fq_files_dict)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/lathe, file=Snakefile
context_key: ['onstart', 'if stat != ""']
    (75, '    if stat != "":')
    (76, "        print(\\'WARNING: Differences to latest version detected. Please reset changes and/or pull repo.\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/lathe, file=Snakefile
context_key: ['if len(span_out) == 3']
    (694, '            if len(span_out) == 3:')
    (695, '                extend = span_out[1].strip()')
    (696, '                extend_cmd = \\\'samtools faidx \\\' + input[3] + \\\' \\\' + extend + " | grep -v \\\'>\\\'" + " >> " + params.outfa + "\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/Mykrobe_tb_workflow, file=Snakefile
context_key: ['if IS_MULTIPLEXED']
    (25, 'if IS_MULTIPLEXED:')
    (26, '    SAMPLES = barcode_parser(config["barcodes"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iqbal-lab-org/Mykrobe_tb_workflow, file=Snakefile
context_key: ['if IS_MULTIPLEXED']
    (40, 'if IS_MULTIPLEXED:')
    (41, '')
    (42, '    include: str(RULES_DIR / "demux.smk")')
    (43, '')
    (44, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RossLab/snakemake-cluster-template, file=Snakefile
context_key: ['if cluster_script == None', 'else', 'rule all', 'rule help', 'rule get_data', 'rule get_results']
    (4, 'if cluster_script == None :')
    (5, '\\tcluster_script = ""')
    (6, 'else :')
    (7, '\\tcluster_script = "scripts/use_local.sh "')
    (8, '')
    (9, 'localrules: all, help, get_results')
    (10, '')
    (11, '### rules for calling')
    (12, '## all')
    (13, 'rule all :')
    (14, '\\tinput : "data/final_result.txt"')
    (15, '')
    (16, '##')
    (17, '## help : print this help')
    (18, 'rule help :')
    (19, '\\tshell :')
    (20, '\\t\\t"sed -n \\\'s/^##//p\\\' Snakefile"')
    (21, '')
    (22, 'rule get_data :')
    (23, '\\tthreads : 1')
    (24, '\\toutput : "data/{sp}.txt"')
    (25, '\\tshell : "scripts/get_data.sh {wildcards.sp} > {output}"')
    (26, '')
    (27, 'rule get_results :')
    (28, '\\tthreads : 1')
    (29, '\\tinput : "data/hummingbird.txt", "data/mealybug.txt"')
    (30, '\\toutput : "data/final_result.txt"')
    (31, '\\tshell : "wc -l {input} 1> {output}"')
    (32, "'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=switt4/confounder-snakemake, file=Snakefile
context_key: ['if not os.path.isdir(config["BIDS_DIR"])']
    (9, 'if not os.path.isdir(config["BIDS_DIR"]):')
    (10, '    raise Exception("Cannot find the dataset at %s" % config["BIDS_DIR"])')
    (11, '')
    (12, "# can also override at command-line with e.g.:  --config bids_dir=\\'path/to/dir\\'  or --configfile ...")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=switt4/confounder-snakemake, file=Snakefile
context_key: ['if len(sessions) > 0']
    (57, 'if len(sessions) > 0:')
    (58, "    subj_sess_dir = join(\\'sub-{subject}\\',\\'ses-{session}\\')")
    (59, "    subj_sess_prefix = \\'sub-{subject}_ses-{session}\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CarolinaPB/calculate-gene-abundance, file=Snakefile
context_key: ['if "OUTDIR" in config']
    (22, 'if "OUTDIR" in config:')
    (23, '    workdir: config["OUTDIR"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kelly-sovacool/meta-repo, file=Snakefile
context_key: ["if \\'token\\' in config and config[\\'token\\']"]
    (5, "if \\'token\\' in config and config[\\'token\\']:")
    (6, "    token = config[\\'token\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kelly-sovacool/meta-repo, file=Snakefile
context_key: ['if delta_days > 0']
    (18, '    if delta_days > 0:')
    (19, '        Path(date_filename).touch()')
    (20, '')
    (21, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ctmrbio/stag-mwc, file=rules/taxonomic_profiling/strainphlan.smk
context_key: ['if not spa_config["clade_of_interest"]']
    (20, '    if not spa_config["clade_of_interest"]:')
    (21, '        available_clades=f"{OUTDIR}/strainphlan/available_clades.txt",')
    (22, '        all_outputs.append(available_clades)')
    (23, '        user_messages.warn("Clade of interest not specified in strainphlan section of config.yaml.")')
    (24, '        user_messages.warn("Based on your samples strainphlan will create a list of available clades in output/strainphlan/available_clades.txt")')
    (25, '        user_messages.warn("If you still want to run strainphlan, please update config.yaml e.g. \\\\"clade_of_interest: s__Bifidobacterium_longum\\\\".")')
    (26, '    if spa_config["clade_of_interest"]:')
    (27, '        spa_alignment=f"{OUTDIR}/strainphlan/{spa_config[\\\'clade_of_interest\\\']}.StrainPhlAn3_concatenated.aln",')
    (28, '        spa_tree=f"{OUTDIR}/strainphlan/RAxML_bestTree.{spa_config[\\\'clade_of_interest\\\']}.StrainPhlAn3.tre",')
    (29, '        all_outputs.append(spa_alignment)')
    (30, '        all_outputs.append(spa_tree)')
    (31, '        user_messages.info("If strainphlan failed, ensure your clade_of_interest is present in output/strainphlan/available_clades.txt.")')
    (32, '        citations.add(publications["MetaPhlAn"])')
    (33, '        citations.add(publications["StrainPhlAn"])')
    (34, '')
    (35, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ctmrbio/stag-mwc, file=Snakefile
context_key: ['if len(SAMPLES) < 1']
    (48, '    if len(SAMPLES) < 1:')
    (49, '        raise WorkflowError("Found no samples! Check input file pattern and path in config.yaml")')
    (50, '    else:')
    (51, '        print(f"Found the following samples in inputdir using input filename pattern \\\'{config[\\\'input_fn_pattern\\\']}\\\':\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ctmrbio/stag-mwc, file=Snakefile
context_key: ['if citations_link.exists()']
    (178, '        if citations_link.exists():')
    (179, '            Path("citations.rst").unlink()')
    (180, '        Path("citations.rst").symlink_to(citation_filename)')
    (181, '')
    (182, '        shell("{snakemake_call} --unlock".format(snakemake_call=argv[0]))')
    (183, '        shell("{snakemake_call} --report {report}-{datetime}.html".format(')
    (184, '            snakemake_call=argv[0],')
    (185, '            report=config["report"],')
    (186, '            datetime=report_datetime,')
    (187, '            )')
    (188, '        )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ctmrbio/stag-mwc, file=rules/taxonomic_profiling/kraken2.smk
context_key: ['if kraken2_config["filter_bracken"]["include"] or kraken2_config["filter_bracken"]["exclude"]']
    (273, '    if kraken2_config["filter_bracken"]["include"] or kraken2_config["filter_bracken"]["exclude"]:')
    (274, '        filtered_brackens = expand(str(OUTDIR/"kraken2/{sample}.{level}.filtered.bracken"), sample=SAMPLES, level=kraken2_config["bracken"]["levels"].split())')
    (275, '        all_table = expand(str(OUTDIR/"kraken2/all_samples.{level}.bracken.txt"), level=kraken2_config["bracken"]["levels"].split())')
    (276, '        all_table_filtered = expand(str(OUTDIR/"kraken2/all_samples.{level}.filtered.bracken.txt"), level=kraken2_config["bracken"]["levels"].split())')
    (277, '')
    (278, '        all_outputs.extend(filtered_brackens)')
    (279, '        all_outputs.append(all_table)')
    (280, '        all_outputs.append(all_table_filtered)')
    (281, '')
    (282, '    citations.add(publications["Bracken"])')
    (283, '')
    (284, '    brackens = expand(str(OUTDIR/"kraken2/{sample}.{level}.bracken"), sample=SAMPLES, level=kraken2_config["bracken"]["levels"].split())')
    (285, '    brackens_mpa_style = expand(str(OUTDIR/"kraken2/{sample}.bracken.mpa_style.txt"), sample=SAMPLES)')
    (286, '    bracken_area_plot = str(OUTDIR/"kraken2/area_plot.bracken.pdf")')
    (287, '    bracken_krona = str(OUTDIR/"kraken2/all_samples.bracken.krona.html")')
    (288, '    all_table_mpa = str(OUTDIR/"kraken2/all_samples.bracken.mpa_style.txt")')
    (289, '')
    (290, '    all_outputs.extend(brackens)')
    (291, '    all_outputs.extend(brackens_mpa_style)')
    (292, '    all_outputs.append(bracken_krona)')
    (293, '    all_outputs.append(all_table_mpa)')
    (294, '    all_outputs.append(bracken_area_plot)')
    (295, '')
    (296, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ctmrbio/stag-mwc, file=rules/preproc/host_removal.smk
context_key: ['if rh_config["keep_fastq"]']
    (23, '    if rh_config["keep_fastq"]:')
    (24, '        all_outputs.extend(filtered_host)')
    (25, '    all_outputs.append(host_proportions)')
    (26, '')
    (27, '    citations.add(publications["Kraken2"])')
    (28, '')
    (29, '    localrules:')
    (30, '        plot_proportion_host')
    (31, '')
    (32, '    rule remove_host:')
    (33, '        """Filter reads matching host database using Kraken2."""')
    (34, '        input:')
    (35, '            read1=OUTDIR/"fastp/{sample}_1.fq.gz",')
    (36, '            read2=OUTDIR/"fastp/{sample}_2.fq.gz",')
    (37, '        output:')
    (38, '            read1=OUTDIR/"host_removal/{sample}_1.fq.gz" if rh_config["keep_fastq"] else temp(OUTDIR/"host_removal/{sample}_1.fq.gz"),')
    (39, '            read2=OUTDIR/"host_removal/{sample}_2.fq.gz" if rh_config["keep_fastq"] else temp(OUTDIR/"host_removal/{sample}_2.fq.gz"),')
    (40, '            host1=OUTDIR/"host_removal/{sample}.host_1.fq.gz" if rh_config["keep_host_fastq"] else temp(OUTDIR/"host_removal/{sample}.host_1.fq.gz"),')
    (41, '            host2=OUTDIR/"host_removal/{sample}.host_2.fq.gz" if rh_config["keep_host_fastq"] else temp(OUTDIR/"host_removal/{sample}.host_2.fq.gz"),')
    (42, '            kraken=OUTDIR/"host_removal/{sample}.kraken" if rh_config["keep_kraken"] else temp(OUTDIR/"host_removal/{sample}.kraken"),')
    (43, '            kreport=OUTDIR/"host_removal/{sample}.kraken" if rh_config["keep_kreport"] else temp(OUTDIR/"host_removal/{sample}.kreport"),')
    (44, '        log:')
    (45, '            stderr=str(LOGDIR/"host_removal/{sample}.kraken2.log"),')
    (46, '        shadow:')
    (47, '            "shallow"')
    (48, '        conda:')
    (49, '            "../../envs/stag-mwc.yaml"')
    (50, '        singularity:')
    (51, '            "oras://ghcr.io/ctmrbio/stag-mwc:stag-mwc"+singularity_branch_tag')
    (52, '        threads:')
    (53, '            cluster_config["remove_host"]["n"] if "remove_host" in cluster_config else 8')
    (54, '        params:')
    (55, '            db=rh_config["db_path"],')
    (56, '            confidence=rh_config["confidence"],')
    (57, '            extra=rh_config["extra"],')
    (58, '            classified=lambda w: f"{OUTDIR}/host_removal/{w.sample}.host#.fq",')
    (59, '            unclassified=lambda w: f"{OUTDIR}/host_removal/{w.sample}#.fq",')
    (60, '            fq_to_compress=lambda w: f"{OUTDIR}/host_removal/{w.sample}*.fq",')
    (61, '        shell:')
    (62, '            """')
    (63, '            kraken2 \\\\')
    (64, '                --db {params.db} \\\\')
    (65, '                --threads {threads} \\\\')
    (66, '                --output {output.kraken} \\\\')
    (67, '                --classified-out {params.classified} \\\\')
    (68, '                --unclassified-out {params.unclassified} \\\\')
    (69, '                --report  {output.kreport} \\\\')
    (70, '                --paired \\\\')
    (71, '                --confidence {params.confidence} \\\\')
    (72, '                {params.extra} \\\\')
    (73, '                {input.read1} {input.read2} \\\\')
    (74, '                2> {log.stderr}')
    (75, '            pigz \\\\')
    (76, '                --processes {threads} \\\\')
    (77, '                --verbose \\\\')
    (78, '                --force \\\\')
    (79, '                {params.fq_to_compress} \\\\')
    (80, '                2>> {log.stderr}')
    (81, '            """')
    (82, '')
    (83, '')
    (84, '    rule plot_proportion_host:')
    (85, '        """Plot proportion of reads that matched the host DB."""')
    (86, '        input:')
    (87, '            expand(str(LOGDIR/"host_removal/{sample}.kraken2.log"), sample=SAMPLES)')
    (88, '        output:')
    (89, '            histogram=report(OUTDIR/"host_removal/host_histogram.pdf",')
    (90, '                       category="Preprocessing",')
    (91, '                       caption="../../report/host_histogram.rst"),')
    (92, '            barplot=report(OUTDIR/"host_removal/host_barplot.pdf",')
    (93, '                       category="Preprocessing",')
    (94, '                       caption="../../report/host_barplot.rst"),')
    (95, '            txt=report(OUTDIR/"host_removal/host_proportions.txt",')
    (96, '                       category="Preprocessing",')
    (97, '                       caption="../../report/host_proportions.rst"),')
    (98, '        log:')
    (99, '            str(LOGDIR/"host_removal/proportion_host.log")')
    (100, '        shadow:')
    (101, '            "shallow"')
    (102, '        conda:')
    (103, '            "../../envs/stag-mwc.yaml"')
    (104, '        singularity:')
    (105, '            "oras://ghcr.io/ctmrbio/stag-mwc:stag-mwc"+singularity_branch_tag')
    (106, '        threads:')
    (107, '            1')
    (108, '        shell:')
    (109, '            """')
    (110, '            scripts/plot_proportion_kraken2.py \\\\')
    (111, '                {input} \\\\')
    (112, '                --histogram {output.histogram} \\\\')
    (113, '                --barplot {output.barplot} \\\\')
    (114, '                --table {output.txt} \\\\')
    (115, '                2>&1 > {log}')
    (116, '            """')
    (117, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ctmrbio/stag-mwc, file=rules/mappers/bbmap.smk
context_key: ['if bbmap_config["counts_table"]["annotations"]', 'if not Path(bbmap_config["counts_table"]["annotations"]).exists()']
    (36, '        if bbmap_config["counts_table"]["annotations"]:')
    (37, '            if not Path(bbmap_config["counts_table"]["annotations"]).exists():')
    (38, '                err_message = "BBMap counts table annotations not found at: \\\'{}\\\'\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ctmrbio/stag-mwc, file=rules/mappers/bbmap.smk
context_key: ['if bbmap_config["featureCounts"]["annotations"]']
    (45, '        if bbmap_config["featureCounts"]["annotations"]:')
    (46, '            if not Path(bbmap_config["featureCounts"]["annotations"]).exists():')
    (47, '                err_message = "BBMap featureCounts annotations not found at: \\\'{}\\\'\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ctmrbio/stag-mwc, file=rules/antibiotic_resistance/amrplusplus.smk
context_key: ['if not Path(amrplusplus_config["megares"]["annotation"]).exists()']
    (23, '    if not Path(amrplusplus_config["megares"]["annotation"]).exists():')
    (24, '        err_message = "No annotations exists at: \\\'{}\\\' !\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Leoberium/pyIPSA, file=workflow/rules/junctions.smk
context_key: ['input', 'run', 'if line.startswith("-")']
    (116, '                    if line.startswith("-"):')
    (117, '                        break')
    (118, '                    left, right = line.strip().split(": ")')
    (119, '                    d[left].append(right)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vibaotram/baseDmux, file=baseDmux/data/Snakefile
context_key: ['if nasID and not os.path.isdir(ssh_dir)']
    (268, '        fi')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbg-ethz/V-pipe, file=resources/auxiliary_workflows/benchmark/workflow/Snakefile
context_key: ['if not line.startswith(group_prefix)']
    (41, '            if not line.startswith(group_prefix):')
    (42, '                continue')
    (43, '')
    (44, '            group = line[len(group_prefix) :].strip()')
    (45, '            break')
    (46, '')
    (47, '    if group == "local":')
    (48, '        method_list_local.append(method)')
    (49, '    elif group == "global":')
    (50, '        method_list_global.append(method)')
    (51, '    else:')
    (52, '        raise RuntimeError(f"Invalid group \\\'{group}\\\' for method \\\'{method}\\\'")')
    (53, '')
    (54, '')
    (55, '# misc setup')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cbg-ethz/V-pipe, file=resources/auxiliary_workflows/benchmark/workflow/Snakefile
context_key: ['if sequencing_mode.startswith("amplicon")']
    (57, 'if sequencing_mode.startswith("amplicon"):')
    (58, '    sequencing_mode = sequencing_mode.split(":")[')
    (59, '        0')
    (60, '    ]  # TODO: Allow different sequencing_modes in parameter file')
    (61, '')
    (62, '')
    (63, '# helper functions')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=Snakefile
context_key: ['if len(config) == 0', 'if os.path.isfile("./config.yaml")']
    (9, 'if len(config) == 0:')
    (10, '    if os.path.isfile("./config.yaml"):')
    (11, '')
    (12, '        configfile: "./config.yaml"')
    (13, '')
    (14, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=Snakefile
context_key: ['if len(config) == 0', 'else']
    (15, '    else:')
    (16, '        sys.exit(')
    (17, '            f"Make sure there is a config.yaml file in {os.getcwd()} or specify one with the --configfile commandline parameter."')
    (18, '        )')
    (19, '')
    (20, '')
    (21, '## Conditional parameters check')
    (22, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=Snakefile
context_key: ['else max(math.trunc(CORES / 3 - 1), 1']
    (61, '    else max(math.trunc(CORES / 3 - 1), 1)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=Snakefile
context_key: ['if (not config["POLYPLOID_ONLY"]) and (not config["DIPLOID_ONLY"])', 'if n_samples_p1 < 2']
    (90, 'if (not config["POLYPLOID_ONLY"]) and (not config["DIPLOID_ONLY"]):')
    (91, '    if n_samples_p1 < 2:')
    (92, '        sys.exit(')
    (93, '            "There seems to be fewer than two samples for your parent1 species, please have at least two samples in your metadata file or make sure the metadata file has the correct formatting (tab-separated columns)"')
    (94, '        )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=Snakefile
context_key: ['if (not config["POLYPLOID_ONLY"]) and (not config["DIPLOID_ONLY"])', 'elif n_samples_p2 < 2']
    (95, '    elif n_samples_p2 < 2:')
    (96, '        sys.exit(')
    (97, '            "There seems to be fewer than two samples for your parent2 species, please have at least two samples in your metadata file or make sure the metadata file has the correct formatting (tab-separated columns)"')
    (98, '        )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=Snakefile
context_key: ['if (not config["POLYPLOID_ONLY"]) and (not config["DIPLOID_ONLY"])', 'elif n_samples_allo < 2']
    (99, '    elif n_samples_allo < 2:')
    (100, '        sys.exit(')
    (101, '            "There seems to be fewer than two samples for your allopolyploid species, please have at least two samples in your metadata file or make sure the metadata file has the correct formatting (tab-separated columns)"')
    (102, '        )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tjbencomo/ngs-pipeline, file=rules/common.smk
context_key: ['if patients.shape[0] != patients.unique().shape[0]']
    (28, 'if patients.shape[0] != patients.unique().shape[0]:')
    (29, '    raise ValueError(f"Duplicate patients present in {config[\\\'patients\\\']}. Remove duplicates!")')
    (30, '')
    (31, '# Logs')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tjbencomo/ngs-pipeline, file=rules/common.smk
context_key: ["if mutect_flags == \\'None\\'"]
    (53, "if mutect_flags == \\'None\\':")
    (54, "    mutect_flags = \\'\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tjbencomo/ngs-pipeline, file=rules/common.smk
context_key: ["if filter_flags == \\'None\\'"]
    (55, "if filter_flags == \\'None\\':")
    (56, "    filter_flags = \\'\\'")
    (57, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tjbencomo/ngs-pipeline, file=rules/common.smk
context_key: ['if tumor_only']
    (58, 'if tumor_only:')
    (59, "    sample_types = [\\'tumor\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tjbencomo/ngs-pipeline, file=rules/common.smk
context_key: ['if use_pon is False']
    (66, 'if use_pon is False:')
    (67, "    pon_vcf = \\'null\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tjbencomo/ngs-pipeline, file=rules/common.smk
context_key: ["if tmp_dir == \\'None\\'"]
    (78, "if tmp_dir == \\'None\\':")
    (79, "    tmp_dir = \\'null\\'")
    (80, '')
    (81, '# Annotations')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_metaassembly
context_key: ['if chrom == prev_chrom', 'if int(start)-int(prev_end)>800000']
    (40, '                if chrom == prev_chrom:')
    (41, '                    # Note that if multiple contigs align to the same reference ')
    (42, '                    # region, this may be negative')
    (43, '                    if int(start)-int(prev_end)>800000:')
    (44, '                        f_out.write(prev_line)')
    (45, '                        f_out.write(line)')
    (46, '                        f_gap.write("\\\\t".join([chrom,prev_end,start,str(int(start)-int(prev_end))])+"\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_metaassembly
context_key: ['if line[']
    (93, '                if line[:5] == "chrom":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_metaassembly
context_key: ['if chrom == centro_chrom and centro_start<=start \\']
    (105, '                    if chrom == centro_chrom and centro_start<=start \\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_metaassembly
context_key: ['if chrom == align_chrom and \\']
    (119, '                    if chrom == align_chrom and \\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_metaassembly
context_key: ['if not s[4] == "complete"']
    (188, '                if not s[4] == "complete":')
    (189, '                    continue')
    (190, '                complete_genes[s[0]] = s')
    (191, '        # Go over the base assembly genes')
    (192, '        with open(input[0],"r") as f_in, open(output[0],"w") as f_out:')
    (193, '            header = ["gene","v2_contig","v2_start","v2_end","wtdbg2_contig",\\\\')
    (194, '                      "v2_start","v2_end"]')
    (195, '            f_out.write("\\\\t".join(header)+"\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_metaassembly
context_key: ['if line[']
    (199, '                if line[:2] in ["ID","=="]:')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_metaassembly
context_key: ['if s[4] == "complete"']
    (205, '                if s[4] == "complete":')
    (206, '                    continue')
    (207, '                # Genes that are not complete in EGYPTREFV2 are also skipped')
    (208, '                if not gene in complete_genes:')
    (209, '                    continue')
    (210, '                # Check if this partial gene is likely "missing something in')
    (211, '                # the center", if not, skip it; this is the case if at least ')
    (212, '                # two regions are aligned')
    (213, '                if not "," in s[5]:')
    (214, '                    continue')
    (215, '                aligned_regions = s[5].split(",")')
    (216, '                start_contig,start_region = aligned_regions[0].split(":")')
    (217, '                end_contig,end_region = aligned_regions[-1].split(":")')
    (218, "                # If start and end contig are not the same, we also don\\'t")
    (219, '                # want to replace this gene')
    (220, '                if not start_contig == end_contig:')
    (221, '                    print("Start and end not on same contig!")')
    (222, '                    continue')
    (223, '                start = start_region.split("-")[0]')
    (224, '                end = end_region.split("-")[1]')
    (225, '                v2_contig,region = complete_genes[gene][-1].split(":")')
    (226, '                v2_start,v2_end = region.split("-")')
    (227, '                info = [gene,start_contig,start,end,v2_contig,v2_start,v2_end]')
    (228, '                f_out.write("\\\\t".join(info)+"\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_metaassembly
context_key: ['if (int(wtdbg2_start)<int(wtdbg2_end) and \\']
    (269, '                if (int(wtdbg2_start)<int(wtdbg2_end) and \\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_metaassembly
context_key: ['else', 'if not wtdbg2_contig in egyptrefwtdbg2_coord']
    (278, '                if not wtdbg2_contig in egyptrefwtdbg2_coord:')
    (279, '                    egyptrefwtdbg2_coord[wtdbg2_contig] = [[left_pos_wtdbg2,right_pos_wtdbg2]]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_metaassembly
context_key: ['else', 'if not contig in egyptrefwtdbg2_coord']
    (292, '                if not contig in egyptrefwtdbg2_coord:')
    (293, "                    new_record = SeqRecord(egyptrefwtdbg2[contig],contig, \\'\\', \\'\\')")
    (294, '                    SeqIO.write(new_record, f_out, "fasta")')
    (295, '                    #print("Wrote "+contig+" Len: "+str(len(egyptrefwtdbg2[contig])))')
    (296, '                    continue')
    (297, '                #print(egyptrefwtdbg2_coord[contig])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_metaassembly
context_key: ['else', 'if last_coord>first_coord']
    (305, '                    if last_coord>first_coord:')
    (306, '                        print("Overlappling genes!")')
    (307, '                        continue')
    (308, '                    # Add base seq between v2 gene seqs to be added')
    (309, '                    new_seq += egyptrefwtdbg2[contig][last_coord:first_coord]')
    (310, '                    # Add gene seq to be added')
    (311, '                    # Check if the start and end sequences match')
    (312, '                    f_replaced.write("Add: "+str(egyptrefv2_geneseq["\\\\t".join([contig,str(coord[0]),str(coord[1])])])[:20]+"\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if not new_disease_id in disease_ids']
    (55, '                if not new_disease_id in disease_ids:')
    (56, '                    f_out.write("\\\\t".join([s[34],s[35],new_disease_id])+"\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if os.path.exists("gwas/results/custom_disease_ids.txt")']
    (66, 'if os.path.exists("gwas/results/custom_disease_ids.txt"):')
    (67, '    with open("gwas/results/custom_disease_ids.txt","r") as f_in:')
    (68, '        for line in f_in:')
    (69, '            # Skip header and skip lines without mapped trait')
    (70, '            if line[:6] == "MAPPED" or line == "\\\\t\\\\t\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if os.path.exists("gwas/results/num_recurrentloci_per_disease.txt")']
    (282, 'if os.path.exists("gwas/results/num_recurrentloci_per_disease.txt"):')
    (283, '    with open("gwas/results/num_recurrentloci_per_disease.txt","r") as f_in:')
    (284, '        for line in f_in:')
    (285, '            s = line.strip("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if not s[1] == "0"']
    (287, '            if not s[1] == "0":')
    (288, '                GWAS_CATALOG_DISEASES_LOCI.append(s[0])')
    (289, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if os.path.exists("gwas/results/num_recurrentlocivcf_per_disease.txt")']
    (307, 'if os.path.exists("gwas/results/num_recurrentlocivcf_per_disease.txt"):')
    (308, '    with open("gwas/results/num_recurrentlocivcf_per_disease.txt","r") as f_in:')
    (309, '        for line in f_in:')
    (310, '            s = line.strip("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if not s[1] in ["0","1"]']
    (312, '            if not s[1] in ["0","1"]:')
    (313, '                GWAS_CATALOG_DISEASES_LOCI_VCF.append(s[0])')
    (314, '')
    (315, '# Compute the LD between any pair of SNPs')
    (316, '# Output the LDs less than 0.8 ')
    (317, '# Compute only for SNPs less than 1MB apart from each other')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if os.path.exists("gwas/results/num_recurrentlocild_per_disease.txt")']
    (346, 'if os.path.exists("gwas/results/num_recurrentlocild_per_disease.txt"):')
    (347, '    with open("gwas/results/num_recurrentlocild_per_disease.txt","r") as f_in:')
    (348, '        for line in f_in:')
    (349, '            s = line.strip("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if not s[1] == "1"']
    (353, '            if not s[1] == "1":')
    (354, '                GWAS_CATALOG_DISEASES_LOCI_LD.append(s[0])')
    (355, '')
    (356, '# The idea is to keep one association per loci, where we define a locus as')
    (357, '# a region of 1MB with a tag association (this means we miss independent')
    (358, '# association signals that are closer than 1 MB from each other)')
    (359, '# We automatically select the association based on the number of occurrences ')
    (360, '# per rsid position plus this positions must be in LD>0.8 with at least one')
    (361, '# other association')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if line[']
    (519, '                if line[:5] == "CHROM":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if chrom[']
    (528, '                if chrom[:3] == "chr":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if not loc in eur_af']
    (540, '                if not loc in eur_af:')
    (541, '                    f_out_multi.write(loc+"\\\\t"+"\\\\t".join(egp_af)+"\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if os.path.exists("gwas/tag_snps/tag_snppos_egyptians.txt")']
    (600, 'if os.path.exists("gwas/tag_snps/tag_snppos_egyptians.txt"):')
    (601, '    with open("gwas/tag_snps/tag_snppos_egyptians.txt","r") as f_in:')
    (602, '        for line in f_in:')
    (603, '            TAG_SNPPOS.append(line.strip("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if line[']
    (750, '                if line[:5] == "CHROM":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if chrom+"\\\\t"+pos+"\\\\tEUR" in num_proxies']
    (758, '                if chrom+"\\\\t"+pos+"\\\\tEUR" in num_proxies:')
    (759, '                    num_eur = num_proxies[chrom+"\\\\t"+pos+"\\\\tEUR"]')
    (760, '                    proxy_sharing = num_proxyoverlap[chrom+"\\\\t"+pos]')
    (761, '                else:')
    (762, '                    num_eur = "NA"')
    (763, '                    proxy_sharing = "NA\\\\tNA\\\\tNA"')
    (764, '                if chrom+"\\\\t"+pos+"\\\\tEGP" in num_proxies:')
    (765, '                    num_egp = num_proxies[chrom+"\\\\t"+pos+"\\\\tEGP"]')
    (766, '                else:')
    (767, '                    num_egp = "NA"')
    (768, '                f_out.write(line.strip("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if line[']
    (831, '                        if line[:5] == "CHROM":')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if not chrom in [str(x) for x in range(23)]']
    (918, '                if not chrom in [str(x) for x in range(23)]:')
    (919, '                    continue')
    (920, '                # Skip GWAS catalog associations without position specified')
    (921, '                if s[12] == "" or " x " in s[12] or ";" in s[12]:')
    (922, '                    continue')
    (923, '                pos = int(s[12])')
    (924, '                for variant in pop_spec[chrom]:')
    (925, '                    if pos-dist <= variant[1] <= pos+dist:')
    (926, '                        f_out.write(line.strip("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_gwas
context_key: ['if not chrom in [str(x) for x in range(23)]']
    (996, '                if not chrom in [str(x) for x in range(23)]:')
    (997, '                    continue')
    (998, '                # Skip GWAS catalog associations without position specified')
    (999, '                if s[12] == "" or " x " in s[12] or ";" in s[12]:')
    (1000, '                    continue')
    (1001, '                pos = int(s[12])')
    (1002, '                # For insertions, e.g., there are not SVs on all chromosomes')
    (1003, '                if not chrom in svs:')
    (1004, '                    continue')
    (1005, '                for variant in svs[chrom]:')
    (1006, '                    if pos-dist <= variant[1] <= pos+dist:')
    (1007, '                        f_out.write(line.strip("\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_nui
context_key: ['if not len(s) == 5 or "Excluding internal overlap" in linea']
    (67, '                        if not len(s) == 5 or "Excluding internal overlap" in linea:')
    (68, '                            continue')
    (69, '                        info = " ".join(s[0].strip().split(" ")[:-2])')
    (70, '                        #print(info)')
    (71, '                        chrom_start, chrom_end = [int(x) for x in s[0].strip().split(" ")[-2:]]')
    (72, '                        #print(chrom_start)')
    (73, '                        #print(chrom_end)')
    (74, '                        ctg_start, ctg_end = [int(x) for x in s[1].strip().split(" ")]')
    (75, '                        #print(ctg_start)')
    (76, '                        #print(ctg_end)')
    (77, '                        len_chrom, len_ctg = [int(x) for x in s[2].strip().split(" ")]')
    (78, '                        assert(len_chrom == abs(chrom_end-chrom_start)+1)')
    (79, '                        assert(len_ctg == abs(ctg_end-ctg_start)+1)')
    (80, '                        ident = float(s[3].strip())')
    (81, '                        #print(ident)')
    (82, '                        chrom, ctg = s[4].strip().split(" ")')
    (83, '                        #print(chrom)')
    (84, '                        #print(ctg)')
    (85, '                        if ctg == ctg_nui and min(ctg_start,ctg_end)<start_nui<max(ctg_start,ctg_end) or \\\\')
    (86, '                                              min(ctg_start,ctg_end)<ctg_start<stop_nui<max(ctg_start,ctg_end):')
    (87, '                            f_out.write(linea)')
    (88, '                        ')
    (89, "# Get for every sample the number of reads that couldn\\'t be mapped using the ")
    (90, '# reference genome and the GATK bundle sequences')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_nui
context_key: ["if line_snv[0] == \\'#\\'"]
    (187, "                        if line_snv[0] == \\'#\\':")
    (188, '                            continue')
    (189, '                        s = line_snv.split("\\\\t")')
    (190, '                        ctg = s[0]')
    (191, '                        pos = s[1]')
    (192, '                        ref = s[3]')
    (193, '                        alt = s[4]')
    (194, '                        qual = s[5]')
    (195, '                        ac,an = s[7].split(";")[10:12]')
    (196, '                        ac_string,ac = ac.split("=")')
    (197, '                        an_string,an = an.split("=")')
    (198, '                        if not ac_string == "AC" or not an_string=="AN":')
    (199, '                            ac,an = s[7].split(";")[9:11]')
    (200, '                            ac_string,ac = ac.split("=")')
    (201, '                            an_string,an = an.split("=")')
    (202, '                        assert(ac_string == "AC")')
    (203, '                        assert(an_string == "AN")')
    (204, '                        if ctg == nui_ctg and int(nui_start)<=int(pos)<=int(nui_end):')
    (205, '                            out_string = [nui_ctg,nui_start,nui_end]')
    (206, '                            out_string += [ctg,pos,ref,alt,qual,ac,an,"SNV"]')
    (207, '                            f_out.write("\\\\t".join(out_string)+"\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=iwohlers/lied_egypt_genome, file=Snakefile_nui
context_key: ["if line_indel[0] == \\'#\\'"]
    (211, "                        if line_indel[0] == \\'#\\':")
    (212, '                            continue')
    (213, '                        s = line_indel.split("\\\\t")')
    (214, '                        ctg = s[0]')
    (215, '                        pos = s[1]')
    (216, '                        ref = s[3]')
    (217, '                        alt = s[4]')
    (218, '                        qual = s[5]')
    (219, '                        ac,an = s[7].split(";")[10:12]')
    (220, '                        ac = ac.split("=")[1]')
    (221, '                        an = an.split("=")[1]')
    (222, '                        if ctg == nui_ctg and int(nui_start)<=int(pos)<=int(nui_end):')
    (223, '                            out_string = [nui_ctg,nui_start,nui_end]')
    (224, '                            out_string += [ctg,pos,ref,alt,qual,ac,an,"INDEL"]')
    (225, '                            f_out.write("\\\\t".join(out_string)+"\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akhanf/structural_connectome_smk, file=Snakefile
context_key: ['if len(sessions) > 0']
    (32, 'if len(sessions) > 0:')
    (33, "    subj_sess_dir = join(\\'sub-{subject}\\',\\'ses-{session}\\')")
    (34, "    subj_sess_prefix = \\'sub-{subject}_ses-{session}\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AAFC-BICoE/snakemake-partial-genome-pipeline, file=Snakefile
context_key: ['if new_basename[0].isdigit()']
    (18, "    if new_basename[0].isdigit(): # Phyluce doesn\\'t work with samples starting with digits")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imcsegmentation, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'if len(fol_path_spillover_slide_acs) > 0']
    (117, 'if do_compensation:')
    (118, '    if len(file_path_orig_sm) == 0:')
    (119, '        if len(fol_path_spillover_slide_acs) > 0:')
    (120, "            spillover_mode = \\'estimate_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imcsegmentation, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'else']
    (121, '        else:')
    (122, '            raise ValueError(\\\'Either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (123, "                             \\'needs to be provided.\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imcsegmentation, file=workflow/Snakefile
context_key: ['if do_compensation', 'elif len(fol_path_spillover_slide_acs) == 0']
    (124, '    elif len(fol_path_spillover_slide_acs) == 0:')
    (125, "            spillover_mode = \\'copy_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imcsegmentation, file=workflow/Snakefile
context_key: ['if do_compensation', 'else']
    (126, '    else:')
    (127, '        raise ValueError(\\\'Only provide either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (128, "                         \\'but not both.\\')")
    (129, '')
    (130, '# Produce a list of all cellprofiler output files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imcsegmentation, file=workflow/Snakefile
context_key: ['if not do_compensation']
    (267, 'if not do_compensation:')
    (268, "    config_dict_cp[\\'measuremasks\\'] = {")
    (269, '        \\\'message\\\':  """\\\\')
    (270, '                    Measure masks ')
    (271, '                    ')
    (272, '                    Measure segmentation masks')
    (273, '                    """,')
    (274, "        \\'run_size\\': 1,")
    (275, "        \\'plugins\\': cp_plugins,")
    (276, "        \\'pipeline\\': \\'resources/cp4_pipelines/adapted/3_measure_mask_basic.cppipe\\',")
    (277, "        \\'input_files\\': [")
    (278, '            # Folder containing segmentation masks')
    (279, '            fol_path_cellmasks,')
    (280, '            # Folder containing image stacks to measure')
    (281, '            fol_path_full,')
    (282, '            # Folder containing cell probabilities')
    (283, '            fol_path_probabilities,')
    (284, '            # Acquisition metadata')
    (285, '            file_path_acmeta,')
    (286, '            # Channel metadata')
    (287, '            file_path_channelmeta_full,')
    (288, '            file_path_channelmeta_cellprobab')
    (289, '        ],')
    (290, "        \\'output_patterns\\': {\\'.\\':")
    (291, '                            # Folder containing cellprofiler measurement')
    (292, '                                cp_measurements_output_files,')
    (293, '                            # Subfolder containing segmentation masks used')
    (294, '                            # for the measurements')
    (295, "                            \\'masks\\': directory(fol_path_cp_masks),")
    (296, '                            # Subfolder containing the images used for the')
    (297, '                            # measurements.')
    (298, "                            \\'images\\': directory(fol_path_cp_images)},")
    (299, "        \\'input_folder\\': fol_path_cp_input")
    (300, '    }')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imcsegmentation, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'params']
    (454, 'if do_compensation:')
    (455, '    if spillover_mode == "estimate_sm":')
    (456, '        rule get_ss_panel_from_panel:')
    (457, '            input: csv_panel')
    (458, '            output: file_path_ss_panel')
    (459, '            params:')
    (460, '                col_metal=csv_panel_metal,')
    (461, '                col_spillover=csv_panel_spillover')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yplakaka/imcsegmentation, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'run']
    (462, '            run:')
    (463, '                dat_csv = pd.read_csv(csv_panel)')
    (464, '                assert params.col_spillover in dat_csv.columns, ValueError(')
    (465, '                    f"The panel csv {csv_panel} should contain a boolean column"')
    (466, '                    f"{params.col_spillover} indicating the metals used for the"')
    (467, '                    f"spillover acquisitions.")')
    (468, '                metals = dat_csv.query(')
    (469, "                    f\\'{params.col_spillover} == 1\\')[params.col_metal].values")
    (470, "                with open(output[0], \\'w\\') as f:")
    (471, '                    for m in metals:')
    (472, "                        f.write(m+\\'\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kcivkulis/stag-mwc, file=Snakefile
context_key: ['if len(SAMPLES) < 1']
    (42, 'if len(SAMPLES) < 1:')
    (43, '    raise WorkflowError("Found no samples! Check input file pattern and path in config.yaml")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kcivkulis/stag-mwc, file=Snakefile
context_key: ['if citations_link.exists()']
    (157, '        if citations_link.exists():')
    (158, '            Path("citations.rst").unlink()')
    (159, '        Path("citations.rst").symlink_to(citation_filename)')
    (160, '')
    (161, '        shell("{snakemake_call} --unlock".format(snakemake_call=argv[0]))')
    (162, '        shell("{snakemake_call} --report {report}-{datetime}.html".format(')
    (163, '            snakemake_call=argv[0],')
    (164, '            report=config["report"],')
    (165, '            datetime=report_datetime,')
    (166, '            )')
    (167, '        )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kcivkulis/stag-mwc, file=rules/taxonomic_profiling/kraken2.smk
context_key: ['if kraken2_config["filter_bracken"]["include"] or kraken2_config["filter_bracken"]["exclude"]']
    (271, '    if kraken2_config["filter_bracken"]["include"] or kraken2_config["filter_bracken"]["exclude"]:')
    (272, '        filtered_brackens = expand(str(OUTDIR/"kraken2/{sample}.{level}.filtered.bracken"), sample=SAMPLES, level=kraken2_config["bracken"]["levels"].split())')
    (273, '        all_table = expand(str(OUTDIR/"kraken2/all_samples.{level}.bracken.txt"), level=kraken2_config["bracken"]["levels"].split())')
    (274, '        all_table_filtered = expand(str(OUTDIR/"kraken2/all_samples.{level}.filtered.bracken.txt"), level=kraken2_config["bracken"]["levels"].split())')
    (275, '')
    (276, '        all_outputs.extend(filtered_brackens)')
    (277, '        all_outputs.append(all_table)')
    (278, '        all_outputs.append(all_table_filtered)')
    (279, '')
    (280, '    citations.add(publications["Bracken"])')
    (281, '')
    (282, '    brackens = expand(str(OUTDIR/"kraken2/{sample}.{level}.bracken"), sample=SAMPLES, level=kraken2_config["bracken"]["levels"].split())')
    (283, '    brackens_mpa_style = expand(str(OUTDIR/"kraken2/{sample}.bracken.mpa_style.txt"), sample=SAMPLES)')
    (284, '    bracken_area_plot = str(OUTDIR/"kraken2/area_plot.bracken.pdf")')
    (285, '    bracken_krona = str(OUTDIR/"kraken2/all_samples.bracken.krona.html")')
    (286, '    all_table_mpa = str(OUTDIR/"kraken2/all_samples.bracken.mpa_style.txt")')
    (287, '')
    (288, '    all_outputs.extend(brackens)')
    (289, '    all_outputs.extend(brackens_mpa_style)')
    (290, '    all_outputs.append(bracken_krona)')
    (291, '    all_outputs.append(all_table_mpa)')
    (292, '    all_outputs.append(bracken_area_plot)')
    (293, '')
    (294, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kcivkulis/stag-mwc, file=rules/mappers/bbmap.smk
context_key: ['if bbmap_config["counts_table"]["annotations"]', 'if not Path(bbmap_config["counts_table"]["annotations"]).exists()']
    (36, '        if bbmap_config["counts_table"]["annotations"]:')
    (37, '            if not Path(bbmap_config["counts_table"]["annotations"]).exists():')
    (38, '                err_message = "BBMap counts table annotations not found at: \\\'{}\\\'\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kcivkulis/stag-mwc, file=rules/mappers/bbmap.smk
context_key: ['if bbmap_config["featureCounts"]["annotations"]']
    (45, '        if bbmap_config["featureCounts"]["annotations"]:')
    (46, '            if not Path(bbmap_config["featureCounts"]["annotations"]).exists():')
    (47, '                err_message = "BBMap featureCounts annotations not found at: \\\'{}\\\'\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hmgu-itg/single-point-analysis-pipeline, file=workflow/rules/meta-analysis.smk
context_key: ['if [ -f "{output.missnp}"']
    (113, '        if [ -f "{output.missnp}" ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=osvaldoreisss/polya-seq_workflow_analysis, file=workflow/rules/filter.smk
context_key: ['if pvalue < 0.05']
    (639, '                if pvalue < 0.05:')
    (640, '                    significance_list.append(True)')
    (641, '                else:')
    (642, '                    significance_list.append(False)')
    (643, '')
    (644, '        labels = list(data.columns)')
    (645, '        x_pos = np.arange(len(labels))')
    (646, '        means = list(data.mean())')
    (647, '        stds = list(data.std())')
    (648, '        print(labels)')
    (649, '        print(means)')
    (650, '        print(stds)')
    (651, '')
    (652, '')
    (653, '        plt.rcParams["figure.figsize"] = [6.8, 6.8]')
    (654, '        plt.rcParams["figure.autolayout"] = True')
    (655, '        print(plt.rcParams["figure.figsize"])')
    (656, '')
    (657, '')
    (658, '        fig = plt.figure()')
    (659, '        ax = plt.gca()')
    (660, '')
    (661, '        bars = ax.bar(')
    (662, '            x_pos,')
    (663, '            means,')
    (664, '            yerr=stds,')
    (665, '            align="center",')
    (666, '            alpha=0.5,')
    (667, '            ecolor="black",')
    (668, '            capsize=10,')
    (669, '            color="#0072b2",')
    (670, '        )')
    (671, '        ax.bar_label(bars, fmt="%.1f%%")')
    (672, '        ax.set_ylabel("% of transcripts")')
    (673, '        ax.set_xticks(x_pos)')
    (674, '        ax.set_xticklabels(labels)')
    (675, '        ax.set_title("Premature Polyadenylation in different conditions")')
    (676, '        plt.xticks(rotation=90)')
    (677, '')
    (678, '        for p, sig in zip(ax.patches, significance_list):')
    (679, '            if sig == True:')
    (680, '                ax.annotate(')
    (681, '                    "*",')
    (682, '                    (p.get_x() + p.get_width() / 2.0, p.get_height() + 0.7),')
    (683, '                    ha="center",')
    (684, '                    va="center",')
    (685, '                    fontsize=11,')
    (686, '                    color="black",')
    (687, '                    rotation=90,')
    (688, '                    xytext=(0, 10),')
    (689, '                    textcoords="offset points",')
    (690, '                )')
    (691, '')
    (692, '        bottom, top = plt.ylim()')
    (693, '        plt.ylim(bottom, top + 1)')
    (694, '        plt.tight_layout()')
    (695, '        ax.figure.savefig(output[0], transparent=True)')
    (696, '        ax.figure.savefig(output[1], transparent=True)')
    (697, '        ax.figure.savefig(output[2], transparent=True)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/RiboReport, file=Snakefile
context_key: ['onstart', 'if not os.path.exists("logs")']
    (11, '   if not os.path.exists("logs"):')
    (12, '     os.makedirs("logs")')
    (13, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RickGelhausen/RiboReport, file=Snakefile
context_key: ['if "RIBO" not in samples["method"].unique()']
    (19, 'if "RIBO" not in samples["method"].unique():')
    (20, '    hasRIBO=False')
    (21, '    print("No Ribo-seq libraries were detected. No prediction tools for this setup are currently implemented. If you have pure Ribo-seq libraries, please use the method tag RIBO. Continuing...")')
    (22, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=laelbarlow/amoebae, file=workflow/Snakefile
context_key: ["if f.endswith(\\'.fna\\') or f.endswith(\\'.faa\\') or f.endswith(\\'.gff3\\')"]
    (52, "    if f.endswith(\\'.fna\\') or f.endswith(\\'.faa\\') or f.endswith(\\'.gff3\\'):")
    (53, '        # Check that input file name is in the list from the CSV file.')
    (54, '        assert os.path.basename(f) in add_to_dbs_params.keys(),\\\\')
    (55, '        """Error: File with filename %s is not listed in CSV file %s.""" % \\\\')
    (56, '        (os.path.basename(f), database_csv)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=laelbarlow/amoebae, file=workflow/Snakefile
context_key: ["if f.endswith(\\'.faa\\') or f.endswith(\\'.afaa\\')"]
    (58, "    if f.endswith(\\'.faa\\') or f.endswith(\\'.afaa\\'):")
    (59, '        # Check that file is listed in input CSV file.')
    (60, '        assert os.path.basename(f) in single_fasta_query_names_all + multi_fasta_query_names_all,\\\\')
    (61, '        """Error: File with filename %s is not listed in CSV file %s.""" % \\\\')
    (62, '        (os.path.basename(f), query_csv)')
    (63, '')
    (64, '# Copy template data directory from the resources directory to the results')
    (65, "# directory if it doesn\\'t already exist.")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=laelbarlow/amoebae, file=workflow/Snakefile
context_key: ["if not os.path.isdir(\\'results/AMOEBAE_Data\\')"]
    (66, "if not os.path.isdir(\\'results/AMOEBAE_Data\\'):")
    (67, "    shutil.copytree(\\'resources/AMOEBAE_Data_template_directory\\', \\'results/AMOEBAE_Data\\')")
    (68, '')
    (69, '# Define rules to be run on the head/login node of a cluster as opposed to')
    (70, '# being submitted as jobs via the job scheduler.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=euronion/snakemake-demo, file=workflow/Snakefile
context_key: ['if True', 'rule greet_giessen', 'input']
    (5, 'if True:')
    (6, '')
    (7, '    rule greet_giessen:')
    (8, '        input:')
    (9, '            city="data/giessen.txt",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=euronion/snakemake-demo, file=workflow/Snakefile
context_key: ['if True', 'rule greet_giessen', 'output']
    (10, '        output:')
    (11, '            city="results/greetings_giessen.txt",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=euronion/snakemake-demo, file=workflow/Snakefile
context_key: ['if True', 'rule greet_giessen', 'script']
    (12, '        script:')
    (13, '            "scripts/greet_city.py"')
    (14, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=euronion/snakemake-demo, file=workflow/Snakefile
context_key: ['if True', 'rule greet_berlin', 'input']
    (15, '    rule greet_berlin:')
    (16, '        input:')
    (17, '            city="data/berlin.txt",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=euronion/snakemake-demo, file=workflow/Snakefile
context_key: ['if True', 'rule greet_berlin', 'output']
    (18, '        output:')
    (19, '            city="results/berlin.txt",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=euronion/snakemake-demo, file=workflow/Snakefile
context_key: ['if True', 'rule greet_berlin', 'script']
    (20, '        script:')
    (21, '            "scripts/greet_city.py"')
    (22, '')
    (23, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/qc, file=workflow/rules/common.smk
context_key: ['if not workflow.overwrite_configfiles']
    (17, 'if not workflow.overwrite_configfiles:')
    (18, '    sys.exit("At least one config file must be passed using --configfile/--configfiles, by command line or a profile!")')
    (19, '')
    (20, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vdjonsson/rna-seq-pizzly, file=workflow/rules/requant.smk
context_key: ["if fasta.endswith(\\'.gz\\')"]
    (1, "    if fasta.endswith(\\'.gz\\'):")
    (2, '        return "<(zcat {})".format(fasta)')
    (3, '    else:')
    (4, '        return fasta')
    (5, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hydra-genetics/annotation, file=workflow/rules/common.smk
context_key: ['if not workflow.overwrite_configfiles']
    (17, 'if not workflow.overwrite_configfiles:')
    (18, '    sys.exit("At least one config file must be passed using --configfile/--configfiles, by command line or a profile!")')
    (19, '')
    (20, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AAFC-BICoE/snakemake-amplicon-metagenomics, file=Snakefile
context_key: ['if not os.path.isabs(config["initial_input_dir"])', 'if not os.path.isabs(config["workdir"])']
    (26, 'if not os.path.isabs(config["initial_input_dir"]):')
    (27, '    INPUTDIR=join(config["workdir"],config["initial_input_dir"])')
    (28, '    if not os.path.isabs(config["workdir"]):')
    (29, '        INPUTDIR=join(os.getcwd(),config["initial_input_dir"])')
    (30, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AAFC-BICoE/snakemake-amplicon-metagenomics, file=Snakefile
context_key: ['if f.endswith(config["input_file_forward_postfix"]']
    (34, '    if f.endswith(config["input_file_forward_postfix"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AAFC-BICoE/snakemake-amplicon-metagenomics, file=Snakefile
context_key: ['if f.endswith(config["input_file_extension"]']
    (40, '    if f.endswith(config["input_file_extension"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AAFC-BICoE/snakemake-amplicon-metagenomics, file=Snakefile
context_key: ['if not os.path.isabs(config["reference_fasta"])', 'if not os.path.isabs(config["workdir"])']
    (48, 'if not os.path.isabs(config["reference_fasta"]):')
    (49, '    if not os.path.isabs(config["workdir"]):')
    (50, '        config["reference_fasta"]=join(os.getcwd(),config["reference_fasta"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AAFC-BICoE/snakemake-amplicon-metagenomics, file=Snakefile
context_key: ['if not os.path.isabs(config["reference_fasta"])', 'else']
    (51, '    else:')
    (52, '        config["reference_fasta"]=join(config["workdir"],config["reference_fasta"])')
    (53, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CalebPecka/ATAC-Seq-Pipeline, file=workflow/rules/BCrank.smk
context_key: ['if perfectConfig == "TRUE"', 'else', 'rule BCrank']
    (4, 'if perfectConfig == "TRUE":')
    (5, '\\tcondaLocation="../envs/R.yaml"')
    (6, 'else:')
    (7, '\\tcondaLocation="../envs/Rconfig.yaml"')
    (8, '')
    (9, 'rule BCrank:')
    (10, '\\tinput:')
    (11, '\\t\\tseq="../results/{sample}.results/upstreamPeak_Sequences.fasta"')
    (12, '\\toutput:')
    (13, '\\t\\tdirectory("../results/{sample}.results/BCrankOutput/")')
    (14, '\\tparams:')
    (15, '\\t\\tseedSize=config["seedSize"],')
    (16, '\\t\\trestartSize=config["restartSize"]')
    (17, '\\tconda:')
    (18, '\\t\\tcondaLocation')
    (19, '\\tlog:')
    (20, '\\t\\t"logs/{sample}.BCrank.log"')
    (21, '\\tshell:')
    (22, '\\t\\t"mkdir -p {output} ;"')
    (23, '\\t\\t"Rscript scripts/BCrank.R {params.seedSize} {params.restartSize} {input.seq} ../results/{wildcards.sample}.results/BCrankOutput/"')
    (24, "'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CalebPecka/ATAC-Seq-Pipeline, file=workflow/rules/NarrowPeakSummitTracker.smk
context_key: ['if perfectConfig == "TRUE"', 'else', 'rule NarrowPeakSummitTracker']
    (4, 'if perfectConfig == "TRUE":')
    (5, '\\tcondaLocation="../envs/R.yaml"')
    (6, 'else:')
    (7, '\\tcondaLocation="../envs/Rconfig.yaml"')
    (8, '')
    (9, 'rule NarrowPeakSummitTracker:')
    (10, '\\tinput:')
    (11, '\\t\\tNApeak="../results/{sample}.results/NA_peaks.narrowPeak",')
    (12, '\\t\\tgtf=config["gtf"]')
    (13, '\\toutput:')
    (14, '\\t\\t"../results/{sample}.results/upstreamPeaks.tsv"')
    (15, '\\tconda:')
    (16, '\\t\\tcondaLocation')
    (17, '\\tlog:')
    (18, '\\t\\t"logs/{sample}.MACS2.log"')
    (19, '\\tshell:')
    (20, '\\t\\t"Rscript scripts/NarrowPeakSummitTracker.R {input.gtf} {input.NApeak} ../results/{wildcards.sample}.results/upstreamPeaks.tsv"')
    (21, "'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CalebPecka/ATAC-Seq-Pipeline, file=workflow/rules/BCrankProcessing.smk
context_key: ['if perfectConfig == "TRUE"', 'else', 'rule BCrankProcessing']
    (4, 'if perfectConfig == "TRUE":')
    (5, '\\tcondaLocation="../envs/R.yaml"')
    (6, 'else:')
    (7, '\\tcondaLocation="../envs/Rconfig.yaml"')
    (8, '')
    (9, 'rule BCrankProcessing:')
    (10, '\\tinput:')
    (11, '\\t\\tdirectory="../results/{sample}.results/BCrankOutput/",')
    (12, '\\t\\tseq="../results/{sample}.results/upstreamPeak_Sequences.fasta",')
    (13, '\\t\\tNApeak="../results/{sample}.results/NA_peaks.narrowPeak"')
    (14, '\\toutput:')
    (15, '\\t\\t"../results/{sample}.results/matchingSiteTable.csv"')
    (16, '\\tparams:')
    (17, '\\t\\tseedSize=config["seedSize"]')
    (18, '\\tconda:')
    (19, '\\t\\tcondaLocation')
    (20, '\\tlog:')
    (21, '\\t\\t"logs/{sample}.BCrankProcessing.log"')
    (22, '\\tshell:')
    (23, '\\t\\t"Rscript scripts/BCrankProcessing.R {params.seedSize} {input.directory} {input.seq} {input.NApeak} ../results/{wildcards.sample}.results/matchingSiteTable.csv"')
    (24, "'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CalebPecka/ATAC-Seq-Pipeline, file=workflow/rules/MACS2.smk
context_key: ['if perfectConfig == "TRUE"', 'else', 'rule MACS2']
    (4, 'if perfectConfig == "TRUE":')
    (5, '\\tcondaLocation="../envs/macs.yaml"')
    (6, 'else:')
    (7, '\\tcondaLocation="../envs/macsConfig.yaml"')
    (8, '')
    (9, 'rule MACS2:')
    (10, '\\tinput:')
    (11, '\\t\\tbam=expand("{sample}", sample=config["samples"])')
    (12, '\\toutput:')
    (13, '\\t\\tNApeak="../results/{sample}.results/NA_peaks.narrowPeak"')
    (14, '\\tparams:')
    (15, '\\t\\tdirName="../results/{sample}.results/",')
    (16, '\\t\\tgenomeType=config["genomeType"],')
    (17, '\\t\\tpeakFileType=config["peakFileType"],')
    (18, '\\t\\tpeakPVal=config["peakPVal"],')
    (19, '\\t\\tseed="0"')
    (20, '\\tconda:')
    (21, '\\t\\t"../envs/macs.yaml"')
    (22, '\\tlog:')
    (23, '\\t\\t"logs/{sample}.MACS2.log"')
    (24, '\\tshell:')
    (25, '\\t\\t"macs2 callpeak -t /{sample} -g {params.genomeType} -f {params.peakFileType} -p {params.peakPVal} --seed {params.seed} --outdir {params.dirName}"')
    (26, "'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CalebPecka/ATAC-Seq-Pipeline, file=workflow/rules/SummitFASTA.smk
context_key: ['if perfectConfig == "TRUE"', 'else', 'rule CreateRefGenome', 'rule SummitFASTA']
    (4, 'if perfectConfig == "TRUE":')
    (5, '\\tcondaLocation="../envs/R.yaml"')
    (6, 'else:')
    (7, '\\tcondaLocation="../envs/Rconfig.yaml"')
    (8, '')
    (9, 'rule CreateRefGenome:')
    (10, '\\toutput:')
    (11, '\\t\\t"../config/HG38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"')
    (12, '\\tlog:')
    (13, '\\t\\t"logs/CreateRefGenome.log"')
    (14, '\\tshell:')
    (15, '\\t\\t"curl -LJO ftp://ftp.ncbi.nlm.nih.gov/genomes/archive/old_genbank/Eukaryotes/vertebrates_mammals/Homo_sapiens/GRCh38/seqs_for_alignment_pipelines/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz > {output}"')
    (16, '')
    (17, 'rule SummitFASTA:')
    (18, '\\tinput:')
    (19, '\\t\\trefGenome={rules.CreateRefGenome.output},')
    (20, '\\t\\tupstreamPeaks="../results/{sample}.results/upstreamPeaks.tsv"')
    (21, '\\toutput:')
    (22, '\\t\\t"../results/{sample}.results/upstreamPeak_Sequences.fasta"')
    (23, '\\tparams:')
    (24, '\\t\\tmotifSize=config["motifSize"]')
    (25, '\\tconda:')
    (26, '\\t\\tcondaLocation')
    (27, '\\tlog:')
    (28, '\\t\\t"logs/{sample}.SummitFASTA.log"')
    (29, '\\tshell:')
    (30, '\\t\\t"Rscript scripts/SummitFASTA.R {input.refGenome} {input.upstreamPeaks} {params.motifSize} ../results/{wildcards.sample}.results/upstreamPeak_Sequences.fasta"')
    (31, "'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=BodenmillerGroup/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'if len(fol_path_spillover_slide_acs) > 0']
    (117, 'if do_compensation:')
    (118, '    if len(file_path_orig_sm) == 0:')
    (119, '        if len(fol_path_spillover_slide_acs) > 0:')
    (120, "            spillover_mode = \\'estimate_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=BodenmillerGroup/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'if len(file_path_orig_sm) == 0', 'else']
    (121, '        else:')
    (122, '            raise ValueError(\\\'Either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (123, "                             \\'needs to be provided.\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=BodenmillerGroup/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'elif len(fol_path_spillover_slide_acs) == 0']
    (124, '    elif len(fol_path_spillover_slide_acs) == 0:')
    (125, "            spillover_mode = \\'copy_sm\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=BodenmillerGroup/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'else']
    (126, '    else:')
    (127, '        raise ValueError(\\\'Only provide either "folder_spillover_slide_acs" or "fn_spillover_matrix\\\'')
    (128, "                         \\'but not both.\\')")
    (129, '')
    (130, '# Produce a list of all cellprofiler output files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=BodenmillerGroup/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if not do_compensation']
    (267, 'if not do_compensation:')
    (268, "    config_dict_cp[\\'measuremasks\\'] = {")
    (269, '        \\\'message\\\':  """\\\\')
    (270, '                    Measure masks ')
    (271, '                    ')
    (272, '                    Measure segmentation masks')
    (273, '                    """,')
    (274, "        \\'run_size\\': 1,")
    (275, "        \\'plugins\\': cp_plugins,")
    (276, "        \\'pipeline\\': \\'resources/cp4_pipelines/adapted/3_measure_mask_basic.cppipe\\',")
    (277, "        \\'input_files\\': [")
    (278, '            # Folder containing segmentation masks')
    (279, '            fol_path_cellmasks,')
    (280, '            # Folder containing image stacks to measure')
    (281, '            fol_path_full,')
    (282, '            # Folder containing cell probabilities')
    (283, '            fol_path_probabilities,')
    (284, '            # Acquisition metadata')
    (285, '            file_path_acmeta,')
    (286, '            # Channel metadata')
    (287, '            file_path_channelmeta_full,')
    (288, '            file_path_channelmeta_cellprobab')
    (289, '        ],')
    (290, "        \\'output_patterns\\': {\\'.\\':")
    (291, '                            # Folder containing cellprofiler measurement')
    (292, '                                cp_measurements_output_files,')
    (293, '                            # Subfolder containing segmentation masks used')
    (294, '                            # for the measurements')
    (295, "                            \\'masks\\': directory(fol_path_cp_masks),")
    (296, '                            # Subfolder containing the images used for the')
    (297, '                            # measurements.')
    (298, "                            \\'images\\': directory(fol_path_cp_images)},")
    (299, "        \\'input_folder\\': fol_path_cp_input")
    (300, '    }')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=BodenmillerGroup/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'params']
    (454, 'if do_compensation:')
    (455, '    if spillover_mode == "estimate_sm":')
    (456, '        rule get_ss_panel_from_panel:')
    (457, '            input: csv_panel')
    (458, '            output: file_path_ss_panel')
    (459, '            params:')
    (460, '                col_metal=csv_panel_metal,')
    (461, '                col_spillover=csv_panel_spillover')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=BodenmillerGroup/ImcSegmentationSnakemake, file=workflow/Snakefile
context_key: ['if do_compensation', 'if spillover_mode == "estimate_sm"', 'rule get_ss_panel_from_panel', 'run']
    (462, '            run:')
    (463, '                dat_csv = pd.read_csv(csv_panel)')
    (464, '                assert params.col_spillover in dat_csv.columns, ValueError(')
    (465, '                    f"The panel csv {csv_panel} should contain a boolean column"')
    (466, '                    f"{params.col_spillover} indicating the metals used for the"')
    (467, '                    f"spillover acquisitions.")')
    (468, '                metals = dat_csv.query(')
    (469, "                    f\\'{params.col_spillover} == 1\\')[params.col_metal].values")
    (470, "                with open(output[0], \\'w\\') as f:")
    (471, '                    for m in metals:')
    (472, "                        f.write(m+\\'\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Martlan97/BI11a-Ap, file=workflow/Snakefile
context_key: ['if e.errno != errno.EEXIST']
    (6, '    if e.errno != errno.EEXIST:')
    (7, '        raise')
    (8, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bihealth/varfish-db-downloader, file=snakefiles/GRCh37/exac.smk
context_key: ['if ($popmax == "NA") {']
    (134, '                    if ($popmax == "NA") {{')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bihealth/varfish-db-downloader, file=snakefiles/GRCh37/exac.smk
context_key: ['else {']
    (141, '                    else {{')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bihealth/varfish-db-downloader, file=snakefiles/GRCh38/gnomad.smk
context_key: ['if ($nonpar == ".") {']
    (145, '                    if ($nonpar == ".") {{')
    (146, '                        # hemi values')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bihealth/varfish-db-downloader, file=snakefiles/GRCh38/gnomad.smk
context_key: ['if ($nonpar == ".") {']
    (194, '                    if ($nonpar == ".") {{')
    (195, '                        # hemi values')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bihealth/varfish-db-downloader, file=snakefiles/GRCh38/gnomad.smk
context_key: ['if ($i == "NA") {']
    (256, '                        if ($i == "NA") {{')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dancooke/syntumorsizer, file=workflow/Snakefile
context_key: ['if "bamout" in config and config["bamout"]']
    (17, 'if "bamout" in config and config["bamout"]:')
    (18, '    bams = [')
    (19, '        f"results/{config[\\\'sample\\\']}_{config[\\\'tumour\\\']}.{config[\\\'reference\\\']}.{config[\\\'mapper\\\']}.bam",')
    (20, '        f"results/{config[\\\'sample\\\']}_{config[\\\'tumour\\\']}.{config[\\\'reference\\\']}.{config[\\\'mapper\\\']}.bam.bai"')
    (21, '    ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zwebbs/Hansen-et-al-RNA-seq, file=Snakefile
context_key: ['if cluster_id == "GARDNER"']
    (20, 'if cluster_id == "GARDNER":')
    (21, '    cfg_schema="CFG_GARDNER_BASIC"')
    (22, '    data_schema="META_GARDNER_SEQ_BASIC"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zwebbs/Hansen-et-al-RNA-seq, file=Snakefile
context_key: ['elif cluster_id == "MIDWAY2"']
    (23, 'elif cluster_id == "MIDWAY2":')
    (24, '    cfg_schema = None  # TODO: implement gs midway2 schema and replace')
    (25, '    data_schema = None  # TODO: implement gs midway2 schema and replace')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zwebbs/Hansen-et-al-RNA-seq, file=Snakefile
context_key: ['if Path(genome_index_dir).exists()', 'rule Verify_Index_Contents', 'shell']
    (66, 'if Path(genome_index_dir).exists():')
    (67, '    rule Verify_Index_Contents:')
    (68, '        params: **CH.get_parameters("STAR_Create_Genome_Index")')
    (69, '        output: **DM.get_rule_data("STAR_Create_Genome_Index", ["static_outputs"])')
    (70, '        resources: **CH.get_resources("Verify_Index_Contents")')
    (71, '        shell:')
    (72, '            "check_directory -o {output.rc_out}"')
    (73, '            " {params.genome_index_manifest} {params.genome_index_dir}"')
    (74, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlthafSinghawansaUHN/cfmedipseq_pipeline, file=Snakefile
context_key: ['if excluded_cases']
    (14, 'if excluded_cases:')
    (15, "    print(\\'Excluding cases:\\')")
    (16, '    for case in excluded_cases:')
    (17, '        print(case)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/SnakeRNASeq, file=Snakefile
context_key: ['if not config["trimming"]["skip"]']
    (18, 'if not config["trimming"]["skip"]:')
    (19, '    TRIMMED_R1= expand("/scratch/ac32082/02.PeanutRNASeq/01.analysis/peanut_rna_seq_analysis/results/02_trim/{smp}_R1_val_1.fq.gz", smp=sample_id)')
    (20, '    TRIMMED_R2= expand("/scratch/ac32082/02.PeanutRNASeq/01.analysis/peanut_rna_seq_analysis/results/02_trim/{smp}_R2_val_2.fq.gz", smp=sample_id)')
    (21, '    TRIMMED_MQC= expand("/scratch/ac32082/02.PeanutRNASeq/01.analysis/peanut_rna_seq_analysis/results/02_trim/trim_galore_multiqc_report.html")')
    (22, '    TRIMMED_FQC=expand("/scratch/ac32082/02.PeanutRNASeq/01.analysis/peanut_rna_seq_analysis/results/01_qc/01b_multiqc/multiqc_trim/multiqc_report_trim_galore.html")')
    (23, '    ALL_TARGET.extend(TRIMMED_R1)')
    (24, '    ALL_TARGET.extend(TRIMMED_R2)')
    (25, '    ALL_TARGET.extend(TRIMMED_FQC)')
    (26, '    ALL_TARGET.extend(TRIMMED_MQC)')
    (27, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/SnakeRNASeq, file=Snakefile
context_key: ['if not config["decontamination"]["skip"]']
    (28, 'if not config["decontamination"]["skip"]:')
    (29, '    CHECK_RRNA= expand("/scratch/ac32082/02.PeanutRNASeq/01.analysis/peanut_rna_seq_analysis/results/03_decontamination/03a_rrna_check/rrna_multiqc_report.html")')
    (30, '    CLEAN_R1= expand("/scratch/ac32082/02.PeanutRNASeq/01.analysis/peanut_rna_seq_analysis/results/03_decontamination/03b_rrna_cleaned/{smp}_R1_clean.fq", smp=sample_id)')
    (31, '    CLEAN_R2= expand("/scratch/ac32082/02.PeanutRNASeq/01.analysis/peanut_rna_seq_analysis/results/03_decontamination/03b_rrna_cleaned/{smp}_R2_clean.fq", smp=sample_id)')
    (32, '    CHECK_CLEAN= expand("/scratch/ac32082/02.PeanutRNASeq/01.analysis/peanut_rna_seq_analysis/results/03_decontamination/03c_rrna_cleaned_check/rrna_clean_multiqc_report.html")')
    (33, '    CLEAN_MQC= expand("/scratch/ac32082/02.PeanutRNASeq/01.analysis/peanut_rna_seq_analysis/results/01_qc/01b_multiqc/multiqc_clean/multiqc_report_decont.html")')
    (34, '    ALL_TARGET.extend(CHECK_RRNA)')
    (35, '    ALL_TARGET.extend(CLEAN_R1)')
    (36, '    ALL_TARGET.extend(CLEAN_R2)')
    (37, '    ALL_TARGET.extend(CHECK_CLEAN)')
    (38, '    ALL_TARGET.extend(CLEAN_MQC)')
    (39, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crs4/fair-crcc-send-data, file=workflow/rules/common.smk
context_key: ['if not glob_ext.startswith(".")']
    (13, 'if not glob_ext.startswith("."):')
    (14, '    raise ValueError("sources.glob_extension must start with a \\\'.\\\'")')
    (15, '')
    (16, '')
    (17, '#### Environment configuration ####')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crs4/fair-crcc-send-data, file=workflow/rules/common.smk
context_key: ['if workflow.use_singularity', 'if repository == work_dir or repository in work_dir.parents']
    (19, 'if workflow.use_singularity:')
    (20, '    # Bind mount the repository path into container.')
    (21, '    # Ideally we want to mount the repository in read-only mode.')
    (22, '    # To avoid making the working directory read-only should it be inside')
    (23, '    # or the same path as the working directory, we check for this case')
    (24, '    # and if true we mount read-write.')
    (25, '    repository = Path(config["repository"]["path"]).resolve()')
    (26, '    work_dir = Path.cwd()')
    (27, '    if repository == work_dir or repository in work_dir.parents:')
    (28, '        mount_options = "rw"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crs4/fair-crcc-send-data, file=workflow/rules/common.smk
context_key: ['if workflow.use_singularity', 'else']
    (29, '    else:')
    (30, '        mount_options = "ro"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crs4/fair-crcc-send-data, file=workflow/rules/common.smk
context_key: ['if workflow.use_singularity']
    (31, "    workflow.singularity_args += \\' \\'.join([")
    (32, '        # Use --cleanenv to work around singularity exec overriding env')
    (33, '        # vars from docker image with host values (https://github.com/sylabs/singularity/issues/533)')
    (34, '        " --cleanenv",')
    (35, '        f" --bind {repository}:{repository}:{mount_options}"])')
    (36, '')
    (37, '')
    (38, '##### Helper functions #####')
    (39, '')
    (40, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=greydongilmore/clinical_dicom2bids_smk, file=workflow/Snakefile
context_key: ["if not exists(join(config[\\'out_dir\\'], \\'logs\\'))"]
    (17, "if not exists(join(config[\\'out_dir\\'], \\'logs\\')):")
    (18, "    makedirs(join(config[\\'out_dir\\'], \\'logs\\'))")
    (19, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=greydongilmore/clinical_dicom2bids_smk, file=workflow/Snakefile
context_key: ["if exists(config[\\'participants_tsv\\'])", 'if isinstance(subjects[0],str)']
    (20, "if exists(config[\\'participants_tsv\\']):")
    (21, "    df = pd.read_table(config[\\'participants_tsv\\'])")
    (22, '    subjects=df.participant_id.to_list()')
    (23, '    if isinstance(subjects[0],str):')
    (24, "        subjects = [ s.strip(\\'sub-P\\') for s in subjects]")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=greydongilmore/clinical_dicom2bids_smk, file=workflow/Snakefile
context_key: ["if exists(config[\\'participants_tsv\\'])", 'else']
    (25, '    else:')
    (26, '        subjects = [ str(s).zfill(3) for s in subjects]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lasejour/snakemake-scRNA-seq, file=rules/diffexp.smk
context_key: ['if celltypes', 'try']
    (2, '    if celltypes:')
    (3, '        try:')
    (4, '            parents = markers.loc[celltypes, "parent"].unique()')
    (5, '        except KeyError:')
    (6, '            raise WorkflowError(')
    (7, '                "Given celltypes {} not defined in markers "')
    (8, '                "(see config).".format(celltypes)')
    (9, '            )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lasejour/snakemake-scRNA-seq, file=rules/diffexp.smk
context_key: ['if celltypes', 'else']
    (10, '    else:')
    (11, '        parents = markers["parent"].unique()')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lasejour/snakemake-scRNA-seq, file=rules/diffexp.smk
context_key: ['if celltypes']
    (12, '    return expand("analysis/cellassign.{parent}.rds", parent=parents)')
    (13, '')
    (14, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lasejour/snakemake-scRNA-seq, file=Snakefile
context_key: ['if "markers" in config.get("celltype", {})']
    (17, 'if "markers" in config.get("celltype", {}):')
    (18, '    markers = pd.read_csv(config["celltype"]["markers"], sep="\\\\t").set_index("name", drop=False)')
    (19, '    markers.loc[:, "parent"].fillna("root", inplace=True)')
    (20, '')
    (21, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_make_bam_links_overlapping_SNPs_pe', 'input']
    (66, 'elif  config["Haplotyper"] is True and config["Validation"] is not None:')
    (67, '    rule haplotype_make_bam_links_overlapping_SNPs_pe:')
    (68, '        """')
    (69, '        Make symlinks to run rad_haplotyper')
    (70, '        """')
    (71, '        input:')
    (72, '            vcf = HAPLOTYPE_DIR + "Overlapping_filtered_snps.recode.vcf",')
    (73, '            bam = MAP_DIR_PE + "{sample}.sorted.bam",')
    (74, '            bai = MAP_DIR_PE + "{sample}.sorted.bam.bai"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_make_bam_links_overlapping_SNPs_pe', 'output']
    (75, '        output:')
    (76, '            bam = HAPLOTYPE_DIR + "{sample}.bam",')
    (77, '            bai = HAPLOTYPE_DIR + "{sample}.bam.bai"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_make_bam_links_overlapping_SNPs_pe', 'threads']
    (78, '        threads:')
    (79, '            1')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_make_bam_links_overlapping_SNPs_pe', 'log']
    (80, '        log:')
    (81, '            HAPLOTYPE_DOC + "make_bam_links.{sample}.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_make_bam_links_overlapping_SNPs_pe', 'benchmark']
    (82, '        benchmark:')
    (83, '            HAPLOTYPE_DOC + "make_bam_links.{sample}.json"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_make_bam_links_overlapping_SNPs_pe', 'shell']
    (84, '        shell:')
    (85, '            "ln -s $(readlink -f {input.bam}) {output.bam} 2> {log}; "')
    (86, '            "ln -s $(readlink -f {input.bai}) {output.bai} 2> {log}; "')
    (87, '')
    (88, '')
    (89, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['else', 'if  config["Haplotyper"] is True and config["Validation"] is None', 'rule haplotype_run_rad_haplotyper', 'input']
    (192, 'if  config["Haplotyper"] is True and config["Validation"] is None:')
    (193, '    rule haplotype_run_rad_haplotyper:')
    (194, '        """')
    (195, '        Run rad_haplotyper')
    (196, '        """')
    (197, '        input:')
    (198, '            popmap = HAPLOTYPE_DIR + "popmap",')
    (199, '            vcf = HAPLOTYPE_DIR + "filtered_snps.recode.vcf"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['else', 'if  config["Haplotyper"] is True and config["Validation"] is None', 'output']
    (200, '        output:')
    (201, '            output_1 = HAPLOTYPE_DIR + "haplotypes.done",')
    (202, '            output_2 = FINAL_HAPLOTYPES + "haplotypes.done"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['else', 'if  config["Haplotyper"] is True and config["Validation"] is None', 'threads']
    (203, '        threads:')
    (204, '            20')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['else', 'if  config["Haplotyper"] is True and config["Validation"] is None', 'params']
    (205, '        params:')
    (206, '            hap_dir = HAPLOTYPE_DIR,')
    (207, '            hap_dir_final_results = FINAL_HAPLOTYPES,')
    (208, '            home_dir = "../..",')
    (209, '            Haplotyping_params = config["Haplotyping_params"]["rad_haplotyper_params"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['else', 'if  config["Haplotyper"] is True and config["Validation"] is None', 'log']
    (210, '        log:')
    (211, '            HAPLOTYPE_DOC + "rad_haplotyper.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['else', 'if  config["Haplotyper"] is True and config["Validation"] is None', 'benchmark']
    (212, '        benchmark:')
    (213, '            HAPLOTYPE_DOC + "rad_haplotyper.json"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['else', 'if  config["Haplotyper"] is True and config["Validation"] is None', 'shell']
    (214, '        shell:')
    (215, '            "cd {params.hap_dir}; "')
    (216, '            "rad_haplotyper.pl -v filtered_snps.recode.vcf -x {threads} -g haps.gen -p popmap {params.Haplotyping_params}; "')
    (217, '            "cd {params.home_dir}; "')
    (218, '            "touch {output.output_1}; "')
    (219, '            "cp {output.output_1} {output.output_2}; "')
    (220, '            "cp results/Haplotype/codes.haps.gen results/Final_results/Haplotypes/; "')
    (221, '            "cp results/Haplotype/stats.out results/Final_results/Haplotypes/; "')
    (222, '            "cp results/Haplotype/ind_stats.out results/Final_results/Haplotypes/; "')
    (223, '            "cp results/Haplotype/haps.gen results/Final_results/Haplotypes/; "')
    (224, '            "cp results/Haplotype/hap_loci.txt results/Final_results/Haplotypes/; "')
    (225, '')
    (226, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe', 'input']
    (227, 'elif  config["Haplotyper"] is True and config["Validation"] is not None:')
    (228, '    rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe:')
    (229, '        """')
    (230, '        Run rad_haplotyper')
    (231, '        """')
    (232, '        input:')
    (233, '            popmap = HAPLOTYPE_DIR + "popmap",')
    (234, '            vcf = HAPLOTYPE_DIR + "Overlapping_filtered_snps.recode.vcf"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe', 'output']
    (235, '        output:')
    (236, '            output_1 = HAPLOTYPE_DIR + "haplotypes.done",')
    (237, '            output_2 = FINAL_HAPLOTYPES + "haplotypes.done"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe', 'threads']
    (238, '        threads:')
    (239, '            20')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe', 'params']
    (240, '        params:')
    (241, '            hap_dir = HAPLOTYPE_DIR,')
    (242, '            hap_dir_final_results = FINAL_HAPLOTYPES,')
    (243, '            home_dir = "../..",')
    (244, '            Haplotyping_params = config["Haplotyping_params"]["rad_haplotyper_params"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe', 'log']
    (245, '        log:')
    (246, '            HAPLOTYPE_DOC + "rad_haplotyper.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe', 'benchmark']
    (247, '        benchmark:')
    (248, '            HAPLOTYPE_DOC + "rad_haplotyper.json"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe', 'shell']
    (249, '        shell:')
    (250, '            "cd {params.hap_dir}; "')
    (251, '            "rad_haplotyper.pl -v Overlapping_filtered_snps.recode.vcf -x {threads} -g haps.gen -p popmap {params.Haplotyping_params}; "')
    (252, '            "cd {params.home_dir}; "')
    (253, '            "touch {output.output_1}; "')
    (254, '            "cp {output.output_1} {output.output_2}; "')
    (255, '            "cp results/Haplotype/codes.haps.gen results/Final_results/Haplotypes/; "')
    (256, '            "cp results/Haplotype/stats.out results/Final_results/Haplotypes/; "')
    (257, '            "cp results/Haplotype/ind_stats.out results/Final_results/Haplotypes/; "')
    (258, '            "cp results/Haplotype/haps.gen results/Final_results/Haplotypes/; "')
    (259, '            "cp results/Haplotype/hap_loci.txt results/Final_results/Haplotypes/; "')
    (260, '')
    (261, '')
    (262, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_make_bam_links_overlapping_SNPs_pe', 'input']
    (66, 'elif  config["Haplotyper"] is True and config["Validation"] is not None:')
    (67, '    rule haplotype_make_bam_links_overlapping_SNPs_pe:')
    (68, '        """')
    (69, '        Make symlinks to run rad_haplotyper')
    (70, '        """')
    (71, '        input:')
    (72, '            vcf = HAPLOTYPE_DIR + "Overlapping_filtered_snps.recode.vcf",')
    (73, '            bam = MAP_DIR_PE + "{sample}.sorted.bam",')
    (74, '            bai = MAP_DIR_PE + "{sample}.sorted.bam.bai"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_make_bam_links_overlapping_SNPs_pe', 'output']
    (75, '        output:')
    (76, '            bam = HAPLOTYPE_DIR + "{sample}.bam",')
    (77, '            bai = HAPLOTYPE_DIR + "{sample}.bam.bai"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_make_bam_links_overlapping_SNPs_pe', 'threads']
    (78, '        threads:')
    (79, '            1')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_make_bam_links_overlapping_SNPs_pe', 'log']
    (80, '        log:')
    (81, '            HAPLOTYPE_DOC + "make_bam_links.{sample}.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_make_bam_links_overlapping_SNPs_pe', 'benchmark']
    (82, '        benchmark:')
    (83, '            HAPLOTYPE_DOC + "make_bam_links.{sample}.json"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_make_bam_links_overlapping_SNPs_pe', 'shell']
    (84, '        shell:')
    (85, '            "ln -s $(readlink -f {input.bam}) {output.bam} 2> {log}; "')
    (86, '            "ln -s $(readlink -f {input.bai}) {output.bai} 2> {log}; "')
    (87, '')
    (88, '')
    (89, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['else', 'if  config["Haplotyper"] is True and config["Validation"] is None', 'rule haplotype_run_rad_haplotyper', 'input']
    (192, 'if  config["Haplotyper"] is True and config["Validation"] is None:')
    (193, '    rule haplotype_run_rad_haplotyper:')
    (194, '        """')
    (195, '        Run rad_haplotyper')
    (196, '        """')
    (197, '        input:')
    (198, '            popmap = HAPLOTYPE_DIR + "popmap",')
    (199, '            vcf = HAPLOTYPE_DIR + "filtered_snps.recode.vcf"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['else', 'if  config["Haplotyper"] is True and config["Validation"] is None', 'output']
    (200, '        output:')
    (201, '            output_1 = HAPLOTYPE_DIR + "haplotypes.done",')
    (202, '            output_2 = FINAL_HAPLOTYPES + "haplotypes.done"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['else', 'if  config["Haplotyper"] is True and config["Validation"] is None', 'threads']
    (203, '        threads:')
    (204, '            20')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['else', 'if  config["Haplotyper"] is True and config["Validation"] is None', 'params']
    (205, '        params:')
    (206, '            hap_dir = HAPLOTYPE_DIR,')
    (207, '            hap_dir_final_results = FINAL_HAPLOTYPES,')
    (208, '            home_dir = "../..",')
    (209, '            Haplotyping_params = config["Haplotyping_params"]["rad_haplotyper_params"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['else', 'if  config["Haplotyper"] is True and config["Validation"] is None', 'log']
    (210, '        log:')
    (211, '            HAPLOTYPE_DOC + "rad_haplotyper.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['else', 'if  config["Haplotyper"] is True and config["Validation"] is None', 'benchmark']
    (212, '        benchmark:')
    (213, '            HAPLOTYPE_DOC + "rad_haplotyper.json"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['else', 'if  config["Haplotyper"] is True and config["Validation"] is None', 'shell']
    (214, '        shell:')
    (215, '            "cd {params.hap_dir}; "')
    (216, '            "rad_haplotyper.pl -v filtered_snps.recode.vcf -x {threads} -g haps.gen -p popmap {params.Haplotyping_params}; "')
    (217, '            "cd {params.home_dir}; "')
    (218, '            "touch {output.output_1}; "')
    (219, '            "cp {output.output_1} {output.output_2}; "')
    (220, '            "cp results/Haplotype/codes.haps.gen results/Final_results/Haplotypes/; "')
    (221, '            "cp results/Haplotype/stats.out results/Final_results/Haplotypes/; "')
    (222, '            "cp results/Haplotype/ind_stats.out results/Final_results/Haplotypes/; "')
    (223, '            "cp results/Haplotype/haps.gen results/Final_results/Haplotypes/; "')
    (224, '            "cp results/Haplotype/hap_loci.txt results/Final_results/Haplotypes/; "')
    (225, '')
    (226, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe', 'input']
    (227, 'elif  config["Haplotyper"] is True and config["Validation"] is not None:')
    (228, '    rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe:')
    (229, '        """')
    (230, '        Run rad_haplotyper')
    (231, '        """')
    (232, '        input:')
    (233, '            popmap = HAPLOTYPE_DIR + "popmap",')
    (234, '            vcf = HAPLOTYPE_DIR + "Overlapping_filtered_snps.recode.vcf"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe', 'output']
    (235, '        output:')
    (236, '            output_1 = HAPLOTYPE_DIR + "haplotypes.done",')
    (237, '            output_2 = FINAL_HAPLOTYPES + "haplotypes.done"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe', 'threads']
    (238, '        threads:')
    (239, '            20')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe', 'params']
    (240, '        params:')
    (241, '            hap_dir = HAPLOTYPE_DIR,')
    (242, '            hap_dir_final_results = FINAL_HAPLOTYPES,')
    (243, '            home_dir = "../..",')
    (244, '            Haplotyping_params = config["Haplotyping_params"]["rad_haplotyper_params"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe', 'log']
    (245, '        log:')
    (246, '            HAPLOTYPE_DOC + "rad_haplotyper.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe', 'benchmark']
    (247, '        benchmark:')
    (248, '            HAPLOTYPE_DOC + "rad_haplotyper.json"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=skyriakidis/Snakemake_haps, file=bin/snakefiles/haplotype.py
context_key: ['elif  config["Haplotyper"] is True and config["Validation"] is not None', 'rule haplotype_run_rad_haplotyper_overlapping_SNPs_pe', 'shell']
    (249, '        shell:')
    (250, '            "cd {params.hap_dir}; "')
    (251, '            "rad_haplotyper.pl -v Overlapping_filtered_snps.recode.vcf -x {threads} -g haps.gen -p popmap {params.Haplotyping_params}; "')
    (252, '            "cd {params.home_dir}; "')
    (253, '            "touch {output.output_1}; "')
    (254, '            "cp {output.output_1} {output.output_2}; "')
    (255, '            "cp results/Haplotype/codes.haps.gen results/Final_results/Haplotypes/; "')
    (256, '            "cp results/Haplotype/stats.out results/Final_results/Haplotypes/; "')
    (257, '            "cp results/Haplotype/ind_stats.out results/Final_results/Haplotypes/; "')
    (258, '            "cp results/Haplotype/haps.gen results/Final_results/Haplotypes/; "')
    (259, '            "cp results/Haplotype/hap_loci.txt results/Final_results/Haplotypes/; "')
    (260, '')
    (261, '')
    (262, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SchulzLab/SOS, file=Snakefile
context_key: ['else', 'if interleaved', 'else', 'if config["Assembly"]["kpname"]=="SOAPdenovo-Trans-31mer all" or config["Assembly"]["kpname"]=="SOAPdenovo-Trans-127mer all"', 'rule all', 'rule preprocess', 'rule error_correction']
    (28, 'if interleaved:')
    (29, '\\tREADFILENAME=["read_1."+readformat, "read_2."+readformat]')
    (30, 'else:')
    (31, "\\tREADFILENAME=[path.split(\\'/\\')[-1] for path in INPUT]\\t")
    (32, '')
    (33, 'if config["Assembly"]["kpname"]=="SOAPdenovo-Trans-31mer all" or config["Assembly"]["kpname"]=="SOAPdenovo-Trans-127mer all":')
    (34, '\\tconfig["Assembly"]["kpname"]="soapdenovo-trans"')
    (35, '')
    (36, 'rule all:')
    (37, '\\tinput:')
    (38, '\\t\\texpand(outdir+"/read_{number}."+readformat, number={1,2}) if interleaved else [],')
    (39, '\\t\\texpand(outdir+"/ErrorCorrected/{reads}_corrected.fa",reads=READFILENAME),')
    (40, '\\t\\t#expand(outdir+"/Normalized/Normalized_{number}.fa", number={1,2}) if len(INPUT)>1 else expand(outdir+"/Normalized.fa"),')
    (41, '\\t\\texpand(outdir+"/configKreation.txt"),')
    (42, '\\t\\texpand(outdir+"/Assembly/Final/p_value.txt"),')
    (43, '\\t\\texpand(outdir+"/Quantification/Quant/quant.sf")')
    (44, '')
    (45, 'rule preprocess:')
    (46, '\\tinput:')
    (47, '\\t\\texpand("{reads}",reads=INPUT)')
    (48, '\\toutput:')
    (49, '\\t\\texpand(outdir+"/read_{number}."+readformat, number={1,2}) if interleaved else [],')
    (50, '\\t\\texpand(outdir+"LogFiles/")')
    (51, '\\tpriority:')
    (52, '\\t\\t1')
    (53, '\\tmessage:')
    (54, '\\t\\t"Executing preprocess"\\t\\t')
    (55, '\\trun:')
    (56, '\\t\\tif interleaved:')
    (57, '\\t\\t\\tshell("perl scripts/deinterleave.pl {input} "+outdir+" "+readformat)')
    (58, '')
    (59, 'rule error_correction:')
    (60, '\\tinput:')
    (61, '\\t\\trules.preprocess.output\\tif interleaved else expand("{reads}",reads=INPUT)\\t')
    (62, '\\toutput:')
    (63, '\\t\\texpand(outdir+"/ErrorCorrected/{reads}_corrected.fa",reads=READFILENAME)')
    (64, '\\tpriority:')
    (65, '\\t\\t2')
    (66, '\\tmessage:')
    (67, '\\t\\t"Executing error correction for the pipeline"')
    (68, '\\tbenchmark:')
    (69, '       \\t\\toutdir+"/LogFiles/TimeCorrection.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SchulzLab/SOS, file=Snakefile
context_key: ['if norm']
    (76, 'if norm:')
    (77, '\\trule read_normalization:')
    (78, '\\t\\tinput:')
    (79, '\\t\\t\\texpand(outdir+"/ErrorCorrected/{reads}_corrected.fa",reads=READFILENAME)')
    (80, '\\t\\toutput:')
    (81, '\\t\\t\\texpand(outdir+"/Normalized/Normalized_{number}.fa", number={1,2}) if len(INPUT)>1 else expand(outdir+"/{reads}_Normalized.fa", reads=READFILENAME) ')
    (82, '\\t\\tparams:')
    (83, '\\t\\t\\tbs={base},')
    (84, '\\t\\t\\tkmr={okmer},')
    (85, '\\t\\t\\tlinput = 2 if len(INPUT)>1 else 1')
    (86, '\\t\\tmessage:')
    (87, '\\t\\t\\t"Executing normalization"')
    (88, '\\t\\tconda: f"config/conda/orna.yaml"')
    (89, '\\t\\tbenchmark:')
    (90, '        \\t\\toutdir+"/LogFiles/TimeNormalization.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sunjiangming/scRNASeq, file=rules/cell-type.smk
context_key: ['if wildcards.parent != "root"']
    (1, '    if wildcards.parent != "root":')
    (2, '        parent = markers.loc[wildcards.parent, "parent"]')
    (3, '        return f"analysis/cellassign.{parent}.rds"')
    (4, '    else:')
    (5, '        return []')
    (6, '')
    (7, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sunjiangming/scRNASeq, file=Snakefile
context_key: ['if "markers" in config.get("celltype", {})']
    (17, 'if "markers" in config.get("celltype", {}):')
    (18, '    markers = pd.read_csv(config["celltype"]["markers"], sep="\\\\t").set_index("name", drop=False)')
    (19, '    markers.loc[:, "parent"].fillna("root", inplace=True)')
    (20, '')
    (21, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=waglecn/mabs, file=Snakefile
context_key: ['if len(config) == 0']
    (5, 'if len(config) == 0:')
    (6, "    exit(\\'no configfile specified: use --configfile [file]\\')")
    (7, '')
    (8, '# TODO -- from')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=waglecn/mabs, file=stage1.smk
context_key: ['input', "if read == \\'R1\\'"]
    (12, "    if read == \\'R1\\':")
    (13, '        fastqs = [')
    (14, '            reads for reads in sample_dict[sample_name] if')
    (15, "            # reads.split(\\'_\\')[-1].endswith(\\'R1_001.fastq.gz\\')")
    (16, "            \\'_R1\\' in reads")
    (17, '        ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=waglecn/mabs, file=stage1.smk
context_key: ['input', "elif read == \\'R2\\'"]
    (18, "    elif read == \\'R2\\':")
    (19, '        fastqs = [')
    (20, '            reads for reads in sample_dict[sample_name] if')
    (21, "            # reads.split(\\'_\\')[-1].endswith(\\'R2_001.fastq.gz\\')")
    (22, "            \\'_R2\\' in reads")
    (23, '        ]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=waglecn/mabs, file=stage1.smk
context_key: ['input', 'if len(reads) == 0']
    (32, '    if len(reads) == 0:')
    (33, "        exit(\\'no reads found\\')")
    (34, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['if (TRANSCRIPTS != None)', 'rule PolyAAnalysis_trimm', 'input']
    (2, 'if (TRANSCRIPTS != None):')
    (3, '    rule PolyAAnalysis_trimm:')
    (4, '        input:')
    (5, '            PolyAAnalysis_trimm_input')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['if (TRANSCRIPTS != None)', 'rule PolyAAnalysis_trimm', 'output']
    (6, '        output:')
    (7, '            tmp + "/{stem}_R1_trimmedPolyA.fastq.gz",')
    (8, '            tmp + "/{stem}_R2_trimmedPolyA.fastq.gz",')
    (9, '            tmp + "/{stem}_PolyA.fastq.gz",')
    (10, '            tmp + "/{stem}_discarded.fastq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['if (TRANSCRIPTS != None)', 'rule PolyAAnalysis_trimm', 'benchmark']
    (11, '        benchmark:')
    (12, '            BENCHMARKS + "/{stem}_trim_polyA_reads.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['if (TRANSCRIPTS != None)', 'rule PolyAAnalysis_trimm', 'log']
    (13, '        log:')
    (14, '            LOGS + "/TRIMMING-POLYA/{stem}.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['if (TRANSCRIPTS != None)', 'rule PolyAAnalysis_trimm', 'params']
    (15, '        params:')
    (16, '            gz = TRANSCRIPTS,')
    (17, '            output_stem = tmp + "/{stem}"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['if (TRANSCRIPTS != None)', 'rule PolyAAnalysis_trimm', 'threads']
    (18, '        threads:')
    (19, '            JULIA_THREADS')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['if (TRANSCRIPTS != None)', 'rule PolyAAnalysis_trimm', 'shell']
    (20, '        shell:')
    (21, '            "julia --depwarn=no PolyAAnalysis.jl/scripts/mark_poly_A.jl -i -p {threads} " +')
    (22, '            "-a {input[0]} -b {input[1]} -o {params.output_stem} " +')
    (23, '            "-r {params.gz} 2>&1 | tee -a {log}"')
    (24, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['elif (gff != None and REFERENCE != None)', 'rule PolyAAnalysis_trimm', 'input']
    (25, 'elif (gff != None and REFERENCE != None):')
    (26, '    rule PolyAAnalysis_trimm:')
    (27, '            input:')
    (28, '                PolyAAnalysis_trimm_input')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['elif (gff != None and REFERENCE != None)', 'rule PolyAAnalysis_trimm', 'output']
    (29, '            output:')
    (30, '                tmp + "/{stem}_R1_trimmedPolyA.fastq.gz",')
    (31, '                tmp + "/{stem}_R2_trimmedPolyA.fastq.gz",')
    (32, '                tmp + "/{stem}_PolyA.fastq.gz",')
    (33, '                tmp + "/{stem}_discarded.fastq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['elif (gff != None and REFERENCE != None)', 'rule PolyAAnalysis_trimm', 'benchmark']
    (34, '            benchmark:')
    (35, '                BENCHMARKS + "/{stem}_trim_polyA_reads.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['elif (gff != None and REFERENCE != None)', 'rule PolyAAnalysis_trimm', 'log']
    (36, '            log:')
    (37, '                LOGS + "/TRIMMING-POLYA/{stem}.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['elif (gff != None and REFERENCE != None)', 'rule PolyAAnalysis_trimm', 'params']
    (38, '            params:')
    (39, '                output_stem = tmp + "/{stem}",')
    (40, '                gff = gff,')
    (41, '                ref = REFERENCE')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['elif (gff != None and REFERENCE != None)', 'rule PolyAAnalysis_trimm', 'threads']
    (42, '            threads:')
    (43, '                JULIA_THREADS')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['elif (gff != None and REFERENCE != None)', 'rule PolyAAnalysis_trimm', 'shell']
    (44, '            shell:')
    (45, '                "julia --depwarn=no PolyAAnalysis.jl/scripts/mark_poly_A.jl -i -p {threads} " +')
    (46, '                "-a {input[0]} -b {input[1]} -o {params.output_stem} " +')
    (47, '                "-g {params.ref} -f {params.gff} 2>&1 | tee -a {log}"')
    (48, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['elif (REFERENCE != None)', 'rule PolyAAnalysis_trimm', 'input']
    (49, 'elif (REFERENCE != None):')
    (50, '    rule PolyAAnalysis_trimm:')
    (51, '            input:')
    (52, '                PolyAAnalysis_trimm_input')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['elif (REFERENCE != None)', 'rule PolyAAnalysis_trimm', 'output']
    (53, '            output:')
    (54, '                tmp + "/{stem}_R1_trimmedPolyA.fastq.gz",')
    (55, '                tmp + "/{stem}_R2_trimmedPolyA.fastq.gz",')
    (56, '                tmp + "/{stem}_PolyA.fastq.gz",')
    (57, '                tmp + "/{stem}_discarded.fastq.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['elif (REFERENCE != None)', 'rule PolyAAnalysis_trimm', 'benchmark']
    (58, '            benchmark:')
    (59, '                BENCHMARKS + "/{stem}_trim_polyA_reads.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['elif (REFERENCE != None)', 'rule PolyAAnalysis_trimm', 'log']
    (60, '            log:')
    (61, '                LOGS + "/TRIMMING-POLYA/{stem}.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['elif (REFERENCE != None)', 'rule PolyAAnalysis_trimm', 'params']
    (62, '            params:')
    (63, '                output_stem = tmp + "/{stem}",')
    (64, '                ref = REFERENCE')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['elif (REFERENCE != None)', 'rule PolyAAnalysis_trimm', 'threads']
    (65, '            threads:')
    (66, '                JULIA_THREADS')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ThermofisherAndBabraham/polyAterminus, file=snakefiles/3_1_trimming_polyA.smk
context_key: ['elif (REFERENCE != None)', 'rule PolyAAnalysis_trimm', 'shell']
    (67, '            shell:')
    (68, '                "julia --depwarn=no PolyAAnalysis.jl/scripts/mark_poly_A.jl -i -p {threads} " +')
    (69, '                "-a {input[0]} -b {input[1]} -o {params.output_stem} " +')
    (70, '                "-g {params.ref} 2>&1 | tee -a {log}"')
    (71, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/qc.smk
context_key: ['if "restrict_regions" in config["processing"]', 'rule bed_to_interval', 'input']
    (46, 'if "restrict_regions" in config["processing"]:')
    (47, '    rule bed_to_interval:')
    (48, '        input:')
    (49, '            file=config["processing"]["restrict_regions"],')
    (50, '            SD=config["ref"]["genome"],')
    (51, '            dict=os.path.splitext(config["ref"]["genome"])[0] + ".dict"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/qc.smk
context_key: ['if "restrict_regions" in config["processing"]', 'rule bed_to_interval', 'output']
    (52, '        output:')
    (53, '            temp(f"{OUTDIR}/regions.intervals")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/qc.smk
context_key: ['if "restrict_regions" in config["processing"]', 'rule bed_to_interval', 'conda']
    (54, '        conda:')
    (55, '            "../envs/picard.yaml"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/qc.smk
context_key: ['if "restrict_regions" in config["processing"]', 'rule bed_to_interval', 'resources']
    (56, '        resources:')
    (57, '            mem_mb = get_resource("bed_to_interval","mem"),')
    (58, '            walltime = get_resource("bed_to_interval","walltime")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/qc.smk
context_key: ['if "restrict_regions" in config["processing"]', 'rule bed_to_interval', 'params']
    (59, '        params:')
    (60, '            extra = "-Xmx{}m".format(get_resource("bed_to_interval","mem"))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/qc.smk
context_key: ['if "restrict_regions" in config["processing"]', 'rule bed_to_interval', 'shell']
    (61, '        shell:')
    (62, '            "picard BedToIntervalList {params.extra} I={input.file} O={output} SD={input.SD}"')
    (63, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/qc.smk
context_key: ['if "restrict_regions" in config["processing"]', 'rule picard_collect_hs_metrics', 'input']
    (64, '    rule picard_collect_hs_metrics:')
    (65, '        input:')
    (66, '            bam=f"{OUTDIR}/recal/{{sample}}-{{unit}}.bam",')
    (67, '            reference=config["ref"]["genome"],')
    (68, '            bait_intervals=f"{OUTDIR}/regions.intervals",')
    (69, '            target_intervals=f"{OUTDIR}/regions.intervals"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/qc.smk
context_key: ['if "restrict_regions" in config["processing"]', 'rule picard_collect_hs_metrics', 'output']
    (70, '        output:')
    (71, '            f"{OUTDIR}/qc/picard/{{sample}}-{{unit}}.txt"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/qc.smk
context_key: ['if "restrict_regions" in config["processing"]', 'rule picard_collect_hs_metrics', 'log']
    (72, '        log:')
    (73, '            f"{LOGDIR}/picard_collect_hs_metrics/{{sample}}-{{unit}}.log"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/qc.smk
context_key: ['if "restrict_regions" in config["processing"]', 'rule picard_collect_hs_metrics', 'threads']
    (74, '        threads: get_resource("picard_collect_hs_metrics","threads")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/qc.smk
context_key: ['if "restrict_regions" in config["processing"]', 'rule picard_collect_hs_metrics', 'resources']
    (75, '        resources:')
    (76, '            mem_mb = get_resource("picard_collect_hs_metrics","mem"),')
    (77, '            walltime = get_resource("picard_collect_hs_metrics","walltime")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/qc.smk
context_key: ['if "restrict_regions" in config["processing"]', 'rule picard_collect_hs_metrics', 'params']
    (78, '        params:')
    (79, '            extra = ""')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/qc.smk
context_key: ['if "restrict_regions" in config["processing"]', 'rule picard_collect_hs_metrics', 'wrapper']
    (80, '        wrapper:')
    (81, '            "0.79.0/bio/picard/collecthsmetrics"')
    (82, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/umi_bclconvert_se.smk
context_key: ['if units.loc[wildcards.unit,["fq2"]].isna().all()']
    (2, '    if units.loc[wildcards.unit,["fq2"]].isna().all():')
    (3, '        print("SE")')
    (4, '        # print(units.loc[wildcards.unit,["fq1"]].dropna()[0])')
    (5, '        return expand_filepath(units.loc[wildcards.unit,["fq1"]].dropna()[0])')
    (6, '    else:')
    (7, '        print("PE")')
    (8, '        # print(units.loc[wildcards.unit,["fq1"]].dropna()[0],units.loc[wildcards.unit,["fq2"]].dropna()[0])')
    (9, '        return expand_filepath(units.loc[wildcards.unit,["fq1"]].dropna()[0]),expand_filepath(units.loc[wildcards.unit,["fq2"]].dropna()[0])')
    (10, '')
    (11, '')
    (12, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/umi_bclconvert.smk
context_key: ['if units.loc[wildcards.unit,["fq2"]].isna().all()']
    (2, '    if units.loc[wildcards.unit,["fq2"]].isna().all():')
    (3, '        print("SE")')
    (4, '        # print(units.loc[wildcards.unit,["fq1"]].dropna()[0])')
    (5, '        return expand_filepath(units.loc[wildcards.unit,["fq1"]].dropna()[0])')
    (6, '    else:')
    (7, '        print("PE")')
    (8, '        # print(units.loc[wildcards.unit,["fq1"]].dropna()[0],units.loc[wildcards.unit,["fq2"]].dropna()[0])')
    (9, '        return expand_filepath(units.loc[wildcards.unit,["fq1"]].dropna()[0]),expand_filepath(units.loc[wildcards.unit,["fq2"]].dropna()[0])')
    (10, '')
    (11, '')
    (12, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/bsqr.smk
context_key: ['if len(known_sites) == 0']
    (3, '    if len(known_sites) == 0:')
    (4, '        known_sites = known_variants.keys()')
    (5, '    for k, v in known_variants.items():')
    (6, '        if k in known_sites:')
    (7, '            ks.append("--known-sites {} ".format(resolve_single_filepath(')
    (8, '                *references_abs_path(), v)[0]))')
    (9, '    return "".join(ks)')
    (10, '')
    (11, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/umi_se.smk
context_key: ['if units.loc[wildcards.unit,["fq2"]].isna().all()']
    (2, '    if units.loc[wildcards.unit,["fq2"]].isna().all():')
    (3, '        print("SE")')
    (4, '        # print(units.loc[wildcards.unit,["fq1"]].dropna()[0])')
    (5, '        return expand_filepath(units.loc[wildcards.unit,["fq1"]].dropna()[0])')
    (6, '    else:')
    (7, '        print("PE")')
    (8, '        # print(units.loc[wildcards.unit,["fq1"]].dropna()[0],units.loc[wildcards.unit,["fq2"]].dropna()[0])')
    (9, '        return expand_filepath(units.loc[wildcards.unit,["fq1"]].dropna()[0]),expand_filepath(units.loc[wildcards.unit,["fq2"]].dropna()[0]),expand_filepath(units.loc[wildcards.unit,["fq3"]].dropna()[0])')
    (10, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/niasmic, file=rules/umi.smk
context_key: ['if units.loc[wildcards.unit,["fq2"]].isna().all()']
    (2, '    if units.loc[wildcards.unit,["fq2"]].isna().all():')
    (3, '        print("SE")')
    (4, '        # print(units.loc[wildcards.unit,["fq1"]].dropna()[0])')
    (5, '        return expand_filepath(units.loc[wildcards.unit,["fq1"]].dropna()[0])')
    (6, '    else:')
    (7, '        print("PE")')
    (8, '        # print(units.loc[wildcards.unit,["fq1"]].dropna()[0],units.loc[wildcards.unit,["fq2"]].dropna()[0])')
    (9, '        return expand_filepath(units.loc[wildcards.unit,["fq1"]].dropna()[0]),expand_filepath(units.loc[wildcards.unit,["fq2"]].dropna()[0]),expand_filepath(units.loc[wildcards.unit,["fq3"]].dropna()[0])')
    (10, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sinanugur/spatial-workflow, file=workflow/Snakefile
context_key: ['if route == "standard"', 'rule all', 'input']
    (12, 'if route == "standard":')
    (13, '    rule all:')
    (14, '        input:')
    (15, '            expand("results/{sample}/clusteringTree/clusteringTree-{sample}.pdf",sample=files),')
    (16, '            expand(["results/{sample}/resolution-" + x + "/{sample}.umap.spatial.pdf" for x in resolution],sample=files),')
    (17, '            expand(["results/{sample}/resolution-" + x + "/{sample}.number-of-cells-per-cluster.xlsx" for x in resolution],sample=files),')
    (18, '            expand(["results/{sample}/resolution-" + x + "/{sample}.all-markers-forAllClusters.xlsx" for x in resolution],sample=files),')
    (19, '            expand(["results/{sample}/resolution-" + x + "/{sample}.positive-markers-forAllClusters.xlsx" for x in resolution],sample=files),')
    (20, '            expand("results/{sample}/spatial-markers/{sample}.spatial_markers.xlsx",sample=files),')
    (21, '            expand("results/{sample}/technicals/{sample}.n_counts.pdf",sample=files),')
    (22, '            expand("results/{sample}/technicals/{sample}.normalization.pdf",sample=files),')
    (23, '            expand("results/{sample}/TissueImage/{sample}.png",sample=files),')
    (24, '            expand("results/{sample}/selected-markers/plots",sample=files)')
    (25, '')
    (26, '')
    (27, '  ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sinanugur/spatial-workflow, file=workflow/Snakefile
context_key: ['elif route == "decon"', 'rule deconvolution', 'input']
    (28, 'elif route == "decon":')
    (29, '    scrnadatafiles, = glob_wildcards("scrna/{datafile}.rds")')
    (30, '    models, = glob_wildcards("models/{modelfile}.rds")')
    (31, '    rule deconvolution:')
    (32, '        input:')
    (33, '            expand("results/{sample}/deconvolution/spotlight/{sample}-{datafile}-spotlight.pdf",datafile=scrnadatafiles,sample=files),')
    (34, '            expand("results/{sample}/deconvolution/gbm/{sample}-{modelfile}-gbm.pdf",modelfile=models,sample=files),')
    (35, '            expand("results/{sample}/deconvolution/seurat/{sample}-{datafile}-seurat.pdf",datafile=scrnadatafiles,sample=files),')
    (36, '            expand("results/{sample}/deconvolution/rctd/{sample}-{datafile}-rctd.pdf",datafile=scrnadatafiles,sample=files)')
    (37, '            #expand("results/{sample}/deconvolution/spotlight/{datafile}/"),')
    (38, '            #expand("results/{sample}/deconvolution/inhouse_gbm/{datafile}/"),')
    (39, '            #expand("results/{sample}/deconvolution/seurat/{datafile}/")')
    (40, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sinanugur/spatial-workflow, file=workflow/Snakefile
context_key: ['elif route == "dwls"', 'rule experimental_decon', 'input']
    (41, 'elif route == "dwls":')
    (42, '    scrnadatafiles, = glob_wildcards("scrna/{datafile}.rds")')
    (43, '    models, = glob_wildcards("models/{modelfile}.rds")')
    (44, '    rule experimental_decon:')
    (45, '        input:')
    (46, '            expand("results/{sample}/deconvolution/dwls/{sample}-{datafile}-dwls.pdf",datafile=scrnadatafiles,sample=files)')
    (47, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sinanugur/spatial-workflow, file=workflow/Snakefile
context_key: ['elif route == "tangram"', 'rule tangram', 'input']
    (48, 'elif route == "tangram":')
    (49, '    scrnadatafiles, = glob_wildcards("scrna/{datafile}.rds")')
    (50, '    models, = glob_wildcards("models/{modelfile}.rds")')
    (51, '    rule tangram:')
    (52, '        input:')
    (53, '            expand("results/{sample}/deconvolution/tangram/{sample}-{datafile}-tangram.pdf",datafile=scrnadatafiles,sample=files)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sinanugur/spatial-workflow, file=workflow/Snakefile
context_key: ['elif route == "tangramgene"', 'rule tangramgene', 'input']
    (54, 'elif route == "tangramgene":')
    (55, '    scrnadatafiles, = glob_wildcards("scrna/{datafile}.rds")')
    (56, '    models, = glob_wildcards("models/{modelfile}.rds")')
    (57, '    rule tangramgene:')
    (58, '        input:')
    (59, '            expand("results/{sample}/deconvolution/tangramgene/{sample}-{datafile}-tangramgene.pdf",datafile=scrnadatafiles,sample=files)')
    (60, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sinanugur/spatial-workflow, file=workflow/Snakefile
context_key: ['elif route == "clusterplots"', 'rule plotting', 'input']
    (61, 'elif route == "clusterplots":')
    (62, '    rule plotting:')
    (63, '        input:')
    (64, '            expand(["results/{sample}/resolution-" + x + "/markers" for x in resolution],sample=files)')
    (65, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=BleekerLab/freebayes_snp_calling, file=Snakefile
context_key: ['if len(SAMPLES) == 1']
    (125, 'if len(SAMPLES) == 1: # only one sample')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Finn-Lab/skin_microbiome, file=prok.Snakefile
context_key: ['if not os.path.exists("logs")']
    (39, 'if not os.path.exists("logs"):')
    (40, '    os.makedirs("logs")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Finn-Lab/skin_microbiome, file=euk.Snakefile
context_key: ['if "TotalBases" in line']
    (201, '                    if "TotalBases" in line:')
    (202, '                        cols = line.split()')
    (203, '                        lenref = int(cols[1])')
    (204, '                        lenquer = int(cols[2])')
    (205, '                    if "AlignedBases" in line:')
    (206, '                        cols = line.split()')
    (207, '                        aliref = cols[1].split("(")[-1].split("%")[0]')
    (208, '                        alique = cols[2].split("(")[-1].split("%")[0]')
    (209, '                    if "AvgIdentity" in line:')
    (210, '                        cols = line.split()')
    (211, '                        ident = float(cols[1])')
    (212, '            line = "%s\\\\t%s\\\\t%i\\\\t%.2f\\\\t%i\\\\t%.2f\\\\t%.2f" % (ref, quer, lenref, float(aliref), lenquer, float(alique), float(ident))')
    (213, '            outf.writelines(line + "\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/rules/Snakefile_DeepVariant
context_key: ['if clusterMode == "gcp" or useRemoteFiles']
    (11, 'if clusterMode == "gcp" or useRemoteFiles:')
    (12, '    from snakemake.remote.GS import RemoteProvider as GSRemoteProvider')
    (13, '    GS = GSRemoteProvider()')
    (14, '')
    (15, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/rules/Snakefile_harmonize
context_key: ['if clusterMode == "gcp" or useRemoteFiles']
    (8, 'if clusterMode == "gcp" or useRemoteFiles:')
    (9, '    from snakemake.remote.GS import RemoteProvider as GSRemoteProvider')
    (10, '    GS = GSRemoteProvider()')
    (11, '')
    (12, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/rules/Snakefile_Strelka2
context_key: ['if clusterMode == "gcp" or useRemoteFiles']
    (8, 'if clusterMode == "gcp" or useRemoteFiles:')
    (9, '    from snakemake.remote.GS import RemoteProvider as GSRemoteProvider')
    (10, '    GS = GSRemoteProvider()')
    (11, '')
    (12, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/rules/Snakefile_preprocess
context_key: ['if clusterMode == "gcp"']
    (7, 'if clusterMode == "gcp":')
    (8, '    GS = GSRemoteProvider()      ')
    (9, '        ')
    (10, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/rules/Snakefile_preprocess
context_key: ['if clusterMode == "gcp"']
    (7, 'if clusterMode == "gcp":')
    (8, '    GS = GSRemoteProvider()      ')
    (9, '        ')
    (10, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/rules/common.smk
context_key: ['if clusterMode == "gcp" or useRemoteFiles']
    (7, 'if clusterMode == "gcp" or useRemoteFiles:')
    (8, '    from snakemake.remote.GS import RemoteProvider as GSRemoteProvider')
    (9, '    GS = GSRemoteProvider()')
    (10, '')
    (11, '# The pipeline will terminate with an error without .bai files')
    (12, '# The assumption is enforced in input to DV, HC and Strelka2 calling rules')
    (13, '# NOT USED')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/Snakefile
context_key: ['if clusterMode == "gcp" or useRemoteFiles']
    (46, 'if clusterMode == "gcp" or useRemoteFiles:')
    (47, '    from snakemake.remote.GS import RemoteProvider as GSRemoteProvider')
    (48, '    GS = GSRemoteProvider()')
    (49, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/Snakefile
context_key: ['if clusterMode == "cluster"']
    (50, 'if clusterMode == "cluster":')
    (51, '   outputDir = "./"')
    (52, '')
    (53, '    ')
    (54, '#including the common functions')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/Snakefile
context_key: ['if dv_mode']
    (58, 'if dv_mode:')
    (59, "    #modelPath = config[\\'modelPath\\']")
    (60, "    #useShards = config[\\'useShards\\']")
    (61, "    glnexus_dv_config = config[\\'glnexus_dv_config\\']")
    (62, "    model_type = config[\\'model_type\\']")
    (63, "    dv_memGB = config[\\'dv_memGB\\']")
    (64, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/Snakefile
context_key: ['if strelka2_mode']
    (65, 'if strelka2_mode:')
    (66, "    bedFileGZ = config[\\'bedFileGZ\\']")
    (67, "    glnexus_strelka2_config = config[\\'glnexus_strelka2_config\\']")
    (68, "    exome_param = config[\\'exome_param\\']")
    (69, "    strelka2_memGB = config[\\'strelka2_memGB\\']")
    (70, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/Snakefile
context_key: ['if hc_mode']
    (71, 'if hc_mode:')
    (72, "    glnexus_gatk_config = config[\\'glnexus_gatk_config\\']")
    (73, '    ')
    (74, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/Snakefile
context_key: ['if strelka2_mode and not dv_mode and not hc_mode', 'rule all', 'input']
    (116, 'if strelka2_mode and not dv_mode and not hc_mode:')
    (117, "    include: \\'rules/Snakefile_Strelka2\\'")
    (118, '    rule all:')
    (119, '        input:')
    (120, "            \\'strelka2/genotyped/strelka2_variants.vcf.gz\\'")
    (121, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/Snakefile
context_key: ['if hc_mode and not dv_mode and not strelka2_mode', 'rule all', 'input']
    (122, 'if hc_mode and not dv_mode and not strelka2_mode:')
    (123, "    include: \\'rules/Snakefile_HaplotypeCaller\\'")
    (124, '    rule all:')
    (125, '        input:')
    (126, "            \\'HaplotypeCaller/genotyped/HC_variants.vcf.gz\\'")
    (127, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/Snakefile
context_key: ['if dv_mode and not hc_mode and not strelka2_mode', 'rule all', 'input']
    (128, 'if dv_mode and not hc_mode and not strelka2_mode:')
    (129, "    include: \\'rules/Snakefile_DeepVariant\\'")
    (130, '    rule all:')
    (131, '        input:')
    (132, "            expand(\\'deepVariant/called/vcfs/{sample}_all_chroms.vcf.gz\\', sample=sampleList) if by_chrom else expand(\\'deepVariant/called_by_sample/{sample}.vcf.gz\\', sample=sampleList),")
    (133, "            \\'deepVariant/genotyped/DV_variants.vcf.gz\\'")
    (134, '            ')
    (135, '            ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/Snakefile
context_key: ['if hc_mode and dv_mode and strelka2_mode and not har_mode', 'rule all', 'input']
    (136, 'if hc_mode and dv_mode and strelka2_mode and not har_mode:')
    (137, "    include: \\'rules/Snakefile_HaplotypeCaller\\'")
    (138, "    include: \\'rules/Snakefile_DeepVariant\\'")
    (139, "    include: \\'rules/Snakefile_Strelka2\\'")
    (140, '    rule all:')
    (141, '        input:')
    (142, "            \\'HaplotypeCaller/genotyped/HC_variants.vcf.gz\\',")
    (143, "            \\'deepVariant/genotyped/DV_variants.vcf.gz\\',   ")
    (144, "            \\'strelka2/genotyped/strelka2_variants.vcf.gz\\'")
    (145, '            ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/Snakefile
context_key: ['if hc_mode and dv_mode and strelka2_mode and har_mode', 'rule all', 'input']
    (146, 'if hc_mode and dv_mode and strelka2_mode and har_mode:')
    (147, "    include: \\'rules/Snakefile_HaplotypeCaller\\'")
    (148, "    include: \\'rules/Snakefile_DeepVariant\\'")
    (149, "    include: \\'rules/Snakefile_Strelka2\\'")
    (150, "    include: \\'rules/Snakefile_harmonize\\'")
    (151, '    rule all:')
    (152, '        input:')
    (153, "            \\'ensemble/all_callers_merged_genotypes.vcf.gz\\',")
    (154, "            \\'ensemble/all_callers_merged_genotypes.vcf.gz.tbi\\'")
    (155, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/GEMSCAN, file=workflow/rules/Snakefile_HaplotypeCaller
context_key: ['if clusterMode == "gcp" or useRemoteFiles']
    (8, 'if clusterMode == "gcp" or useRemoteFiles:')
    (9, '    from snakemake.remote.GS import RemoteProvider as GSRemoteProvider')
    (10, '    GS = GSRemoteProvider()')
    (11, '')
    (12, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if len(ensemble) > 1']
    (139, 'if len(ensemble) > 1:')
    (140, "    results.extend(expand(f\\'{eval_teacher_ens_dir}/{{dataset}}.metrics\\', dataset=eval_datasets))")
    (141, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if install_deps']
    (142, 'if install_deps:')
    (143, '    results.append("/tmp/flags/setup.done")')
    (144, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if not backward_pretrained']
    (145, 'if not backward_pretrained:')
    (146, "    # don\\'t evaluate pretrained model")
    (147, "    results.extend(expand(f\\'{eval_backward_dir}/{{dataset}}.metrics\\',dataset=eval_datasets))")
    (148, '    do_train_backward=True')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if bicleaner_type']
    (158, 'if bicleaner_type:')
    (159, "    clean_corpus_prefix = f\\'{biclean}/corpus\\'")
    (160, "    teacher_corpus = f\\'{biclean}/corpus\\'")
    (161, '    use_bicleaner = True')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if mono_trg_datasets']
    (173, 'if mono_trg_datasets:')
    (174, "    teacher_corpus = f\\'{augmented}/corpus\\'")
    (175, '    augment_corpus = True')
    (176, '    final_teacher_dir = teacher_finetuned_dir')
    (177, "    results.extend(expand(f\\'{eval_res_dir}/teacher-finetuned{{ens}}/{{dataset}}.metrics\\',ens=ensemble, dataset=eval_datasets))")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if install_deps', 'rule setup']
    (219, 'if install_deps:')
    (220, '    rule setup:')
    (221, '        message: "Installing dependencies"')
    (222, '        log: f"{log_dir}/install-deps.log"')
    (223, '        conda: "envs/base.yml"')
    (224, '        priority: 99')
    (225, "        # group: \\'setup\\'")
    (226, '        output: touch("/tmp/flags/setup.done")  # specific to local machine')
    (227, "        shell: \\'bash pipeline/setup/install-deps.sh >> {log} 2>&1\\'")
    (228, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if use_bicleaner', 'rule kenlm']
    (331, 'if use_bicleaner:')
    (332, '    rule kenlm:')
    (333, '        message: "Installing kenlm"')
    (334, '        log: f"{log_dir}/kenlm.log"')
    (335, '        conda: bicleaner_env')
    (336, '        threads: 4')
    (337, "#        group: \\'setup\\'")
    (338, '        output: directory(f"{bin}/kenlm")')
    (339, "        shell: \\'bash pipeline/setup/install-kenlm.sh {kenlm} {threads}  >> {log} 2>&1\\'")
    (340, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if use_bicleaner', 'rule bicleaner_pack']
    (341, '    rule bicleaner_pack:')
    (342, '        message: f"Downloading language pack for bicleaner"')
    (343, '        log: f"{log_dir}/bicleaner_pack.log"')
    (344, '        conda: bicleaner_env')
    (345, '#        group: "clean_corpus"')
    (346, '        threads: 1')
    (347, '        input: rules.kenlm.output')
    (348, '        output: directory(f"{biclean}/pack")')
    (349, '        shell: \\\'\\\'\\\'bash pipeline/bicleaner/download-pack.sh "{output}" {bicleaner_type} >> {log} 2>&1\\\'\\\'\\\'')
    (350, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if use_bicleaner', 'rule bicleaner']
    (351, '    rule bicleaner:')
    (352, '        message: f"Cleaning corpus using {bicleaner_type}"')
    (353, '        log: f"{log_dir}/bicleaner/{{dataset}}.log"')
    (354, '        conda: bicleaner_env')
    (355, '#       group: "bicleaner"')
    (356, '        threads: gpus_num * 2 if bicleaner_type == "bicleaner-ai" else workflow.cores')
    (357, '        resources: gpu=gpus_num if bicleaner_type == "bicleaner-ai" else 0')
    (358, '        input: ancient(rules.kenlm.output), multiext(f"{clean}/corpus/{{dataset}}", f".{src}.gz", f".{trg}.gz"),')
    (359, '                pack_dir=rules.bicleaner_pack.output')
    (360, '        output: multiext(f"{biclean}/corpus/{{dataset}}", f".{src}.gz", f".{trg}.gz")')
    (361, '        params:')
    (362, '            prefix_input=f"{clean}/corpus/{{dataset}}",prefix_output=f"{biclean}/corpus/{{dataset}}",')
    (363, '            threshold=lambda wildcards: bicl_dataset_thresholds[wildcards.dataset]')
    (364, '                                            if wildcards.dataset in bicl_dataset_thresholds')
    (365, '                                            else bicl_default_threshold')
    (366, "        shell: \\'\\'\\'bash pipeline/bicleaner/bicleaner.sh \\\\")
    (367, '                    "{params.prefix_input}" "{params.prefix_output}" {params.threshold} {bicleaner_type} {threads} \\\\')
    (368, '                    "{input.pack_dir}" >> {log} 2>&1\\\'\\\'\\\'')
    (369, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if not vocab_pretrained', 'rule train_vocab']
    (410, 'if not vocab_pretrained:')
    (411, '    rule train_vocab:')
    (412, '        message: "Training spm vocab"')
    (413, '        log: f"{log_dir}/train_vocab.log"')
    (414, '        conda: "envs/base.yml"')
    (415, '        threads: 2')
    (416, '        input: bin=ancient(spm_trainer), corpus_src=clean_corpus_src, corpus_trg=clean_corpus_trg')
    (417, '        output: vocab_path')
    (418, '        params: prefix_train=clean_corpus_prefix,prefix_test=f"{original}/devset"')
    (419, '        shell: \\\'\\\'\\\'bash pipeline/train/spm-vocab.sh "{input.corpus_src}" "{input.corpus_trg}" "{output}" {spm_sample_size} \\\\')
    (420, "                    >> {log} 2>&1\\'\\'\\'")
    (421, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if do_train_backward', 'rule train_backward', 'input']
    (422, 'if do_train_backward:')
    (423, '    rule train_backward:')
    (424, '        message: "Training backward model"')
    (425, '        log: f"{log_dir}/train_backward.log"')
    (426, '        conda: "envs/base.yml"')
    (427, '        threads: gpus_num * 2')
    (428, '        resources: gpu=gpus_num')
    (429, "        #group \\'backward\\'")
    (430, '        input:')
    (431, '            rules.merge_devset.output, train_src=clean_corpus_src,train_trg=clean_corpus_trg,')
    (432, '            bin=ancient(trainer), vocab=vocab_path,')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if do_train_backward', 'rule train_backward', 'output']
    (433, "        output:  model=f\\'{backward_dir}/{best_model}\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if do_train_backward', 'rule train_backward', 'params']
    (434, '        params: prefix_train=clean_corpus_prefix,prefix_test=f"{original}/devset",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if do_train_backward', 'rule train_backward']
    (435, '                args=get_args("training-backward")')
    (436, "        shell: \\'\\'\\'bash pipeline/train/train.sh \\\\")
    (437, '                    backward train {trg} {src} "{params.prefix_train}" "{params.prefix_test}" "{backward_dir}" \\\\')
    (438, '                    "{input.vocab}" "{best_model_metric}" {params.args} >> {log} 2>&1\\\'\\\'\\\'')
    (439, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if augment_corpus']
    (440, 'if augment_corpus:')
    (441, '    checkpoint split_mono_trg:')
    (442, '        message: "Splitting monolingual trg dataset"')
    (443, '        log: f"{log_dir}/split_mono_trg.log"')
    (444, '        conda: "envs/base.yml"')
    (445, '        threads: 1')
    (446, '        input: corpora=f"{clean}/mono.{trg}.gz", bin=ancient(deduper)')
    (447, "        output: directory(f\\'{translated}/mono_trg\\')")
    (448, "        shell: \\'bash pipeline/translate/split-mono.sh {input.corpora} {output} {split_length} >> {log} 2>&1\\'")
    (449, '')
    (450, '    rule translate_mono_trg:')
    (451, '        message: "Translating monolingual trg dataset with backward model"')
    (452, '        log: f"{log_dir}/translate_mono_trg/{{part}}.log"')
    (453, '        conda: "envs/base.yml"')
    (454, '        threads: gpus_num * 2')
    (455, '        resources: gpu=gpus_num')
    (456, '        input:')
    (457, "            bin=ancient(decoder), file=f\\'{translated}/mono_trg/file.{{part}}\\',")
    (458, "            vocab=vocab_path, model=f\\'{backward_dir}/{best_model}\\'")
    (459, "        output: f\\'{translated}/mono_trg/file.{{part}}.out\\'")
    (460, '        params: args = get_args("decoding-backward")')
    (461, '        shell: \\\'\\\'\\\'bash pipeline/translate/translate.sh "{input.file}" "{input.vocab}" {input.model} {params.args} \\\\')
    (462, "                >> {log} 2>&1\\'\\'\\'")
    (463, '')
    (464, '    rule collect_mono_trg:')
    (465, '        message: "Collecting translated mono trg dataset"')
    (466, '        log: f"{log_dir}/collect_mono_trg.log"')
    (467, '        conda: "envs/base.yml"')
    (468, '        threads: 4')
    (469, "        #group \\'mono_trg\\'")
    (470, '        input:')
    (471, '            lambda wildcards: expand(f"{translated}/mono_trg/file.{{part}}.out",')
    (472, '                part=find_parts(wildcards, checkpoints.split_mono_trg))')
    (473, "        output: f\\'{translated}/mono.{src}.gz\\'")
    (474, '        params: src_mono=f"{clean}/mono.{trg}.gz",dir=directory(f\\\'{translated}/mono_trg\\\')')
    (475, '        shell: \\\'bash pipeline/translate/collect.sh "{params.dir}" "{output}" "{params.src_mono}" >> {log} 2>&1\\\'')
    (476, '')
    (477, '    rule merge_augmented:')
    (478, '        message: "Merging augmented dataset"')
    (479, '        log: f"{log_dir}/merge_augmented.log"')
    (480, '        conda: "envs/base.yml"')
    (481, '        threads: 4')
    (482, "        #group \\'mono_trg\\'")
    (483, '        input:')
    (484, '            src1=clean_corpus_src,src2=rules.collect_mono_trg.output,')
    (485, '            trg1=clean_corpus_trg,trg2=rules.split_mono_trg.input,')
    (486, '            bin=ancient(deduper)')
    (487, "        output: res_src=f\\'{augmented}/corpus.{src}.gz\\',res_trg=f\\'{augmented}/corpus.{trg}.gz\\'")
    (488, "        shell: \\'\\'\\'bash pipeline/translate/merge-corpus.sh \\\\")
    (489, '                    "{input.src1}" "{input.src2}" "{input.trg1}" "{input.trg2}" "{output.res_src}" "{output.res_trg}" \\\\')
    (490, "                      >> {log} 2>&1\\'\\'\\'")
    (491, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if augment_corpus', 'rule finetune_teacher', 'input']
    (508, 'if augment_corpus:')
    (509, '    rule finetune_teacher:')
    (510, '        message: "Finetune teacher on parallel corpus"')
    (511, '        log: f"{log_dir}/finetune_teacher{{ens}}.log"')
    (512, '        conda: "envs/base.yml"')
    (513, '        threads: gpus_num * 2')
    (514, '        resources: gpu=gpus_num')
    (515, '        input:')
    (516, "            rules.merge_devset.output, model=f\\'{teacher_base_dir}{{ens}}/{best_model}\\',")
    (517, '            train_src=clean_corpus_src, train_trg=clean_corpus_trg,')
    (518, '            bin=ancient(trainer), vocab=vocab_path')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if augment_corpus', 'rule finetune_teacher', 'output']
    (519, "        output: model=f\\'{teacher_finetuned_dir}{{ens}}/{best_model}\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if augment_corpus', 'rule finetune_teacher', 'params']
    (520, '        params: prefix_train=clean_corpus_prefix, prefix_test=f"{original}/devset",')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=mozilla/firefox-translations-training, file=Snakefile
context_key: ['if augment_corpus', 'rule finetune_teacher']
    (521, "                dir=directory(f\\'{teacher_finetuned_dir}{{ens}}\\'),")
    (522, '                args=get_args("training-teacher-finetuned")')
    (523, "        shell: \\'\\'\\'bash pipeline/train/train.sh \\\\")
    (524, '                    teacher train {src} {trg} "{params.prefix_train}" "{params.prefix_test}" "{params.dir}" \\\\')
    (525, '                    "{input.vocab}" --pretrained-model "{input.model}" {params.args} >> {log} 2>&1\\\'\\\'\\\'')
    (526, '')
    (527, '### translation with teacher')
    (528, '')
    (529, '# corpus')
    (530, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=osvaldoreisss/transcriptome_assembly_workflow, file=workflow/rules/assembly.smk
context_key: ['input', 'run', 'if sample==sample_old']
    (14, '                if sample==sample_old:')
    (15, '                    continue')
    (16, '                condition = samples.loc[(sample), ["condition"]].dropna()[\\\'condition\\\'][0]')
    (17, '                left=file')
    (18, "                right=file.replace(\\'.R1.\\',\\'.R2.\\')")
    (19, '                out.write(f"{condition}\\\\t{sample}\\\\t{left}\\\\t{right}\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pughlab/PLBR_MedRemixBEDPE, file=workflow/Snakefile
context_key: ['if input_ftype == "fastq"']
    (15, 'if input_ftype == "fastq":')
    (16, '    include: "rules/process_fastq.smk"           ## consensusCruncher & trim-galore')
    (17, '    include: "rules/bwa_align_sort_merge.smk"    ## bwa align, samtools sort, merge')
    (18, '    include: "rules/qc.smk"                      ## fastQC on fastq and bam')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pughlab/PLBR_MedRemixBEDPE, file=workflow/Snakefile
context_key: ['elif input_ftype == "bam"']
    (19, 'elif input_ftype == "bam":')
    (20, '    include: "rules/process_bam.smk"             ## if input_ftype == bam, softlink bam to proper directory and index bam')
    (21, '    include: "rules/qc.smk"                      ## fastQC on fastq and bam')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pughlab/PLBR_MedRemixBEDPE, file=workflow/Snakefile
context_key: ['elif input_ftype == "bedpe"']
    (22, 'elif input_ftype == "bedpe":')
    (23, '    include: "rules/process_bedpe.smk"           ## if input_ftype == bedpe, softlink bedpe to proper directory')
    (24, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pughlab/PLBR_MedRemixBEDPE, file=workflow/Snakefile
context_key: ["if bam_or_bedpe == \\'bedpe\\'"]
    (37, "if bam_or_bedpe == \\'bedpe\\':")
    (38, '    use_bam_or_bedpe.append(')
    (39, '        expand(')
    (40, "            [path_to_data + \\'/{cohort}/results/bedpe_cfmedip_nbglm/bedpe_{sample}_fit_nbglm.bedgraph\\'.format(")
    (41, '                cohort=v[0],')
    (42, '                sample=v[1]) for v in get_all_samples_with_cohorts()])')
    (43, '    )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pughlab/PLBR_MedRemixBEDPE, file=workflow/Snakefile
context_key: ["elif bam_or_bedpe == \\'bam\\'"]
    (44, "elif bam_or_bedpe == \\'bam\\':")
    (45, '    use_bam_or_bedpe.append(')
    (46, '        expand(')
    (47, "            [path_to_data + \\'/{cohort}/results/bam_cfmedip_nbglm/bam_{sample}_fit_nbglm.bedgraph\\'.format(")
    (48, '                cohort=v[0],')
    (49, '                sample=v[1]) for v in get_all_samples_with_cohorts()])')
    (50, '    )')
    (51, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pughlab/PLBR_MedRemixBEDPE, file=workflow/Snakefile
context_key: ["if input_ftype == \\'fastq\\' or input_ftype == \\'bam\\'"]
    (53, "if input_ftype == \\'fastq\\' or input_ftype == \\'bam\\':")
    (54, '    output_qc_or_not.append(')
    (55, '        expand(')
    (56, "            [path_to_data + \\'/{cohort}/qc/{sample}_qc_full.txt\\'.format(")
    (57, '                cohort=v[0],')
    (58, '                sample=v[1]) for v in get_all_samples_with_cohorts()])')
    (59, '    )')
    (60, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/single-cell-rna-seq, file=Snakefile
context_key: ['if "markers" in config.get("celltype", {})']
    (20, 'if "markers" in config.get("celltype", {}):')
    (21, '    markers = pd.read_csv(config["celltype"]["markers"], sep="\\\\t").set_index(')
    (22, '        "name", drop=False')
    (23, '    )')
    (24, '    markers.loc[:, "parent"].fillna("root", inplace=True)')
    (25, '')
    (26, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=VUEG/bdes_to, file=Snakefile
context_key: ['if org_raster in NORMALIZED_DATASETS.keys()']
    (447, '                if org_raster in NORMALIZED_DATASETS.keys():')
    (448, '                    rescaled_raster = harmonized_raster.replace(org_raster,')
    (449, '                                                                NORMALIZED_DATASETS[org_raster])')
    (450, '                    llogger.info("{0} Rescaling dataset {1}".format(prefix, harmonized_raster))')
    (451, '                    llogger.debug("{0} Target dataset {1}".format(prefix, rescaled_raster))')
    (452, '                    spatutils.rescale_raster(harmonized_raster, rescaled_raster,')
    (453, '                                             method="normalize",')
    (454, '                                             only_positive=True, verbose=False)')
    (455, '                    os.remove(harmonized_raster)')
    (456, '                    llogger.debug("{0} Renaming dataset {1} to {2}".format(prefix, rescaled_raster, harmonized_raster))')
    (457, '                    os.rename(rescaled_raster, harmonized_raster)')
    (458, '                    harmonized_raster = rescaled_raster')
    (459, '')
    (460, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ax-Sch/AlphScore, file=workflow/Snakefile
context_key: ['if testing == True', 'rule all', 'rule download_AlphaFold_files', 'rule download_dbNSFP_files', 'rule split_dbNSFP', 'rule get_FEATURE_DSSP', 'rule get_feature_protinter_interactions', 'rule get_feature_HSE', 'rule get_feature_pLDDT', 'rule combine_features_pdb_level', 'rule add_graph_based_analysis_pdb_level', 'rule combine_pdb_level_files_to_protein_level']
    (13, 'if testing == True:')
    (14, '\\tgrid_search_table=grid_search_table.iloc[[0,5]] # testing')
    (15, '\\tdbNSFP_file=dbNSFP_file.iloc[1000:1200]')
    (16, '\\tpd_DMS_uniprot_ids=pd.DataFrame(DMS_uniprot_ids, columns=["uniprot_ids"])')
    (17, '\\tdbNSFP_file=dbNSFP_file.append(pd_DMS_uniprot_ids)')
    (18, '')
    (19, 'PDB_file=pd.read_csv("config/pdb_ids.txt", names=["PDB_ID"])')
    (20, 'PDB_file[["prefix","uniprot_ids","model","postfix"]]=PDB_file["PDB_ID"].str.split("-", expand=True,)')
    (21, 'PDB_dbNSFP=PDB_file[PDB_file.uniprot_ids.isin(dbNSFP_file.uniprot_ids)]')
    (22, 'relevant_uniprot_ids=PDB_dbNSFP["uniprot_ids"].tolist()')
    (23, 'relevant_uniprot_ids=list(set(relevant_uniprot_ids)) # remove duplicates')
    (24, 'relevant_alphafold_models=PDB_dbNSFP["PDB_ID"].tolist()')
    (25, '')
    (26, '')
    (27, 'rule all:')
    (28, '\\tinput:')
    (29, '\\t\\texpand("results/network/{alphmodel}_combined_w_network_and_dbnsfp.csv.gz", alphmodel=relevant_alphafold_models ),')
    (30, '\\t\\t"results/train_testset1/gnomad_extracted_prepro_rec.csv.gz",')
    (31, '\\t\\texpand("results/prediction/{prefix}_results.tsv", prefix=grid_search_table["prefix"].to_list()),')
    (32, '\\t\\t"results/joined_grid/joined_grid.tsv",')
    (33, '\\t\\t"results/prediction_final/final_regular_written_full_model.RData",')
    (34, '\\t\\t"results/prediction_final/final_nopLDDT_written_full_model.RData",')
    (35, '\\t\\t"results/prediction_final/pre_final_model_NullModel_variants.csv.gz",')
    (36, '\\t\\t"results/validation_set/validation_set_w_AlphScore.csv.gz",')
    (37, '\\t\\t"results/analyse_score/spearman_plot.pdf",')
    (38, '\\t\\t"results/plot_k/barplot_preprocessed.pdf",')
    (39, '\\t\\t"results/plot_k/pre_final_model_regular_importance_permutation.pdf",')
    (40, '\\t\\t"results/clinvar2022/values_of_clinvar_variants.tsv.gz",')
    (41, '\\t\\t"results/merge_eval/all_possible_values_concat.csv.gz",')
    (42, '\\t\\t"results/merge_final/all_possible_values_concat.csv.gz",')
    (43, '\\t\\t"results/clinvar2022/values_of_clinvar_variants_FINAL.tsv.gz",')
    (44, '\\t\\t"results/merge_final/all_deduplicated.tsv.gz",')
    (45, '\\t\\t"results/final_model_curves/AlphScorePlot_FINAL.pdf",')
    (46, '\\t\\t"results/clinvar2022_alphafold/ClinVar_val_REVEL.pdf",')
    (47, '\\t\\t"results/pLDDT/arranged_plddt_plots.pdf"')
    (48, '')
    (49, '')
    (50, 'rule download_AlphaFold_files:')
    (51, '\\toutput:')
    (52, '\\t\\texpand(config["pdb_dir"]+"{pdb_name}.pdb.gz", pdb_name=relevant_alphafold_models)')
    (53, '\\tparams:')
    (54, '\\t\\talphafold="results/pdb_files",')
    (55, '\\t\\tpartition=config["long_partition"]')
    (56, '\\tresources: time_job=4800, mem_mb=8000')
    (57, '\\tshell:')
    (58, '\\t\\t"""')
    (59, '\\t\\thomedir=$(pwd)')
    (60, '\\t\\t')
    (61, '\\t\\tmkdir -p "{params.alphafold}"')
    (62, '\\t\\t')
    (63, '\\t\\tcd "{params.alphafold}"')
    (64, '\\t\\trm -f UP000005640_9606_HUMAN_v2.tar')
    (65, '\\t\\twget {config[alphafold_download_adress]}')
    (66, '\\t\\ttar -xf UP000005640_9606_HUMAN_v2.tar')
    (67, '\\t\\trm UP000005640_9606_HUMAN_v2.tar')
    (68, '\\t\\t"""')
    (69, '')
    (70, 'rule download_dbNSFP_files:')
    (71, '\\toutput:')
    (72, '\\t\\t"results/dbNSFP_raw/download_ok",')
    (73, '\\tparams:')
    (74, '\\t\\tdbNSFP="results/dbNSFP_raw",')
    (75, '\\t\\tpartition=config["long_partition"]')
    (76, '\\tresources: time_job=4800, mem_mb=8000')
    (77, '\\tshell:')
    (78, '\\t\\t"""')
    (79, '\\t\\thomedir=$(pwd)')
    (80, '\\t\\tmkdir -p "{params.dbNSFP}"')
    (81, '\\t\\tcd {params.dbNSFP}')
    (82, '\\t\\t')
    (83, '\\t\\trm -f dbNSFP4.2a.zip')
    (84, '\\t\\twget {config[dbNSFP_download_adress]}')
    (85, '\\t\\tunzip dbNSFP4.2a.zip')
    (86, '\\t\\trm dbNSFP4.2a.zip')
    (87, '\\t\\t')
    (88, '\\t\\tcd $homedir')
    (89, '\\t\\ttouch {output}')
    (90, '\\t\\t"""')
    (91, '')
    (92, '')
    (93, 'rule split_dbNSFP:')
    (94, '\\tinput:')
    (95, '\\t\\t"results/dbNSFP_raw/download_ok",')
    (96, '\\toutput:')
    (97, '\\t\\t"results/split_dbNSFP/chr{chr}_ok"')
    (98, '\\tparams:')
    (99, '\\t\\ttmp="results/split_dbNSFP/tmp/chr{chr}/",')
    (100, '\\t\\tdb_file="results/dbNSFP_raw/dbNSFP4.2a_variant.chr{chr}.gz",')
    (101, '\\t\\toutdir="results/split_dbNSFP/by_uniprotID/",')
    (102, '\\t\\tpartition=config["short_partition"]')
    (103, '\\tresources: time_job=480, mem_mb=5000')
    (104, '\\tshell:')
    (105, '\\t\\t"""')
    (106, '\\t\\thomedir=$(pwd)')
    (107, '\\t\\tmkdir -p "{params.tmp}"')
    (108, '\\t\\tmkdir -p "{params.outdir}"')
    (109, '\\t\\tcd "{params.tmp}"')
    (110, '\\t\\tmkdir -p header')
    (111, '\\t\\t')
    (112, '\\t\\tif (( $(zcat $homedir"/{params.db_file}" | head -n1 > header/header.txt) )) # pipe / head combination produces non-0-exit')
    (113, '\\t\\tthen')
    (114, '\\t\\techo "not ok"')
    (115, '\\t\\tfi')
    (116, '\\t\\t')
    (117, '\\t\\tzcat $homedir"/{params.db_file}" | tail -n+2 | split -l 20000 - {wildcards.chr}')
    (118, '\\t\\t')
    (119, '\\t\\tfor file in $(find * -maxdepth 0 -type f)')
    (120, '\\t\\tdo')
    (121, '\\t\\tpython $homedir"/workflow/scripts/split_dbNSFP_unnest.py" $homedir"/config/pdb_ids.txt" $(pwd)"/header/header.txt" $file $homedir"/"{params.outdir}')
    (122, '\\t\\tdone')
    (123, '\\t\\t')
    (124, '\\t\\tcd $homedir')
    (125, '\\t\\ttouch "{output}"')
    (126, '\\t\\t"""')
    (127, '')
    (128, 'rule get_FEATURE_DSSP:')
    (129, '\\tinput:')
    (130, '\\t\\tpdb=config["pdb_dir"]+"{pdb_name}.pdb.gz"')
    (131, '\\toutput:')
    (132, '\\t\\t"results/pdb_features/{pdb_name}/{pdb_name}_core.txt"')
    (133, '\\tparams:')
    (134, '\\t\\tpartition=config["short_partition"],')
    (135, '\\tresources: time_job=20')
    (136, '\\tconda: "envs/dssp.yaml"')
    (137, '\\tshell:')
    (138, '\\t\\t"""')
    (139, '\\t\\tfeature="$(pwd)/"{config[feature_folder]}"bin/featurize"')
    (140, '\\t\\texport FEATURE_DIR="$(pwd)/"{config[feature_folder]}"data/"')
    (141, '\\t\\twork_dir="results/pdb_features/{wildcards.pdb_name}/"')
    (142, '\\t\\ttemp_pdb=$work_dir"1A1A.pdb"')
    (143, '\\t\\t')
    (144, '\\t\\texport DSSP_DIR=$work_dir')
    (145, '\\t\\texport PDB_DIR=$work_dir')
    (146, '\\t\\t')
    (147, '\\t\\tmkdir -p $work_dir')
    (148, '\\t\\tgunzip -c "{input}" > $temp_pdb')
    (149, '\\t\\t')
    (150, '\\t\\t#run dssp')
    (151, '\\t\\tcat $temp_pdb | {config[dssp_bin]} -i /dev/stdin > $work_dir"1A1A.dssp"')
    (152, '\\t\\techo 1A1A > $work_dir"pdb_id"')
    (153, '\\t\\t')
    (154, '\\t\\t$feature $temp_pdb -n1 -w0.2 | grep ":A@CA$" > $work_dir"{wildcards.pdb_name}_core.txt"')
    (155, '\\t\\t$feature $temp_pdb -n1 -w3 | grep ":A@CA$" > $work_dir"{wildcards.pdb_name}_3A.txt"')
    (156, '\\t\\t$feature $temp_pdb -n1 -w6 | grep ":A@CA$" > $work_dir"{wildcards.pdb_name}_6A.txt"')
    (157, '\\t\\t$feature $temp_pdb -n1 -w9 | grep ":A@CA$" > $work_dir"{wildcards.pdb_name}_9A.txt"')
    (158, '\\t\\t$feature $temp_pdb -n1 -w12 | grep ":A@CA$" > $work_dir"{wildcards.pdb_name}_12A.txt"')
    (159, '\\t\\t')
    (160, '\\t\\trm $temp_pdb*')
    (161, '\\t\\t"""')
    (162, '')
    (163, 'rule get_feature_protinter_interactions:')
    (164, '\\tinput:')
    (165, '\\t\\tpdb=config["pdb_dir"]+"{pdb_name}.pdb.gz"')
    (166, '\\toutput:')
    (167, '\\t\\t"results/pdb_features/{pdb_name}/result_hbond_side_side.csv"')
    (168, '\\tparams:')
    (169, '\\t\\tpartition=config["short_partition"]')
    (170, '\\tresources: time_job=60')
    (171, '\\tshell:')
    (172, '\\t\\t"""')
    (173, '\\t\\tprotinter="$(pwd)/"{config[protinter_bin]}')
    (174, '\\t\\twork_dir="results/pdb_features/{wildcards.pdb_name}/"')
    (175, '\\t\\tmkdir -p $work_dir')
    (176, '\\t\\t')
    (177, '\\t\\tgunzip -c "{input}" > $work_dir"/pdbfile.pdb"')
    (178, '\\t\\tcd $work_dir')
    (179, '\\t\\t')
    (180, '\\t\\tpython3 $protinter pdbfile.pdb -hydrophobic -csv')
    (181, '\\t\\tpython3 $protinter pdbfile.pdb -disulphide -csv')
    (182, '\\t\\tpython3 $protinter pdbfile.pdb -ionic -csv')
    (183, '\\t\\tpython3 $protinter pdbfile.pdb -aroaro -csv')
    (184, '\\t\\tpython3 $protinter pdbfile.pdb -arosul -csv')
    (185, '\\t\\tpython3 $protinter pdbfile.pdb -catpi -csv')
    (186, '\\t\\t#python3 $protinter pdbfile.pdb -hb1 -csv')
    (187, '\\t\\tpython3 $protinter pdbfile.pdb -hb2 -csv')
    (188, '\\t\\tpython3 $protinter pdbfile.pdb -hb3 -csv')
    (189, '\\t\\tpython3 $protinter pdbfile.pdb -within_radius -csv')
    (190, '\\t\\t')
    (191, '\\t\\tsed -i "s/ //g" result_*')
    (192, '\\t\\trm pdbfile.pdb')
    (193, '\\t\\t"""')
    (194, '')
    (195, 'rule get_feature_HSE:')
    (196, '\\tinput:')
    (197, '\\t\\tpdb=config["pdb_dir"]+"{pdb_name}.pdb.gz"')
    (198, '\\toutput:')
    (199, '\\t\\t"results/pdb_features/{pdb_name}/HSEs.csv"')
    (200, '\\tparams:')
    (201, '\\t\\tpdb_path=config["pdb_dir"]+"{pdb_name}.pdb",')
    (202, '\\t\\tpartition=config["short_partition"]')
    (203, '\\tresources: time_job=20')
    (204, '\\trun:')
    (205, '\\t\\timport os')
    (206, '\\t\\tfrom Bio.PDB.PDBParser import PDBParser')
    (207, '\\t\\timport Bio.PDB as bpd')
    (208, '\\t\\timport pandas as pd')
    (209, '\\t\\tpdb_path=params[0]')
    (210, '\\t\\tos.system(\\\'gunzip -c "\\\' + pdb_path + \\\'.gz" > "\\\' + pdb_path + \\\'X"\\\' )')
    (211, '\\t\\t')
    (212, '\\t\\tparser = PDBParser()')
    (213, '\\t\\tstructure = parser.get_structure("test", pdb_path + \\\'X\\\')')
    (214, '\\t\\thse = bpd.HSExposure')
    (215, '\\t\\texp_ca = hse.HSExposureCB(structure)')
    (216, '\\t\\tos.system(\\\'rm "\\\' + pdb_path + \\\'X"\\\' )')
    (217, '\\t\\tresi_count=len(exp_ca.keys())')
    (218, '\\t\\tlist_RESI_HSE=[ [exp_ca.keys()[i][1][1], exp_ca.property_list[i][1][0], exp_ca.property_list[i][1][1] ] for i in range(0, resi_count)]')
    (219, '\\t\\tdf_HSE=pd.DataFrame(list_RESI_HSE)')
    (220, '\\t\\t# manual check: HSE1=upper sphere, HSE2=lower sphere')
    (221, "\\t\\tdf_HSE.columns =[\\'RESI\\',\\'HSE1\\',\\'HSE2\\']")
    (222, '\\t\\tdf_HSE.index=df_HSE.RESI')
    (223, '\\t\\tdf_HSE.to_csv(output[0], index=False)')
    (224, '')
    (225, '')
    (226, 'rule get_feature_pLDDT:')
    (227, '\\tinput:')
    (228, '\\t\\tpdb=config["pdb_dir"]+"{pdb_name}.pdb.gz"')
    (229, '\\toutput:')
    (230, '\\t\\t"results/pdb_features/{pdb_name}/b_factors.csv"')
    (231, '\\tparams:')
    (232, '\\t\\tpartition=config["short_partition"]')
    (233, '\\tresources: time_job=20')
    (234, '\\trun:')
    (235, '\\t\\timport pandas as pd ')
    (236, '\\t\\tfrom biopandas.pdb import PandasPdb')
    (237, '\\t\\timport numpy as np')
    (238, '\\t\\tppdb_load = PandasPdb()')
    (239, '\\t\\tppdb_load.read_pdb(input[0])')
    (240, '\\t\\tpd_df=ppdb_load.df["ATOM"][["residue_number","atom_name","b_factor"]].to_numpy()')
    (241, '\\t\\tnp.savetxt(output[0], pd_df[pd_df[:,1]=="CA"], fmt=\\\'%s\\\')')
    (242, '')
    (243, '')
    (244, 'rule combine_features_pdb_level:')
    (245, '\\tinput:')
    (246, '\\t\\t"results/pdb_features/{pdb_name}/result_hbond_side_side.csv",')
    (247, '\\t\\t"results/pdb_features/{pdb_name}/b_factors.csv",')
    (248, '\\t\\t"results/pdb_features/{pdb_name}/{pdb_name}_core.txt",')
    (249, '\\t\\t"results/pdb_features/{pdb_name}/HSEs.csv"')
    (250, '\\toutput:')
    (251, '\\t\\t"results/combine1_pdb_results/{pdb_name}_combined_features.csv.gz"')
    (252, '\\tparams:')
    (253, '\\t\\tpdb_directory="results/pdb_features/{pdb_name}/",')
    (254, '\\t\\theader_feature="resources/header_featurize.txt",')
    (255, '\\t\\tpartition=config["short_partition"]')
    (256, '\\tresources: time_job=480, mem_mb=8000')
    (257, '\\tscript:')
    (258, '\\t\\t"scripts/combine_features_pdb_level.py"')
    (259, '')
    (260, '')
    (261, 'rule add_graph_based_analysis_pdb_level:')
    (262, '\\tinput:')
    (263, '\\t\\tcomb1="results/combine1_pdb_results/{pdb_name}_combined_features.csv.gz",')
    (264, '\\t\\tpdb=config["pdb_dir"]+"{pdb_name}.pdb.gz",')
    (265, '\\t\\tdbnsfp=expand("results/split_dbNSFP/chr{chr}_ok",chr=chroms),')
    (266, '\\toutput:')
    (267, '\\t\\t"results/network/{pdb_name}_combined_w_network_and_dbnsfp.csv.gz"')
    (268, '\\tparams:')
    (269, '\\t\\t"results/split_dbNSFP/by_uniprotID/",')
    (270, '\\t\\tpdb_path=config["pdb_dir"]+"{pdb_name}.pdb",')
    (271, '\\t\\tpartition=config["short_partition"]')
    (272, '\\tresources: time_job=240, mem_mb=15000')
    (273, '\\tscript:')
    (274, '\\t\\t"scripts/graph_based_analysis_pdb_level.py"')
    (275, '\\t\\t')
    (276, '')
    (277, 'rule combine_pdb_level_files_to_protein_level:')
    (278, '\\tinput:')
    (279, '\\t\\tlambda wildcards : ["results/network/" + el.rstrip("\\')
    (280, '") + "_combined_w_network_and_dbnsfp.csv.gz" for el in relevant_alphafold_models if el.split("-")[1] in wildcards.uniprot_id] # AF-A0A087WUL8-F3-model_v1_combined_features')
    (281, '\\toutput:')
    (282, '\\t\\t"results/combine2_protein/{uniprot_id}_w_AFfeatures.csv.gz"')
    (283, '\\tparams:')
    (284, '\\t\\t"results/split_dbNSFP/by_uniprotID/",')
    (285, '\\t\\tpartition=config["short_partition"]')
    (286, '\\tresources: time_job=240, mem_mb=lambda wildcards : 3000+1000*len([el.rstrip("\\')
    (287, '") for el in relevant_alphafold_models if el.split("-")[1] in wildcards.uniprot_id])')
    (288, '\\trun:')
    (289, '\\t\\timport pandas as pd')
    (290, '\\t\\timport os')
    (291, '\\t\\tli =[]')
    (292, '\\t\\tfor filename in input:')
    (293, '\\t\\t\\ttry:')
    (294, '\\t\\t\\t\\tdf = pd.read_csv(filename, low_memory=False)')
    (295, '\\t\\t\\t\\tli.append(df)')
    (296, '\\t\\t\\texcept:')
    (297, '\\t\\t\\t\\tprint(filename + " error")')
    (298, '\\t\\ttry:')
    (299, '\\t\\t\\tframe = pd.concat(li, axis=0, ignore_index=True)')
    (300, '\\t\\texcept:')
    (301, '\\t\\t\\tprint("empty df")')
    (302, "\\t\\t\\tos.system(\\'touch \\' + output[0])")
    (303, '        \\t\\tos._exit(0)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=percyfal/assemblyeval-smk, file=workflow/rules/common.smk
context_key: ['if WRAPPER_PREFIX == SMK_WRAPPER_PREFIX']
    (15, 'if WRAPPER_PREFIX == SMK_WRAPPER_PREFIX:')
    (16, '    # Change main to version number once we start sem-versioning')
    (17, '    WRAPPER_PREFIX = "https://raw.githubusercontent.com/percyfal/assemblyeval-smk/main/workflow/wrappers"')
    (18, '')
    (19, '# this container defines the underlying OS for each job when using the workflow')
    (20, '# with --use-conda --use-singularity')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['try', 'if cgr_data is False']
    (98, '        if cgr_data is False:')
    (99, "            fq1 = header.index(\\'fq1\\')")
    (100, "            fq2 = header.index(\\'fq2\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['try', 'if cgr_data']
    (102, '        if cgr_data:')
    (103, "            sys.exit(\\'ERROR: Manifest file \\' + meta_man_fullpath + \\' must contain headers Run-ID and Project-ID\\')")
    (104, '        else:')
    (105, "            sys.exit(\\'ERROR: Manifest file \\' + meta_man_fullpath + \\' must contain headers Run-ID, Project-ID, fq1, and fq2\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['try', 'if l[0] in sampleDict.keys()']
    (108, '        if l[0] in sampleDict.keys():')
    (109, "            sys.exit(\\'ERROR: Duplicate sample IDs detected in \\' + meta_man_fullpath)")
    (110, '        if cgr_data is True:')
    (111, '            sampleDict[l[0]] = (l[runID], l[projID])  # SampleID, Run-ID, Project-ID')
    (112, '        else:')
    (113, '            sampleDict[l[0]] = (l[runID], l[projID], l[fq1], l[fq2])')
    (114, '        RUN_IDS.append(l[runID])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if Q2_2017']
    (235, 'if Q2_2017:')
    (236, '    include: "rules/Snakefile_2017.11"')
    (237, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not cgr_data', 'rule fix_qiita_fastq_header_r1', 'input']
    (278, 'if not cgr_data:')
    (279, '    rule fix_qiita_fastq_header_r1:')
    (280, '        """QIITA data has a header that breaks fq spec - this checks and corrects it.')
    (281, "        If there\\'s no space in the first :-delimited field, then the file is just renamed.")
    (282, '')
    (283, '        E.g.:')
    (284, '        Original header: @12015.MIC2055.0003_34 M05314:89:000000000-BPV43:1:1102:14089:1660 1:N:0:1 orig_bc=TATTGAATATTG new_bc=TATTGAATATTG bc_diffs=0')
    (285, '        changed to @M05314:89:000000000-BPV43:1:1102:14089:1660 1:N:0:1 orig_bc=TATTGAATATTG new_bc=TATTGAATATTG bc_diffs=0 orig_header=@12015.MIC2055.0003_34 M05314')
    (286, '        """')
    (287, '        input:')
    (288, "            out_dir + \\'fastqs/{sample}_R1.fastq.gz\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not cgr_data', 'rule fix_qiita_fastq_header_r1', 'output']
    (289, '        output:')
    (290, "            temp(out_dir + \\'fastqs/{sample}_R1_fixed.fastq.gz\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not cgr_data', 'rule fix_qiita_fastq_header_r1', 'benchmark']
    (291, '        benchmark:')
    (292, "            out_dir + \\'run_times/fix_qiita_fastq_header_r1/{sample}.tsv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not cgr_data', 'rule fix_qiita_fastq_header_r1', 'shell']
    (293, '        shell:')
    (294, '            \\\'if [[ $(zcat {input} | head -n1 | cut -f1 -d\\\\":\\\\") =~ " " ]]; then \\\\')
    (295, '                zcat {input} | awk \\\\\\\'{{if (NR % 4 == 1) {{n=split($0, arr, " "); split(arr[2],tag,":"); printf "@%s ", arr[2]; for (i=3; i<=n; i++) printf "%s ",arr[i]; printf "orig_header=@%s %s\\\\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['else \\']
    (297, '            else \\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['else \\']
    (319, '            else \\\\')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule dada2_denoise', 'input']
    (445, 'if not Q2_2017:')
    (446, '    rule dada2_denoise:')
    (447, '        """ Generates feature tables and feature sequences')
    (448, '        This method denoises paired-end sequences, dereplicates them, and filters chimeras.')
    (449, '        Each feature in the table is represented by one sequence (joined paired-end).')
    (450, '')
    (451, '        See notes above.')
    (452, '        """')
    (453, '        input:')
    (454, "            qza = out_dir + \\'import_and_demultiplex/{runID}.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule dada2_denoise', 'output']
    (455, '        output:')
    (456, "            features = out_dir + \\'denoising/feature_tables/{runID}.qza\\',")
    (457, "            seqs = out_dir + \\'denoising/sequence_tables/{runID}.qza\\',")
    (458, "            stats = out_dir + \\'denoising/stats/{runID}.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule dada2_denoise', 'params']
    (459, '        params:')
    (460, '            trim_l_f = trim_left_f,')
    (461, '            trim_l_r = trim_left_r,')
    (462, '            trun_len_f = trunc_len_f,')
    (463, '            trun_len_r = trunc_len_r,')
    (464, '            min_fold = min_fold')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule dada2_denoise', 'benchmark']
    (465, '        benchmark:')
    (466, "            out_dir + \\'run_times/dada2_denoise/{runID}.tsv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule dada2_denoise', 'threads']
    (467, '        threads: 8')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule dada2_denoise', 'run']
    (468, '        run:')
    (469, "            shell(\\'qiime dada2 denoise-paired \\\\")
    (470, '                --verbose \\\\')
    (471, '                --p-n-threads {threads} \\\\')
    (472, '                --i-demultiplexed-seqs {input.qza} \\\\')
    (473, '                --o-table {output.features} \\\\')
    (474, '                --o-representative-sequences {output.seqs} \\\\')
    (475, '                --o-denoising-stats {output.stats} \\\\')
    (476, '                --p-trim-left-f {params.trim_l_f} \\\\')
    (477, '                --p-trim-left-r {params.trim_l_r} \\\\')
    (478, '                --p-trunc-len-f {params.trun_len_f} \\\\')
    (479, '                --p-trunc-len-r {params.trun_len_r} \\\\')
    (480, "                --p-min-fold-parent-over-abundance {params.min_fold}\\')")
    (481, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule dada2_stats_visualization', 'input']
    (482, '    rule dada2_stats_visualization:')
    (483, '        """Generating visualization for DADA2 stats by flowcell.')
    (484, '')
    (485, '        SS: This can actually be used for both both 2017 and 2019 (currently only under 2019)')
    (486, '')
    (487, '        https://docs.qiime2.org/2017.10/plugins/available/metadata/tabulate/')
    (488, '        """')
    (489, '        input:')
    (490, "            out_dir + \\'denoising/stats/{runID}.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule dada2_stats_visualization', 'output']
    (491, '        output:')
    (492, "            out_dir + \\'denoising/stats/{runID}.qzv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule dada2_stats_visualization', 'benchmark']
    (493, '        benchmark:')
    (494, "            out_dir + \\'run_times/dada2_stats_visualization/{runID}.tsv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule dada2_stats_visualization', 'shell']
    (495, '        shell:')
    (496, "            \\'qiime metadata tabulate \\\\")
    (497, '                --m-input-file {input} \\\\')
    (498, "                --o-visualization {output}\\'")
    (499, '')
    (500, '# to add in metadata, start here: plus re- run perl check!  Must have the qza results for samples')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_samples_with_low_read_count', 'input']
    (565, 'if not Q2_2017:')
    (566, '    rule remove_samples_with_low_read_count:')
    (567, '        """Remove samples that have less than min # reads')
    (568, '')
    (569, '        See https://docs.qiime2.org/2019.1/tutorials/filtering/ for')
    (570, '        additional explanation of this and subsequent filtering rules')
    (571, '        """')
    (572, '        input:')
    (573, "            out_dir + \\'denoising/feature_tables/merged.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_samples_with_low_read_count', 'output']
    (574, '        output:')
    (575, "            out_dir + \\'read_feature_and_sample_filtering/feature_tables/1_remove_samples_with_low_read_count.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_samples_with_low_read_count', 'params']
    (576, '        params:')
    (577, '            f = min_num_reads_per_sample')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_samples_with_low_read_count', 'benchmark']
    (578, '        benchmark:')
    (579, "            out_dir + \\'run_times/remove_samples_with_low_read_count/remove_samples_with_low_read_count.tsv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_samples_with_low_read_count', 'shell']
    (580, '        shell:')
    (581, "            \\'qiime feature-table filter-samples \\\\")
    (582, '                --i-table {input} \\\\')
    (583, '                --p-min-frequency {params.f} \\\\')
    (584, "                --o-filtered-table {output}\\'")
    (585, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_features_with_low_read_count', 'input']
    (586, '    rule remove_features_with_low_read_count:')
    (587, '        """Remove features that have less than min # reads')
    (588, '        """')
    (589, '        input:')
    (590, "            out_dir + \\'read_feature_and_sample_filtering/feature_tables/1_remove_samples_with_low_read_count.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_features_with_low_read_count', 'output']
    (591, '        output:')
    (592, "            out_dir + \\'read_feature_and_sample_filtering/feature_tables/2_remove_features_with_low_read_count.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_features_with_low_read_count', 'params']
    (593, '        params:')
    (594, '            f = min_num_reads_per_feature')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_features_with_low_read_count', 'benchmark']
    (595, '        benchmark:')
    (596, "            out_dir + \\'run_times/remove_features_with_low_read_count/remove_features_with_low_read_count.tsv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_features_with_low_read_count', 'shell']
    (597, '        shell:')
    (598, "            \\'qiime feature-table filter-features \\\\")
    (599, '                --i-table {input} \\\\')
    (600, '                --p-min-frequency {params.f} \\\\')
    (601, "                --o-filtered-table {output}\\'")
    (602, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_features_with_low_sample_count', 'input']
    (603, '    rule remove_features_with_low_sample_count:')
    (604, '        """Remove features that occur in less than min # samples')
    (605, '        """')
    (606, '        input:')
    (607, "            out_dir + \\'read_feature_and_sample_filtering/feature_tables/2_remove_features_with_low_read_count.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_features_with_low_sample_count', 'output']
    (608, '        output:')
    (609, "            out_dir + \\'read_feature_and_sample_filtering/feature_tables/3_remove_features_with_low_sample_count.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_features_with_low_sample_count', 'params']
    (610, '        params:')
    (611, '            f = min_num_samples_per_feature')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_features_with_low_sample_count', 'benchmark']
    (612, '        benchmark:')
    (613, "            out_dir + \\'run_times/remove_features_with_low_sample_count/remove_features_with_low_sample_count.tsv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_features_with_low_sample_count', 'shell']
    (614, '        shell:')
    (615, "            \\'qiime feature-table filter-features \\\\")
    (616, '                --i-table {input} \\\\')
    (617, '                --p-min-samples {params.f} \\\\')
    (618, "                --o-filtered-table {output}\\'")
    (619, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_samples_with_low_feature_count', 'input']
    (620, '    rule remove_samples_with_low_feature_count:')
    (621, '        """ Remove samples that have less than min # features')
    (622, '')
    (623, '        Min of at least 1 is necessary to remove 0 read samples')
    (624, '        (e.g. blanks) for downstream PhyloSeq manipulation.')
    (625, '        """')
    (626, '        input:')
    (627, "            out_dir + \\'read_feature_and_sample_filtering/feature_tables/3_remove_features_with_low_sample_count.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_samples_with_low_feature_count', 'output']
    (628, '        output:')
    (629, "            out_dir + \\'read_feature_and_sample_filtering/feature_tables/4_remove_samples_with_low_feature_count.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_samples_with_low_feature_count', 'params']
    (630, '        params:')
    (631, '            f = min_num_features_per_sample')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_samples_with_low_feature_count', 'benchmark']
    (632, '        benchmark:')
    (633, "            out_dir + \\'run_times/remove_samples_with_low_feature_count/remove_samples_with_low_feature_count.tsv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_samples_with_low_feature_count', 'shell']
    (634, '        shell:')
    (635, "            \\'qiime feature-table filter-samples \\\\")
    (636, '                --i-table {input} \\\\')
    (637, '                --p-min-features {params.f} \\\\')
    (638, "                --o-filtered-table {output}\\'")
    (639, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule filtered_feature_table_visualization', 'input']
    (640, '    rule filtered_feature_table_visualization:')
    (641, '        """Generate visual and tabular summaries of a feature table')
    (642, '        Generate information on how many sequences are associated with each sample')
    (643, '        and with each feature, histograms of those distributions, and some related')
    (644, '        summary statistics.')
    (645, '        """')
    (646, '        input:')
    (647, "            qza1 = out_dir + \\'read_feature_and_sample_filtering/feature_tables/1_remove_samples_with_low_read_count.qza\\',")
    (648, "            qza2 = out_dir + \\'read_feature_and_sample_filtering/feature_tables/2_remove_features_with_low_read_count.qza\\',")
    (649, "            qza3 = out_dir + \\'read_feature_and_sample_filtering/feature_tables/3_remove_features_with_low_sample_count.qza\\',")
    (650, "            qza4 = out_dir + \\'read_feature_and_sample_filtering/feature_tables/4_remove_samples_with_low_feature_count.qza\\',")
    (651, "            q2_manifest = out_dir + \\'manifests/manifest_qiime2.tsv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule filtered_feature_table_visualization', 'output']
    (652, '        output:')
    (653, "            qzv1 = out_dir + \\'read_feature_and_sample_filtering/feature_tables/1_remove_samples_with_low_read_count.qzv\\',")
    (654, "            qzv2 = out_dir + \\'read_feature_and_sample_filtering/feature_tables/2_remove_features_with_low_read_count.qzv\\',")
    (655, "            qzv3 = out_dir + \\'read_feature_and_sample_filtering/feature_tables/3_remove_features_with_low_sample_count.qzv\\',")
    (656, "            qzv4 = out_dir + \\'read_feature_and_sample_filtering/feature_tables/4_remove_samples_with_low_feature_count.qzv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule filtered_feature_table_visualization', 'benchmark']
    (657, '        benchmark:')
    (658, "            out_dir + \\'run_times/filtered_feature_table_visualization/feature_table_visualization.tsv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule filtered_feature_table_visualization', 'shell']
    (659, '        shell:')
    (660, "            \\'qiime feature-table summarize \\\\")
    (661, '                --i-table {input.qza1} \\\\')
    (662, '                --o-visualization {output.qzv1} \\\\')
    (663, '                --m-sample-metadata-file {input.q2_manifest} && \\\\')
    (664, '            qiime feature-table summarize \\\\')
    (665, '                --i-table {input.qza2} \\\\')
    (666, '                --o-visualization {output.qzv2} \\\\')
    (667, '                --m-sample-metadata-file {input.q2_manifest} && \\\\')
    (668, '            qiime feature-table summarize \\\\')
    (669, '                --i-table {input.qza3} \\\\')
    (670, '                --o-visualization {output.qzv3} \\\\')
    (671, '                --m-sample-metadata-file {input.q2_manifest} && \\\\')
    (672, '            qiime feature-table summarize \\\\')
    (673, '                --i-table {input.qza4} \\\\')
    (674, '                --o-visualization {output.qzv4} \\\\')
    (675, "                --m-sample-metadata-file {input.q2_manifest}\\'")
    (676, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule apply_filters_to_sequence_tables', 'input']
    (677, '    rule apply_filters_to_sequence_tables:')
    (678, '        input:')
    (679, "            feat1 = out_dir + \\'read_feature_and_sample_filtering/feature_tables/1_remove_samples_with_low_read_count.qza\\',")
    (680, "            feat2 = out_dir + \\'read_feature_and_sample_filtering/feature_tables/2_remove_features_with_low_read_count.qza\\',")
    (681, "            feat3 = out_dir + \\'read_feature_and_sample_filtering/feature_tables/3_remove_features_with_low_sample_count.qza\\',")
    (682, "            feat4 = out_dir + \\'read_feature_and_sample_filtering/feature_tables/4_remove_samples_with_low_feature_count.qza\\',")
    (683, "            seq_table = out_dir + \\'denoising/sequence_tables/merged.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule apply_filters_to_sequence_tables', 'output']
    (684, '        output:')
    (685, "            seq1 = out_dir + \\'read_feature_and_sample_filtering/sequence_tables/1_remove_samples_with_low_read_count.qza\\',")
    (686, "            seq2 = out_dir + \\'read_feature_and_sample_filtering/sequence_tables/2_remove_features_with_low_read_count.qza\\',")
    (687, "            seq3 = out_dir + \\'read_feature_and_sample_filtering/sequence_tables/3_remove_features_with_low_sample_count.qza\\',")
    (688, "            seq4 = out_dir + \\'read_feature_and_sample_filtering/sequence_tables/4_remove_samples_with_low_feature_count.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule apply_filters_to_sequence_tables', 'benchmark']
    (689, '        benchmark:')
    (690, "            out_dir + \\'run_times/apply_filters_to_sequence_tables/apply_filters_to_sequence_tables.tsv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule apply_filters_to_sequence_tables', 'shell']
    (691, '        shell:')
    (692, "            \\'qiime feature-table filter-seqs --i-data {input.seq_table} --i-table {input.feat1} --o-filtered-data {output.seq1} && \\\\")
    (693, '                qiime feature-table filter-seqs --i-data {input.seq_table} --i-table {input.feat2} --o-filtered-data {output.seq2} && \\\\')
    (694, '                qiime feature-table filter-seqs --i-data {input.seq_table} --i-table {input.feat3} --o-filtered-data {output.seq3} && \\\\')
    (695, "                qiime feature-table filter-seqs --i-data {input.seq_table} --i-table {input.feat4} --o-filtered-data {output.seq4}\\'")
    (696, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule filtered_sequence_table_visualization', 'input']
    (697, '    rule filtered_sequence_table_visualization:')
    (698, '        """Generate visual and tabular summaries for sequences')
    (699, '        Generate a mapping of feature IDs to sequences, and provide links to easily')
    (700, '        BLAST each sequence against the NCBI nt database.')
    (701, '        """')
    (702, '        input:')
    (703, "            qza1 = out_dir + \\'read_feature_and_sample_filtering/sequence_tables/1_remove_samples_with_low_read_count.qza\\',")
    (704, "            qza2 = out_dir + \\'read_feature_and_sample_filtering/sequence_tables/2_remove_features_with_low_read_count.qza\\',")
    (705, "            qza3 = out_dir + \\'read_feature_and_sample_filtering/sequence_tables/3_remove_features_with_low_sample_count.qza\\',")
    (706, "            qza4 = out_dir + \\'read_feature_and_sample_filtering/sequence_tables/4_remove_samples_with_low_feature_count.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule filtered_sequence_table_visualization', 'output']
    (707, '        output:')
    (708, "            qzv1 = out_dir + \\'read_feature_and_sample_filtering/sequence_tables/1_remove_samples_with_low_read_count.qzv\\',")
    (709, "            qzv2 = out_dir + \\'read_feature_and_sample_filtering/sequence_tables/2_remove_features_with_low_read_count.qzv\\',")
    (710, "            qzv3 = out_dir + \\'read_feature_and_sample_filtering/sequence_tables/3_remove_features_with_low_sample_count.qzv\\',")
    (711, "            qzv4 = out_dir + \\'read_feature_and_sample_filtering/sequence_tables/4_remove_samples_with_low_feature_count.qzv\\',")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule filtered_sequence_table_visualization', 'benchmark']
    (712, '        benchmark:')
    (713, "            out_dir + \\'run_times/sequence_table_visualization/filtered_sequence_table_visualization.tsv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule filtered_sequence_table_visualization', 'shell']
    (714, '        shell:')
    (715, "            \\'qiime feature-table tabulate-seqs \\\\")
    (716, '                --i-data {input.qza1} \\\\')
    (717, '                --o-visualization {output.qzv1} && \\\\')
    (718, '            qiime feature-table tabulate-seqs \\\\')
    (719, '                --i-data {input.qza2} \\\\')
    (720, '                --o-visualization {output.qzv2} && \\\\')
    (721, '            qiime feature-table tabulate-seqs \\\\')
    (722, '                --i-data {input.qza3} \\\\')
    (723, '                --o-visualization {output.qzv3} && \\\\')
    (724, '            qiime feature-table tabulate-seqs \\\\')
    (725, '                --i-data {input.qza4} \\\\')
    (726, "                --o-visualization {output.qzv4}\\'")
    (727, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['input', 'run', 'if Q2_2017']
    (844, '        if Q2_2017:')
    (845, '            shell("mv {input} {output.o3} && touch {output.o1} {output.o2}")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_non_bacterial_taxa_sequence_table', 'input']
    (994, 'if not Q2_2017:')
    (995, '    rule remove_non_bacterial_taxa_sequence_table:')
    (996, '        """Remove taxa with non bacterial sequences and bacteria with unannotated phyla ')
    (997, '')
    (998, '        Recommended by Greg Caporaso')
    (999, '        Number of samples will be also dropped because of taxa drops.')
    (1000, '        NOTE: This is necessary for downstream unweighted unifrac weird cluster issue.')
    (1001, '        """')
    (1002, '        input:')
    (1003, "            bacterial_features = out_dir + \\'bacteria_only/feature_tables/{ref}/merged.qza\\',")
    (1004, "            seqs = out_dir + \\'denoising/sequence_tables/merged.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_non_bacterial_taxa_sequence_table', 'output']
    (1005, '        output:')
    (1006, "            out_dir + \\'bacteria_only/sequence_tables/{ref}/merged.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_non_bacterial_taxa_sequence_table', 'benchmark']
    (1007, '        benchmark:')
    (1008, "            out_dir + \\'run_times/remove_non_bacterial_taxa_sequence_table/{ref}.tsv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule remove_non_bacterial_taxa_sequence_table', 'shell']
    (1009, '        shell:')
    (1010, "            \\'qiime feature-table filter-seqs \\\\")
    (1011, '                --i-data {input.seqs} \\\\')
    (1012, '                --i-table {input.bacterial_features} \\\\')
    (1013, "                --o-filtered-data {output}\\'")
    (1014, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule bacteria_only_table_visualization', 'input']
    (1015, '    rule bacteria_only_table_visualization:')
    (1016, '        input:')
    (1017, "            features = out_dir + \\'bacteria_only/feature_tables/{ref}/merged.qza\\',")
    (1018, "            seqs = out_dir + \\'bacteria_only/sequence_tables/{ref}/merged.qza\\',")
    (1019, "            q2_manifest = out_dir + \\'manifests/manifest_qiime2.tsv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule bacteria_only_table_visualization', 'output']
    (1020, '        output:')
    (1021, "            features = out_dir + \\'bacteria_only/feature_tables/{ref}/merged.qzv\\',")
    (1022, "            seqs = out_dir + \\'bacteria_only/sequence_tables/{ref}/merged.qzv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule bacteria_only_table_visualization', 'shell']
    (1023, '        shell:')
    (1024, "            \\'qiime feature-table summarize \\\\")
    (1025, '                --i-table {input.features} \\\\')
    (1026, '                --o-visualization {output.features} \\\\')
    (1027, '                --m-sample-metadata-file {input.q2_manifest} && \\\\')
    (1028, '            qiime feature-table tabulate-seqs \\\\')
    (1029, '                --i-data {input.seqs} \\\\')
    (1030, "                --o-visualization {output.seqs}\\'")
    (1031, '')
    (1032, '    # note that phylogenetics are done with original taxa, including non-bacterial and phylum-unclassified taxa')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule phylogenetic_tree', 'input']
    (1033, '    rule phylogenetic_tree:')
    (1034, '        """Sequence alignment, phylogentic tree assignment, rooting at midpoint')
    (1035, '        Starts by creating a sequence alignment using MAFFT, remove any phylogenetically')
    (1036, '        uninformative or ambiguously aligned reads, infer a phylogenetic tree')
    (1037, '        and then root at its midpoint.')
    (1038, '')
    (1039, '        Note: It appears that downstream analysis (e.g. weighted unifrac) is not')
    (1040, '        substantially affected by using pre- or post-non-bacterial-sequence removal')
    (1041, '        sequence tables.')
    (1042, '        """')
    (1043, '        input:')
    (1044, "            out_dir + \\'read_feature_and_sample_filtering/sequence_tables/4_remove_samples_with_low_feature_count.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule phylogenetic_tree', 'output']
    (1045, '        output:')
    (1046, "            msa = out_dir + \\'phylogenetics/msa.qza\\',")
    (1047, "            masked_msa = out_dir + \\'phylogenetics/masked_msa.qza\\',")
    (1048, "            unrooted_tree = out_dir + \\'phylogenetics/unrooted_tree.qza\\',")
    (1049, "            rooted_tree = out_dir + \\'phylogenetics/rooted_tree.qza\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule phylogenetic_tree', 'benchmark']
    (1050, '        benchmark:')
    (1051, "            out_dir + \\'run_times/phylogenetic_tree/phylogenetic_tree.tsv\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/QIIME_pipeline, file=workflow/Snakefile
context_key: ['if not Q2_2017', 'rule phylogenetic_tree', 'shell']
    (1052, '        shell:')
    (1053, "            \\'qiime phylogeny align-to-tree-mafft-fasttree \\\\")
    (1054, '                --i-sequences {input} \\\\')
    (1055, '                --o-alignment {output.msa} \\\\')
    (1056, '                --o-masked-alignment {output.masked_msa} \\\\')
    (1057, '                --o-tree {output.unrooted_tree} \\\\')
    (1058, "                --o-rooted-tree {output.rooted_tree}\\'")
    (1059, '')
    (1060, '# note that alpha and beta diversity are done with filtered taxa, which excludes non-bacterial and phylum-unclassified taxa')
    (1061, '# possible site of entry if you want to change sampling depth threshold!')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=genomic-medicine-sweden/pgx, file=workflow/rules/common.smk
context_key: ['if not workflow.overwrite_configfiles']
    (23, 'if not workflow.overwrite_configfiles:')
    (24, '    sys.exit("At least one config file must be passed using --configfile/--configfiles, by command line or a profile!")')
    (25, '')
    (26, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=rusch-emma/polychrom-workflow, file=workflow/Snakefile
context_key: ['if not config']
    (6, 'if not config:')
    (7, '    configfile: "config/config.yaml"')
    (8, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=rusch-emma/polychrom-workflow, file=workflow/Snakefile
context_key: ['if "parameters_file" in sim_config and os.path.isfile(sim_config["parameters_file"])']
    (19, 'if "parameters_file" in sim_config and os.path.isfile(sim_config["parameters_file"]):')
    (20, '    # use provided parameter space from table')
    (21, '    param_df = pd.read_csv(sim_config["parameters_file"], sep="\\\\t")')
    (22, '    paramspace = Paramspace(param_df, filename_params="*", param_sep="=")')
    (23, '    params_pattern = paramspace.wildcard_pattern')
    (24, '    params_instances = paramspace.instance_patterns')
    (25, '    param_keys = param_df.columns.values')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=rusch-emma/polychrom-workflow, file=examples/sim_hic/workflow/Snakefile
context_key: ['if not config']
    (6, 'if not config:')
    (7, '    configfile: "config/config.yaml"')
    (8, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=rusch-emma/polychrom-workflow, file=examples/sim_hic/workflow/Snakefile
context_key: ['if "parameters_file" in sim_config and os.path.isfile(sim_config["parameters_file"])']
    (19, 'if "parameters_file" in sim_config and os.path.isfile(sim_config["parameters_file"]):')
    (20, '    # use provided parameter space from table')
    (21, '    param_df = pd.read_csv(sim_config["parameters_file"], sep="\\\\t")')
    (22, '    paramspace = Paramspace(param_df, filename_params="*", param_sep="=")')
    (23, '    params_pattern = paramspace.wildcard_pattern')
    (24, '    params_instances = paramspace.instance_patterns')
    (25, '    param_keys = param_df.columns.values')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sravel/RattleSNP, file=rattleSNP/snakefiles/Snakefile
context_key: ['Examples', 'if cluster_config', 'if rule in cluster_config', "if \\'threads\\' in cluster_config[rule]"]
    (59, '    if cluster_config:')
    (60, '        if rule in cluster_config:')
    (61, "            if \\'threads\\' in cluster_config[rule]:")
    (62, "                return int(cluster_config[rule][\\'threads\\'])")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sravel/RattleSNP, file=rattleSNP/snakefiles/Snakefile
context_key: ['Examples', 'if cluster_config', 'if rule in cluster_config', "elif \\'cpus-per-task\\' in cluster_config[rule]"]
    (63, "            elif \\'cpus-per-task\\' in cluster_config[rule]:")
    (64, "                return int(cluster_config[rule][\\'cpus-per-task\\'])")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sravel/RattleSNP, file=rattleSNP/snakefiles/Snakefile
context_key: ['Examples', 'if cluster_config', "elif \\'__default__\\' in cluster_config", "if \\'cpus-per-task\\' in cluster_config[\\'__default__\\']"]
    (65, "        elif \\'__default__\\' in cluster_config:")
    (66, "            if \\'cpus-per-task\\' in cluster_config[\\'__default__\\']:")
    (67, "                return int(cluster_config[\\'__default__\\'][\\'cpus-per-task\\'])")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sravel/RattleSNP, file=rattleSNP/snakefiles/Snakefile
context_key: ['Examples', 'if cluster_config', "elif \\'threads\\' in cluster_config[\\'__default__\\']"]
    (68, "        elif \\'threads\\' in cluster_config[\\'__default__\\']:")
    (69, "            return int(cluster_config[\\'__default__\\'][\\'threads\\'])")
    (70, '    # if local')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sravel/RattleSNP, file=rattleSNP/snakefiles/Snakefile
context_key: ['Examples', 'elif workflow.global_resources["_cores"]']
    (71, '    elif workflow.global_resources["_cores"]:')
    (72, '        return workflow.global_resources["_cores"]')
    (73, '    # if cluster not rule and not default or local not _cores return value from call')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tgac-vumc/QDNAseq.snakemake, file=Snakefile
context_key: ['if setting == "service"']
    (32, 'if setting == "service": #rule service')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tgac-vumc/QDNAseq.snakemake, file=Snakefile
context_key: ['elif setting == "research"']
    (36, 'elif setting == "research": #rule research')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

