repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if config.get("grch37")']
    (124, 'def get_genome_build():')
    (125, '    if config.get("grch37"):')
    (126, '        return "grch37"')
    (127, '    else:')
    (128, '        return "grch38"')
    (129, '')
    (130, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if config.get("limit-reads")']
    (222, 'def get_read_limit_param(wildcards, input):')
    (223, '    if config.get("limit-reads"):')
    (224, '        return "| head -n 110000"  # a bit more than 100000 reads because we also have the header')
    (225, '    else:')
    (226, '        return ""')
    (227, '')
    (228, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-benchmark, file=workflow/rules/common.smk
context_key: ['if config.get("limit-reads")']
    (275, 'def get_norm_params(wildcards):')
    (276, '    target = ""')
    (277, '    if config.get("limit-reads"):')
    (278, '        target = "--targets 1"')
    (279, '    return f"--atomize --check-ref s --rm-dup exact {target}"')
    (280, '')
    (281, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CVUA-RRW/FooDMe, file=workflow/rules/common.smk
context_key: ['if config["taxid_filter"] == "None"']
    (32, 'def get_mask():')
    (33, '    if config["taxid_filter"] == "None":')
    (34, '        return "common/nomask"')
    (35, '    else:')
    (36, '        return "common/taxid_mask.txt"')
    (37, '')
    (38, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=CVUA-RRW/FooDMe, file=workflow/rules/common.smk
context_key: ['if config["blocklist"] == "None"']
    (39, 'def get_blocklist():')
    (40, '    if config["blocklist"] == "None":')
    (41, '        return "common/noblock"')
    (42, '    elif config["blocklist"] == "extinct":')
    (43, '        return os.path.join(workflow.basedir, "..", "data", "blocklist.txt")')
    (44, '    else:')
    (45, '        return config["blocklist"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ovewh/Climaso, file=workflow/Snakefile
context_key: ["if config[\\'out_dir\\']"]
    (143, 'def get_control_path(w, variable, grid_label=None):')
    (144, '    if grid_label == None:')
    (145, "        grid_label = config[\\'default_grid_label\\']")
    (146, "    if w.get(\\'experiment\\', None) == \\'piControl\\':")
    (147, "        paths = get_paths(w, variable,\\'piControl\\', grid_label,activity=\\'CMIP\\', control=False)")
    (148, "    elif w.get(\\'experiment\\',None) == \\'histSST\\':")
    (149, "        paths = get_paths(w, variable, \\'histSST-piAer\\', grid_label, activity=\\'AerChemMIP\\',control=False)")
    (150, '    else:')
    (151, '        try:')
    (152, "            paths = get_paths(w, variable,\\'piClim-control\\', grid_label,activity=\\'RFMIP\\', control=True)")
    (153, '        except KeyError:')
    (154, "            paths = get_paths(w, variable,\\'piClim-control\\', grid_label,activity=\\'AerChemMIP\\', control=True)")
    (155, '    if not paths:')
    (156, '        print(paths)')
    (157, '        import sys; sys.exit()')
    (158, '    return paths')
    (159, '')
    (160, 'output_format = {}')
    (161, '')
    (162, '')
    (163, '')
    (164, '# set root outdir path')
    (165, "if config[\\'out_dir\\']:")
    (166, "    outdir = config[\\'out_dir\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ovewh/Climaso, file=workflow/Snakefile
context_key: ["if config[\\'output_format\\']==\\'aerocom_column\\'"]
    (143, 'def get_control_path(w, variable, grid_label=None):')
    (144, '    if grid_label == None:')
    (145, "        grid_label = config[\\'default_grid_label\\']")
    (146, "    if w.get(\\'experiment\\', None) == \\'piControl\\':")
    (147, "        paths = get_paths(w, variable,\\'piControl\\', grid_label,activity=\\'CMIP\\', control=False)")
    (148, "    elif w.get(\\'experiment\\',None) == \\'histSST\\':")
    (149, "        paths = get_paths(w, variable, \\'histSST-piAer\\', grid_label, activity=\\'AerChemMIP\\',control=False)")
    (150, '    else:')
    (151, '        try:')
    (152, "            paths = get_paths(w, variable,\\'piClim-control\\', grid_label,activity=\\'RFMIP\\', control=True)")
    (153, '        except KeyError:')
    (154, "            paths = get_paths(w, variable,\\'piClim-control\\', grid_label,activity=\\'AerChemMIP\\', control=True)")
    (155, '    if not paths:')
    (156, '        print(paths)')
    (157, '        import sys; sys.exit()')
    (158, '    return paths')
    (159, '')
    (160, 'output_format = {}')
    (161, '')
    (162, '')
    (163, '')
    (164, '# set root outdir path')
    (165, "if config[\\'out_dir\\']:")
    (166, "    outdir = config[\\'out_dir\\']")
    (167, 'else:')
    (168, "    outdir = \\'results/\\'")
    (169, '')
    (170, "if config[\\'output_format\\']==\\'aerocom_column\\':")
    (171, "    output_format[\\'single_variable\\'] = outdir+\\'{model}_{experiment}/renamed/aerocom3_{model}_{experiment}_{variable}_Column_{freq}_monthly.nc\\'")
    (172, '    print(output_format)')
    (173, "    valid_frequencies = \\'daily|annual|2010|monthly\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Ovewh/Climaso, file=workflow/Snakefile
context_key: ["if config[\\'no_intermediate_files\\']"]
    (143, 'def get_control_path(w, variable, grid_label=None):')
    (144, '    if grid_label == None:')
    (145, "        grid_label = config[\\'default_grid_label\\']")
    (146, "    if w.get(\\'experiment\\', None) == \\'piControl\\':")
    (147, "        paths = get_paths(w, variable,\\'piControl\\', grid_label,activity=\\'CMIP\\', control=False)")
    (148, "    elif w.get(\\'experiment\\',None) == \\'histSST\\':")
    (149, "        paths = get_paths(w, variable, \\'histSST-piAer\\', grid_label, activity=\\'AerChemMIP\\',control=False)")
    (150, '    else:')
    (151, '        try:')
    (152, "            paths = get_paths(w, variable,\\'piClim-control\\', grid_label,activity=\\'RFMIP\\', control=True)")
    (153, '        except KeyError:')
    (154, "            paths = get_paths(w, variable,\\'piClim-control\\', grid_label,activity=\\'AerChemMIP\\', control=True)")
    (155, '    if not paths:')
    (156, '        print(paths)')
    (157, '        import sys; sys.exit()')
    (158, '    return paths')
    (159, '')
    (160, 'output_format = {}')
    (161, '')
    (162, '')
    (163, '')
    (164, '# set root outdir path')
    (165, "if config[\\'out_dir\\']:")
    (166, "    outdir = config[\\'out_dir\\']")
    (167, 'else:')
    (168, "    outdir = \\'results/\\'")
    (169, '')
    (170, "if config[\\'output_format\\']==\\'aerocom_column\\':")
    (171, "    output_format[\\'single_variable\\'] = outdir+\\'{model}_{experiment}/renamed/aerocom3_{model}_{experiment}_{variable}_Column_{freq}_monthly.nc\\'")
    (172, '    print(output_format)')
    (173, "    valid_frequencies = \\'daily|annual|2010|monthly\\'")
    (174, 'else:')
    (175, "    output_format[\\'single_variable\\'] = outdir+\\'{experiment}/{variable}/{variable}_{experiment}_{model}_{freq}.nc\\'")
    (176, "    valid_frequencies = \\'Aday|Amon|clim|Ayear\\'")
    (177, '')
    (178, "if config[\\'no_intermediate_files\\']:")
    (179, '    output_format = {key:temp(item) for key, item in output_format.items()}')
    (180, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/common.smk
context_key: ['if config["settings"]["trimming"]["activate"]']
    (193, 'def get_multiqc_input(wildcards):')
    (194, '    """Get multiqc input."""')
    (195, '    multiqc_input = []')
    (196, '    multiqc_input.extend(')
    (197, '        expand(')
    (198, '            [')
    (199, '                "results/qc/fastqc/{u.sample_name}/{u.library_name}_fastqc.zip",')
    (200, '            ],')
    (201, '            u=samples.itertuples()')
    (202, '        )')
    (203, '    )')
    (204, '    if config["settings"]["trimming"]["activate"]:')
    (205, '        multiqc_input.extend(')
    (206, '            expand(')
    (207, '                [')
    (208, '                    "results/trimmed/{u.sample_name}/{u.library_name}.paired.qc.txt",')
    (209, '                ],')
    (210, '                u=samples.itertuples()')
    (211, '            )')
    (212, '        )')
    (213, '    return multiqc_input')
    (214, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/kGWASflow, file=workflow/rules/common.smk
context_key: ['if config["settings"]["trimming"]["activate"]']
    (220, 'def get_input_path_for_generate_input_lists():')
    (221, '    """Get input path of reads to create input lists."""')
    (222, '    if config["settings"]["trimming"]["activate"]:')
    (223, '        return "results/trimmed"')
    (224, '    else:')
    (225, '        return "results/reads"')
    (226, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=friendsofstrandseq/mosaicatcher-pipeline, file=workflow/rules/common.smk
context_key: ['if config["genecore"] is True']
    (267, 'def findstem(arr):')
    (268, '')
    (269, '    # Determine size of the array')
    (270, '    n = len(arr)')
    (271, '')
    (272, '    # Take first word from array')
    (273, '    # as reference')
    (274, '    s = arr[0]')
    (275, '    l = len(s)')
    (276, '')
    (277, '    res = ""')
    (278, '')
    (279, '    for i in range(l):')
    (280, '        for j in range(i + 1, l + 1):')
    (281, '')
    (282, '            # generating all possible substrings')
    (283, '            # of our reference string arr[0] i.e s')
    (284, '            stem = s[i:j]')
    (285, '            k = 1')
    (286, '            for k in range(1, n):')
    (287, '')
    (288, '                # Check if the generated stem is')
    (289, '                # common to all words')
    (290, '                if stem not in arr[k]:')
    (291, '                    break')
    (292, '')
    (293, '            # If current substring is present in')
    (294, '            # all strings and its length is greater')
    (295, '            # than current result')
    (296, '            if k + 1 == n and len(res) < len(stem):')
    (297, '                res = stem')
    (298, '')
    (299, '    return res')
    (300, '')
    (301, '')
    (302, '# Create configuration file with samples')
    (303, '')
    (304, 'c = HandleInput(')
    (305, '    input_path=config["data_location"],')
    (306, '    genecore_path="{genecore_prefix}/{genecore_date_folder}".format(')
    (307, '        genecore_prefix=config["genecore_prefix"],')
    (308, '        genecore_date_folder=config["genecore_date_folder"],')
    (309, '    ),')
    (310, '    output_path="{data_location}/config/config_df_ashleys.tsv".format(')
    (311, '        data_location=config["data_location"]')
    (312, '    ),')
    (313, '    check_sm_tag=False,')
    (314, '    bam=False,')
    (315, '    genecore=config["genecore"],')
    (316, ')')
    (317, '# df_config_files = c.df_config_files')
    (318, 'if config["genecore"] is True:')
    (319, '    d_master = c.d_master')
    (320, '')
    (321, '# Read config file previously produced')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=friendsofstrandseq/mosaicatcher-pipeline, file=workflow/rules/common.smk
context_key: ['if config["GC_analysis"] is Tru']
    (267, 'def findstem(arr):')
    (268, '')
    (269, '    # Determine size of the array')
    (270, '    n = len(arr)')
    (271, '')
    (272, '    # Take first word from array')
    (273, '    # as reference')
    (274, '    s = arr[0]')
    (275, '    l = len(s)')
    (276, '')
    (277, '    res = ""')
    (278, '')
    (279, '    for i in range(l):')
    (280, '        for j in range(i + 1, l + 1):')
    (281, '')
    (282, '            # generating all possible substrings')
    (283, '            # of our reference string arr[0] i.e s')
    (284, '            stem = s[i:j]')
    (285, '            k = 1')
    (286, '            for k in range(1, n):')
    (287, '')
    (288, '                # Check if the generated stem is')
    (289, '                # common to all words')
    (290, '                if stem not in arr[k]:')
    (291, '                    break')
    (292, '')
    (293, '            # If current substring is present in')
    (294, '            # all strings and its length is greater')
    (295, '            # than current result')
    (296, '            if k + 1 == n and len(res) < len(stem):')
    (297, '                res = stem')
    (298, '')
    (299, '    return res')
    (300, '')
    (301, '')
    (302, '# Create configuration file with samples')
    (303, '')
    (304, 'c = HandleInput(')
    (305, '    input_path=config["data_location"],')
    (306, '    genecore_path="{genecore_prefix}/{genecore_date_folder}".format(')
    (307, '        genecore_prefix=config["genecore_prefix"],')
    (308, '        genecore_date_folder=config["genecore_date_folder"],')
    (309, '    ),')
    (310, '    output_path="{data_location}/config/config_df_ashleys.tsv".format(')
    (311, '        data_location=config["data_location"]')
    (312, '    ),')
    (313, '    check_sm_tag=False,')
    (314, '    bam=False,')
    (315, '    genecore=config["genecore"],')
    (316, ')')
    (317, '# df_config_files = c.df_config_files')
    (318, 'if config["genecore"] is True:')
    (319, '    d_master = c.d_master')
    (320, '')
    (321, '# Read config file previously produced')
    (322, 'df_config_files = c.df_config_files')
    (323, 'df_config_files["Selected"] = True')
    (324, '')
    (325, '# List of available samples')
    (326, 'samples = list(sorted(list(df_config_files.Sample.unique().tolist())))')
    (327, '')
    (328, '')
    (329, '')
    (330, '')
    (331, '# Creation of dicts to be used in the rules')
    (332, 'dict_cells_nb_per_sample = (')
    (333, '    df_config_files.loc[df_config_files["Selected"] == True]')
    (334, '    .groupby("Sample")["Cell"]')
    (335, '    .nunique()')
    (336, '    .to_dict()')
    (337, ')')
    (338, '')
    (339, 'allbams_per_sample = df_config_files.groupby("Sample")["Cell"].apply(list).to_dict()')
    (340, 'cell_per_sample = (')
    (341, '    df_config_files.loc[df_config_files["Selected"] == True]')
    (342, '    .groupby("Sample")["Cell"]')
    (343, '    .unique()')
    (344, '    .apply(list)')
    (345, '    .to_dict()')
    (346, ')')
    (347, 'bam_per_sample_local = (')
    (348, '    df_config_files.loc[df_config_files["Selected"] == True]')
    (349, '    .groupby("Sample")["Cell"]')
    (350, '    .unique()')
    (351, '    .apply(list)')
    (352, '    .to_dict()')
    (353, ')')
    (354, 'bam_per_sample = (')
    (355, '    df_config_files.loc[df_config_files["Selected"] == True]')
    (356, '    .groupby("Sample")["Cell"]')
    (357, '    .unique()')
    (358, '    .apply(list)')
    (359, '    .to_dict()')
    (360, ')')
    (361, '')
    (362, 'plottype_counts = (')
    (363, '    config["plottype_counts"]')
    (364, '    if config["GC_analysis"] is True')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/musta, file=workflow/rules/common.smk
context_key: ['if config.get("run").get("annotate")']
    (188, 'def get_annotation_input():')
    (189, '    if config.get("run").get("annotate"):')
    (190, '        return lambda wildcards: get_vcf_list(wildcards)')
    (191, '    else:')
    (192, '        return resolve_results_filepath(')
    (193, '            config.get("paths").get("results_dir"),')
    (194, '            "results/{sample}_somatic_filtered_selected.vcf.gz",')
    (195, '        )')
    (196, '')
    (197, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/musta, file=workflow/rules/common.smk
context_key: ['if config.get("run").get("analysis")']
    (205, 'def get_maf_file_input():')
    (206, '    if config.get("run").get("analysis"):')
    (207, '        return lambda wildcards: get_maf_list(wildcards)')
    (208, '    else:')
    (209, '        return expand(')
    (210, '            resolve_results_filepath(')
    (211, '                config.get("paths").get("results_dir"),')
    (212, '                "results/annotation/funcotator/{sample}_funcotated.maf",')
    (213, '            ),')
    (214, '            sample=list(samples_master.keys()),')
    (215, '        )')
    (216, '')
    (217, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if config["primers"]["trimming"].get("tsv", ""']
    (43, 'def _group_or_sample(row):')
    (44, '    group = row.get("group", None)')
    (45, '    if pd.isnull(group):')
    (46, '        return row["sample_name"]')
    (47, '    return group')
    (48, '')
    (49, '')
    (50, 'samples["group"] = [_group_or_sample(row) for _, row in samples.iterrows()]')
    (51, 'validate(samples, schema="../schemas/samples.schema.yaml")')
    (52, '')
    (53, '')
    (54, 'groups = samples["group"].unique()')
    (55, '')
    (56, 'if "groups" in config:')
    (57, '    group_annotation = (')
    (58, '        pd.read_csv(config["groups"], sep="\\\\t", dtype={"group": str})')
    (59, '        .set_index("group")')
    (60, '        .sort_index()')
    (61, '    )')
    (62, '    group_annotation = group_annotation.loc[groups]')
    (63, 'else:')
    (64, '    group_annotation = pd.DataFrame({"group": groups}).set_index("group")')
    (65, '')
    (66, 'units = (')
    (67, '    pd.read_csv(')
    (68, '        config["units"],')
    (69, '        sep="\\\\t",')
    (70, '        dtype={"sample_name": str, "unit_name": str},')
    (71, '        comment="#",')
    (72, '    )')
    (73, '    .set_index(["sample_name", "unit_name"], drop=False)')
    (74, '    .sort_index()')
    (75, ')')
    (76, '')
    (77, 'if "umis" not in units.columns:')
    (78, '    units["umis"] = pd.NA')
    (79, '')
    (80, 'validate(units, schema="../schemas/units.schema.yaml")')
    (81, '')
    (82, 'primer_panels = (')
    (83, '    (')
    (84, '        pd.read_csv(')
    (85, '            config["primers"]["trimming"]["tsv"],')
    (86, '            sep="\\\\t",')
    (87, '            dtype={"panel": str, "fa1": str, "fa2": str},')
    (88, '            comment="#",')
    (89, '        )')
    (90, '        .set_index(["panel"], drop=False)')
    (91, '        .sort_index()')
    (92, '    )')
    (93, '    if config["primers"]["trimming"].get("tsv", "")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if config["report"]["activate"]']
    (98, 'def get_final_output(wildcards):')
    (99, '')
    (100, '    final_output = expand(')
    (101, '        "results/qc/multiqc/{group}.html",')
    (102, '        group=groups,')
    (103, '    )')
    (104, '')
    (105, '    if config["report"]["activate"]:')
    (106, '        final_output.extend(')
    (107, '            expand(')
    (108, '                "results/datavzrd-report/{batch}.{event}.fdr-controlled",')
    (109, '                event=config["calling"]["fdr-control"]["events"],')
    (110, '                batch=get_report_batches(),')
    (111, '            )')
    (112, '        )')
    (113, '    else:')
    (114, '        final_output.extend(')
    (115, '            expand(')
    (116, '                "results/final-calls/{group}.{event}.fdr-controlled.bcf",')
    (117, '                group=groups,')
    (118, '                event=config["calling"]["fdr-control"]["events"],')
    (119, '            )')
    (120, '        )')
    (121, '')
    (122, '    if config["tables"]["activate"]:')
    (123, '        final_output.extend(')
    (124, '            expand(')
    (125, '                "results/tables/{group}.{event}.fdr-controlled.tsv",')
    (126, '                group=groups,')
    (127, '                event=config["calling"]["fdr-control"]["events"],')
    (128, '            )')
    (129, '        )')
    (130, '        if config["tables"].get("generate_excel", False):')
    (131, '            final_output.extend(')
    (132, '                expand(')
    (133, '                    "results/tables/{group}.{event}.fdr-controlled.xlsx",')
    (134, '                    group=groups,')
    (135, '                    event=config["calling"]["fdr-control"]["events"],')
    (136, '                )')
    (137, '            )')
    (138, '')
    (139, '    final_output.extend(get_mutational_burden_targets())')
    (140, '')
    (141, '    return final_output')
    (142, '')
    (143, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if config["annotations"]["dgidb"].get("datasources", "")']
    (804, 'def get_dgidb_datasources():')
    (805, '    if config["annotations"]["dgidb"].get("datasources", ""):')
    (806, '        return "-s {}".format(" ".join(config["annotations"]["dgidb"]["datasources"]))')
    (807, '    return ""')
    (808, '')
    (809, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if config["primers"]["trimming"].get("library_length", 0) != 0']
    (810, 'def get_bowtie_insertsize():')
    (811, '    if config["primers"]["trimming"].get("library_length", 0) != 0:')
    (812, '        return "-X {}".format(config["primers"]["trimming"].get("library_length"))')
    (813, '    return ""')
    (814, '')
    (815, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-varlociraptor, file=workflow/rules/common.smk
context_key: ['if config["ref"]["species"] == "homo_sapiens"']
    (848, 'def get_varsome_url():')
    (849, '    if config["ref"]["species"] == "homo_sapiens":')
    (850, '        build = "hg38" if config["ref"]["build"] == "GRCh38" else "hg19"')
    (851, '        return f"https://varsome.com/variant/{build}/chr"')
    (852, '    else:')
    (853, '        return None')
    (854, '')
    (855, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=nikostr/dna-seq-deepvariant-glnexus-variant-calling, file=workflow/rules/common.smk
context_key: ['if config["bwa_mem"]["wrapper"] == "bwa/mem"']
    (55, 'def get_bwa_index(wildcards):')
    (56, '    assert config["bwa_mem"]["wrapper"] in [')
    (57, '        "bwa/mem",')
    (58, '        "bwa-mem2/mem",')
    (59, '    ], "BWA-MEM wrapper must be either bwa/mem or bwa-mem2/mem"')
    (60, '    if config["bwa_mem"]["wrapper"] == "bwa/mem":')
    (61, '        return "resources/genome.fasta.sa"')
    (62, '    else:')
    (63, '        return "resources/genome.fasta.0123"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/rna-seq-star-deseq2, file=workflow/rules/common.smk
context_key: ['if config["trimming"]["activate"]', 'if is_paired_end(wildcards.sample)']
    (88, 'def get_fq(wildcards):')
    (89, '    if config["trimming"]["activate"]:')
    (90, '        # activated trimming, use trimmed data')
    (91, '        if is_paired_end(wildcards.sample):')
    (92, '            # paired-end sample')
    (93, '            return dict(')
    (94, '                zip(')
    (95, '                    ["fq1", "fq2"],')
    (96, '                    expand(')
    (97, '                        "results/trimmed/{sample}_{unit}_{group}.fastq.gz",')
    (98, '                        group=["R1", "R2"],')
    (99, '                        **wildcards,')
    (100, '                    ),')
    (101, '                )')
    (102, '            )')
    (103, '        # single end sample')
    (104, '        return {"fq1": "trimmed/{sample}_{unit}_single.fastq.gz".format(**wildcards)}')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/rna-seq-star-deseq2, file=workflow/rules/common.smk
context_key: ['if config["trimming"]["activate"]', 'else', 'if pd.isna(u["fq1"])']
    (88, 'def get_fq(wildcards):')
    (89, '    if config["trimming"]["activate"]:')
    (90, '        # activated trimming, use trimmed data')
    (91, '        if is_paired_end(wildcards.sample):')
    (92, '            # paired-end sample')
    (93, '            return dict(')
    (94, '                zip(')
    (95, '                    ["fq1", "fq2"],')
    (96, '                    expand(')
    (97, '                        "results/trimmed/{sample}_{unit}_{group}.fastq.gz",')
    (98, '                        group=["R1", "R2"],')
    (99, '                        **wildcards,')
    (100, '                    ),')
    (101, '                )')
    (102, '            )')
    (103, '        # single end sample')
    (104, '        return {"fq1": "trimmed/{sample}_{unit}_single.fastq.gz".format(**wildcards)}')
    (105, '    else:')
    (106, '        # no trimming, use raw reads')
    (107, '        u = units.loc[(wildcards.sample, wildcards.unit), ["fq1", "fq2"]].dropna()')
    (108, '        if pd.isna(u["fq1"]):')
    (109, '            # SRA sample (always paired-end for now)')
    (110, '            accession = u["sra"]')
    (111, '            return dict(')
    (112, '                zip(')
    (113, '                    ["fq1", "fq2"],')
    (114, '                    expand(')
    (115, '                        "sra/{accession}_{group}.fastq",')
    (116, '                        accession=accession,')
    (117, '                        group=["R1", "R2"],')
    (118, '                    ),')
    (119, '                )')
    (120, '            )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/rna-seq-star-deseq2, file=workflow/rules/common.smk
context_key: ['if config["trimming"]["activate"]', 'else', 'if not is_paired_end(wildcards.sample)']
    (88, 'def get_fq(wildcards):')
    (89, '    if config["trimming"]["activate"]:')
    (90, '        # activated trimming, use trimmed data')
    (91, '        if is_paired_end(wildcards.sample):')
    (92, '            # paired-end sample')
    (93, '            return dict(')
    (94, '                zip(')
    (95, '                    ["fq1", "fq2"],')
    (96, '                    expand(')
    (97, '                        "results/trimmed/{sample}_{unit}_{group}.fastq.gz",')
    (98, '                        group=["R1", "R2"],')
    (99, '                        **wildcards,')
    (100, '                    ),')
    (101, '                )')
    (102, '            )')
    (103, '        # single end sample')
    (104, '        return {"fq1": "trimmed/{sample}_{unit}_single.fastq.gz".format(**wildcards)}')
    (105, '    else:')
    (106, '        # no trimming, use raw reads')
    (107, '        u = units.loc[(wildcards.sample, wildcards.unit), ["fq1", "fq2"]].dropna()')
    (108, '        if pd.isna(u["fq1"]):')
    (109, '            # SRA sample (always paired-end for now)')
    (110, '            accession = u["sra"]')
    (111, '            return dict(')
    (112, '                zip(')
    (113, '                    ["fq1", "fq2"],')
    (114, '                    expand(')
    (115, '                        "sra/{accession}_{group}.fastq",')
    (116, '                        accession=accession,')
    (117, '                        group=["R1", "R2"],')
    (118, '                    ),')
    (119, '                )')
    (120, '            )')
    (121, '        if not is_paired_end(wildcards.sample):')
    (122, '            return {"fq1": f"{u.fq1}"}')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/rna-seq-star-deseq2, file=workflow/rules/common.smk
context_key: ['if config["trimming"]["activate"]', 'else', 'else']
    (88, 'def get_fq(wildcards):')
    (89, '    if config["trimming"]["activate"]:')
    (90, '        # activated trimming, use trimmed data')
    (91, '        if is_paired_end(wildcards.sample):')
    (92, '            # paired-end sample')
    (93, '            return dict(')
    (94, '                zip(')
    (95, '                    ["fq1", "fq2"],')
    (96, '                    expand(')
    (97, '                        "results/trimmed/{sample}_{unit}_{group}.fastq.gz",')
    (98, '                        group=["R1", "R2"],')
    (99, '                        **wildcards,')
    (100, '                    ),')
    (101, '                )')
    (102, '            )')
    (103, '        # single end sample')
    (104, '        return {"fq1": "trimmed/{sample}_{unit}_single.fastq.gz".format(**wildcards)}')
    (105, '    else:')
    (106, '        # no trimming, use raw reads')
    (107, '        u = units.loc[(wildcards.sample, wildcards.unit), ["fq1", "fq2"]].dropna()')
    (108, '        if pd.isna(u["fq1"]):')
    (109, '            # SRA sample (always paired-end for now)')
    (110, '            accession = u["sra"]')
    (111, '            return dict(')
    (112, '                zip(')
    (113, '                    ["fq1", "fq2"],')
    (114, '                    expand(')
    (115, '                        "sra/{accession}_{group}.fastq",')
    (116, '                        accession=accession,')
    (117, '                        group=["R1", "R2"],')
    (118, '                    ),')
    (119, '                )')
    (120, '            )')
    (121, '        if not is_paired_end(wildcards.sample):')
    (122, '            return {"fq1": f"{u.fq1}"}')
    (123, '        else:')
    (124, '            return {"fq1": f"{u.fq1}", "fq2": f"{u.fq2}"}')
    (125, '')
    (126, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/rna-seq-star-deseq2, file=workflow/rules/common.smk
context_key: ['if config["trimming"]["activate"]']
    (154, 'def get_fastqs(wc):')
    (155, '    if config["trimming"]["activate"]:')
    (156, '        return expand(')
    (157, '            "results/trimmed/{sample}/{unit}_{read}.fastq.gz",')
    (158, '            unit=units.loc[wc.sample, "unit_name"],')
    (159, '            sample=wc.sample,')
    (160, '            read=wc.read,')
    (161, '        )')
    (162, '    unit = units.loc[wc.sample]')
    (163, '    if all(pd.isna(unit["fq1"])):')
    (164, '        # SRA sample (always paired-end for now)')
    (165, '        accession = unit["sra"]')
    (166, '        return expand(')
    (167, '            "sra/{accession}_{read}.fastq", accession=accession, read=wc.read[-1]')
    (168, '        )')
    (169, '    fq = "fq{}".format(wc.read[-1])')
    (170, '    return units.loc[wc.sample, fq].tolist()')
    (171, '')
    (172, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=EichlerLab/ONT_pipelines, file=workflow/rules/common.smk
context_key: ['if config.get("CHRS") == None']
    (25, 'def find_clair_chrs(wildcards):')
    (26, '    if config.get("CHRS") == None:')
    (27, '        with open(f"{REF}.fai", "r") as infile:')
    (28, '            chroms = [line.split("\\\\t")[0] for line in infile]')
    (29, '    else:')
    (30, '        chroms = config.get("CHRS")')
    (31, '    return expand(')
    (32, '        "tmp/alignments/{{sample}}/{chrom}/{{bc_vers}}/{{seq}}/merge_output.vcf.gz",')
    (33, '        chrom=chroms,')
    (34, '    )')
    (35, '')
    (36, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=EichlerLab/ONT_pipelines, file=workflow/rules/common.smk
context_key: ['if config.get("METHYL_WINDOW_FILE") != None']
    (60, 'def find_all_windows(wildcards):')
    (61, '    if config.get("METHYL_WINDOW_FILE") != None:')
    (62, '        with open(config.get("WINDOW_FILE"), "r") as infile:')
    (63, '            windows = [x.rstrip() for x in infile]')
    (64, '        return expand(')
    (65, '            rules.nanopolish.output.methyl,')
    (66, '            window=windows,')
    (67, '            sample=wildcards.sample,')
    (68, '            bc_vers=wildcards.bc_vers,')
    (69, '            seq=wildcards.seq,')
    (70, '        )')
    (71, '    elif config.get("METHYL_CLAIR_CHRS") != None:')
    (72, '        windows = config.get("CHRS")')
    (73, '        return expand(')
    (74, '            rules.nanopolish.output.methyl,')
    (75, '            window=windows,')
    (76, '            sample=wildcards.sample,')
    (77, '            bc_vers=wildcards.bc_vers,')
    (78, '            seq=wildcards.seq,')
    (79, '        )')
    (80, '    else:')
    (81, '        with open(f"{REF}.fai", "r") as infile:')
    (82, '            windows = [x.split("\\\\t")[0] for x in infile]')
    (83, '        return expand(')
    (84, '            rules.nanopolish.output.methyl,')
    (85, '            window=windows,')
    (86, '            sample=wildcards.sample,')
    (87, '            bc_vers=wildcards.bc_vers,')
    (88, '            seq=wildcards.seq,')
    (89, '        )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SilkeAllmannLab/pacbio_snakemake, file=workflow/Snakefile
context_key: ['if config["genome"]["map_to_genome"] == True', 'rule all', 'input']
    (43, 'def get_subread_file(wildcards):')
    (44, '    subread_bam_file = samples_df.loc[(wildcards.sample), "pacbio"]  ')
    (45, '    return subread_bam_file')
    (46, '')
    (47, '##############')
    (48, '# TARGET RULES')
    (49, '##############')
    (50, '')
    (51, '# with a reference genome, export both annotation (GTF) and collapsed mRNA isoforms')
    (52, 'GTFs = expand(config["result_dir"] + "{sample}.collapsed.gtf", sample=SAMPLES)')
    (53, 'FASTAs = expand(config["result_dir"] + "{sample}.collapsed.fasta", sample=SAMPLES)')
    (54, '')
    (55, '# without a genome reference')
    (56, '# only exports uncollapsed transcripts in FASTA format ')
    (57, 'TRANSCRIPTS = expand(config["result_dir"] + "{sample}.transcripts.fasta", sample=SAMPLES)')
    (58, '')
    (59, 'if config["genome"]["map_to_genome"] == True:')
    (60, '    rule all:')
    (61, '        input:')
    (62, '            GTFs, FASTAs')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=hzi-bifo/phylogeny-of-single-cells, file=workflow/rules/common.smk
context_key: ['if config["univec"]["activate"]']
    (120, 'def get_univec_reference_input(wildcards):')
    (121, '    if config["univec"]["activate"]:')
    (122, '        core = "_Core" if config["univec"]["core"] else ""')
    (123, '        return f"resources/reference/UniVec_Core/UniVec{core}.fa"')
    (124, '    else:')
    (125, '        return "/dev/null"')
    (126, '')
    (127, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=friendsofstrandseq/ashleys-qc-pipeline, file=workflow/rules/common.smk
context_key: ['if config["genecore"] is True']
    (221, 'def findstem(arr):')
    (222, '')
    (223, '    # Determine size of the array')
    (224, '    n = len(arr)')
    (225, '')
    (226, '    # Take first word from array')
    (227, '    # as reference')
    (228, '    s = arr[0]')
    (229, '    l = len(s)')
    (230, '')
    (231, '    res = ""')
    (232, '')
    (233, '    for i in range(l):')
    (234, '        for j in range(i + 1, l + 1):')
    (235, '')
    (236, '            # generating all possible substrings')
    (237, '            # of our reference string arr[0] i.e s')
    (238, '            stem = s[i:j]')
    (239, '            k = 1')
    (240, '            for k in range(1, n):')
    (241, '')
    (242, '                # Check if the generated stem is')
    (243, '                # common to all words')
    (244, '                if stem not in arr[k]:')
    (245, '                    break')
    (246, '')
    (247, '            # If current substring is present in')
    (248, '            # all strings and its length is greater')
    (249, '            # than current result')
    (250, '            if k + 1 == n and len(res) < len(stem):')
    (251, '                res = stem')
    (252, '')
    (253, '    return res')
    (254, '')
    (255, '')
    (256, '# Create configuration file with samples')
    (257, '')
    (258, 'c = HandleInput(')
    (259, '    input_path=config["data_location"],')
    (260, '    genecore_path="{genecore_prefix}/{genecore_date_folder}".format(')
    (261, '        genecore_prefix=config["genecore_prefix"],')
    (262, '        genecore_date_folder=config["genecore_date_folder"],')
    (263, '    ),')
    (264, '    output_path="{data_location}/config/config_df_ashleys.tsv".format(')
    (265, '        data_location=config["data_location"]')
    (266, '    ),')
    (267, '    check_sm_tag=False,')
    (268, '    bam=False,')
    (269, '    genecore=config["genecore"],')
    (270, ')')
    (271, 'df_config_files = c.df_config_files')
    (272, 'if config["genecore"] is True:')
    (273, '    d_master = c.d_master')
    (274, '')
    (275, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=friendsofstrandseq/ashleys-qc-pipeline, file=workflow/rules/common.smk
context_key: ['if config["GC_analysis"] is Tru']
    (221, 'def findstem(arr):')
    (222, '')
    (223, '    # Determine size of the array')
    (224, '    n = len(arr)')
    (225, '')
    (226, '    # Take first word from array')
    (227, '    # as reference')
    (228, '    s = arr[0]')
    (229, '    l = len(s)')
    (230, '')
    (231, '    res = ""')
    (232, '')
    (233, '    for i in range(l):')
    (234, '        for j in range(i + 1, l + 1):')
    (235, '')
    (236, '            # generating all possible substrings')
    (237, '            # of our reference string arr[0] i.e s')
    (238, '            stem = s[i:j]')
    (239, '            k = 1')
    (240, '            for k in range(1, n):')
    (241, '')
    (242, '                # Check if the generated stem is')
    (243, '                # common to all words')
    (244, '                if stem not in arr[k]:')
    (245, '                    break')
    (246, '')
    (247, '            # If current substring is present in')
    (248, '            # all strings and its length is greater')
    (249, '            # than current result')
    (250, '            if k + 1 == n and len(res) < len(stem):')
    (251, '                res = stem')
    (252, '')
    (253, '    return res')
    (254, '')
    (255, '')
    (256, '# Create configuration file with samples')
    (257, '')
    (258, 'c = HandleInput(')
    (259, '    input_path=config["data_location"],')
    (260, '    genecore_path="{genecore_prefix}/{genecore_date_folder}".format(')
    (261, '        genecore_prefix=config["genecore_prefix"],')
    (262, '        genecore_date_folder=config["genecore_date_folder"],')
    (263, '    ),')
    (264, '    output_path="{data_location}/config/config_df_ashleys.tsv".format(')
    (265, '        data_location=config["data_location"]')
    (266, '    ),')
    (267, '    check_sm_tag=False,')
    (268, '    bam=False,')
    (269, '    genecore=config["genecore"],')
    (270, ')')
    (271, 'df_config_files = c.df_config_files')
    (272, 'if config["genecore"] is True:')
    (273, '    d_master = c.d_master')
    (274, '')
    (275, '')
    (276, 'samples = list(sorted(list(df_config_files.Sample.unique().tolist())))')
    (277, '')
    (278, '# genecore_mapping = df_config_files.groupby("Genecore_file")["Cell"].unique().apply(lambda r: r[0]).to_dict()')
    (279, '')
    (280, '# Dictionnary of libraries available per sample')
    (281, 'cell_per_sample = (')
    (282, '    df_config_files.groupby("Sample")["Cell"].unique().apply(list).to_dict()')
    (283, ')')
    (284, '')
    (285, '# Plottype options for QC count plot')
    (286, 'plottype_counts = (')
    (287, '    config["plottype_counts"]')
    (288, '    if config["GC_analysis"] is True')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=friendsofstrandseq/ashleys-qc-pipeline, file=workflow/rules/common.smk
context_key: ['if config["GC_analysis"] is True', 'if len(cell_per_sample[sample]) == 96']
    (221, 'def findstem(arr):')
    (222, '')
    (223, '    # Determine size of the array')
    (224, '    n = len(arr)')
    (225, '')
    (226, '    # Take first word from array')
    (227, '    # as reference')
    (228, '    s = arr[0]')
    (229, '    l = len(s)')
    (230, '')
    (231, '    res = ""')
    (232, '')
    (233, '    for i in range(l):')
    (234, '        for j in range(i + 1, l + 1):')
    (235, '')
    (236, '            # generating all possible substrings')
    (237, '            # of our reference string arr[0] i.e s')
    (238, '            stem = s[i:j]')
    (239, '            k = 1')
    (240, '            for k in range(1, n):')
    (241, '')
    (242, '                # Check if the generated stem is')
    (243, '                # common to all words')
    (244, '                if stem not in arr[k]:')
    (245, '                    break')
    (246, '')
    (247, '            # If current substring is present in')
    (248, '            # all strings and its length is greater')
    (249, '            # than current result')
    (250, '            if k + 1 == n and len(res) < len(stem):')
    (251, '                res = stem')
    (252, '')
    (253, '    return res')
    (254, '')
    (255, '')
    (256, '# Create configuration file with samples')
    (257, '')
    (258, 'c = HandleInput(')
    (259, '    input_path=config["data_location"],')
    (260, '    genecore_path="{genecore_prefix}/{genecore_date_folder}".format(')
    (261, '        genecore_prefix=config["genecore_prefix"],')
    (262, '        genecore_date_folder=config["genecore_date_folder"],')
    (263, '    ),')
    (264, '    output_path="{data_location}/config/config_df_ashleys.tsv".format(')
    (265, '        data_location=config["data_location"]')
    (266, '    ),')
    (267, '    check_sm_tag=False,')
    (268, '    bam=False,')
    (269, '    genecore=config["genecore"],')
    (270, ')')
    (271, 'df_config_files = c.df_config_files')
    (272, 'if config["genecore"] is True:')
    (273, '    d_master = c.d_master')
    (274, '')
    (275, '')
    (276, 'samples = list(sorted(list(df_config_files.Sample.unique().tolist())))')
    (277, '')
    (278, '# genecore_mapping = df_config_files.groupby("Genecore_file")["Cell"].unique().apply(lambda r: r[0]).to_dict()')
    (279, '')
    (280, '# Dictionnary of libraries available per sample')
    (281, 'cell_per_sample = (')
    (282, '    df_config_files.groupby("Sample")["Cell"].unique().apply(list).to_dict()')
    (283, ')')
    (284, '')
    (285, '# Plottype options for QC count plot')
    (286, 'plottype_counts = (')
    (287, '    config["plottype_counts"]')
    (288, '    if config["GC_analysis"] is True')
    (289, '    else config["plottype_counts"][0]')
    (290, ')')
    (291, '')
    (292, '# Special row/column mode for GC analysis of a 96-well plate')
    (293, 'if config["GC_analysis"] is True:')
    (294, '')
    (295, '    import string')
    (296, '    import collections')
    (297, '    import numpy as np')
    (298, '')
    (299, '    # Instanciate a dict of dict')
    (300, '    d = collections.defaultdict(dict)')
    (301, '    # Select orientation based on config file (landscape/portrait)')
    (302, '    orientation = (8, 12) if config["plate_orientation"] == "landscape" else (12, 8)')
    (303, '    for sample in samples:')
    (304, '        # If sample contains 96 files')
    (305, '        if len(cell_per_sample[sample]) == 96:')
    (306, '            # Create dict for each row/column & save it into d')
    (307, '            for j, e in enumerate(')
    (308, '                np.reshape(np.array(sorted(cell_per_sample[sample])), orientation)')
    (309, '            ):')
    (310, '                d[sample][list(string.ascii_uppercase)[j]] = e')
    (311, '')
    (312, '# from pprint import pprint')
    (313, '# pprint(cell_per_sample)')
    (314, '# for k in cell_per_sample:')
    (315, '#     print(k)')
    (316, '#     print(len(cell_per_sample[k]))')
    (317, '')
    (318, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=zavolanlab/polyAsite_workflow, file=Snakefile
context_key: ["if config[\\'excluded_chr\\'] is not None"]
    (162, 'def get_excluded_chromosomes(wildcards):')
    (163, "    if config[\\'excluded_chr\\'] is not None:")
    (164, "        excluded = {i:1 for i in config[\\'excluded_chr\\']}")
    (165, '    else:')
    (166, '        excluded = {}')
    (167, '    if samples.loc[wildcards.sample, "sex"] == "F":')
    (168, "        excluded[ config[\\'female_chr\\'] ] = 1")
    (169, '    else:')
    (170, "        if config[\\'female_chr\\'] in excluded:")
    (171, "            del excluded[ config[\\'female_chr\\'] ]")
    (172, '    excluded_list = list(excluded.keys())')
    (173, '    if len(excluded_list) == 0:')
    (174, '        return ""')
    (175, '    else:')
    (176, '        return("--exclude=" + ":".join(excluded_list))')
    (177, '')
    (178, '')
    (179, '#-------------------------------------------------------------------------------')
    (180, '# local rules')
    (181, '#-------------------------------------------------------------------------------')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ianvcaldas/smalldisco, file=workflow/Snakefile
context_key: ['if config["tails_antisense_only"]']
    (38, 'def get_map_antisense(wildcards):')
    (39, '  if config["tails_antisense_only"]:')
    (40, '    return "-S"')
    (41, '  else:')
    (42, '    return ""')
    (43, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/Biscuit_Snakemake_Workflow, file=workflow/Snakefile
context_key: ["if config[\\'output_directory\\'] == \\'\\'"]
    (16, 'def set_output_directory():')
    (17, "    if config[\\'output_directory\\'] == \\'\\':")
    (18, '        return os.getcwd()')
    (19, '    else:')
    (20, "        return config[\\'output_directory\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBISweden/fungal-trans, file=Snakefile
context_key: ['if config["co_assembly"]']
    (10, 'def parse_validation_error(e):')
    (11, '    """')
    (12, '    Catches errors thrown when validating the config file and attempts to')
    (13, '    print more meaningful messages.')
    (14, '')
    (15, '    :param e: Error')
    (16, '    :return:')
    (17, '    """')
    (18, '    instance = ""')
    (19, '    print("ERROR VALIDATING CONFIG FILE")')
    (20, '    for item in str(e).split("\\')
    (21, '"):')
    (22, '        item = item.replace(\\\'"\\\',\\\'\\\')')
    (23, '        if "ValidationError:" in item:')
    (24, '            print(item)')
    (25, '        if item[0:11] == "On instance":')
    (26, '            instance = item.replace("On instance[\\\'", "INCORRECT CONFIG AT: ")')
    (27, '            instance = instance.replace("\\\']:","")')
    (28, '            print(instance)')
    (29, '    return')
    (30, '')
    (31, '# Set temporary dir')
    (32, 'if not os.getenv("TMPDIR"):')
    (33, '    os.environ["TMPDIR"] = "tmp"')
    (34, '    os.makedirs(os.environ["TMPDIR"],exist_ok=True)')
    (35, '')
    (36, 'wildcard_constraints:')
    (37, '    sample_id = "[A-Za-z0-9_\\\\-\\\\.]+",')
    (38, '    assembler = "megahit|trinity|transabyss",')
    (39, '    filter_source = "unfiltered|filtered|taxmapper|bowtie2"')
    (40, '')
    (41, '# Set default config and validate against schema')
    (42, 'if os.path.exists("config.yml"):')
    (43, '    configfile: "config.yml"')
    (44, 'try:')
    (45, '    validate(config, "config/config.schema.yaml")')
    (46, 'except WorkflowError as e:')
    (47, '    parse_validation_error(e)')
    (48, '    sys.exit()')
    (49, '')
    (50, 'workdir: config["workdir"]')
    (51, '')
    (52, '# Get environment info')
    (53, 'pythonpath = sys.executable')
    (54, 'envdir = \\\'/\\\'.join(pythonpath.split("/")[0:-2])')
    (55, 'system = platform.system()')
    (56, 'config["system"] = system')
    (57, '')
    (58, '# Parse samples and assemblies')
    (59, 'samples, map_dict, assemblies = parse_sample_list(config["sample_file_list"], config)')
    (60, '')
    (61, '# Include rules')
    (62, 'include: "source/rules/db.smk"')
    (63, 'include: "source/rules/preprocess.smk"')
    (64, 'include: "source/rules/filter.smk"')
    (65, 'include: "source/rules/assembly.smk"')
    (66, 'include: "source/rules/map.smk"')
    (67, 'include: "source/rules/sourmash.smk"')
    (68, 'include: "source/rules/annotate_single.smk"')
    (69, 'include: "source/rules/annotate_co.smk"')
    (70, 'include: "source/rules/kraken.smk"')
    (71, '')
    (72, '# Define targets')
    (73, '## Preprocessing')
    (74, 'preprocess = expand("results/preprocess/{sample_id}_R{i}.cut.trim.fastq.gz", sample_id = samples.keys(), i = [1,2])')
    (75, 'preprocess += ["results/report/preprocess/preprocess_report.html"]')
    (76, '')
    (77, '## Host reads')
    (78, 'host_reads = expand("results/host/{sample_id}_R{i}.host.fastq.gz",')
    (79, '            sample_id = samples.keys(), i = [1,2])')
    (80, '## Taxmapper filter')
    (81, 'taxmapper_filter = expand("results/taxmapper/{sample_id}/{sample_id}_R1.cut.trim.filtered.fastq.gz", sample_id = samples.keys())')
    (82, 'taxmapper_filter += expand("results/report/taxmapper/taxa_freq_norm_lvl{i}.svg", i = [1,2])')
    (83, '## Bowtie filter')
    (84, 'bowtie_filter = expand("results/{aligner}/{sample_id}/{sample_id}_R{i}.fungi.nohost.fastq.gz",')
    (85, '            sample_id = samples.keys(), i = [1,2], aligner = config["host_aligner"])')
    (86, 'bowtie_filter += ["results/report/filtering/filter_report.html"]')
    (87, 'bowtie_filter += host_reads')
    (88, '## Union filter')
    (89, 'union_filter = expand("results/filtered/{sample_id}/{sample_id}_R{i}.filtered.union.fastq.gz",')
    (90, '            sample_id = samples.keys(), i = [1,2])')
    (91, 'union_filter += host_reads')
    (92, '')
    (93, '## Sourmash')
    (94, 'sourmash = expand("results/sourmash/{sample_id}/{sample_id}.{source}.sig",')
    (95, '            sample_id = samples.keys(), source = config["read_source"])')
    (96, 'sourmash += expand("results/report/sourmash/{source}_sample.dist.labels.txt", source = config["read_source"])')
    (97, 'sourmash_assembly = expand("results/assembly/{assembler}/{source}/{sample_id}/final.sig",')
    (98, '            assembler = config["assembler"], source = config["read_source"], sample_id=samples.keys())')
    (99, 'sourmash_assembly += expand("results/report/sourmash/{source}_assembly_{assembler}.dist.labels.txt",')
    (100, '            source = config["read_source"], assembler = config["assembler"])')
    (101, '## Single-assemblies')
    (102, 'assembly = expand("results/report/assembly/{source}_{assembler}_stats.tsv",')
    (103, '            source = config["read_source"], assembler = config["assembler"])')
    (104, 'assembly += expand("results/assembly/{assembler}/{source}/{sample_id}/final.fa",')
    (105, '                assembler = config["assembler"], source = config["read_source"], sample_id = samples.keys())')
    (106, '## Annotations')
    (107, 'taxonomy = expand("results/collated/{assembler}/{source}/taxonomy/taxonomy.{fc}.tsv",')
    (108, '            assembler = config["assembler"],')
    (109, '            fc = ["tpm","raw"],')
    (110, '            source = config["read_source"])')
    (111, 'dbCAN = expand("results/collated/{assembler}/{source}/dbCAN/dbCAN.{fc}.tsv",')
    (112, '            assembler = config["assembler"],')
    (113, '            fc = ["tpm","raw"],')
    (114, '            source = config["read_source"])')
    (115, 'eggnog = expand("results/collated/{assembler}/{source}/eggNOG/{db}.{fc}.tsv",')
    (116, '            db = ["enzymes","pathways","pathways.norm","modules","kos","tc","cazy"],')
    (117, '            assembler = config["assembler"],')
    (118, '            fc = ["tpm","raw"],')
    (119, '            source = config["read_source"])')
    (120, 'mapres = expand("results/assembly/{assembler}/{source}/{sample_id}/{sample_id}_R{i}.fastq.gz",')
    (121, '            assembler = config["assembler"], source = config["read_source"], sample_id = samples.keys(), i = [1,2])')
    (122, 'mapres += expand("results/report/map/{assembler}_{source}_map_report.html",')
    (123, '            assembler = config["assembler"], source = config["read_source"])')
    (124, 'normalize = expand("results/annotation/{assembler}/{source}/{sample_id}/featureCounts/fc.{fc}.tab",')
    (125, '            assembler = config["assembler"], source = config["read_source"], sample_id = samples.keys(), fc=["tpm","raw"])')
    (126, '### Optional blobtools output')
    (127, 'blobtools = expand("results/annotation/{assembler}/{source}/{sample_id}/taxonomy/{sample_id}.bestsum.phylum.p5.span.100.exclude_other.blobplot.bam0.png",')
    (128, '            assembler = config["assembler"], sample_id = samples.keys(), source = config["read_source"])')
    (129, 'blobtools += expand("results/annotation/{assembler}/{source}/{sample_id}/taxonomy/{sample_id}.bestsum.order.p6.span.100.exclude_other.blobplot.bam0.png",')
    (130, '            assembler = config["assembler"], sample_id = samples.keys(), source = config["read_source"])')
    (131, '')
    (132, '### Optional kraken output')
    (133, 'kraken_output = expand("results/kraken/{sample_id}.{suffix}", sample_id = samples.keys(), suffix = ["out.gz","kreport"])')
    (134, '')
    (135, '## Co-assemblies')
    (136, 'co_assembly = expand("results/co-assembly/{assembler}/{assembly}/final.fa", assembly = assemblies.keys(), assembler = config["assembler"])')
    (137, 'co_assembly_stats = expand("results/report/co-assembly/{assembler}.{assembly}_assembly_stats.tsv", assembly = assemblies.keys(), assembler = config["assembler"])')
    (138, 'co_assembly.append(co_assembly_stats)')
    (139, '## Fastuniq')
    (140, 'fastuniq = expand("results/fastuniq/{assembly}/R{i}.fastuniq.gz", assembly = assemblies.keys(), i=[1,2])')
    (141, '## Annotations')
    (142, 'eggnog_co = expand("results/collated/co-assembly/{assembler}/{assembly}/eggNOG/{db}.{fc}.tsv",')
    (143, '            db = ["enzymes","pathways","pathways.norm","modules","kos","tc","cazy"],')
    (144, '            fc = ["raw","tpm"],')
    (145, '            assembly = assemblies.keys(), assembler = config["assembler"])')
    (146, 'eggnog_co_tax = expand("results/collated/co-assembly/{assembler}/{assembly}/eggNOG_taxonomy/{tax_rank}.{tax_name}.{db}.{fc}.tsv",')
    (147, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (148, '            fc = ["raw","tpm"],')
    (149, '            db = ["kos","enzymes","modules","pathways","tc","cazy"],')
    (150, '            tax_rank = config["tax_rank"],')
    (151, '            tax_name = config["tax_name"])')
    (152, 'abundance_co = expand("results/collated/co-assembly/{assembler}/{assembly}/abundance/{assembly}.{fc}.tsv",')
    (153, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (154, '            fc = ["raw","tpm"])')
    (155, 'abundance_co_tax = expand("results/collated/co-assembly/{assembler}/{assembly}/abundance_taxonomy/{tax_rank}.{tax_name}.{assembly}.{fc}.tsv",')
    (156, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (157, '            fc = ["raw","tpm"],')
    (158, '            tax_rank = config["tax_rank"],')
    (159, '            tax_name = config["tax_name"])')
    (160, 'dbCAN_co = expand("results/collated/co-assembly/{assembler}/{assembly}/dbCAN/dbCAN.{fc}.tsv",')
    (161, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (162, '            fc = ["raw","tpm"])')
    (163, 'dbCAN_co_tax = expand("results/collated/co-assembly/{assembler}/{assembly}/dbCAN_taxonomy/{tax_rank}.{tax_name}.dbCAN.{fc}.tsv",')
    (164, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (165, '            fc = ["raw","tpm"],')
    (166, '            tax_rank = config["tax_rank"],')
    (167, '            tax_name = config["tax_name"])')
    (168, 'taxonomy_co = expand("results/collated/co-assembly/{assembler}/{assembly}/taxonomy/taxonomy.{fc}.tsv",')
    (169, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (170, '            fc = ["raw","tpm"])')
    (171, '# Map and normalize')
    (172, 'map_co = expand("results/report/map/{assembler}.{assembly}_map_report.html", assembly = assemblies.keys(), assembler=config["assembler"])')
    (173, 'normalize_co = expand("results/collated/co-assembly/{assembler}/{assembly}/abundance/{assembly}.{fc}.tsv",')
    (174, '                assembly = assemblies.keys(), assembler = config["assembler"], fc = ["tpm","raw"])')
    (175, '### Optional blobtools output')
    (176, 'blobtools_co = expand("results/annotation/co-assembly/{assembler}/{assembly}/taxonomy/blobtools/{sample_id}.bestsum.phylum.p5.span.100.exclude_other.blobplot.bam0.png",')
    (177, '                 assembly = assemblies.keys(), sample_id = samples.keys(), assembler = config["assembler"])')
    (178, 'blobtools_co += expand("results/annotation/co-assembly/{assembler}/{assembly}/taxonomy/blobtools/{sample_id}.bestsum.phylum.p5.span.100.exclude_other.blobplot.read_cov.bam0.png",')
    (179, '                assembly = assemblies.keys(), sample_id = samples.keys(), assembler = config["assembler"])')
    (180, '')
    (181, 'inputs = []')
    (182, 'inputs += preprocess')
    (183, '')
    (184, 'if config["co_assembly"]:')
    (185, '    inputs += [eggnog_co, map_co, co_assembly_stats, abundance_co, abundance_co_tax, dbCAN_co, taxonomy_co, dbCAN_co_tax, eggnog_co_tax]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBISweden/fungal-trans, file=Snakefile
context_key: ['if config["single_assembly"]']
    (10, 'def parse_validation_error(e):')
    (11, '    """')
    (12, '    Catches errors thrown when validating the config file and attempts to')
    (13, '    print more meaningful messages.')
    (14, '')
    (15, '    :param e: Error')
    (16, '    :return:')
    (17, '    """')
    (18, '    instance = ""')
    (19, '    print("ERROR VALIDATING CONFIG FILE")')
    (20, '    for item in str(e).split("\\')
    (21, '"):')
    (22, '        item = item.replace(\\\'"\\\',\\\'\\\')')
    (23, '        if "ValidationError:" in item:')
    (24, '            print(item)')
    (25, '        if item[0:11] == "On instance":')
    (26, '            instance = item.replace("On instance[\\\'", "INCORRECT CONFIG AT: ")')
    (27, '            instance = instance.replace("\\\']:","")')
    (28, '            print(instance)')
    (29, '    return')
    (30, '')
    (31, '# Set temporary dir')
    (32, 'if not os.getenv("TMPDIR"):')
    (33, '    os.environ["TMPDIR"] = "tmp"')
    (34, '    os.makedirs(os.environ["TMPDIR"],exist_ok=True)')
    (35, '')
    (36, 'wildcard_constraints:')
    (37, '    sample_id = "[A-Za-z0-9_\\\\-\\\\.]+",')
    (38, '    assembler = "megahit|trinity|transabyss",')
    (39, '    filter_source = "unfiltered|filtered|taxmapper|bowtie2"')
    (40, '')
    (41, '# Set default config and validate against schema')
    (42, 'if os.path.exists("config.yml"):')
    (43, '    configfile: "config.yml"')
    (44, 'try:')
    (45, '    validate(config, "config/config.schema.yaml")')
    (46, 'except WorkflowError as e:')
    (47, '    parse_validation_error(e)')
    (48, '    sys.exit()')
    (49, '')
    (50, 'workdir: config["workdir"]')
    (51, '')
    (52, '# Get environment info')
    (53, 'pythonpath = sys.executable')
    (54, 'envdir = \\\'/\\\'.join(pythonpath.split("/")[0:-2])')
    (55, 'system = platform.system()')
    (56, 'config["system"] = system')
    (57, '')
    (58, '# Parse samples and assemblies')
    (59, 'samples, map_dict, assemblies = parse_sample_list(config["sample_file_list"], config)')
    (60, '')
    (61, '# Include rules')
    (62, 'include: "source/rules/db.smk"')
    (63, 'include: "source/rules/preprocess.smk"')
    (64, 'include: "source/rules/filter.smk"')
    (65, 'include: "source/rules/assembly.smk"')
    (66, 'include: "source/rules/map.smk"')
    (67, 'include: "source/rules/sourmash.smk"')
    (68, 'include: "source/rules/annotate_single.smk"')
    (69, 'include: "source/rules/annotate_co.smk"')
    (70, 'include: "source/rules/kraken.smk"')
    (71, '')
    (72, '# Define targets')
    (73, '## Preprocessing')
    (74, 'preprocess = expand("results/preprocess/{sample_id}_R{i}.cut.trim.fastq.gz", sample_id = samples.keys(), i = [1,2])')
    (75, 'preprocess += ["results/report/preprocess/preprocess_report.html"]')
    (76, '')
    (77, '## Host reads')
    (78, 'host_reads = expand("results/host/{sample_id}_R{i}.host.fastq.gz",')
    (79, '            sample_id = samples.keys(), i = [1,2])')
    (80, '## Taxmapper filter')
    (81, 'taxmapper_filter = expand("results/taxmapper/{sample_id}/{sample_id}_R1.cut.trim.filtered.fastq.gz", sample_id = samples.keys())')
    (82, 'taxmapper_filter += expand("results/report/taxmapper/taxa_freq_norm_lvl{i}.svg", i = [1,2])')
    (83, '## Bowtie filter')
    (84, 'bowtie_filter = expand("results/{aligner}/{sample_id}/{sample_id}_R{i}.fungi.nohost.fastq.gz",')
    (85, '            sample_id = samples.keys(), i = [1,2], aligner = config["host_aligner"])')
    (86, 'bowtie_filter += ["results/report/filtering/filter_report.html"]')
    (87, 'bowtie_filter += host_reads')
    (88, '## Union filter')
    (89, 'union_filter = expand("results/filtered/{sample_id}/{sample_id}_R{i}.filtered.union.fastq.gz",')
    (90, '            sample_id = samples.keys(), i = [1,2])')
    (91, 'union_filter += host_reads')
    (92, '')
    (93, '## Sourmash')
    (94, 'sourmash = expand("results/sourmash/{sample_id}/{sample_id}.{source}.sig",')
    (95, '            sample_id = samples.keys(), source = config["read_source"])')
    (96, 'sourmash += expand("results/report/sourmash/{source}_sample.dist.labels.txt", source = config["read_source"])')
    (97, 'sourmash_assembly = expand("results/assembly/{assembler}/{source}/{sample_id}/final.sig",')
    (98, '            assembler = config["assembler"], source = config["read_source"], sample_id=samples.keys())')
    (99, 'sourmash_assembly += expand("results/report/sourmash/{source}_assembly_{assembler}.dist.labels.txt",')
    (100, '            source = config["read_source"], assembler = config["assembler"])')
    (101, '## Single-assemblies')
    (102, 'assembly = expand("results/report/assembly/{source}_{assembler}_stats.tsv",')
    (103, '            source = config["read_source"], assembler = config["assembler"])')
    (104, 'assembly += expand("results/assembly/{assembler}/{source}/{sample_id}/final.fa",')
    (105, '                assembler = config["assembler"], source = config["read_source"], sample_id = samples.keys())')
    (106, '## Annotations')
    (107, 'taxonomy = expand("results/collated/{assembler}/{source}/taxonomy/taxonomy.{fc}.tsv",')
    (108, '            assembler = config["assembler"],')
    (109, '            fc = ["tpm","raw"],')
    (110, '            source = config["read_source"])')
    (111, 'dbCAN = expand("results/collated/{assembler}/{source}/dbCAN/dbCAN.{fc}.tsv",')
    (112, '            assembler = config["assembler"],')
    (113, '            fc = ["tpm","raw"],')
    (114, '            source = config["read_source"])')
    (115, 'eggnog = expand("results/collated/{assembler}/{source}/eggNOG/{db}.{fc}.tsv",')
    (116, '            db = ["enzymes","pathways","pathways.norm","modules","kos","tc","cazy"],')
    (117, '            assembler = config["assembler"],')
    (118, '            fc = ["tpm","raw"],')
    (119, '            source = config["read_source"])')
    (120, 'mapres = expand("results/assembly/{assembler}/{source}/{sample_id}/{sample_id}_R{i}.fastq.gz",')
    (121, '            assembler = config["assembler"], source = config["read_source"], sample_id = samples.keys(), i = [1,2])')
    (122, 'mapres += expand("results/report/map/{assembler}_{source}_map_report.html",')
    (123, '            assembler = config["assembler"], source = config["read_source"])')
    (124, 'normalize = expand("results/annotation/{assembler}/{source}/{sample_id}/featureCounts/fc.{fc}.tab",')
    (125, '            assembler = config["assembler"], source = config["read_source"], sample_id = samples.keys(), fc=["tpm","raw"])')
    (126, '### Optional blobtools output')
    (127, 'blobtools = expand("results/annotation/{assembler}/{source}/{sample_id}/taxonomy/{sample_id}.bestsum.phylum.p5.span.100.exclude_other.blobplot.bam0.png",')
    (128, '            assembler = config["assembler"], sample_id = samples.keys(), source = config["read_source"])')
    (129, 'blobtools += expand("results/annotation/{assembler}/{source}/{sample_id}/taxonomy/{sample_id}.bestsum.order.p6.span.100.exclude_other.blobplot.bam0.png",')
    (130, '            assembler = config["assembler"], sample_id = samples.keys(), source = config["read_source"])')
    (131, '')
    (132, '### Optional kraken output')
    (133, 'kraken_output = expand("results/kraken/{sample_id}.{suffix}", sample_id = samples.keys(), suffix = ["out.gz","kreport"])')
    (134, '')
    (135, '## Co-assemblies')
    (136, 'co_assembly = expand("results/co-assembly/{assembler}/{assembly}/final.fa", assembly = assemblies.keys(), assembler = config["assembler"])')
    (137, 'co_assembly_stats = expand("results/report/co-assembly/{assembler}.{assembly}_assembly_stats.tsv", assembly = assemblies.keys(), assembler = config["assembler"])')
    (138, 'co_assembly.append(co_assembly_stats)')
    (139, '## Fastuniq')
    (140, 'fastuniq = expand("results/fastuniq/{assembly}/R{i}.fastuniq.gz", assembly = assemblies.keys(), i=[1,2])')
    (141, '## Annotations')
    (142, 'eggnog_co = expand("results/collated/co-assembly/{assembler}/{assembly}/eggNOG/{db}.{fc}.tsv",')
    (143, '            db = ["enzymes","pathways","pathways.norm","modules","kos","tc","cazy"],')
    (144, '            fc = ["raw","tpm"],')
    (145, '            assembly = assemblies.keys(), assembler = config["assembler"])')
    (146, 'eggnog_co_tax = expand("results/collated/co-assembly/{assembler}/{assembly}/eggNOG_taxonomy/{tax_rank}.{tax_name}.{db}.{fc}.tsv",')
    (147, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (148, '            fc = ["raw","tpm"],')
    (149, '            db = ["kos","enzymes","modules","pathways","tc","cazy"],')
    (150, '            tax_rank = config["tax_rank"],')
    (151, '            tax_name = config["tax_name"])')
    (152, 'abundance_co = expand("results/collated/co-assembly/{assembler}/{assembly}/abundance/{assembly}.{fc}.tsv",')
    (153, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (154, '            fc = ["raw","tpm"])')
    (155, 'abundance_co_tax = expand("results/collated/co-assembly/{assembler}/{assembly}/abundance_taxonomy/{tax_rank}.{tax_name}.{assembly}.{fc}.tsv",')
    (156, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (157, '            fc = ["raw","tpm"],')
    (158, '            tax_rank = config["tax_rank"],')
    (159, '            tax_name = config["tax_name"])')
    (160, 'dbCAN_co = expand("results/collated/co-assembly/{assembler}/{assembly}/dbCAN/dbCAN.{fc}.tsv",')
    (161, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (162, '            fc = ["raw","tpm"])')
    (163, 'dbCAN_co_tax = expand("results/collated/co-assembly/{assembler}/{assembly}/dbCAN_taxonomy/{tax_rank}.{tax_name}.dbCAN.{fc}.tsv",')
    (164, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (165, '            fc = ["raw","tpm"],')
    (166, '            tax_rank = config["tax_rank"],')
    (167, '            tax_name = config["tax_name"])')
    (168, 'taxonomy_co = expand("results/collated/co-assembly/{assembler}/{assembly}/taxonomy/taxonomy.{fc}.tsv",')
    (169, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (170, '            fc = ["raw","tpm"])')
    (171, '# Map and normalize')
    (172, 'map_co = expand("results/report/map/{assembler}.{assembly}_map_report.html", assembly = assemblies.keys(), assembler=config["assembler"])')
    (173, 'normalize_co = expand("results/collated/co-assembly/{assembler}/{assembly}/abundance/{assembly}.{fc}.tsv",')
    (174, '                assembly = assemblies.keys(), assembler = config["assembler"], fc = ["tpm","raw"])')
    (175, '### Optional blobtools output')
    (176, 'blobtools_co = expand("results/annotation/co-assembly/{assembler}/{assembly}/taxonomy/blobtools/{sample_id}.bestsum.phylum.p5.span.100.exclude_other.blobplot.bam0.png",')
    (177, '                 assembly = assemblies.keys(), sample_id = samples.keys(), assembler = config["assembler"])')
    (178, 'blobtools_co += expand("results/annotation/co-assembly/{assembler}/{assembly}/taxonomy/blobtools/{sample_id}.bestsum.phylum.p5.span.100.exclude_other.blobplot.read_cov.bam0.png",')
    (179, '                assembly = assemblies.keys(), sample_id = samples.keys(), assembler = config["assembler"])')
    (180, '')
    (181, 'inputs = []')
    (182, 'inputs += preprocess')
    (183, '')
    (184, 'if config["co_assembly"]:')
    (185, '    inputs += [eggnog_co, map_co, co_assembly_stats, abundance_co, abundance_co_tax, dbCAN_co, taxonomy_co, dbCAN_co_tax, eggnog_co_tax]')
    (186, 'if config["single_assembly"]:')
    (187, '    inputs += [eggnog, dbCAN, taxonomy]')
    (188, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBISweden/fungal-trans, file=Snakefile
context_key: ['if config["read_source"] == "bowtie2"']
    (10, 'def parse_validation_error(e):')
    (11, '    """')
    (12, '    Catches errors thrown when validating the config file and attempts to')
    (13, '    print more meaningful messages.')
    (14, '')
    (15, '    :param e: Error')
    (16, '    :return:')
    (17, '    """')
    (18, '    instance = ""')
    (19, '    print("ERROR VALIDATING CONFIG FILE")')
    (20, '    for item in str(e).split("\\')
    (21, '"):')
    (22, '        item = item.replace(\\\'"\\\',\\\'\\\')')
    (23, '        if "ValidationError:" in item:')
    (24, '            print(item)')
    (25, '        if item[0:11] == "On instance":')
    (26, '            instance = item.replace("On instance[\\\'", "INCORRECT CONFIG AT: ")')
    (27, '            instance = instance.replace("\\\']:","")')
    (28, '            print(instance)')
    (29, '    return')
    (30, '')
    (31, '# Set temporary dir')
    (32, 'if not os.getenv("TMPDIR"):')
    (33, '    os.environ["TMPDIR"] = "tmp"')
    (34, '    os.makedirs(os.environ["TMPDIR"],exist_ok=True)')
    (35, '')
    (36, 'wildcard_constraints:')
    (37, '    sample_id = "[A-Za-z0-9_\\\\-\\\\.]+",')
    (38, '    assembler = "megahit|trinity|transabyss",')
    (39, '    filter_source = "unfiltered|filtered|taxmapper|bowtie2"')
    (40, '')
    (41, '# Set default config and validate against schema')
    (42, 'if os.path.exists("config.yml"):')
    (43, '    configfile: "config.yml"')
    (44, 'try:')
    (45, '    validate(config, "config/config.schema.yaml")')
    (46, 'except WorkflowError as e:')
    (47, '    parse_validation_error(e)')
    (48, '    sys.exit()')
    (49, '')
    (50, 'workdir: config["workdir"]')
    (51, '')
    (52, '# Get environment info')
    (53, 'pythonpath = sys.executable')
    (54, 'envdir = \\\'/\\\'.join(pythonpath.split("/")[0:-2])')
    (55, 'system = platform.system()')
    (56, 'config["system"] = system')
    (57, '')
    (58, '# Parse samples and assemblies')
    (59, 'samples, map_dict, assemblies = parse_sample_list(config["sample_file_list"], config)')
    (60, '')
    (61, '# Include rules')
    (62, 'include: "source/rules/db.smk"')
    (63, 'include: "source/rules/preprocess.smk"')
    (64, 'include: "source/rules/filter.smk"')
    (65, 'include: "source/rules/assembly.smk"')
    (66, 'include: "source/rules/map.smk"')
    (67, 'include: "source/rules/sourmash.smk"')
    (68, 'include: "source/rules/annotate_single.smk"')
    (69, 'include: "source/rules/annotate_co.smk"')
    (70, 'include: "source/rules/kraken.smk"')
    (71, '')
    (72, '# Define targets')
    (73, '## Preprocessing')
    (74, 'preprocess = expand("results/preprocess/{sample_id}_R{i}.cut.trim.fastq.gz", sample_id = samples.keys(), i = [1,2])')
    (75, 'preprocess += ["results/report/preprocess/preprocess_report.html"]')
    (76, '')
    (77, '## Host reads')
    (78, 'host_reads = expand("results/host/{sample_id}_R{i}.host.fastq.gz",')
    (79, '            sample_id = samples.keys(), i = [1,2])')
    (80, '## Taxmapper filter')
    (81, 'taxmapper_filter = expand("results/taxmapper/{sample_id}/{sample_id}_R1.cut.trim.filtered.fastq.gz", sample_id = samples.keys())')
    (82, 'taxmapper_filter += expand("results/report/taxmapper/taxa_freq_norm_lvl{i}.svg", i = [1,2])')
    (83, '## Bowtie filter')
    (84, 'bowtie_filter = expand("results/{aligner}/{sample_id}/{sample_id}_R{i}.fungi.nohost.fastq.gz",')
    (85, '            sample_id = samples.keys(), i = [1,2], aligner = config["host_aligner"])')
    (86, 'bowtie_filter += ["results/report/filtering/filter_report.html"]')
    (87, 'bowtie_filter += host_reads')
    (88, '## Union filter')
    (89, 'union_filter = expand("results/filtered/{sample_id}/{sample_id}_R{i}.filtered.union.fastq.gz",')
    (90, '            sample_id = samples.keys(), i = [1,2])')
    (91, 'union_filter += host_reads')
    (92, '')
    (93, '## Sourmash')
    (94, 'sourmash = expand("results/sourmash/{sample_id}/{sample_id}.{source}.sig",')
    (95, '            sample_id = samples.keys(), source = config["read_source"])')
    (96, 'sourmash += expand("results/report/sourmash/{source}_sample.dist.labels.txt", source = config["read_source"])')
    (97, 'sourmash_assembly = expand("results/assembly/{assembler}/{source}/{sample_id}/final.sig",')
    (98, '            assembler = config["assembler"], source = config["read_source"], sample_id=samples.keys())')
    (99, 'sourmash_assembly += expand("results/report/sourmash/{source}_assembly_{assembler}.dist.labels.txt",')
    (100, '            source = config["read_source"], assembler = config["assembler"])')
    (101, '## Single-assemblies')
    (102, 'assembly = expand("results/report/assembly/{source}_{assembler}_stats.tsv",')
    (103, '            source = config["read_source"], assembler = config["assembler"])')
    (104, 'assembly += expand("results/assembly/{assembler}/{source}/{sample_id}/final.fa",')
    (105, '                assembler = config["assembler"], source = config["read_source"], sample_id = samples.keys())')
    (106, '## Annotations')
    (107, 'taxonomy = expand("results/collated/{assembler}/{source}/taxonomy/taxonomy.{fc}.tsv",')
    (108, '            assembler = config["assembler"],')
    (109, '            fc = ["tpm","raw"],')
    (110, '            source = config["read_source"])')
    (111, 'dbCAN = expand("results/collated/{assembler}/{source}/dbCAN/dbCAN.{fc}.tsv",')
    (112, '            assembler = config["assembler"],')
    (113, '            fc = ["tpm","raw"],')
    (114, '            source = config["read_source"])')
    (115, 'eggnog = expand("results/collated/{assembler}/{source}/eggNOG/{db}.{fc}.tsv",')
    (116, '            db = ["enzymes","pathways","pathways.norm","modules","kos","tc","cazy"],')
    (117, '            assembler = config["assembler"],')
    (118, '            fc = ["tpm","raw"],')
    (119, '            source = config["read_source"])')
    (120, 'mapres = expand("results/assembly/{assembler}/{source}/{sample_id}/{sample_id}_R{i}.fastq.gz",')
    (121, '            assembler = config["assembler"], source = config["read_source"], sample_id = samples.keys(), i = [1,2])')
    (122, 'mapres += expand("results/report/map/{assembler}_{source}_map_report.html",')
    (123, '            assembler = config["assembler"], source = config["read_source"])')
    (124, 'normalize = expand("results/annotation/{assembler}/{source}/{sample_id}/featureCounts/fc.{fc}.tab",')
    (125, '            assembler = config["assembler"], source = config["read_source"], sample_id = samples.keys(), fc=["tpm","raw"])')
    (126, '### Optional blobtools output')
    (127, 'blobtools = expand("results/annotation/{assembler}/{source}/{sample_id}/taxonomy/{sample_id}.bestsum.phylum.p5.span.100.exclude_other.blobplot.bam0.png",')
    (128, '            assembler = config["assembler"], sample_id = samples.keys(), source = config["read_source"])')
    (129, 'blobtools += expand("results/annotation/{assembler}/{source}/{sample_id}/taxonomy/{sample_id}.bestsum.order.p6.span.100.exclude_other.blobplot.bam0.png",')
    (130, '            assembler = config["assembler"], sample_id = samples.keys(), source = config["read_source"])')
    (131, '')
    (132, '### Optional kraken output')
    (133, 'kraken_output = expand("results/kraken/{sample_id}.{suffix}", sample_id = samples.keys(), suffix = ["out.gz","kreport"])')
    (134, '')
    (135, '## Co-assemblies')
    (136, 'co_assembly = expand("results/co-assembly/{assembler}/{assembly}/final.fa", assembly = assemblies.keys(), assembler = config["assembler"])')
    (137, 'co_assembly_stats = expand("results/report/co-assembly/{assembler}.{assembly}_assembly_stats.tsv", assembly = assemblies.keys(), assembler = config["assembler"])')
    (138, 'co_assembly.append(co_assembly_stats)')
    (139, '## Fastuniq')
    (140, 'fastuniq = expand("results/fastuniq/{assembly}/R{i}.fastuniq.gz", assembly = assemblies.keys(), i=[1,2])')
    (141, '## Annotations')
    (142, 'eggnog_co = expand("results/collated/co-assembly/{assembler}/{assembly}/eggNOG/{db}.{fc}.tsv",')
    (143, '            db = ["enzymes","pathways","pathways.norm","modules","kos","tc","cazy"],')
    (144, '            fc = ["raw","tpm"],')
    (145, '            assembly = assemblies.keys(), assembler = config["assembler"])')
    (146, 'eggnog_co_tax = expand("results/collated/co-assembly/{assembler}/{assembly}/eggNOG_taxonomy/{tax_rank}.{tax_name}.{db}.{fc}.tsv",')
    (147, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (148, '            fc = ["raw","tpm"],')
    (149, '            db = ["kos","enzymes","modules","pathways","tc","cazy"],')
    (150, '            tax_rank = config["tax_rank"],')
    (151, '            tax_name = config["tax_name"])')
    (152, 'abundance_co = expand("results/collated/co-assembly/{assembler}/{assembly}/abundance/{assembly}.{fc}.tsv",')
    (153, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (154, '            fc = ["raw","tpm"])')
    (155, 'abundance_co_tax = expand("results/collated/co-assembly/{assembler}/{assembly}/abundance_taxonomy/{tax_rank}.{tax_name}.{assembly}.{fc}.tsv",')
    (156, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (157, '            fc = ["raw","tpm"],')
    (158, '            tax_rank = config["tax_rank"],')
    (159, '            tax_name = config["tax_name"])')
    (160, 'dbCAN_co = expand("results/collated/co-assembly/{assembler}/{assembly}/dbCAN/dbCAN.{fc}.tsv",')
    (161, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (162, '            fc = ["raw","tpm"])')
    (163, 'dbCAN_co_tax = expand("results/collated/co-assembly/{assembler}/{assembly}/dbCAN_taxonomy/{tax_rank}.{tax_name}.dbCAN.{fc}.tsv",')
    (164, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (165, '            fc = ["raw","tpm"],')
    (166, '            tax_rank = config["tax_rank"],')
    (167, '            tax_name = config["tax_name"])')
    (168, 'taxonomy_co = expand("results/collated/co-assembly/{assembler}/{assembly}/taxonomy/taxonomy.{fc}.tsv",')
    (169, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (170, '            fc = ["raw","tpm"])')
    (171, '# Map and normalize')
    (172, 'map_co = expand("results/report/map/{assembler}.{assembly}_map_report.html", assembly = assemblies.keys(), assembler=config["assembler"])')
    (173, 'normalize_co = expand("results/collated/co-assembly/{assembler}/{assembly}/abundance/{assembly}.{fc}.tsv",')
    (174, '                assembly = assemblies.keys(), assembler = config["assembler"], fc = ["tpm","raw"])')
    (175, '### Optional blobtools output')
    (176, 'blobtools_co = expand("results/annotation/co-assembly/{assembler}/{assembly}/taxonomy/blobtools/{sample_id}.bestsum.phylum.p5.span.100.exclude_other.blobplot.bam0.png",')
    (177, '                 assembly = assemblies.keys(), sample_id = samples.keys(), assembler = config["assembler"])')
    (178, 'blobtools_co += expand("results/annotation/co-assembly/{assembler}/{assembly}/taxonomy/blobtools/{sample_id}.bestsum.phylum.p5.span.100.exclude_other.blobplot.read_cov.bam0.png",')
    (179, '                assembly = assemblies.keys(), sample_id = samples.keys(), assembler = config["assembler"])')
    (180, '')
    (181, 'inputs = []')
    (182, 'inputs += preprocess')
    (183, '')
    (184, 'if config["co_assembly"]:')
    (185, '    inputs += [eggnog_co, map_co, co_assembly_stats, abundance_co, abundance_co_tax, dbCAN_co, taxonomy_co, dbCAN_co_tax, eggnog_co_tax]')
    (186, 'if config["single_assembly"]:')
    (187, '    inputs += [eggnog, dbCAN, taxonomy]')
    (188, '')
    (189, 'if config["read_source"] == "bowtie2":')
    (190, '    filter_input = bowtie_filter')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBISweden/fungal-trans, file=Snakefile
context_key: ['elif config["read_source"] == "taxmapper"']
    (10, 'def parse_validation_error(e):')
    (11, '    """')
    (12, '    Catches errors thrown when validating the config file and attempts to')
    (13, '    print more meaningful messages.')
    (14, '')
    (15, '    :param e: Error')
    (16, '    :return:')
    (17, '    """')
    (18, '    instance = ""')
    (19, '    print("ERROR VALIDATING CONFIG FILE")')
    (20, '    for item in str(e).split("\\')
    (21, '"):')
    (22, '        item = item.replace(\\\'"\\\',\\\'\\\')')
    (23, '        if "ValidationError:" in item:')
    (24, '            print(item)')
    (25, '        if item[0:11] == "On instance":')
    (26, '            instance = item.replace("On instance[\\\'", "INCORRECT CONFIG AT: ")')
    (27, '            instance = instance.replace("\\\']:","")')
    (28, '            print(instance)')
    (29, '    return')
    (30, '')
    (31, '# Set temporary dir')
    (32, 'if not os.getenv("TMPDIR"):')
    (33, '    os.environ["TMPDIR"] = "tmp"')
    (34, '    os.makedirs(os.environ["TMPDIR"],exist_ok=True)')
    (35, '')
    (36, 'wildcard_constraints:')
    (37, '    sample_id = "[A-Za-z0-9_\\\\-\\\\.]+",')
    (38, '    assembler = "megahit|trinity|transabyss",')
    (39, '    filter_source = "unfiltered|filtered|taxmapper|bowtie2"')
    (40, '')
    (41, '# Set default config and validate against schema')
    (42, 'if os.path.exists("config.yml"):')
    (43, '    configfile: "config.yml"')
    (44, 'try:')
    (45, '    validate(config, "config/config.schema.yaml")')
    (46, 'except WorkflowError as e:')
    (47, '    parse_validation_error(e)')
    (48, '    sys.exit()')
    (49, '')
    (50, 'workdir: config["workdir"]')
    (51, '')
    (52, '# Get environment info')
    (53, 'pythonpath = sys.executable')
    (54, 'envdir = \\\'/\\\'.join(pythonpath.split("/")[0:-2])')
    (55, 'system = platform.system()')
    (56, 'config["system"] = system')
    (57, '')
    (58, '# Parse samples and assemblies')
    (59, 'samples, map_dict, assemblies = parse_sample_list(config["sample_file_list"], config)')
    (60, '')
    (61, '# Include rules')
    (62, 'include: "source/rules/db.smk"')
    (63, 'include: "source/rules/preprocess.smk"')
    (64, 'include: "source/rules/filter.smk"')
    (65, 'include: "source/rules/assembly.smk"')
    (66, 'include: "source/rules/map.smk"')
    (67, 'include: "source/rules/sourmash.smk"')
    (68, 'include: "source/rules/annotate_single.smk"')
    (69, 'include: "source/rules/annotate_co.smk"')
    (70, 'include: "source/rules/kraken.smk"')
    (71, '')
    (72, '# Define targets')
    (73, '## Preprocessing')
    (74, 'preprocess = expand("results/preprocess/{sample_id}_R{i}.cut.trim.fastq.gz", sample_id = samples.keys(), i = [1,2])')
    (75, 'preprocess += ["results/report/preprocess/preprocess_report.html"]')
    (76, '')
    (77, '## Host reads')
    (78, 'host_reads = expand("results/host/{sample_id}_R{i}.host.fastq.gz",')
    (79, '            sample_id = samples.keys(), i = [1,2])')
    (80, '## Taxmapper filter')
    (81, 'taxmapper_filter = expand("results/taxmapper/{sample_id}/{sample_id}_R1.cut.trim.filtered.fastq.gz", sample_id = samples.keys())')
    (82, 'taxmapper_filter += expand("results/report/taxmapper/taxa_freq_norm_lvl{i}.svg", i = [1,2])')
    (83, '## Bowtie filter')
    (84, 'bowtie_filter = expand("results/{aligner}/{sample_id}/{sample_id}_R{i}.fungi.nohost.fastq.gz",')
    (85, '            sample_id = samples.keys(), i = [1,2], aligner = config["host_aligner"])')
    (86, 'bowtie_filter += ["results/report/filtering/filter_report.html"]')
    (87, 'bowtie_filter += host_reads')
    (88, '## Union filter')
    (89, 'union_filter = expand("results/filtered/{sample_id}/{sample_id}_R{i}.filtered.union.fastq.gz",')
    (90, '            sample_id = samples.keys(), i = [1,2])')
    (91, 'union_filter += host_reads')
    (92, '')
    (93, '## Sourmash')
    (94, 'sourmash = expand("results/sourmash/{sample_id}/{sample_id}.{source}.sig",')
    (95, '            sample_id = samples.keys(), source = config["read_source"])')
    (96, 'sourmash += expand("results/report/sourmash/{source}_sample.dist.labels.txt", source = config["read_source"])')
    (97, 'sourmash_assembly = expand("results/assembly/{assembler}/{source}/{sample_id}/final.sig",')
    (98, '            assembler = config["assembler"], source = config["read_source"], sample_id=samples.keys())')
    (99, 'sourmash_assembly += expand("results/report/sourmash/{source}_assembly_{assembler}.dist.labels.txt",')
    (100, '            source = config["read_source"], assembler = config["assembler"])')
    (101, '## Single-assemblies')
    (102, 'assembly = expand("results/report/assembly/{source}_{assembler}_stats.tsv",')
    (103, '            source = config["read_source"], assembler = config["assembler"])')
    (104, 'assembly += expand("results/assembly/{assembler}/{source}/{sample_id}/final.fa",')
    (105, '                assembler = config["assembler"], source = config["read_source"], sample_id = samples.keys())')
    (106, '## Annotations')
    (107, 'taxonomy = expand("results/collated/{assembler}/{source}/taxonomy/taxonomy.{fc}.tsv",')
    (108, '            assembler = config["assembler"],')
    (109, '            fc = ["tpm","raw"],')
    (110, '            source = config["read_source"])')
    (111, 'dbCAN = expand("results/collated/{assembler}/{source}/dbCAN/dbCAN.{fc}.tsv",')
    (112, '            assembler = config["assembler"],')
    (113, '            fc = ["tpm","raw"],')
    (114, '            source = config["read_source"])')
    (115, 'eggnog = expand("results/collated/{assembler}/{source}/eggNOG/{db}.{fc}.tsv",')
    (116, '            db = ["enzymes","pathways","pathways.norm","modules","kos","tc","cazy"],')
    (117, '            assembler = config["assembler"],')
    (118, '            fc = ["tpm","raw"],')
    (119, '            source = config["read_source"])')
    (120, 'mapres = expand("results/assembly/{assembler}/{source}/{sample_id}/{sample_id}_R{i}.fastq.gz",')
    (121, '            assembler = config["assembler"], source = config["read_source"], sample_id = samples.keys(), i = [1,2])')
    (122, 'mapres += expand("results/report/map/{assembler}_{source}_map_report.html",')
    (123, '            assembler = config["assembler"], source = config["read_source"])')
    (124, 'normalize = expand("results/annotation/{assembler}/{source}/{sample_id}/featureCounts/fc.{fc}.tab",')
    (125, '            assembler = config["assembler"], source = config["read_source"], sample_id = samples.keys(), fc=["tpm","raw"])')
    (126, '### Optional blobtools output')
    (127, 'blobtools = expand("results/annotation/{assembler}/{source}/{sample_id}/taxonomy/{sample_id}.bestsum.phylum.p5.span.100.exclude_other.blobplot.bam0.png",')
    (128, '            assembler = config["assembler"], sample_id = samples.keys(), source = config["read_source"])')
    (129, 'blobtools += expand("results/annotation/{assembler}/{source}/{sample_id}/taxonomy/{sample_id}.bestsum.order.p6.span.100.exclude_other.blobplot.bam0.png",')
    (130, '            assembler = config["assembler"], sample_id = samples.keys(), source = config["read_source"])')
    (131, '')
    (132, '### Optional kraken output')
    (133, 'kraken_output = expand("results/kraken/{sample_id}.{suffix}", sample_id = samples.keys(), suffix = ["out.gz","kreport"])')
    (134, '')
    (135, '## Co-assemblies')
    (136, 'co_assembly = expand("results/co-assembly/{assembler}/{assembly}/final.fa", assembly = assemblies.keys(), assembler = config["assembler"])')
    (137, 'co_assembly_stats = expand("results/report/co-assembly/{assembler}.{assembly}_assembly_stats.tsv", assembly = assemblies.keys(), assembler = config["assembler"])')
    (138, 'co_assembly.append(co_assembly_stats)')
    (139, '## Fastuniq')
    (140, 'fastuniq = expand("results/fastuniq/{assembly}/R{i}.fastuniq.gz", assembly = assemblies.keys(), i=[1,2])')
    (141, '## Annotations')
    (142, 'eggnog_co = expand("results/collated/co-assembly/{assembler}/{assembly}/eggNOG/{db}.{fc}.tsv",')
    (143, '            db = ["enzymes","pathways","pathways.norm","modules","kos","tc","cazy"],')
    (144, '            fc = ["raw","tpm"],')
    (145, '            assembly = assemblies.keys(), assembler = config["assembler"])')
    (146, 'eggnog_co_tax = expand("results/collated/co-assembly/{assembler}/{assembly}/eggNOG_taxonomy/{tax_rank}.{tax_name}.{db}.{fc}.tsv",')
    (147, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (148, '            fc = ["raw","tpm"],')
    (149, '            db = ["kos","enzymes","modules","pathways","tc","cazy"],')
    (150, '            tax_rank = config["tax_rank"],')
    (151, '            tax_name = config["tax_name"])')
    (152, 'abundance_co = expand("results/collated/co-assembly/{assembler}/{assembly}/abundance/{assembly}.{fc}.tsv",')
    (153, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (154, '            fc = ["raw","tpm"])')
    (155, 'abundance_co_tax = expand("results/collated/co-assembly/{assembler}/{assembly}/abundance_taxonomy/{tax_rank}.{tax_name}.{assembly}.{fc}.tsv",')
    (156, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (157, '            fc = ["raw","tpm"],')
    (158, '            tax_rank = config["tax_rank"],')
    (159, '            tax_name = config["tax_name"])')
    (160, 'dbCAN_co = expand("results/collated/co-assembly/{assembler}/{assembly}/dbCAN/dbCAN.{fc}.tsv",')
    (161, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (162, '            fc = ["raw","tpm"])')
    (163, 'dbCAN_co_tax = expand("results/collated/co-assembly/{assembler}/{assembly}/dbCAN_taxonomy/{tax_rank}.{tax_name}.dbCAN.{fc}.tsv",')
    (164, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (165, '            fc = ["raw","tpm"],')
    (166, '            tax_rank = config["tax_rank"],')
    (167, '            tax_name = config["tax_name"])')
    (168, 'taxonomy_co = expand("results/collated/co-assembly/{assembler}/{assembly}/taxonomy/taxonomy.{fc}.tsv",')
    (169, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (170, '            fc = ["raw","tpm"])')
    (171, '# Map and normalize')
    (172, 'map_co = expand("results/report/map/{assembler}.{assembly}_map_report.html", assembly = assemblies.keys(), assembler=config["assembler"])')
    (173, 'normalize_co = expand("results/collated/co-assembly/{assembler}/{assembly}/abundance/{assembly}.{fc}.tsv",')
    (174, '                assembly = assemblies.keys(), assembler = config["assembler"], fc = ["tpm","raw"])')
    (175, '### Optional blobtools output')
    (176, 'blobtools_co = expand("results/annotation/co-assembly/{assembler}/{assembly}/taxonomy/blobtools/{sample_id}.bestsum.phylum.p5.span.100.exclude_other.blobplot.bam0.png",')
    (177, '                 assembly = assemblies.keys(), sample_id = samples.keys(), assembler = config["assembler"])')
    (178, 'blobtools_co += expand("results/annotation/co-assembly/{assembler}/{assembly}/taxonomy/blobtools/{sample_id}.bestsum.phylum.p5.span.100.exclude_other.blobplot.read_cov.bam0.png",')
    (179, '                assembly = assemblies.keys(), sample_id = samples.keys(), assembler = config["assembler"])')
    (180, '')
    (181, 'inputs = []')
    (182, 'inputs += preprocess')
    (183, '')
    (184, 'if config["co_assembly"]:')
    (185, '    inputs += [eggnog_co, map_co, co_assembly_stats, abundance_co, abundance_co_tax, dbCAN_co, taxonomy_co, dbCAN_co_tax, eggnog_co_tax]')
    (186, 'if config["single_assembly"]:')
    (187, '    inputs += [eggnog, dbCAN, taxonomy]')
    (188, '')
    (189, 'if config["read_source"] == "bowtie2":')
    (190, '    filter_input = bowtie_filter')
    (191, 'elif config["read_source"] == "taxmapper":')
    (192, '    filter_input = taxmapper_filter')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBISweden/fungal-trans, file=Snakefile
context_key: ['elif config["read_source"] == "filtered"']
    (10, 'def parse_validation_error(e):')
    (11, '    """')
    (12, '    Catches errors thrown when validating the config file and attempts to')
    (13, '    print more meaningful messages.')
    (14, '')
    (15, '    :param e: Error')
    (16, '    :return:')
    (17, '    """')
    (18, '    instance = ""')
    (19, '    print("ERROR VALIDATING CONFIG FILE")')
    (20, '    for item in str(e).split("\\')
    (21, '"):')
    (22, '        item = item.replace(\\\'"\\\',\\\'\\\')')
    (23, '        if "ValidationError:" in item:')
    (24, '            print(item)')
    (25, '        if item[0:11] == "On instance":')
    (26, '            instance = item.replace("On instance[\\\'", "INCORRECT CONFIG AT: ")')
    (27, '            instance = instance.replace("\\\']:","")')
    (28, '            print(instance)')
    (29, '    return')
    (30, '')
    (31, '# Set temporary dir')
    (32, 'if not os.getenv("TMPDIR"):')
    (33, '    os.environ["TMPDIR"] = "tmp"')
    (34, '    os.makedirs(os.environ["TMPDIR"],exist_ok=True)')
    (35, '')
    (36, 'wildcard_constraints:')
    (37, '    sample_id = "[A-Za-z0-9_\\\\-\\\\.]+",')
    (38, '    assembler = "megahit|trinity|transabyss",')
    (39, '    filter_source = "unfiltered|filtered|taxmapper|bowtie2"')
    (40, '')
    (41, '# Set default config and validate against schema')
    (42, 'if os.path.exists("config.yml"):')
    (43, '    configfile: "config.yml"')
    (44, 'try:')
    (45, '    validate(config, "config/config.schema.yaml")')
    (46, 'except WorkflowError as e:')
    (47, '    parse_validation_error(e)')
    (48, '    sys.exit()')
    (49, '')
    (50, 'workdir: config["workdir"]')
    (51, '')
    (52, '# Get environment info')
    (53, 'pythonpath = sys.executable')
    (54, 'envdir = \\\'/\\\'.join(pythonpath.split("/")[0:-2])')
    (55, 'system = platform.system()')
    (56, 'config["system"] = system')
    (57, '')
    (58, '# Parse samples and assemblies')
    (59, 'samples, map_dict, assemblies = parse_sample_list(config["sample_file_list"], config)')
    (60, '')
    (61, '# Include rules')
    (62, 'include: "source/rules/db.smk"')
    (63, 'include: "source/rules/preprocess.smk"')
    (64, 'include: "source/rules/filter.smk"')
    (65, 'include: "source/rules/assembly.smk"')
    (66, 'include: "source/rules/map.smk"')
    (67, 'include: "source/rules/sourmash.smk"')
    (68, 'include: "source/rules/annotate_single.smk"')
    (69, 'include: "source/rules/annotate_co.smk"')
    (70, 'include: "source/rules/kraken.smk"')
    (71, '')
    (72, '# Define targets')
    (73, '## Preprocessing')
    (74, 'preprocess = expand("results/preprocess/{sample_id}_R{i}.cut.trim.fastq.gz", sample_id = samples.keys(), i = [1,2])')
    (75, 'preprocess += ["results/report/preprocess/preprocess_report.html"]')
    (76, '')
    (77, '## Host reads')
    (78, 'host_reads = expand("results/host/{sample_id}_R{i}.host.fastq.gz",')
    (79, '            sample_id = samples.keys(), i = [1,2])')
    (80, '## Taxmapper filter')
    (81, 'taxmapper_filter = expand("results/taxmapper/{sample_id}/{sample_id}_R1.cut.trim.filtered.fastq.gz", sample_id = samples.keys())')
    (82, 'taxmapper_filter += expand("results/report/taxmapper/taxa_freq_norm_lvl{i}.svg", i = [1,2])')
    (83, '## Bowtie filter')
    (84, 'bowtie_filter = expand("results/{aligner}/{sample_id}/{sample_id}_R{i}.fungi.nohost.fastq.gz",')
    (85, '            sample_id = samples.keys(), i = [1,2], aligner = config["host_aligner"])')
    (86, 'bowtie_filter += ["results/report/filtering/filter_report.html"]')
    (87, 'bowtie_filter += host_reads')
    (88, '## Union filter')
    (89, 'union_filter = expand("results/filtered/{sample_id}/{sample_id}_R{i}.filtered.union.fastq.gz",')
    (90, '            sample_id = samples.keys(), i = [1,2])')
    (91, 'union_filter += host_reads')
    (92, '')
    (93, '## Sourmash')
    (94, 'sourmash = expand("results/sourmash/{sample_id}/{sample_id}.{source}.sig",')
    (95, '            sample_id = samples.keys(), source = config["read_source"])')
    (96, 'sourmash += expand("results/report/sourmash/{source}_sample.dist.labels.txt", source = config["read_source"])')
    (97, 'sourmash_assembly = expand("results/assembly/{assembler}/{source}/{sample_id}/final.sig",')
    (98, '            assembler = config["assembler"], source = config["read_source"], sample_id=samples.keys())')
    (99, 'sourmash_assembly += expand("results/report/sourmash/{source}_assembly_{assembler}.dist.labels.txt",')
    (100, '            source = config["read_source"], assembler = config["assembler"])')
    (101, '## Single-assemblies')
    (102, 'assembly = expand("results/report/assembly/{source}_{assembler}_stats.tsv",')
    (103, '            source = config["read_source"], assembler = config["assembler"])')
    (104, 'assembly += expand("results/assembly/{assembler}/{source}/{sample_id}/final.fa",')
    (105, '                assembler = config["assembler"], source = config["read_source"], sample_id = samples.keys())')
    (106, '## Annotations')
    (107, 'taxonomy = expand("results/collated/{assembler}/{source}/taxonomy/taxonomy.{fc}.tsv",')
    (108, '            assembler = config["assembler"],')
    (109, '            fc = ["tpm","raw"],')
    (110, '            source = config["read_source"])')
    (111, 'dbCAN = expand("results/collated/{assembler}/{source}/dbCAN/dbCAN.{fc}.tsv",')
    (112, '            assembler = config["assembler"],')
    (113, '            fc = ["tpm","raw"],')
    (114, '            source = config["read_source"])')
    (115, 'eggnog = expand("results/collated/{assembler}/{source}/eggNOG/{db}.{fc}.tsv",')
    (116, '            db = ["enzymes","pathways","pathways.norm","modules","kos","tc","cazy"],')
    (117, '            assembler = config["assembler"],')
    (118, '            fc = ["tpm","raw"],')
    (119, '            source = config["read_source"])')
    (120, 'mapres = expand("results/assembly/{assembler}/{source}/{sample_id}/{sample_id}_R{i}.fastq.gz",')
    (121, '            assembler = config["assembler"], source = config["read_source"], sample_id = samples.keys(), i = [1,2])')
    (122, 'mapres += expand("results/report/map/{assembler}_{source}_map_report.html",')
    (123, '            assembler = config["assembler"], source = config["read_source"])')
    (124, 'normalize = expand("results/annotation/{assembler}/{source}/{sample_id}/featureCounts/fc.{fc}.tab",')
    (125, '            assembler = config["assembler"], source = config["read_source"], sample_id = samples.keys(), fc=["tpm","raw"])')
    (126, '### Optional blobtools output')
    (127, 'blobtools = expand("results/annotation/{assembler}/{source}/{sample_id}/taxonomy/{sample_id}.bestsum.phylum.p5.span.100.exclude_other.blobplot.bam0.png",')
    (128, '            assembler = config["assembler"], sample_id = samples.keys(), source = config["read_source"])')
    (129, 'blobtools += expand("results/annotation/{assembler}/{source}/{sample_id}/taxonomy/{sample_id}.bestsum.order.p6.span.100.exclude_other.blobplot.bam0.png",')
    (130, '            assembler = config["assembler"], sample_id = samples.keys(), source = config["read_source"])')
    (131, '')
    (132, '### Optional kraken output')
    (133, 'kraken_output = expand("results/kraken/{sample_id}.{suffix}", sample_id = samples.keys(), suffix = ["out.gz","kreport"])')
    (134, '')
    (135, '## Co-assemblies')
    (136, 'co_assembly = expand("results/co-assembly/{assembler}/{assembly}/final.fa", assembly = assemblies.keys(), assembler = config["assembler"])')
    (137, 'co_assembly_stats = expand("results/report/co-assembly/{assembler}.{assembly}_assembly_stats.tsv", assembly = assemblies.keys(), assembler = config["assembler"])')
    (138, 'co_assembly.append(co_assembly_stats)')
    (139, '## Fastuniq')
    (140, 'fastuniq = expand("results/fastuniq/{assembly}/R{i}.fastuniq.gz", assembly = assemblies.keys(), i=[1,2])')
    (141, '## Annotations')
    (142, 'eggnog_co = expand("results/collated/co-assembly/{assembler}/{assembly}/eggNOG/{db}.{fc}.tsv",')
    (143, '            db = ["enzymes","pathways","pathways.norm","modules","kos","tc","cazy"],')
    (144, '            fc = ["raw","tpm"],')
    (145, '            assembly = assemblies.keys(), assembler = config["assembler"])')
    (146, 'eggnog_co_tax = expand("results/collated/co-assembly/{assembler}/{assembly}/eggNOG_taxonomy/{tax_rank}.{tax_name}.{db}.{fc}.tsv",')
    (147, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (148, '            fc = ["raw","tpm"],')
    (149, '            db = ["kos","enzymes","modules","pathways","tc","cazy"],')
    (150, '            tax_rank = config["tax_rank"],')
    (151, '            tax_name = config["tax_name"])')
    (152, 'abundance_co = expand("results/collated/co-assembly/{assembler}/{assembly}/abundance/{assembly}.{fc}.tsv",')
    (153, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (154, '            fc = ["raw","tpm"])')
    (155, 'abundance_co_tax = expand("results/collated/co-assembly/{assembler}/{assembly}/abundance_taxonomy/{tax_rank}.{tax_name}.{assembly}.{fc}.tsv",')
    (156, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (157, '            fc = ["raw","tpm"],')
    (158, '            tax_rank = config["tax_rank"],')
    (159, '            tax_name = config["tax_name"])')
    (160, 'dbCAN_co = expand("results/collated/co-assembly/{assembler}/{assembly}/dbCAN/dbCAN.{fc}.tsv",')
    (161, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (162, '            fc = ["raw","tpm"])')
    (163, 'dbCAN_co_tax = expand("results/collated/co-assembly/{assembler}/{assembly}/dbCAN_taxonomy/{tax_rank}.{tax_name}.dbCAN.{fc}.tsv",')
    (164, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (165, '            fc = ["raw","tpm"],')
    (166, '            tax_rank = config["tax_rank"],')
    (167, '            tax_name = config["tax_name"])')
    (168, 'taxonomy_co = expand("results/collated/co-assembly/{assembler}/{assembly}/taxonomy/taxonomy.{fc}.tsv",')
    (169, '            assembly = assemblies.keys(), assembler = config["assembler"],')
    (170, '            fc = ["raw","tpm"])')
    (171, '# Map and normalize')
    (172, 'map_co = expand("results/report/map/{assembler}.{assembly}_map_report.html", assembly = assemblies.keys(), assembler=config["assembler"])')
    (173, 'normalize_co = expand("results/collated/co-assembly/{assembler}/{assembly}/abundance/{assembly}.{fc}.tsv",')
    (174, '                assembly = assemblies.keys(), assembler = config["assembler"], fc = ["tpm","raw"])')
    (175, '### Optional blobtools output')
    (176, 'blobtools_co = expand("results/annotation/co-assembly/{assembler}/{assembly}/taxonomy/blobtools/{sample_id}.bestsum.phylum.p5.span.100.exclude_other.blobplot.bam0.png",')
    (177, '                 assembly = assemblies.keys(), sample_id = samples.keys(), assembler = config["assembler"])')
    (178, 'blobtools_co += expand("results/annotation/co-assembly/{assembler}/{assembly}/taxonomy/blobtools/{sample_id}.bestsum.phylum.p5.span.100.exclude_other.blobplot.read_cov.bam0.png",')
    (179, '                assembly = assemblies.keys(), sample_id = samples.keys(), assembler = config["assembler"])')
    (180, '')
    (181, 'inputs = []')
    (182, 'inputs += preprocess')
    (183, '')
    (184, 'if config["co_assembly"]:')
    (185, '    inputs += [eggnog_co, map_co, co_assembly_stats, abundance_co, abundance_co_tax, dbCAN_co, taxonomy_co, dbCAN_co_tax, eggnog_co_tax]')
    (186, 'if config["single_assembly"]:')
    (187, '    inputs += [eggnog, dbCAN, taxonomy]')
    (188, '')
    (189, 'if config["read_source"] == "bowtie2":')
    (190, '    filter_input = bowtie_filter')
    (191, 'elif config["read_source"] == "taxmapper":')
    (192, '    filter_input = taxmapper_filter')
    (193, 'elif config["read_source"] == "filtered":')
    (194, '    filter_input = union_filter')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NBISweden/fungal-trans, file=source/rules/filter.smk
context_key: ['if config["host_aligner"] == "star"']
    (292, 'def get_host_logs(config, samples):')
    (293, '    if config["host_aligner"] == "star":')
    (294, '        return expand("results/star/{sample_id}/{sample_id}.host.log", sample_id = samples.keys())')
    (295, '    elif config["host_aligner"] == "bowtie2":')
    (296, '        return expand("results/bowtie2/{sample_id}/{sample_id}.host.log", sample_id = samples.keys())')
    (297, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Mar111tiN/RNAseq, file=rules/common.smk
context_key: ["if config[\\'trimming\\'][\\'activate\\']"]
    (139, 'def get_star_fastq(w):')
    (140, "    \\'\\'\\'")
    (141, '    returns the trimmed/untrimmed mates/single fastq depending on config and existence of second fastq in units_df')
    (142, "    \\'\\'\\'")
    (143, '')
    (144, '    # trimming is active')
    (145, "    if config[\\'trimming\\'][\\'activate\\']:")
    (146, '        # activated trimming, use trimmed data')
    (147, '')
    (148, '        return get_trim_fastq(w)')
    (149, '')
    (150, '    # trimming is inactive --> get org fastqs from units_df  ')
    (151, '    return get_raw_fastq(w)')
    (152, '')
    (153, '')
    (154, '############# fastQC ########################')
    (155, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Mar111tiN/RNAseq, file=rules/common.smk
context_key: ["if config[\\'trimming\\'][\\'activate\\']"]
    (156, 'def get_fastqc_list(_):')
    (157, "    \\'\\'\\'")
    (158, '    returns the complete list of required fastqc files depending on trim option')
    (159, "    \\'\\'\\'")
    (160, '    ')
    (161, '    # create file list from the included_files tuple list')
    (162, '    fastqc_folder = "qc/fastqc"')
    (163, '    types = ["raw"]')
    (164, '    # add the trim folder prefix if trimming is active')
    (165, "    if config[\\'trimming\\'][\\'activate\\']:")
    (166, '        types.append("trim")')
    (167, '    reads = ["R1", "R2"]')
    (168, '    # create SE marker for single read units')
    (169, '    units[\\\'SE\\\'] = units[\\\'fastq2\\\'].isna() | (units[\\\'fastq2\\\'] == "")')
    (170, '')
    (171, '    PE_list = [os.path.join(fastqc_folder, f"{u[0]}-{u[1]}_{read}_{t}_fastqc.zip") for u in units[~units[\\\'SE\\\']].index for t in types for read in reads]')
    (172, '    SE_list = [os.path.join(fastqc_folder, f"{u[0]}-{u[1]}_{t}_fastqc.zip") for u in units[units[\\\'SE\\\']].index for t in types]')
    (173, '')
    (174, '    return SE_list + PE_list')
    (175, '')
    (176, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/human_genomics_pipeline, file=workflow/Snakefile
context_key: ['if config[\\\'TRIM\\\'] == "No" or config[\\\'TRIM\\\'] == "no"']
    (18, 'def get_input_fastq(command):')
    (19, '    """Return a string which defines the input fastq files for the bwa_mem and pbrun_germline rules.')
    (20, '    This changes based on the user configurable options for trimming')
    (21, '    """')
    (22, '    ')
    (23, '    input_files = ""')
    (24, '')
    (25, '    if config[\\\'TRIM\\\'] == "No" or config[\\\'TRIM\\\'] == "no":')
    (26, '        input_files = ["../../fastq/{sample}_1.fastq.gz", "../../fastq/{sample}_2.fastq.gz"]')
    (27, '    if config[\\\'TRIM\\\'] == "Yes" or config[\\\'TRIM\\\'] == "yes":')
    (28, '        input_files = ["../results/trimmed/{sample}_1_val_1.fq.gz", "../results/trimmed/{sample}_2_val_2.fq.gz"]')
    (29, '')
    (30, '    return input_files')
    (31, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/human_genomics_pipeline, file=workflow/Snakefile
context_key: ['if config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\'']
    (32, 'def get_output_vcf(config):')
    (33, '    """Return a string which defines the output vcf files for the pbrun_germline rule. This changes based on the')
    (34, '    user configurable options for running single samples or cohorts of samples')
    (35, '    """')
    (36, '    ')
    (37, '    vcf = ""')
    (38, '')
    (39, '    if config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\':')
    (40, '        vcf = "../results/called/{sample}_raw_snps_indels.vcf"')
    (41, '    if config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\':')
    (42, '        vcf = "../results/called/{sample}_raw_snps_indels_tmp.g.vcf"')
    (43, '')
    (44, '    return vcf')
    (45, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/human_genomics_pipeline, file=workflow/Snakefile
context_key: ['if config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\'']
    (46, 'def get_params(command):')
    (47, '    """Return a string which defines some parameters for the pbrun_germline rule. This changes based on the')
    (48, '    user configurable options for running single samples or cohorts of samples')
    (49, '    """')
    (50, '    ')
    (51, '    params = ""')
    (52, '')
    (53, '    if config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\':')
    (54, '        params = ""')
    (55, '    if config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\':')
    (56, '        params = "--gvcf"')
    (57, '')
    (58, '    return params')
    (59, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/human_genomics_pipeline, file=workflow/Snakefile
context_key: ['if config[\\\'GPU_ACCELERATED\\\'] == "Yes" or config[\\\'GPU_ACCELERATED\\\'] == "yes"']
    (92, 'def get_recal_resources_command(resource):')
    (93, '    """Return a string, a portion of the gatk BaseRecalibrator command (used in the gatk_BaseRecalibrator and the')
    (94, '    parabricks_germline rules) which dynamically includes each of the recalibration resources defined by the user')
    (95, '    in the configuration file. For each recalibration resource (element in the list), we construct the command by')
    (96, '    adding either --knownSites (for parabricks) or --known-sites (for gatk4) <recalibration resource file>')
    (97, '    """')
    (98, '    ')
    (99, '    command = ""')
    (100, '    ')
    (101, "    for resource in config[\\'RECALIBRATION\\'][\\'RESOURCES\\']:")
    (102, '        if config[\\\'GPU_ACCELERATED\\\'] == "Yes" or config[\\\'GPU_ACCELERATED\\\'] == "yes":')
    (103, '            command += "--knownSites " + resource + " "')
    (104, '')
    (105, '        if config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no":')
    (106, '            command += "--known-sites " + resource + " "')
    (107, '')
    (108, '    return command')
    (109, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/human_genomics_pipeline, file=workflow/Snakefile
context_key: ['if config[\\\'GPU_ACCELERATED\\\'] == "Yes" or config[\\\'GPU_ACCELERATED\\\'] == "yes"']
    (110, 'def get_wes_intervals_command(resource):')
    (111, '    """Return a string, a portion of the gatk command\\\'s (used in several gatk rules) which builds a flag')
    (112, '    formatted for gatk based on the configuration file. We construct the command by adding either --interval-file')
    (113, '    (for parabricks) or --L (for gatk4) <exome capture file>. If the user provides nothing for these configurable')
    (114, '    options, an empty string is returned')
    (115, '    """')
    (116, '    ')
    (117, '    command = ""')
    (118, '    ')
    (119, '    if config[\\\'GPU_ACCELERATED\\\'] == "Yes" or config[\\\'GPU_ACCELERATED\\\'] == "yes":')
    (120, '        command = "--interval-file " + config[\\\'WES\\\'][\\\'PADDING\\\'] + " "')
    (121, '    if config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no":')
    (122, '        command = "--L " + config[\\\'WES\\\'][\\\'PADDING\\\'] + " "')
    (123, '    if config[\\\'WES\\\'][\\\'INTERVALS\\\'] == "":')
    (124, '        command = ""')
    (125, '')
    (126, '    return command')
    (127, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/human_genomics_pipeline, file=workflow/Snakefile
context_key: ['if config[\\\'GPU_ACCELERATED\\\'] == "Yes" or config[\\\'GPU_ACCELERATED\\\'] == "yes"']
    (128, 'def get_wes_padding_command(resource):')
    (129, '    """Return a string, a portion of the gatk command\\\'s (used in several gatk rules) which builds a flag')
    (130, '    formatted for gatk based on the configuration file. We construct the command by adding either --interval')
    (131, '    (for parabricks) or --ip (for gatk4) <exome capture file>. If the user provides nothing for these configurable')
    (132, '    options, an empty string is returned')
    (133, '    """')
    (134, '    ')
    (135, '    command = ""')
    (136, '')
    (137, '    if config[\\\'GPU_ACCELERATED\\\'] == "Yes" or config[\\\'GPU_ACCELERATED\\\'] == "yes":')
    (138, '        command = "--interval " + config[\\\'WES\\\'][\\\'PADDING\\\'] + " "')
    (139, '    if config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no":')
    (140, '        command = "--ip " + config[\\\'WES\\\'][\\\'PADDING\\\'] + " "')
    (141, '    if config[\\\'WES\\\'][\\\'PADDING\\\'] == "":')
    (142, '        command = ""')
    (143, '')
    (144, '    return command')
    (145, '')
    (146, '#### Set up report #####')
    (147, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/human_genomics_pipeline, file=workflow/Snakefile
context_key: ['if config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\'', 'rule all', 'input']
    (128, 'def get_wes_padding_command(resource):')
    (129, '    """Return a string, a portion of the gatk command\\\'s (used in several gatk rules) which builds a flag')
    (130, '    formatted for gatk based on the configuration file. We construct the command by adding either --interval')
    (131, '    (for parabricks) or --ip (for gatk4) <exome capture file>. If the user provides nothing for these configurable')
    (132, '    options, an empty string is returned')
    (133, '    """')
    (134, '    ')
    (135, '    command = ""')
    (136, '')
    (137, '    if config[\\\'GPU_ACCELERATED\\\'] == "Yes" or config[\\\'GPU_ACCELERATED\\\'] == "yes":')
    (138, '        command = "--interval " + config[\\\'WES\\\'][\\\'PADDING\\\'] + " "')
    (139, '    if config[\\\'GPU_ACCELERATED\\\'] == "No" or config[\\\'GPU_ACCELERATED\\\'] == "no":')
    (140, '        command = "--ip " + config[\\\'WES\\\'][\\\'PADDING\\\'] + " "')
    (141, '    if config[\\\'WES\\\'][\\\'PADDING\\\'] == "":')
    (142, '        command = ""')
    (143, '')
    (144, '    return command')
    (145, '')
    (146, '#### Set up report #####')
    (147, '')
    (148, 'report: "report/workflow.rst"')
    (149, '')
    (150, '##### Target rules #####')
    (151, '')
    (152, 'if config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\':')
    (153, '    rule all:')
    (154, '        input:')
    (155, '            "../results/qc/multiqc_report.html",')
    (156, '            expand("../results/mapped/{sample}_recalibrated.bam", sample = SAMPLES),')
    (157, '            expand("../results/called/{sample}_raw_snps_indels.vcf", sample = SAMPLES),')
    (158, '            ')
    (159, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yztxwd/snakemake-pipeline-general, file=rules/bowtie2.smk
context_key: ['if config["skiptrim"]']
    (12, 'def align_se_find_input(wildcards):')
    (13, '    global samples')
    (14, '    global config')
    (15, '')
    (16, '    trimmer=config["trimmer"]')
    (17, '    if config["skiptrim"]:')
    (18, '        return "data/" + samples.loc[(wildcards.sample, wildcards.rep, wildcards.unit), "fq1"]')
    (19, '    else:')
    (20, '        return f"output/trimmed/{{sample}}-{{rep}}-{{unit}}.{trimmer}.fq.gz" ')
    (21, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/cite-seq-alevin-fry-seurat, file=workflow/rules/common.smk
context_key: ['if config["thresholds"].get("max-hto-count") is None']
    (26, 'def get_targets():')
    (27, '    if config["thresholds"].get("max-hto-count") is None:')
    (28, '        return "results/plots/hto-counts.initial.pdf"')
    (29, '    if config["thresholds"].get("max-adt-count") is None:')
    (30, '        return "results/plots/adt-counts.pdf"')
    (31, '    # TODO go on with later steps')
    (32, '    return []')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=usnistgov/giab-asm-benchmarking, file=workflow/Snakefile
context_key: ['if config["gender"] == "male"', 'rule dipcall_makefile_male', 'input']
    (148, 'def get_hap2(wildcards):')
    (149, '    path = asm.loc[(wildcards.prefix), "maternal_haplotype"]')
    (150, '    return(path)')
    (151, '')
    (152, '################################################################################')
    (153, '## Call variants with Dipcall based on gender')
    (154, '################################################################################')
    (155, '')
    (156, 'if config["gender"] == "male" :')
    (157, '    rule dipcall_makefile_male:')
    (158, '        input:')
    (159, '            h1=get_hap1,')
    (160, '            h2=get_hap2,')
    (161, '            ref= rules.get_ref.output,')
    (162, '            ref_idx= rules.index_ref.output,')
    (163, '            #ref="resources/references/" + ref_id + ".fa",')
    (164, '            par=par_ref')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=BDI-pathogens/ShiverCovid, file=snakemake/Snakefile
context_key: ['if config_value not in valid_value_list']
    (12, 'def check_config(config_name, valid_value_list):')
    (13, '    config_value = config[config_name]')
    (14, '    if config_value not in valid_value_list:')
    (15, '        logging.error(f"{config_name} config \\\'{config_value}\\\' not valid. Options are: {valid_value_list}. Exiting...")')
    (16, '        sys.exit(1)')
    (17, '    return config_value')
    (18, '')
    (19, '')
    (20, '# General config')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/Nanopore_DNA-seq, file=Snakefile
context_key: ['if config["hpv"]=="yes"', 'rule all', 'input']
    (24, 'def parse_sampleID(fname):')
    (25, "    return fname.split(raw)[-1].split(\\'_fast5\\')[0]")
    (26, '')
    (27, "dire = sorted(glob.glob(raw + \\'*_fast5\\'), key=parse_sampleID)")
    (28, '')
    (29, 'd = {}')
    (30, 'for key, value in itertools.groupby(dire, parse_sampleID):')
    (31, '    d[key] = list(value)')
    (32, '')
    (33, 'samples = d.keys()')
    (34, '')
    (35, '# Map to HPV genome or not')
    (36, 'if config["hpv"]=="yes":')
    (37, '   include: "modules/Snakefile_MBC"')
    (38, '   include: "modules/Snakefile_SV"')
    (39, '   include: "modules/Snakefile_MBC_hpv"')
    (40, '   include: "modules/Snakefile_SV_hpv"')
    (41, '   include: "modules/Snakefile_VC"')
    (42, '   include: "modules/Snakefile_VC_hpv"')
    (43, '   rule all:')
    (44, '        input:')
    (45, '              expand(out + "mbc/basic/{sample}/mod_mappings.5mC_sorted.bam.bai",sample=samples),')
    (46, '              expand(out + "mbc/model_based/{sample}/mod_mappings.5mC_sorted.bam.bai",sample=samples),')
    (47, '              expand(out + "sv/{sample}/sv_calls/{sample}_cutesv_filtered_ann_Qfiltered.vcf",sample=samples),')
    (48, '              expand(out + "sv/{sample}/qc",sample=samples),')
    (49, '              expand(out + "mbc_hpv/basic/{sample}/mod_mappings.5mC_sorted.bam.bai",sample=samples),')
    (50, '              expand(out + "mbc_hpv/model_based/{sample}/mod_mappings.5mC_sorted.bam.bai",sample=samples),')
    (51, '              expand(out + "sv_hpv/{sample}/sv_calls/{sample}_cutesv_filtered_ann.vcf",sample=samples),')
    (52, '              expand(out + "sv_hpv/{sample}/qc",sample=samples),')
    (53, '              expand(out + "vc/{sample}/complete.txt",sample=samples),')
    (54, '              expand(out + "vc/{sample}/filter/{sample}_PEPPER_Margin_DeepVariant.phased_ann_Qfiltered_ann.vcf",sample=samples),')
    (55, '              expand(out + "vc_hpv/{sample}/complete.txt",sample=samples),')
    (56, '              expand(out + "vc_hpv/{sample}/{sample}_PEPPER_Margin_DeepVariant.phased_ann.vcf",sample=samples),')
    (57, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=richardshuai/sv2a-rna-seq, file=workflow/rules/common.smk
context_key: ['if config["trimming"]["activate"]']
    (162, 'def get_fastqs(wc):')
    (163, '    if config["trimming"]["activate"]:')
    (164, '        return expand(')
    (165, '            "results/trimmed/{sample}/{unit}_{read}.fastq.gz",')
    (166, '            unit=units.loc[wc.sample, "unit_name"],')
    (167, '            sample=wc.sample,')
    (168, '            read=wc.read,')
    (169, '        )')
    (170, '    unit = units.loc[wc.sample]')
    (171, '    if all(pd.isna(unit["fq1"])):')
    (172, '        # SRA sample (always paired-end for now)')
    (173, '        accession = unit["sra"]')
    (174, '        return expand(')
    (175, '            "sra/{accession}_{read}.fastq", accession=accession, read=wc.read[-1]')
    (176, '        )')
    (177, '    fq = "fq{}".format(wc.read[-1])')
    (178, '    return units.loc[wc.sample, fq].tolist()')
    (179, '')
    (180, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=olabiyi/snakemake-workflow-qiime2, file=Snakefile
context_key: ['if config[\\\'amplicon\\\'] == "16S"', 'rule Function_annotation', 'input']
    (1358, 'def get_out_dir(w, output):')
    (1359, "    parts = output.ec.split(\\'/\\')")
    (1360, '    return parts[0] + "/" + parts[1]')
    (1361, '')
    (1362, 'if config[\\\'amplicon\\\'] == "16S":')
    (1363, '')
    (1364, '    # ------------------ Function analysis using Picrust2 -----------------#')
    (1365, '')
    (1366, '    rule Function_annotation:')
    (1367, '        input:')
    (1368, '            feature_table=rules.Export_tables.output.feature_table_biom,')
    (1369, '            rep_seqs=rules.Export_tables.output.rep_seqs')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ramay/dada2_snakemake_workflow, file=Snakefile
context_key: ['if config["qc_only"]']
    (16, 'def all_input_reads(qc):')
    (17, '    if config["qc_only"]:')
    (18, '        return expand("output/fastqc/{sample}" + config["R1"] + "_fastqc.html", sample=SAMPLES)')
    (19, '    else:')
    (20, '#                return expand("output/cutadapt/{sample}_R1.fastq.gz",sample=SAMPLES)')
    (21, '                return expand("output/fastqc/{sample}" + config["R1"] + "_fastqc.html", sample=SAMPLES)')
    (22, '')
    (23, '')
    (24, '')
    (25, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if config["epitope_prediction"]["activate"]']
    (45, 'def get_final_output():')
    (46, '    if config["epitope_prediction"]["activate"]:')
    (47, '        final_output = expand(')
    (48, '            "results/neoantigens/{mhc}/{S.sample}.{S.sequencing_type}.xlsx",')
    (49, '            S=units.loc[samples[samples.type == "tumor"]["sample"]]')
    (50, '            .drop_duplicates(["sample", "sequencing_type"])')
    (51, '            .itertuples(),')
    (52, '            mhc=list(')
    (53, '                filter(')
    (54, '                    None,')
    (55, '                    [')
    (56, '                        "netMHCpan" if is_activated("affinity/netMHCpan") else None,')
    (57, '                        "netMHCIIpan" if is_activated("affinity/netMHCIIpan") else None,')
    (58, '                    ],')
    (59, '                )')
    (60, '            ),')
    (61, '        )')
    (62, '    else:')
    (63, '        if config["HLAtyping"]["HLA_LA"]["activate"]:')
    (64, '            final_output = expand(')
    (65, '                [')
    (66, '                    "results/optitype/{sample}/hla_alleles_{sample}.tsv",')
    (67, '                    "results/HLA-LA/hlaI_{sample}.tsv",')
    (68, '                    "results/HLA-LA/hlaII_{sample}.tsv",')
    (69, '                ],')
    (70, '                sample=samples["sample"],')
    (71, '            )')
    (72, '        else:')
    (73, '            final_output = expand(')
    (74, '                "results/optitype/{sample}/hla_alleles_{sample}.tsv",')
    (75, '                sample=samples["sample"],')
    (76, '            )')
    (77, '    return final_output')
    (78, '')
    (79, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if config["fusion"]["arriba"]["activate"]']
    (80, 'def get_fusion_output():')
    (81, '    if config["fusion"]["arriba"]["activate"]:')
    (82, '        fusion_output = expand(')
    (83, '            "results/fusion/arriba/{sample}.fusions.tsv",')
    (84, '            sample=units[units["sequencing_type"] == "RNA"]["sample"],')
    (85, '        )')
    (86, '    else:')
    (87, '        fusion_output = []')
    (88, '    return fusion_output')
    (89, '')
    (90, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/dna-seq-neoantigen-prediction, file=workflow/rules/common.smk
context_key: ['if config["trimming"]["activate"]']
    (165, 'def get_fastqs(wc):')
    (166, '    if config["trimming"]["activate"]:')
    (167, '        return expand(')
    (168, '            "results/trimmed/{sample}/{seqtype}/{unit}_{read}.fastq.gz",')
    (169, '            unit=units.loc[wc.seqtype].loc[wc.sample, "unit_name"],')
    (170, '            sample=wc.sample,')
    (171, '            read=wc.read,')
    (172, '            seqtype=wc.seqtype,')
    (173, '        )')
    (174, '    unit = units.loc[wc.sample].loc[wc.seqtype]')
    (175, '    if all(pd.isna(unit["fq1"])):')
    (176, '        # SRA sample (always paired-end for now)')
    (177, '        accession = unit["sra"]')
    (178, '        return expand(')
    (179, '            "sra/{accession}_{read}.fastq", accession=accession, read=wc.read[-1]')
    (180, '        )')
    (181, '    fq = "fq{}".format(wc.read[-1])')
    (182, '    return units.loc[wc.sample].loc[wc.seqtype, fq].tolist()')
    (183, '')
    (184, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SaGeTOPK/sage-topk-experiments, file=Snakefile
context_key: ['if config["experiments"][xp]["check"]']
    (61, 'def xp_files(wcs):')
    (62, '    name = config["name"]')
    (63, '    output = config["output"]')
    (64, '    for xp in config["experiments"]:')
    (65, '        if config["experiments"][xp]["check"]:')
    (66, '            return [f"{output}/{name}/run.csv", f"{output}/{name}/check.csv"]')
    (67, '    return [f"{output}/{name}/run.csv"]')
    (68, '')
    (69, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lparsons/kas-seq-workflow, file=rules/common.smk
context_key: ['if config["trimming"]["skip"]', 'if is_single_end(wildcards.sample, wildcards.unit)']
    (12, 'def get_fq(wildcards):')
    (13, '    if config["trimming"]["skip"]:')
    (14, '        # no trimming, use raw reads')
    (15, '        u = units.loc[(wildcards.sample, wildcards.unit), ["fq1", "fq2"]].dropna()')
    (16, '        if is_single_end(wildcards.sample, wildcards.unit):')
    (17, '            return [f"{u.fq1}"]')
    (18, '        else:')
    (19, '            return [f"{u.fq1}", f"{u.fq2}"]')
    (20, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lparsons/kas-seq-workflow, file=rules/common.smk
context_key: ['if config["trimming"]["skip"]', 'else', 'if not is_single_end(wildcards.sample, wildcards.unit)']
    (12, 'def get_fq(wildcards):')
    (13, '    if config["trimming"]["skip"]:')
    (14, '        # no trimming, use raw reads')
    (15, '        u = units.loc[(wildcards.sample, wildcards.unit), ["fq1", "fq2"]].dropna()')
    (16, '        if is_single_end(wildcards.sample, wildcards.unit):')
    (17, '            return [f"{u.fq1}"]')
    (18, '        else:')
    (19, '            return [f"{u.fq1}", f"{u.fq2}"]')
    (20, '')
    (21, '    else:')
    (22, '        # yes trimming, use trimmed data')
    (23, '        if not is_single_end(wildcards.sample, wildcards.unit):')
    (24, '            # paired-end sample')
    (25, '            return expand(')
    (26, '                "results/trimmed/{sample}-{unit}.{group}.fastq.gz",')
    (27, '                group=[1, 2],')
    (28, '                sample=wildcards.sample,')
    (29, '                unit=wildcards.unit,')
    (30, '            )')
    (31, '        # single end sample')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lparsons/kas-seq-workflow, file=rules/common.smk
context_key: ['if config["trimming"]["skip"]', 'else']
    (12, 'def get_fq(wildcards):')
    (13, '    if config["trimming"]["skip"]:')
    (14, '        # no trimming, use raw reads')
    (15, '        u = units.loc[(wildcards.sample, wildcards.unit), ["fq1", "fq2"]].dropna()')
    (16, '        if is_single_end(wildcards.sample, wildcards.unit):')
    (17, '            return [f"{u.fq1}"]')
    (18, '        else:')
    (19, '            return [f"{u.fq1}", f"{u.fq2}"]')
    (20, '')
    (21, '    else:')
    (22, '        # yes trimming, use trimmed data')
    (23, '        if not is_single_end(wildcards.sample, wildcards.unit):')
    (24, '            # paired-end sample')
    (25, '            return expand(')
    (26, '                "results/trimmed/{sample}-{unit}.{group}.fastq.gz",')
    (27, '                group=[1, 2],')
    (28, '                sample=wildcards.sample,')
    (29, '                unit=wildcards.unit,')
    (30, '            )')
    (31, '        # single end sample')
    (32, '        return [')
    (33, '            "results/trimmed/{sample}-{unit}.fastq.gz".format(')
    (34, '                sample=wildcards.sample, unit=wildcards.unit')
    (35, '            )')
    (36, '        ]')
    (37, '')
    (38, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=solida-core/ride, file=rules/reads_feature_count.smk
context_key: ['if config.get("read_type")=="se"']
    (1, 'def fastq_input(r1):')
    (2, '    if config.get("read_type")=="se":')
    (3, '        return r1')
    (4, '    else:')
    (5, '        r2=r1.replace("-R1-", "-R2-")')
    (6, '        reads=[r1,r2]')
    (7, '        return " ".join(reads)')
    (8, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=imgag/ont_tools, file=workflow/rules/helper_scripts.smk
context_key: ["if config[\\'verbose\\']"]
    (2, 'def get_input_folders(wc):')
    (3, '    """')
    (4, '    Return the FASTQ data folder for a sample.')
    (5, '        Allows multiple runs per sample (restarted, repeated)')
    (6, '        Demultiplexed samples on a single flowcell with barcode information')
    (7, '        Includes folder with failed reads when specified in config')
    (8, '')
    (9, '        New option: Ifq folder contains a subfolder called "fastq_rebasecalled", ')
    (10, '        reads will be taken only from this folder. Option can be turned off in config option')
    (11, "        \\'fastq_prefer_rebasecalled\\' ")
    (12, '    """')
    (13, '    folders = map_samples_folder[wc.sample].copy()')
    (14, '    if config[\\\'verbose\\\']: print("Input Folders:" + str(folders), end = \\\'\\\')')
    (15, '    ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=imgag/ont_tools, file=workflow/rules/helper_scripts.smk
context_key: ["if config[\\'verbose\\']"]
    (34, 'def get_input_folders_fast5(wc):')
    (35, '    """')
    (36, '    Return the FAST5 data folder for a sample.')
    (37, '        Allows multiple runs per sample (restarted, repeated)')
    (38, '        Demultiplexed samples on a single flowcell with barcode information')
    (39, '        Includes folder with failed reads when specified in config')
    (40, '    """')
    (41, '    folders = map_samples_folder[wc.sample].copy()')
    (42, "    folders.append([\\'/fast5_pass/\\'.join(x) for x in map_samples_barcode[wc.sample]])")
    (43, '    if config[\\\'verbose\\\']: print("Input Folders:" + str(folders[0]))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=imgag/ont_tools, file=workflow/rules/helper_scripts.smk
context_key: ["if config[\\'use_failed_reads\\']"]
    (34, 'def get_input_folders_fast5(wc):')
    (35, '    """')
    (36, '    Return the FAST5 data folder for a sample.')
    (37, '        Allows multiple runs per sample (restarted, repeated)')
    (38, '        Demultiplexed samples on a single flowcell with barcode information')
    (39, '        Includes folder with failed reads when specified in config')
    (40, '    """')
    (41, '    folders = map_samples_folder[wc.sample].copy()')
    (42, "    folders.append([\\'/fast5_pass/\\'.join(x) for x in map_samples_barcode[wc.sample]])")
    (43, '    if config[\\\'verbose\\\']: print("Input Folders:" + str(folders[0]))')
    (44, "    if config[\\'use_failed_reads\\']:")
    (45, "       folders.append([\\'/fast5_fail/\\'.join(x) for x in map_samples_barcode[wc.sample]])")
    (46, '    folders_exist = [x for x in folders[0] if os.path.exists(x)]')
    (47, "    return{\\'folders\\': folders[0]}")
    (48, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=uaauaguga/cfRNA-pipe, file=Snakefile
context_key: ["if config[\\'quality_control\\']"]
    (29, 'def get_output(config):')
    (30, '    # get output requested in configure file')
    (31, '    outputs = {}')
    (32, "    if config[\\'quality_control\\']:")
    (33, '        outputs["qc-raw"] = expand(config[\\\'output_dir\\\'] + "/qc-raw/{sample_id}_{mate}_fastqc.html",')
    (34, '                                   sample_id = sample_ids,mate=["1","2"])')
    (35, '')
    (36, "    if config[\\'trim_adapter\\']:")
    (37, '        outputs["trimmed"] = expand(config[\\\'output_dir\\\'] + "/trimmed/{sample_id}_{mate}.fastq.gz", ')
    (38, '                                   sample_id = sample_ids,mate=["1","2"])')
    (39, "        if config[\\'quality_control\\']:")
    (40, '            outputs["qc-trimmed"] = expand(config[\\\'output_dir\\\'] + "/qc-trimmed/{sample_id}_{mate}_fastqc.html",')
    (41, '                                   sample_id = sample_ids,mate=["1","2"])')
    (42, '')
    (43, "    if config[\\'remove_unwanted_sequences\\']:")
    (44, '        outputs["cleaned"] = expand(config[\\\'output_dir\\\'] + "/cleaned/{sample_id}_{mate}.fastq.gz", ')
    (45, '                                   sample_id = sample_ids,mate=["1","2"])')
    (46, "        if config[\\'quality_control\\']:")
    (47, '            outputs["qc-cleaned"] = expand(config[\\\'output_dir\\\'] + "/qc-cleaned/{sample_id}_{mate}_fastqc.html",')
    (48, '                                   sample_id = sample_ids,mate=["1","2"])')
    (49, '')
    (50, "    if config[\\'genome_mapping\\']:")
    (51, '        outputs["genome"] = expand(config[\\\'output_dir\\\'] + "/bam/{sample_id}/genome.bam", sample_id = sample_ids)')
    (52, '')
    (53, "    if config[\\'circrna_mapping\\']:")
    (54, '        outputs["circrna"] = expand(config[\\\'output_dir\\\'] + "/bam/{sample_id}/circrna.bam", sample_id = sample_ids)')
    (55, '')
    (56, "    if config[\\'metagenomic_classification\\']:")
    (57, '        outputs["microbe-counts"] = expand(config[\\\'output_dir\\\'] + "/microbe/report/{sample_id}.txt", sample_id = sample_ids)')
    (58, "    if config[\\'count_gene\\']:")
    (59, '        outputs["gene-counts"] = config[\\\'output_dir\\\'] + \\\'/counts/matrix/gene.txt\\\'')
    (60, "    if config[\\'count_circrna\\']:")
    (61, '        outputs["circrna-counts"] = config[\\\'output_dir\\\'] + \\\'/counts/matrix/circrna.txt\\\'')
    (62, '    if config["count_editing"]:')
    (63, '        outputs["editing-edited"] = config[\\\'output_dir\\\'] + \\\'/editing/matrix/editing-coverage.txt\\\',')
    (64, '        outputs["editing-ref"] =  config[\\\'output_dir\\\'] + \\\'/editing/matrix/reference-coverage.txt\\\'')
    (65, "    if config[\\'splicing\\']:")
    (66, '        outputs[\\\'splicing\\\'] = expand(config[\\\'output_dir\\\'] + \\\'/splicing/events/{event}.MATS.JC.txt\\\', event = ["A3SS","A5SS","MXE","RI","SE"])')
    (67, '    if config["APA"]:')
    (68, '        outputs["PDUI"] = config["output_dir"] + \\\'/APA/PDUI.txt\\\'')
    (69, '    return list(outputs.values())')
    (70, '        ')
    (71, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=lculibrk/Ploidetect-pipeline, file=Snakefile
context_key: ['if config["ploidetect_local_clone"] and config["ploidetect_local_clone"] != "None"']
    (134, 'def devtools_install():')
    (135, '    if config["ploidetect_local_clone"] and config["ploidetect_local_clone"] != "None":')
    (136, '        install_path = config["ploidetect_local_clone"].format(**config)')
    (137, '        devtools_cmd = f"devtools::install_local(\\\'{install_path}\\\', force = TRUE)"')
    (138, '    else:')
    (139, '        ver = config["ploidetect_ver"]')
    (140, '        devtools_cmd = f"devtools::install_github(\\\'lculibrk/Ploidetect\\\', ref = \\\'{ver}\\\')"')
    (141, '    return f\\\'"{devtools_cmd}"\\\'')
    (142, '')
    (143, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=SycuroLab/metqc, file=Snakefile
context_key: ['if config["qc_only"]']
    (23, 'def all_input_reads(qc):')
    (24, '    if config["qc_only"]:')
    (25, '        return expand(os.path.join(config["input_dir"],"{sample}"+config["forward_read_suffix"]), sample=SAMPLES)')
    (26, '    else:')
    (27, '        if config["run_bbmap"]:')
    (28, '            return expand(os.path.join(config["output_dir"],"bbmap","{sample}_bbmapped_1.fastq"), sample=SAMPLES)')
    (29, '        else:')
    (30, '            if config["run_bmtagger"]:')
    (31, '                return expand(os.path.join(config["output_dir"],"bmtagger","{sample}_bmtagged_1.fastq"), sample=SAMPLES)')
    (32, '            else:')
    (33, '                return expand(os.path.join(config["output_dir"],"prinseq","{sample}_filtered_1.fastq"), sample=SAMPLES)')
    (34, '')
    (35, '# **** Rules ****')
    (36, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=NCI-CGR/Nanopore_RNA-seq, file=Snakefile
context_key: ['if config["hpv"]=="yes"', 'rule all', 'input']
    (19, 'def parse_sampleID(fname):')
    (20, "    return fname.split(raw)[-1].split(\\'_fast5\\')[0]")
    (21, '')
    (22, "dire = sorted(glob.glob(raw + \\'*_fast5\\'), key=parse_sampleID)")
    (23, '')
    (24, 'd = {}')
    (25, 'for key, value in itertools.groupby(dire, parse_sampleID):')
    (26, '    d[key] = list(value)')
    (27, '')
    (28, 'samples = d.keys()')
    (29, '')
    (30, 'if config["hpv"]=="yes":')
    (31, '   include: "modules/Snakefile_map_hpv"')
    (32, '   include: "modules/Snakefile_stringtie_hpv"')
    (33, '   include: "modules/Snakefile_freddie_hpv"')
    (34, '   include: "modules/Snakefile_map"')
    (35, '   include: "modules/Snakefile_stringtie"')
    (36, '   include: "modules/Snakefile_freddie"')
    (37, '   rule all:')
    (38, '        input:')
    (39, '              expand(out + "alignment/{sample}/qc",sample=samples),')
    (40, '              expand(out + "alignment_hpv/{sample}/qc",sample=samples),')
    (41, '              expand(out + "stringtie_hpv/{sample}/{sample}_gffcompare/complete.txt",sample=samples),')
    (42, '              expand(out + "stringtie_hpv/{sample}/{sample}_gffread/{sample}_gffread.fa",sample=samples),')
    (43, '              expand(out + "freddie_hpv/{sample}/{sample}_gffcompare/complete.txt",sample=samples),')
    (44, '              expand(out + "freddie_hpv/{sample}/{sample}_gffread/{sample}_gffread.fa",sample=samples),')
    (45, '              expand(out + "stringtie/{sample}/{sample}_gffcompare/complete.txt",sample=samples),')
    (46, '              expand(out + "stringtie/{sample}/{sample}_gffread/{sample}_gffread.fa",sample=samples),')
    (47, '              expand(out + "freddie/{sample}/{sample}_gffcompare/complete.txt",sample=samples),')
    (48, '              expand(out + "freddie/{sample}/{sample}_gffread/{sample}_gffread.fa",sample=samples)')
    (49, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ebi-gene-expression-group/wf-bulk-indexing, file=Snakefile-bioentities
context_key: ['if config[\\\'target\\\'] == "load"']
    (18, 'def get_outputs_bioentities(wildcards):')
    (19, '    print(f"Using target: {config[\\\'target\\\']}")')
    (20, '    if config[\\\'target\\\'] == "load":')
    (21, '        outputs = [f"{config[\\\'species\\\']}.index.loaded",')
    (22, '                "exp_designs_updates.done",')
    (23, '                "coexpression_updates.done",')
    (24, '                "load_bulk_analytics_index.done"]')
    (25, "        if \\'exp_update_sync_dest\\' in config:")
    (26, '            outputs.append("sync_exp_designs.done")')
    (27, '        return outputs')
    (28, '    elif config[\\\'target\\\'] == "load_bioentities_only":')
    (29, '        return [f"{config[\\\'species\\\']}.index.loaded"]')
    (30, '    elif config[\\\'target\\\'] == "load_analytics_only":')
    (31, '        return ["load_bulk_analytics_index.done"]')
    (32, '    elif config[\\\'target\\\'] == "jsonl":')
    (33, '        return get_jsonl_paths()')
    (34, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jhamman/chains, file=Snakefile
context_key: ["if config.get(\\'verbose\\')"]
    (48, 'def _expand(*args, **kwargs):')
    (49, "    if config.get(\\'verbose\\'):")
    (50, '        print(args)')
    (51, '        print(kwargs)')
    (52, '    return expand(*args, **kwargs)')
    (53, '')
    (54, '')
    (55, '# Workflow bookends')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tniranjan1/RRBS_PIPELINE, file=workflow/Snakefile
context_key: ["if config[\\'remove_samples\\'][\\'genotype_discordant\\'][\\'activate\\']", 'if not os.path.isfile(vcf_source)']
    (60, 'def distinguish_lrs_rrbs(middle_folder, return_value):')
    (61, "    samples2use = lrs_sample_names if middle_folder == \\'lrs_methyl\\' else rrbs_sample_names")
    (62, '    expansion = []')
    (63, '    for u in samples2use: expansion.append(f"{results_dir}/{middle_folder}/samples/" + u + ".agePrediction.txt")')
    (64, '    if return_value == "files":')
    (65, '        return expansion')
    (66, '    else:')
    (67, '        return samples2use')
    (68, '')
    (69, '# Install DMRFinder from github if not done already')
    (70, "DMRFinder_repo_path = abspath(\\'resources/DMRFinder\\')")
    (71, 'if not os.path.isdir(DMRFinder_repo_path):')
    (72, "    git.Repo.clone_from(config[\\'DMRfinder\\'][\\'DMRFinder_git_URL\\'], DMRFinder_repo_path)")
    (73, 'DMRFinder_repo = git.Repo(DMRFinder_repo_path)')
    (74, '# check that the repository loaded correctly')
    (75, 'if DMRFinder_repo.bare:')
    (76, "    DMRF_error = \\'Could not load repository at {}.\\'.format(DMRFinder_repo_path)")
    (77, "    DMRF_error = DMRF_error + \\' Recommend checking DMRFinder git url in ../config/config.yaml. Workflow aborted.\\'")
    (78, '    print(DMRF_error)')
    (79, '')
    (80, '# Which Cytosine contexts to Analyze')
    (81, 'context_truth = {')
    (82, '    "CpG" : True, # Default is to always analyze CpG contexts')
    (83, '    "CHG" : config[\\\'cytosine_sites\\\'][\\\'CHG\\\'],')
    (84, '    "CHH" : config[\\\'cytosine_sites\\\'][\\\'CHH\\\']')
    (85, '}')
    (86, 'context_to_use = [ key for key in list(context_truth.keys()) if context_truth[key] ]')
    (87, '')
    (88, '# Clean set of chromosomes to run on')
    (89, 'chromosomes = chromosome_constraint(reference_genome_path)')
    (90, '')
    (91, '# Verify vcf file')
    (92, 'vcf_file = f"{genotype_dir}/sample.original_genotypes.vcf.gz"')
    (93, "if config[\\'remove_samples\\'][\\'genotype_discordant\\'][\\'activate\\']:")
    (94, "    vcf_source = config[\\'remove_samples\\'][\\'genotype_discordant\\'][\\'vcf\\']")
    (95, '    if not os.path.isfile(vcf_source):')
    (96, '        print("VCF file ({}) could not be found. Please check path. Workflow aborted.".format(vcf_source))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dilworthlab/CnT_pipeline_snakemake, file=Snakefile
context_key: ["if config[\\'Species\\'] == \\'Mus musculus\\'"]
    (73, 'def findmd5(ROOT_DIR):')
    (74, '    try:')
    (75, '        for _file_ in os.listdir(READS_DIR):')
    (76, "            if fnmatch.fnmatch(_file_, \\'*md5*.txt\\'):")
    (77, '                PATH2MD5 = os.path.join(READS_DIR, _file_)')
    (78, "                logger.info(f\\'md5sum file: {PATH2MD5}\\')")
    (79, '                return PATH2MD5')
    (80, '')
    (81, '    except:')
    (82, "        logger.info(f\\'Can not locate md5sum.txt, make sure it is included in the raw-reads directory \\')")
    (83, '')
    (84, '')
    (85, '')
    (86, '# Describing wildcards')
    (87, "SINGLE_READ = f\\'{READS_DIR}/{{fastqfile}}_{{read}}.fastq.gz\\'")
    (88, '')
    (89, '')
    (90, 'READS = set(glob_wildcards(SINGLE_READ).read)')
    (91, 'FASTQFILES = set(glob_wildcards(SINGLE_READ).fastqfile)')
    (92, '')
    (93, '# Control vs non-Control files')
    (94, "IGGREADS = set( (samples.loc[samples[\\'condition\\'].str.contains(\\'IgG\\', case=False)].index).to_list() )")
    (95, "TARGETS = set( (samples.loc[~samples[\\'condition\\'].str.contains(\\'IgG\\', case=False)].index).to_list() )")
    (96, '')
    (97, '')
    (98, "logger.info(f\\'Sample file format: {SINGLE_READ}\\')")
    (99, "logger.info(f\\'These are the sample names: {FASTQFILES}\\')")
    (100, "logger.info(f\\'Each sample has {READS}\\')")
    (101, "logger.info(f\\'IgG control: {IGGREADS}\\')")
    (102, "logger.info(f\\'Target files: {TARGETS}\\')")
    (103, '')
    (104, '')
    (105, "EGS_GRCh38 = {\\'50\\': \\'2308125349\\', \\'75\\': \\'2747877777\\', \\'100\\': \\'2805636331\\', \\'150\\': \\'2862010578\\', \\'200\\': \\'2887553303\\'}")
    (106, "EGS_GRCm38 = {\\'50\\': \\'2308125349\\', \\'75\\': \\'2407883318\\', \\'100\\': \\'2467481108\\', \\'150\\': \\'2494787188\\', \\'200\\': \\'2520869189\\'}")
    (107, '')
    (108, '')
    (109, '# Species')
    (110, '')
    (111, "READLENGHT = config[\\'read_lenght\\']")
    (112, '')
    (113, "if config[\\'Species\\'] == \\'Mus musculus\\':")
    (114, '    EFFECTIVEGENOMESIZE = EGS_GRCm38[READLENGHT]')
    (115, '')
    (116, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dilworthlab/CnT_pipeline_snakemake, file=Snakefile
context_key: ["if config[\\'Species\\'] == \\'Homo sapiens\\'"]
    (73, 'def findmd5(ROOT_DIR):')
    (74, '    try:')
    (75, '        for _file_ in os.listdir(READS_DIR):')
    (76, "            if fnmatch.fnmatch(_file_, \\'*md5*.txt\\'):")
    (77, '                PATH2MD5 = os.path.join(READS_DIR, _file_)')
    (78, "                logger.info(f\\'md5sum file: {PATH2MD5}\\')")
    (79, '                return PATH2MD5')
    (80, '')
    (81, '    except:')
    (82, "        logger.info(f\\'Can not locate md5sum.txt, make sure it is included in the raw-reads directory \\')")
    (83, '')
    (84, '')
    (85, '')
    (86, '# Describing wildcards')
    (87, "SINGLE_READ = f\\'{READS_DIR}/{{fastqfile}}_{{read}}.fastq.gz\\'")
    (88, '')
    (89, '')
    (90, 'READS = set(glob_wildcards(SINGLE_READ).read)')
    (91, 'FASTQFILES = set(glob_wildcards(SINGLE_READ).fastqfile)')
    (92, '')
    (93, '# Control vs non-Control files')
    (94, "IGGREADS = set( (samples.loc[samples[\\'condition\\'].str.contains(\\'IgG\\', case=False)].index).to_list() )")
    (95, "TARGETS = set( (samples.loc[~samples[\\'condition\\'].str.contains(\\'IgG\\', case=False)].index).to_list() )")
    (96, '')
    (97, '')
    (98, "logger.info(f\\'Sample file format: {SINGLE_READ}\\')")
    (99, "logger.info(f\\'These are the sample names: {FASTQFILES}\\')")
    (100, "logger.info(f\\'Each sample has {READS}\\')")
    (101, "logger.info(f\\'IgG control: {IGGREADS}\\')")
    (102, "logger.info(f\\'Target files: {TARGETS}\\')")
    (103, '')
    (104, '')
    (105, "EGS_GRCh38 = {\\'50\\': \\'2308125349\\', \\'75\\': \\'2747877777\\', \\'100\\': \\'2805636331\\', \\'150\\': \\'2862010578\\', \\'200\\': \\'2887553303\\'}")
    (106, "EGS_GRCm38 = {\\'50\\': \\'2308125349\\', \\'75\\': \\'2407883318\\', \\'100\\': \\'2467481108\\', \\'150\\': \\'2494787188\\', \\'200\\': \\'2520869189\\'}")
    (107, '')
    (108, '')
    (109, '# Species')
    (110, '')
    (111, "READLENGHT = config[\\'read_lenght\\']")
    (112, '')
    (113, "if config[\\'Species\\'] == \\'Mus musculus\\':")
    (114, '    EFFECTIVEGENOMESIZE = EGS_GRCm38[READLENGHT]')
    (115, '')
    (116, '')
    (117, "if config[\\'Species\\'] == \\'Homo sapiens\\':")
    (118, '    EFFECTIVEGENOMESIZE = EGS_GRCh38[READLENGHT]')
    (119, '')
    (120, '')
    (121, '# Spike')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dilworthlab/CnT_pipeline_snakemake, file=Snakefile
context_key: ["if config[\\'Spikein\\'] == \\'Amp\\'"]
    (73, 'def findmd5(ROOT_DIR):')
    (74, '    try:')
    (75, '        for _file_ in os.listdir(READS_DIR):')
    (76, "            if fnmatch.fnmatch(_file_, \\'*md5*.txt\\'):")
    (77, '                PATH2MD5 = os.path.join(READS_DIR, _file_)')
    (78, "                logger.info(f\\'md5sum file: {PATH2MD5}\\')")
    (79, '                return PATH2MD5')
    (80, '')
    (81, '    except:')
    (82, "        logger.info(f\\'Can not locate md5sum.txt, make sure it is included in the raw-reads directory \\')")
    (83, '')
    (84, '')
    (85, '')
    (86, '# Describing wildcards')
    (87, "SINGLE_READ = f\\'{READS_DIR}/{{fastqfile}}_{{read}}.fastq.gz\\'")
    (88, '')
    (89, '')
    (90, 'READS = set(glob_wildcards(SINGLE_READ).read)')
    (91, 'FASTQFILES = set(glob_wildcards(SINGLE_READ).fastqfile)')
    (92, '')
    (93, '# Control vs non-Control files')
    (94, "IGGREADS = set( (samples.loc[samples[\\'condition\\'].str.contains(\\'IgG\\', case=False)].index).to_list() )")
    (95, "TARGETS = set( (samples.loc[~samples[\\'condition\\'].str.contains(\\'IgG\\', case=False)].index).to_list() )")
    (96, '')
    (97, '')
    (98, "logger.info(f\\'Sample file format: {SINGLE_READ}\\')")
    (99, "logger.info(f\\'These are the sample names: {FASTQFILES}\\')")
    (100, "logger.info(f\\'Each sample has {READS}\\')")
    (101, "logger.info(f\\'IgG control: {IGGREADS}\\')")
    (102, "logger.info(f\\'Target files: {TARGETS}\\')")
    (103, '')
    (104, '')
    (105, "EGS_GRCh38 = {\\'50\\': \\'2308125349\\', \\'75\\': \\'2747877777\\', \\'100\\': \\'2805636331\\', \\'150\\': \\'2862010578\\', \\'200\\': \\'2887553303\\'}")
    (106, "EGS_GRCm38 = {\\'50\\': \\'2308125349\\', \\'75\\': \\'2407883318\\', \\'100\\': \\'2467481108\\', \\'150\\': \\'2494787188\\', \\'200\\': \\'2520869189\\'}")
    (107, '')
    (108, '')
    (109, '# Species')
    (110, '')
    (111, "READLENGHT = config[\\'read_lenght\\']")
    (112, '')
    (113, "if config[\\'Species\\'] == \\'Mus musculus\\':")
    (114, '    EFFECTIVEGENOMESIZE = EGS_GRCm38[READLENGHT]')
    (115, '')
    (116, '')
    (117, "if config[\\'Species\\'] == \\'Homo sapiens\\':")
    (118, '    EFFECTIVEGENOMESIZE = EGS_GRCh38[READLENGHT]')
    (119, '')
    (120, '')
    (121, '# Spike')
    (122, "if config[\\'Spikein\\'] == \\'Amp\\':")
    (123, "    SPIKEINDEX = config[\\'Spikein_index_amp\\']")
    (124, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dilworthlab/CnT_pipeline_snakemake, file=Snakefile
context_key: ["if config[\\'Spikein\\'] == \\'Bacteria\\'"]
    (73, 'def findmd5(ROOT_DIR):')
    (74, '    try:')
    (75, '        for _file_ in os.listdir(READS_DIR):')
    (76, "            if fnmatch.fnmatch(_file_, \\'*md5*.txt\\'):")
    (77, '                PATH2MD5 = os.path.join(READS_DIR, _file_)')
    (78, "                logger.info(f\\'md5sum file: {PATH2MD5}\\')")
    (79, '                return PATH2MD5')
    (80, '')
    (81, '    except:')
    (82, "        logger.info(f\\'Can not locate md5sum.txt, make sure it is included in the raw-reads directory \\')")
    (83, '')
    (84, '')
    (85, '')
    (86, '# Describing wildcards')
    (87, "SINGLE_READ = f\\'{READS_DIR}/{{fastqfile}}_{{read}}.fastq.gz\\'")
    (88, '')
    (89, '')
    (90, 'READS = set(glob_wildcards(SINGLE_READ).read)')
    (91, 'FASTQFILES = set(glob_wildcards(SINGLE_READ).fastqfile)')
    (92, '')
    (93, '# Control vs non-Control files')
    (94, "IGGREADS = set( (samples.loc[samples[\\'condition\\'].str.contains(\\'IgG\\', case=False)].index).to_list() )")
    (95, "TARGETS = set( (samples.loc[~samples[\\'condition\\'].str.contains(\\'IgG\\', case=False)].index).to_list() )")
    (96, '')
    (97, '')
    (98, "logger.info(f\\'Sample file format: {SINGLE_READ}\\')")
    (99, "logger.info(f\\'These are the sample names: {FASTQFILES}\\')")
    (100, "logger.info(f\\'Each sample has {READS}\\')")
    (101, "logger.info(f\\'IgG control: {IGGREADS}\\')")
    (102, "logger.info(f\\'Target files: {TARGETS}\\')")
    (103, '')
    (104, '')
    (105, "EGS_GRCh38 = {\\'50\\': \\'2308125349\\', \\'75\\': \\'2747877777\\', \\'100\\': \\'2805636331\\', \\'150\\': \\'2862010578\\', \\'200\\': \\'2887553303\\'}")
    (106, "EGS_GRCm38 = {\\'50\\': \\'2308125349\\', \\'75\\': \\'2407883318\\', \\'100\\': \\'2467481108\\', \\'150\\': \\'2494787188\\', \\'200\\': \\'2520869189\\'}")
    (107, '')
    (108, '')
    (109, '# Species')
    (110, '')
    (111, "READLENGHT = config[\\'read_lenght\\']")
    (112, '')
    (113, "if config[\\'Species\\'] == \\'Mus musculus\\':")
    (114, '    EFFECTIVEGENOMESIZE = EGS_GRCm38[READLENGHT]')
    (115, '')
    (116, '')
    (117, "if config[\\'Species\\'] == \\'Homo sapiens\\':")
    (118, '    EFFECTIVEGENOMESIZE = EGS_GRCh38[READLENGHT]')
    (119, '')
    (120, '')
    (121, '# Spike')
    (122, "if config[\\'Spikein\\'] == \\'Amp\\':")
    (123, "    SPIKEINDEX = config[\\'Spikein_index_amp\\']")
    (124, '')
    (125, "if config[\\'Spikein\\'] == \\'Bacteria\\':")
    (126, "    SPIKEINDEX = config[\\'Spikein_index_Ecoli\\']")
    (127, '')
    (128, '')
    (129, '#localrules: Clean_up')
    (130, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=vari-bbc/Peaks_workflow, file=Snakefile
context_key: ["if config[\\'run_hmmratac\\']"]
    (1294, 'def get_merged_beds (wildcards):')
    (1295, "    atac_peak_types = [\\'macs2_ENCODE_atac_broad\\',\\'macs2_ENCODE_atac_narrow\\',\\'macs2_nfr_broad\\',\\'macs2_nfr_narrow\\',\\'macs2_narrow\\',\\'macs2_broad\\']")
    (1296, "    if config[\\'run_hmmratac\\']:")
    (1297, "        atac_peak_types = atac_peak_types + [\\'hmmratac_nfr\\']# if config[\\'run_hmmratac\\']")
    (1298, '    merged_beds = expand("analysis/merge_all_peaks/all_merged_{peak_type}.bed", peak_type=atac_peak_types if config[\\\'atacseq\\\'] else [\\\'macs2_narrow\\\',\\\'macs2_broad\\\'])')
    (1299, '    return (merged_beds)')
    (1300, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=genomewalker/aMGSIM-smk, file=Snakefile
context_key: ['if config["seq_library"][0] == "single"']
    (5, 'def get_md5sum(x):')
    (6, '    import hashlib')
    (7, '')
    (8, '    return hashlib.md5(x.encode("utf-8")).hexdigest()[:10]')
    (9, '')
    (10, '')
    (11, '"""')
    (12, 'Author: A. Fernandez-Guerra')
    (13, 'Affiliation: Lundbeck Foundation GeoGenetics Centre')
    (14, 'Aim: Create synthetic data to test metaDMG')
    (15, 'Run: snakemake   -s Snakefile')
    (16, '"""')
    (17, '#')
    (18, '##### set minimum snakemake version #####')
    (19, 'min_version("5.20.1")')
    (20, '')
    (21, '# configfile: "config/config.yaml"')
    (22, '# report: "report/workflow.rst"')
    (23, '')
    (24, '# This should be placed in the Snakefile.')
    (25, '')
    (26, '"""')
    (27, 'Working directory')
    (28, '"""')
    (29, '')
    (30, '')
    (31, 'workdir: config["wdir"]')
    (32, '')
    (33, '')
    (34, '# message("The current working directory is " + WDIR)')
    (35, '')
    (36, '"""')
    (37, ' The list of samples to be processed')
    (38, '"""')
    (39, '')
    (40, 'sample_table_read = pd.read_table(')
    (41, '    config["sample_file_read"], sep="\\\\t", lineterminator="\\')
    (42, '"')
    (43, ')')
    (44, '')
    (45, "# let\\'s check that the basic columns are present")
    (46, 'if not all(')
    (47, '    item in sample_table_read.columns')
    (48, '    for item in [')
    (49, '        "label",')
    (50, '        "libprep",')
    (51, '        "read_length_freqs_file",')
    (52, '        "mapping_stats_filtered_file",')
    (53, '        "metadmg_results",')
    (54, '        "metadmg_misincorporations",')
    (55, '    ]')
    (56, '):')
    (57, '    raise ValueError("The sample table must contain the columns \\\'label\\\' and \\\'file\\\'")')
    (58, '    exit(1)')
    (59, '')
    (60, 'if not "libprep" in sample_table_read.columns:')
    (61, '    sample_table_read["libprep"] = "double"')
    (62, '')
    (63, 'sample_table_read = sample_table_read.drop_duplicates(')
    (64, '    subset="label", keep="first", inplace=False')
    (65, ')')
    (66, 'sample_table_read = sample_table_read.dropna()')
    (67, '')
    (68, 'if not "short_label" in sample_table_read.columns:')
    (69, '    sample_table_read["short_label"] = sample_table_read.apply(')
    (70, '        lambda row: get_md5sum(row.label), axis=1')
    (71, '    )')
    (72, '')
    (73, 'sample_table_read.set_index("short_label", inplace=True)')
    (74, 'sample_label_dict_read = sample_table_read.to_dict()["label"]')
    (75, 'sample_label_read = sample_table_read.index.values')
    (76, '')
    (77, '')
    (78, 'done_art_sr = []')
    (79, 'done_art_p1 = []')
    (80, 'done_art_p2 = []')
    (81, 'done_fragsim = []')
    (82, 'done_deamsim = []')
    (83, 'if config["seq_library"][0] == "single":')
    (84, '    done_art_sr = (')
    (85, '        expand(')
    (86, '            f\\\'{config["rdir"]}/{{smp}}/{{seqlib}}/{{num_reads}}/reads/{{smp}}_art.fq.gz\\\',')
    (87, '            smp=sample_label_read,')
    (88, '            seqlib=config["seq_library"],')
    (89, '            num_reads=[str(int(float(i))) for i in config["num_reads"]],')
    (90, '        ),')
    (91, '    )')
    (92, '    done_fragsim = (')
    (93, '        expand(')
    (94, '            f\\\'{config["rdir"]}/{{smp}}/{{seqlib}}/{{num_reads}}/reads/{{smp}}_fragSim.fa.gz\\\',')
    (95, '            smp=sample_label_read,')
    (96, '            seqlib=config["seq_library"],')
    (97, '            num_reads=[str(int(float(i))) for i in config["num_reads"]],')
    (98, '        ),')
    (99, '    )')
    (100, '    done_deamsim = (')
    (101, '        expand(')
    (102, '            f\\\'{config["rdir"]}/{{smp}}/{{seqlib}}/{{num_reads}}/reads/{{smp}}_deamSim.fa.gz\\\',')
    (103, '            smp=sample_label_read,')
    (104, '            seqlib=config["seq_library"],')
    (105, '            num_reads=[str(int(float(i))) for i in config["num_reads"]],')
    (106, '        ),')
    (107, '    )')
    (108, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=genomewalker/aMGSIM-smk, file=Snakefile
context_key: ['if config["seq_library"][0] == "double"']
    (5, 'def get_md5sum(x):')
    (6, '    import hashlib')
    (7, '')
    (8, '    return hashlib.md5(x.encode("utf-8")).hexdigest()[:10]')
    (9, '')
    (10, '')
    (11, '"""')
    (12, 'Author: A. Fernandez-Guerra')
    (13, 'Affiliation: Lundbeck Foundation GeoGenetics Centre')
    (14, 'Aim: Create synthetic data to test metaDMG')
    (15, 'Run: snakemake   -s Snakefile')
    (16, '"""')
    (17, '#')
    (18, '##### set minimum snakemake version #####')
    (19, 'min_version("5.20.1")')
    (20, '')
    (21, '# configfile: "config/config.yaml"')
    (22, '# report: "report/workflow.rst"')
    (23, '')
    (24, '# This should be placed in the Snakefile.')
    (25, '')
    (26, '"""')
    (27, 'Working directory')
    (28, '"""')
    (29, '')
    (30, '')
    (31, 'workdir: config["wdir"]')
    (32, '')
    (33, '')
    (34, '# message("The current working directory is " + WDIR)')
    (35, '')
    (36, '"""')
    (37, ' The list of samples to be processed')
    (38, '"""')
    (39, '')
    (40, 'sample_table_read = pd.read_table(')
    (41, '    config["sample_file_read"], sep="\\\\t", lineterminator="\\')
    (42, '"')
    (43, ')')
    (44, '')
    (45, "# let\\'s check that the basic columns are present")
    (46, 'if not all(')
    (47, '    item in sample_table_read.columns')
    (48, '    for item in [')
    (49, '        "label",')
    (50, '        "libprep",')
    (51, '        "read_length_freqs_file",')
    (52, '        "mapping_stats_filtered_file",')
    (53, '        "metadmg_results",')
    (54, '        "metadmg_misincorporations",')
    (55, '    ]')
    (56, '):')
    (57, '    raise ValueError("The sample table must contain the columns \\\'label\\\' and \\\'file\\\'")')
    (58, '    exit(1)')
    (59, '')
    (60, 'if not "libprep" in sample_table_read.columns:')
    (61, '    sample_table_read["libprep"] = "double"')
    (62, '')
    (63, 'sample_table_read = sample_table_read.drop_duplicates(')
    (64, '    subset="label", keep="first", inplace=False')
    (65, ')')
    (66, 'sample_table_read = sample_table_read.dropna()')
    (67, '')
    (68, 'if not "short_label" in sample_table_read.columns:')
    (69, '    sample_table_read["short_label"] = sample_table_read.apply(')
    (70, '        lambda row: get_md5sum(row.label), axis=1')
    (71, '    )')
    (72, '')
    (73, 'sample_table_read.set_index("short_label", inplace=True)')
    (74, 'sample_label_dict_read = sample_table_read.to_dict()["label"]')
    (75, 'sample_label_read = sample_table_read.index.values')
    (76, '')
    (77, '')
    (78, 'done_art_sr = []')
    (79, 'done_art_p1 = []')
    (80, 'done_art_p2 = []')
    (81, 'done_fragsim = []')
    (82, 'done_deamsim = []')
    (83, 'if config["seq_library"][0] == "single":')
    (84, '    done_art_sr = (')
    (85, '        expand(')
    (86, '            f\\\'{config["rdir"]}/{{smp}}/{{seqlib}}/{{num_reads}}/reads/{{smp}}_art.fq.gz\\\',')
    (87, '            smp=sample_label_read,')
    (88, '            seqlib=config["seq_library"],')
    (89, '            num_reads=[str(int(float(i))) for i in config["num_reads"]],')
    (90, '        ),')
    (91, '    )')
    (92, '    done_fragsim = (')
    (93, '        expand(')
    (94, '            f\\\'{config["rdir"]}/{{smp}}/{{seqlib}}/{{num_reads}}/reads/{{smp}}_fragSim.fa.gz\\\',')
    (95, '            smp=sample_label_read,')
    (96, '            seqlib=config["seq_library"],')
    (97, '            num_reads=[str(int(float(i))) for i in config["num_reads"]],')
    (98, '        ),')
    (99, '    )')
    (100, '    done_deamsim = (')
    (101, '        expand(')
    (102, '            f\\\'{config["rdir"]}/{{smp}}/{{seqlib}}/{{num_reads}}/reads/{{smp}}_deamSim.fa.gz\\\',')
    (103, '            smp=sample_label_read,')
    (104, '            seqlib=config["seq_library"],')
    (105, '            num_reads=[str(int(float(i))) for i in config["num_reads"]],')
    (106, '        ),')
    (107, '    )')
    (108, '')
    (109, 'if config["seq_library"][0] == "double":')
    (110, '    done_art_p1 = (')
    (111, '        (')
    (112, '            expand(')
    (113, '                f\\\'{config["rdir"]}/{{smp}}/{{seqlib}}/{{num_reads}}/reads/{{smp}}_art.1.fq.gz\\\',')
    (114, '                smp=sample_label_read,')
    (115, '                seqlib=config["seq_library"],')
    (116, '                num_reads=[str(int(float(i))) for i in config["num_reads"]],')
    (117, '            ),')
    (118, '        ),')
    (119, '    )')
    (120, '    done_art_p2 = (')
    (121, '        (')
    (122, '            expand(')
    (123, '                f\\\'{config["rdir"]}/{{smp}}/{{seqlib}}/{{num_reads}}/reads/{{smp}}_art.2.fq.gz\\\',')
    (124, '                smp=sample_label_read,')
    (125, '                seqlib=config["seq_library"],')
    (126, '                num_reads=[str(int(float(i))) for i in config["num_reads"]],')
    (127, '            ),')
    (128, '        ),')
    (129, '    )')
    (130, '    done_fragsim = (')
    (131, '        expand(')
    (132, '            f\\\'{config["rdir"]}/{{smp}}/{{seqlib}}/{{num_reads}}/reads/{{smp}}_fragSim.fa.gz\\\',')
    (133, '            smp=sample_label_read,')
    (134, '            seqlib=config["seq_library"],')
    (135, '            num_reads=[str(int(float(i))) for i in config["num_reads"]],')
    (136, '        ),')
    (137, '    )')
    (138, '    done_deamsim = (')
    (139, '        expand(')
    (140, '            f\\\'{config["rdir"]}/{{smp}}/{{seqlib}}/{{num_reads}}/reads/{{smp}}_deamSim.fa.gz\\\',')
    (141, '            smp=sample_label_read,')
    (142, '            seqlib=config["seq_library"],')
    (143, '            num_reads=[str(int(float(i))) for i in config["num_reads"]],')
    (144, '        ),')
    (145, '    )')
    (146, '')
    (147, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\'']
    (22, 'def get_genmod_score_input(command):')
    (23, '    """Return a string which defines the input file for the genmod_score rule. This changes based on the')
    (24, '    user configurable options for running single samples or cohorts of samples')
    (25, '    """')
    (26, '    ')
    (27, '    input_files = ""')
    (28, '')
    (29, '    if config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\':')
    (30, '        input_files = "../results/readyforscout/{sample}_filtered_annotated_multiallelicsites.vcf.gz"')
    (31, '    if config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\':')
    (32, '        input_files = "../results/readyforscout/{sample}_filtered_annotated_multiallelicsites_probandonly.vcf"')
    (33, '')
    (34, '    return input_files')
    (35, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ESR-NZ/vcf_annotation_pipeline, file=workflow/Snakefile
context_key: ['if config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\'']
    (36, 'def get_genmod_score_params(command):')
    (37, '    """Return a string which defines some of the parameters for the genmod_score rule. This changes based on the')
    (38, '    user configurable options for running single samples or cohorts of samples')
    (39, '    """')
    (40, '    ')
    (41, '    params = ""')
    (42, '')
    (43, '    if config[\\\'DATA\\\'] == "Single" or config[\\\'DATA\\\'] == \\\'single\\\':')
    (44, '        params = "--score_config scripts/score_single.ini"')
    (45, '    if config[\\\'DATA\\\'] == "Cohort" or config[\\\'DATA\\\'] == \\\'cohort\\\':')
    (46, '        params = "--score_config scripts/score_cohort.ini --family_file ../../pedigrees/{sample}_pedigree.ped"')
    (47, '')
    (48, '    return params')
    (49, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PyPSA/pypsa-eur-sec, file=Snakefile
context_key: ['if config["foresight"] == "myopic"']
    (632, '    def solved_previous_horizon(wildcards):')
    (633, '        planning_horizons = config["scenario"]["planning_horizons"]')
    (634, '        i = planning_horizons.index(int(wildcards.planning_horizons))')
    (635, '        planning_horizon_p = str(planning_horizons[i-1])')
    (636, '        return RDIR + "/postnetworks/elec_s{simpl}_{clusters}_lv{lv}_{opts}_{sector_opts}_" + planning_horizon_p + ".nc"')
    (637, '')
    (638, '')
    (639, '    rule add_brownfield:')
    (640, '        input:')
    (641, '            overrides="data/override_component_attrs",')
    (642, "            network=RDIR + \\'/prenetworks/elec_s{simpl}_{clusters}_lv{lv}_{opts}_{sector_opts}_{planning_horizons}.nc\\',")
    (643, '            network_p=solved_previous_horizon, #solved network at previous time step')
    (644, '            costs=CDIR + "costs_{planning_horizons}.csv",')
    (645, '            cop_soil_total="resources/cop_soil_total_elec_s{simpl}_{clusters}.nc",')
    (646, '            cop_air_total="resources/cop_air_total_elec_s{simpl}_{clusters}.nc"')
    (647, '        output: RDIR + "/prenetworks-brownfield/elec_s{simpl}_{clusters}_lv{lv}_{opts}_{sector_opts}_{planning_horizons}.nc"')
    (648, '        threads: 4')
    (649, '        resources: mem_mb=10000')
    (650, "        benchmark: RDIR + \\'/benchmarks/add_brownfield/elec_s{simpl}_{clusters}_lv{lv}_{opts}_{sector_opts}_{planning_horizons}\\'")
    (651, '        script: "scripts/add_brownfield.py"')
    (652, '')
    (653, '')
    (654, '    ruleorder: add_existing_baseyear > add_brownfield')
    (655, '')
    (656, '')
    (657, '    rule solve_network_myopic:')
    (658, '        input:')
    (659, '            overrides="data/override_component_attrs",')
    (660, '            network=RDIR + "/prenetworks-brownfield/elec_s{simpl}_{clusters}_lv{lv}_{opts}_{sector_opts}_{planning_horizons}.nc",')
    (661, '            costs=CDIR + "costs_{planning_horizons}.csv",')
    (662, "            config=SDIR + \\'/configs/config.yaml\\'")
    (663, '        output: RDIR + "/postnetworks/elec_s{simpl}_{clusters}_lv{lv}_{opts}_{sector_opts}_{planning_horizons}.nc"')
    (664, '        shadow: "shallow"')
    (665, '        log:')
    (666, '            solver=RDIR + "/logs/elec_s{simpl}_{clusters}_lv{lv}_{opts}_{sector_opts}_{planning_horizons}_solver.log",')
    (667, '            python=RDIR + "/logs/elec_s{simpl}_{clusters}_lv{lv}_{opts}_{sector_opts}_{planning_horizons}_python.log",')
    (668, '            memory=RDIR + "/logs/elec_s{simpl}_{clusters}_lv{lv}_{opts}_{sector_opts}_{planning_horizons}_memory.log"')
    (669, '        threads: 4')
    (670, "        resources: mem_mb=config[\\'solving\\'][\\'mem\\']")
    (671, '        benchmark: RDIR + "/benchmarks/solve_network/elec_s{simpl}_{clusters}_lv{lv}_{opts}_{sector_opts}_{planning_horizons}"')
    (672, '        script: "scripts/solve_network.py"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if config["eigenvector"]["do"]']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if config["saddle"]["do"]']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]', 'if shifts > 0']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]', 'if config["pileups"]["expected"]']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]', 'if len(pileup_norms) == 0']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
    (183, '    pileup_resolution_limits = config["pileups"]["resolution_limits"]')
    (184, '    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]')
    (185, '    pileup_resolutions = list(')
    (186, '        filter(')
    (187, '            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],')
    (188, '            resolutions,')
    (189, '        )')
    (190, '    )')
    (191, '    mindists = [')
    (192, '        int(pileups_mindist * 2**i)')
    (193, '        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))')
    (194, '    ]')
    (195, '    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]')
    (196, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if config["insulation"]["do"]']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
    (183, '    pileup_resolution_limits = config["pileups"]["resolution_limits"]')
    (184, '    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]')
    (185, '    pileup_resolutions = list(')
    (186, '        filter(')
    (187, '            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],')
    (188, '            resolutions,')
    (189, '        )')
    (190, '    )')
    (191, '    mindists = [')
    (192, '        int(pileups_mindist * 2**i)')
    (193, '        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))')
    (194, '    ]')
    (195, '    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]')
    (196, '')
    (197, 'if config["insulation"]["do"]:')
    (198, '    insul_res_win = []')
    (199, '    for resolution in config["insulation"]["resolutions"]:')
    (200, '        for win in config["insulation"]["resolutions"][resolution]:')
    (201, '            insul_res_win.append(f"{resolution}_{win}")')
    (202, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if config["call_TADs"]["do"]']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
    (183, '    pileup_resolution_limits = config["pileups"]["resolution_limits"]')
    (184, '    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]')
    (185, '    pileup_resolutions = list(')
    (186, '        filter(')
    (187, '            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],')
    (188, '            resolutions,')
    (189, '        )')
    (190, '    )')
    (191, '    mindists = [')
    (192, '        int(pileups_mindist * 2**i)')
    (193, '        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))')
    (194, '    ]')
    (195, '    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]')
    (196, '')
    (197, 'if config["insulation"]["do"]:')
    (198, '    insul_res_win = []')
    (199, '    for resolution in config["insulation"]["resolutions"]:')
    (200, '        for win in config["insulation"]["resolutions"][resolution]:')
    (201, '            insul_res_win.append(f"{resolution}_{win}")')
    (202, '')
    (203, 'if config["call_TADs"]["do"]:')
    (204, '    tad_res_win = []')
    (205, '    for resolution in config["call_TADs"]["resolutions"]:')
    (206, '        for win in config["call_TADs"]["resolutions"][resolution]:')
    (207, '            tad_res_win.append(f"{resolution}_{win}")')
    (208, '')
    (209, "# chroms = cooler.Cooler(f\\'{coolers_folder}/{coolfiles[0]}::resolutions/{resolutions[0]}\\').chroms()[:]")
    (210, '')
    (211, '# bedpe_mindists = [int(bedpe_pileups_mindist*2**i) for i in np.arange(0, np.log2(bedpe_pileups_maxdist/bedpe_pileups_mindist))]')
    (212, "# bedpe_separations = [f\\'{mindist}-{mindist*2}\\' for mindist in bedpe_mindists]")
    (213, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if config["compare_boundaries"]["do"']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
    (183, '    pileup_resolution_limits = config["pileups"]["resolution_limits"]')
    (184, '    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]')
    (185, '    pileup_resolutions = list(')
    (186, '        filter(')
    (187, '            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],')
    (188, '            resolutions,')
    (189, '        )')
    (190, '    )')
    (191, '    mindists = [')
    (192, '        int(pileups_mindist * 2**i)')
    (193, '        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))')
    (194, '    ]')
    (195, '    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]')
    (196, '')
    (197, 'if config["insulation"]["do"]:')
    (198, '    insul_res_win = []')
    (199, '    for resolution in config["insulation"]["resolutions"]:')
    (200, '        for win in config["insulation"]["resolutions"][resolution]:')
    (201, '            insul_res_win.append(f"{resolution}_{win}")')
    (202, '')
    (203, 'if config["call_TADs"]["do"]:')
    (204, '    tad_res_win = []')
    (205, '    for resolution in config["call_TADs"]["resolutions"]:')
    (206, '        for win in config["call_TADs"]["resolutions"][resolution]:')
    (207, '            tad_res_win.append(f"{resolution}_{win}")')
    (208, '')
    (209, "# chroms = cooler.Cooler(f\\'{coolers_folder}/{coolfiles[0]}::resolutions/{resolutions[0]}\\').chroms()[:]")
    (210, '')
    (211, '# bedpe_mindists = [int(bedpe_pileups_mindist*2**i) for i in np.arange(0, np.log2(bedpe_pileups_maxdist/bedpe_pileups_mindist))]')
    (212, "# bedpe_separations = [f\\'{mindist}-{mindist*2}\\' for mindist in bedpe_mindists]")
    (213, '')
    (214, 'expecteds = expand(')
    (215, '    f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",')
    (216, '    sample=samples,')
    (217, '    resolution=resolutions,')
    (218, ')')
    (219, '')
    (220, 'diff_boundaries = (')
    (221, '    expand(')
    (222, '        f"{boundary_folder}/Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (223, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}.bed",')
    (224, '        insul_res_win=insul_res_win,')
    (225, '    )')
    (226, '    if config["compare_boundaries"]["do"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if config["compare_boundaries"]["do"] and config["pileups"]["do"']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
    (183, '    pileup_resolution_limits = config["pileups"]["resolution_limits"]')
    (184, '    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]')
    (185, '    pileup_resolutions = list(')
    (186, '        filter(')
    (187, '            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],')
    (188, '            resolutions,')
    (189, '        )')
    (190, '    )')
    (191, '    mindists = [')
    (192, '        int(pileups_mindist * 2**i)')
    (193, '        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))')
    (194, '    ]')
    (195, '    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]')
    (196, '')
    (197, 'if config["insulation"]["do"]:')
    (198, '    insul_res_win = []')
    (199, '    for resolution in config["insulation"]["resolutions"]:')
    (200, '        for win in config["insulation"]["resolutions"][resolution]:')
    (201, '            insul_res_win.append(f"{resolution}_{win}")')
    (202, '')
    (203, 'if config["call_TADs"]["do"]:')
    (204, '    tad_res_win = []')
    (205, '    for resolution in config["call_TADs"]["resolutions"]:')
    (206, '        for win in config["call_TADs"]["resolutions"][resolution]:')
    (207, '            tad_res_win.append(f"{resolution}_{win}")')
    (208, '')
    (209, "# chroms = cooler.Cooler(f\\'{coolers_folder}/{coolfiles[0]}::resolutions/{resolutions[0]}\\').chroms()[:]")
    (210, '')
    (211, '# bedpe_mindists = [int(bedpe_pileups_mindist*2**i) for i in np.arange(0, np.log2(bedpe_pileups_maxdist/bedpe_pileups_mindist))]')
    (212, "# bedpe_separations = [f\\'{mindist}-{mindist*2}\\' for mindist in bedpe_mindists]")
    (213, '')
    (214, 'expecteds = expand(')
    (215, '    f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",')
    (216, '    sample=samples,')
    (217, '    resolution=resolutions,')
    (218, ')')
    (219, '')
    (220, 'diff_boundaries = (')
    (221, '    expand(')
    (222, '        f"{boundary_folder}/Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (223, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}.bed",')
    (224, '        insul_res_win=insul_res_win,')
    (225, '    )')
    (226, '    if config["compare_boundaries"]["do"]')
    (227, '    else []')
    (228, ')')
    (229, 'diff_boundaries_pileups = (')
    (230, '    expand(')
    (231, '        f"{pileups_folder}/{boundary_folder_name}/{{sample}}-{{resolution}}_over_Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (232, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}_{{norm}}_local.clpy",')
    (233, '        sample=samples,')
    (234, '        resolution=pileup_resolutions,')
    (235, '        insul_res_win=insul_res_win,')
    (236, '        norm=pileup_norms,')
    (237, '    )')
    (238, '    if config["compare_boundaries"]["do"] and config["pileups"]["do"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if config["call_TADs"]["do"']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
    (183, '    pileup_resolution_limits = config["pileups"]["resolution_limits"]')
    (184, '    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]')
    (185, '    pileup_resolutions = list(')
    (186, '        filter(')
    (187, '            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],')
    (188, '            resolutions,')
    (189, '        )')
    (190, '    )')
    (191, '    mindists = [')
    (192, '        int(pileups_mindist * 2**i)')
    (193, '        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))')
    (194, '    ]')
    (195, '    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]')
    (196, '')
    (197, 'if config["insulation"]["do"]:')
    (198, '    insul_res_win = []')
    (199, '    for resolution in config["insulation"]["resolutions"]:')
    (200, '        for win in config["insulation"]["resolutions"][resolution]:')
    (201, '            insul_res_win.append(f"{resolution}_{win}")')
    (202, '')
    (203, 'if config["call_TADs"]["do"]:')
    (204, '    tad_res_win = []')
    (205, '    for resolution in config["call_TADs"]["resolutions"]:')
    (206, '        for win in config["call_TADs"]["resolutions"][resolution]:')
    (207, '            tad_res_win.append(f"{resolution}_{win}")')
    (208, '')
    (209, "# chroms = cooler.Cooler(f\\'{coolers_folder}/{coolfiles[0]}::resolutions/{resolutions[0]}\\').chroms()[:]")
    (210, '')
    (211, '# bedpe_mindists = [int(bedpe_pileups_mindist*2**i) for i in np.arange(0, np.log2(bedpe_pileups_maxdist/bedpe_pileups_mindist))]')
    (212, "# bedpe_separations = [f\\'{mindist}-{mindist*2}\\' for mindist in bedpe_mindists]")
    (213, '')
    (214, 'expecteds = expand(')
    (215, '    f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",')
    (216, '    sample=samples,')
    (217, '    resolution=resolutions,')
    (218, ')')
    (219, '')
    (220, 'diff_boundaries = (')
    (221, '    expand(')
    (222, '        f"{boundary_folder}/Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (223, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}.bed",')
    (224, '        insul_res_win=insul_res_win,')
    (225, '    )')
    (226, '    if config["compare_boundaries"]["do"]')
    (227, '    else []')
    (228, ')')
    (229, 'diff_boundaries_pileups = (')
    (230, '    expand(')
    (231, '        f"{pileups_folder}/{boundary_folder_name}/{{sample}}-{{resolution}}_over_Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (232, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}_{{norm}}_local.clpy",')
    (233, '        sample=samples,')
    (234, '        resolution=pileup_resolutions,')
    (235, '        insul_res_win=insul_res_win,')
    (236, '        norm=pileup_norms,')
    (237, '    )')
    (238, '    if config["compare_boundaries"]["do"] and config["pileups"]["do"]')
    (239, '    else []')
    (240, ')')
    (241, '')
    (242, 'tads = (')
    (243, '    expand(')
    (244, '        f"{tad_folder}/TADs_{{sampleTADs}}_{{tad_res_win}}.bed",')
    (245, '        sampleTADs=config["call_TADs"]["samples"],')
    (246, '        tad_res_win=tad_res_win,')
    (247, '    )')
    (248, '    if config["call_TADs"]["do"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if config["call_TADs"]["do"] and config["pileups"]["do"']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
    (183, '    pileup_resolution_limits = config["pileups"]["resolution_limits"]')
    (184, '    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]')
    (185, '    pileup_resolutions = list(')
    (186, '        filter(')
    (187, '            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],')
    (188, '            resolutions,')
    (189, '        )')
    (190, '    )')
    (191, '    mindists = [')
    (192, '        int(pileups_mindist * 2**i)')
    (193, '        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))')
    (194, '    ]')
    (195, '    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]')
    (196, '')
    (197, 'if config["insulation"]["do"]:')
    (198, '    insul_res_win = []')
    (199, '    for resolution in config["insulation"]["resolutions"]:')
    (200, '        for win in config["insulation"]["resolutions"][resolution]:')
    (201, '            insul_res_win.append(f"{resolution}_{win}")')
    (202, '')
    (203, 'if config["call_TADs"]["do"]:')
    (204, '    tad_res_win = []')
    (205, '    for resolution in config["call_TADs"]["resolutions"]:')
    (206, '        for win in config["call_TADs"]["resolutions"][resolution]:')
    (207, '            tad_res_win.append(f"{resolution}_{win}")')
    (208, '')
    (209, "# chroms = cooler.Cooler(f\\'{coolers_folder}/{coolfiles[0]}::resolutions/{resolutions[0]}\\').chroms()[:]")
    (210, '')
    (211, '# bedpe_mindists = [int(bedpe_pileups_mindist*2**i) for i in np.arange(0, np.log2(bedpe_pileups_maxdist/bedpe_pileups_mindist))]')
    (212, "# bedpe_separations = [f\\'{mindist}-{mindist*2}\\' for mindist in bedpe_mindists]")
    (213, '')
    (214, 'expecteds = expand(')
    (215, '    f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",')
    (216, '    sample=samples,')
    (217, '    resolution=resolutions,')
    (218, ')')
    (219, '')
    (220, 'diff_boundaries = (')
    (221, '    expand(')
    (222, '        f"{boundary_folder}/Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (223, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}.bed",')
    (224, '        insul_res_win=insul_res_win,')
    (225, '    )')
    (226, '    if config["compare_boundaries"]["do"]')
    (227, '    else []')
    (228, ')')
    (229, 'diff_boundaries_pileups = (')
    (230, '    expand(')
    (231, '        f"{pileups_folder}/{boundary_folder_name}/{{sample}}-{{resolution}}_over_Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (232, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}_{{norm}}_local.clpy",')
    (233, '        sample=samples,')
    (234, '        resolution=pileup_resolutions,')
    (235, '        insul_res_win=insul_res_win,')
    (236, '        norm=pileup_norms,')
    (237, '    )')
    (238, '    if config["compare_boundaries"]["do"] and config["pileups"]["do"]')
    (239, '    else []')
    (240, ')')
    (241, '')
    (242, 'tads = (')
    (243, '    expand(')
    (244, '        f"{tad_folder}/TADs_{{sampleTADs}}_{{tad_res_win}}.bed",')
    (245, '        sampleTADs=config["call_TADs"]["samples"],')
    (246, '        tad_res_win=tad_res_win,')
    (247, '    )')
    (248, '    if config["call_TADs"]["do"]')
    (249, '    else []')
    (250, ')')
    (251, '')
    (252, 'tads_pileups = (')
    (253, '    expand(')
    (254, '        f"{pileups_folder}/{tad_folder_name}/{{sample}}-{{resolution}}_over_TADs_{{sampleTADs}}_{{tad_res_win}}_{{norm}}_local_rescaled.clpy",')
    (255, '        sample=samples,')
    (256, '        resolution=pileup_resolutions,')
    (257, '        sampleTADs=config["call_TADs"]["samples"],')
    (258, '        tad_res_win=tad_res_win,')
    (259, '        norm=pileup_norms,')
    (260, '    )')
    (261, '    if config["call_TADs"]["do"] and config["pileups"]["do"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if config["pileups"]["do"]', 'if row[mode]']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
    (183, '    pileup_resolution_limits = config["pileups"]["resolution_limits"]')
    (184, '    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]')
    (185, '    pileup_resolutions = list(')
    (186, '        filter(')
    (187, '            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],')
    (188, '            resolutions,')
    (189, '        )')
    (190, '    )')
    (191, '    mindists = [')
    (192, '        int(pileups_mindist * 2**i)')
    (193, '        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))')
    (194, '    ]')
    (195, '    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]')
    (196, '')
    (197, 'if config["insulation"]["do"]:')
    (198, '    insul_res_win = []')
    (199, '    for resolution in config["insulation"]["resolutions"]:')
    (200, '        for win in config["insulation"]["resolutions"][resolution]:')
    (201, '            insul_res_win.append(f"{resolution}_{win}")')
    (202, '')
    (203, 'if config["call_TADs"]["do"]:')
    (204, '    tad_res_win = []')
    (205, '    for resolution in config["call_TADs"]["resolutions"]:')
    (206, '        for win in config["call_TADs"]["resolutions"][resolution]:')
    (207, '            tad_res_win.append(f"{resolution}_{win}")')
    (208, '')
    (209, "# chroms = cooler.Cooler(f\\'{coolers_folder}/{coolfiles[0]}::resolutions/{resolutions[0]}\\').chroms()[:]")
    (210, '')
    (211, '# bedpe_mindists = [int(bedpe_pileups_mindist*2**i) for i in np.arange(0, np.log2(bedpe_pileups_maxdist/bedpe_pileups_mindist))]')
    (212, "# bedpe_separations = [f\\'{mindist}-{mindist*2}\\' for mindist in bedpe_mindists]")
    (213, '')
    (214, 'expecteds = expand(')
    (215, '    f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",')
    (216, '    sample=samples,')
    (217, '    resolution=resolutions,')
    (218, ')')
    (219, '')
    (220, 'diff_boundaries = (')
    (221, '    expand(')
    (222, '        f"{boundary_folder}/Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (223, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}.bed",')
    (224, '        insul_res_win=insul_res_win,')
    (225, '    )')
    (226, '    if config["compare_boundaries"]["do"]')
    (227, '    else []')
    (228, ')')
    (229, 'diff_boundaries_pileups = (')
    (230, '    expand(')
    (231, '        f"{pileups_folder}/{boundary_folder_name}/{{sample}}-{{resolution}}_over_Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (232, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}_{{norm}}_local.clpy",')
    (233, '        sample=samples,')
    (234, '        resolution=pileup_resolutions,')
    (235, '        insul_res_win=insul_res_win,')
    (236, '        norm=pileup_norms,')
    (237, '    )')
    (238, '    if config["compare_boundaries"]["do"] and config["pileups"]["do"]')
    (239, '    else []')
    (240, ')')
    (241, '')
    (242, 'tads = (')
    (243, '    expand(')
    (244, '        f"{tad_folder}/TADs_{{sampleTADs}}_{{tad_res_win}}.bed",')
    (245, '        sampleTADs=config["call_TADs"]["samples"],')
    (246, '        tad_res_win=tad_res_win,')
    (247, '    )')
    (248, '    if config["call_TADs"]["do"]')
    (249, '    else []')
    (250, ')')
    (251, '')
    (252, 'tads_pileups = (')
    (253, '    expand(')
    (254, '        f"{pileups_folder}/{tad_folder_name}/{{sample}}-{{resolution}}_over_TADs_{{sampleTADs}}_{{tad_res_win}}_{{norm}}_local_rescaled.clpy",')
    (255, '        sample=samples,')
    (256, '        resolution=pileup_resolutions,')
    (257, '        sampleTADs=config["call_TADs"]["samples"],')
    (258, '        tad_res_win=tad_res_win,')
    (259, '        norm=pileup_norms,')
    (260, '    )')
    (261, '    if config["call_TADs"]["do"] and config["pileups"]["do"]')
    (262, '    else []')
    (263, ')')
    (264, 'dot_methods = [')
    (265, '    m for m in config["call_dots"]["methods"] if config["call_dots"]["methods"][m]["do"]')
    (266, ']')
    (267, 'if dot_methods:')
    (268, '    loops = expand(')
    (269, '        f"{loop_folder}/merged_resolutions/Loops_{{method}}_{{sampleLoops}}.bedpe",')
    (270, '        method=dot_methods,')
    (271, '        sampleLoops=config["call_dots"]["samples"],')
    (272, '    )')
    (273, '')
    (274, '    loops_pileups = (')
    (275, '        expand(')
    (276, '            f"{pileups_folder}/{loop_folder_name}/{{sample}}-{{resolution}}_over_Loops_{{method}}_{{sampleLoops}}_{{norm}}_{{mode}}.clpy",')
    (277, '            sample=samples,')
    (278, '            resolution=pileup_resolutions,')
    (279, '            method=dot_methods,')
    (280, '            sampleLoops=config["call_dots"]["samples"],')
    (281, '            norm=pileup_norms,')
    (282, '            mode=["distal", "by_distance"],')
    (283, '        )')
    (284, '        if config["pileups"]["do"]')
    (285, '        else []')
    (286, '    )')
    (287, 'else:')
    (288, '    loops = []')
    (289, '    loops_pileups = []')
    (290, '')
    (291, 'for file in loops:')
    (292, '    name = path.splitext(path.basename(file))[0]')
    (293, '    bedfiles_dict[name] = file')
    (294, '    bedtype_dict[name] = "bedpe"')
    (295, 'for file in tads + diff_boundaries:')
    (296, '    name = path.splitext(path.basename(file))[0]')
    (297, '    bedfiles_dict[name] = file')
    (298, '    bedtype_dict[name] = "bed"')
    (299, '')
    (300, 'beds_pileups = []')
    (301, 'if config["pileups"]["do"]:')
    (302, '    for bedname, row in bed_df.iterrows():')
    (303, '        modes = []')
    (304, '        for mode in pileup_params.keys():')
    (305, '            if row[mode]:')
    (306, '                modes += [mode]')
    (307, '')
    (308, '        for sample in samples:')
    (309, '            if sample not in samples_annotations.index:')
    (310, '                continue')
    (311, '            if (')
    (312, '                bedname in samples_annotations.columns')
    (313, '                and not samples_annotations.loc[sample, bedname]')
    (314, '            ):')
    (315, '                continue')
    (316, '            beds_pileups += expand(')
    (317, '                f"{pileups_folder}/{beds_folder_name}/{sample}-{{resolution}}_over_{bedname}_{{norm}}_{{mode}}.clpy",')
    (318, '                resolution=pileup_resolutions,')
    (319, '                norm=pileup_norms,')
    (320, '                mode=modes,')
    (321, '            )')
    (322, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=open2c/quaich, file=workflow/Snakefile
context_key: ['if config["saddle"]["do"']
    (99, 'def make_local_path(bedname, kind):')
    (100, '    if kind == "bed":')
    (101, '        return f"{beds_folder}/{bedname}.bed"')
    (102, '    elif kind == "bedpe":')
    (103, '        return f"{bedpes_folder}/{bedname}.bedpe"')
    (104, '    else:')
    (105, '        raise ValueError("Only bed and bedpe file types are supported")')
    (106, '')
    (107, '')
    (108, 'bedfiles_local = get_files(beds_folder, "bed")')
    (109, 'bedpefiles_local = get_files(bedpes_folder, "bedpe")')
    (110, '')
    (111, 'local_bed_names = {')
    (112, '    path.splitext(bedfile)[0]: f"{beds_folder}/{bedfile}" for bedfile in bedfiles_local')
    (113, '}')
    (114, 'local_bedpe_names = {')
    (115, '    path.splitext(bedpefile)[0]: f"{bedpes_folder}/{bedpefile}"')
    (116, '    for bedpefile in bedpefiles_local')
    (117, '}')
    (118, '')
    (119, 'bed_df = pd.read_csv(config["annotations"], sep="\\\\t", header=0, comment="#")')
    (120, 'bed_df.loc[:, "will_download"] = bed_df.file.apply(will_download)')
    (121, 'bed_df.loc[:, "local_path"] = bed_df.apply(')
    (122, '    lambda x: make_local_path(x.bedname, x.format) if x.will_download else x.file,')
    (123, '    axis=1,')
    (124, ')')
    (125, 'bed_df = bed_df.set_index("bedname").replace("-", np.nan)')
    (126, '')
    (127, 'pileup_params = config["pileups"]["arguments"]')
    (128, '')
    (129, 'bed_df[list(pileup_params.keys())] = ~bed_df[list(pileup_params.keys())].isna()')
    (130, '')
    (131, 'bedlinks_dict = dict(')
    (132, '    bed_df.query("will_download")["file"]')
    (133, ')  # dict with beds to be downloaded')
    (134, 'bedfiles_dict = dict(bed_df["local_path"])')
    (135, 'bedfiles_dict.update(local_bed_names)')
    (136, 'bedfiles_dict.update(local_bedpe_names)')
    (137, 'bedfiles = list(bedfiles_dict.keys())')
    (138, "# bedfiles_pileups = [bf for bf in bedfiles if bed_df.loc[bf, \\'pileups\\']]")
    (139, 'bedtype_dict = dict(bed_df["format"])')
    (140, "# bedpe_pileups_mindist, bedpe_pileups_maxdist = config[\\'bedpe_pileups_distance_limits\\']")
    (141, '')
    (142, 'samples_annotations = ~pd.read_csv(')
    (143, '    config["samples_annotations_combinations"],')
    (144, '    sep="\\\\t",')
    (145, '    header=0,')
    (146, '    index_col=0,')
    (147, '    comment="#",')
    (148, ').isna()')
    (149, '### Data resolutions')
    (150, 'ignore_resolutions_more_than = config["ignore_resolutions_more_than"]')
    (151, 'resolutions = config["resolutions"]  ##### Assume same resolutions in all coolers')
    (152, 'minresolution = min(resolutions)')
    (153, 'resolutions = list(filter(lambda x: x <= ignore_resolutions_more_than, resolutions))')
    (154, '')
    (155, 'if config["eigenvector"]["do"]:')
    (156, '    eigenvector_resolution_limits = config["eigenvector"]["resolution_limits"]')
    (157, '    eigenvector_resolutions = list(')
    (158, '        filter(')
    (159, '            lambda x: eigenvector_resolution_limits[0]')
    (160, '            <= x')
    (161, '            <= eigenvector_resolution_limits[1],')
    (162, '            resolutions,')
    (163, '        )')
    (164, '    )')
    (165, '')
    (166, 'if config["saddle"]["do"]:')
    (167, '    saddle_mindist, saddle_maxdist = config["saddle"]["distance_limits"]')
    (168, '    saddle_mindists = [')
    (169, '        int(saddle_mindist * 2**i)')
    (170, '        for i in np.arange(0, np.log2(saddle_maxdist / saddle_mindist))')
    (171, '    ]')
    (172, '    saddle_separations = [f"_dist_{mindist}-{mindist*2}" for mindist in saddle_mindists]')
    (173, '')
    (174, 'if config["pileups"]["do"] or config["pileups"]["bed_pairs"]["do"]:')
    (175, '    shifts = config["pileups"]["shifts"]')
    (176, '    pileup_norms = []')
    (177, '    if shifts > 0:')
    (178, '        pileup_norms.append(f"{shifts}-shifts")')
    (179, '    if config["pileups"]["expected"]:')
    (180, '        pileup_norms.append("expected")')
    (181, '    if len(pileup_norms) == 0:')
    (182, '        raise ValueError("Please use expected or shifts to normalize pileups")')
    (183, '    pileup_resolution_limits = config["pileups"]["resolution_limits"]')
    (184, '    pileups_mindist, pileups_maxdist = config["pileups"]["distance_limits"]')
    (185, '    pileup_resolutions = list(')
    (186, '        filter(')
    (187, '            lambda x: pileup_resolution_limits[0] <= x <= pileup_resolution_limits[1],')
    (188, '            resolutions,')
    (189, '        )')
    (190, '    )')
    (191, '    mindists = [')
    (192, '        int(pileups_mindist * 2**i)')
    (193, '        for i in np.arange(0, np.log2(pileups_maxdist / pileups_mindist))')
    (194, '    ]')
    (195, '    separations = [f"_dist_{mindist}-{mindist*2}" for mindist in mindists]')
    (196, '')
    (197, 'if config["insulation"]["do"]:')
    (198, '    insul_res_win = []')
    (199, '    for resolution in config["insulation"]["resolutions"]:')
    (200, '        for win in config["insulation"]["resolutions"][resolution]:')
    (201, '            insul_res_win.append(f"{resolution}_{win}")')
    (202, '')
    (203, 'if config["call_TADs"]["do"]:')
    (204, '    tad_res_win = []')
    (205, '    for resolution in config["call_TADs"]["resolutions"]:')
    (206, '        for win in config["call_TADs"]["resolutions"][resolution]:')
    (207, '            tad_res_win.append(f"{resolution}_{win}")')
    (208, '')
    (209, "# chroms = cooler.Cooler(f\\'{coolers_folder}/{coolfiles[0]}::resolutions/{resolutions[0]}\\').chroms()[:]")
    (210, '')
    (211, '# bedpe_mindists = [int(bedpe_pileups_mindist*2**i) for i in np.arange(0, np.log2(bedpe_pileups_maxdist/bedpe_pileups_mindist))]')
    (212, "# bedpe_separations = [f\\'{mindist}-{mindist*2}\\' for mindist in bedpe_mindists]")
    (213, '')
    (214, 'expecteds = expand(')
    (215, '    f"{expected_folder}/{{sample}}_{{resolution}}.expected.tsv",')
    (216, '    sample=samples,')
    (217, '    resolution=resolutions,')
    (218, ')')
    (219, '')
    (220, 'diff_boundaries = (')
    (221, '    expand(')
    (222, '        f"{boundary_folder}/Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (223, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}.bed",')
    (224, '        insul_res_win=insul_res_win,')
    (225, '    )')
    (226, '    if config["compare_boundaries"]["do"]')
    (227, '    else []')
    (228, ')')
    (229, 'diff_boundaries_pileups = (')
    (230, '    expand(')
    (231, '        f"{pileups_folder}/{boundary_folder_name}/{{sample}}-{{resolution}}_over_Insulation_{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][0]}_not_"')
    (232, '        f"{config[\\\'compare_boundaries\\\'][\\\'samples\\\'][1]}_{{insul_res_win}}_{{norm}}_local.clpy",')
    (233, '        sample=samples,')
    (234, '        resolution=pileup_resolutions,')
    (235, '        insul_res_win=insul_res_win,')
    (236, '        norm=pileup_norms,')
    (237, '    )')
    (238, '    if config["compare_boundaries"]["do"] and config["pileups"]["do"]')
    (239, '    else []')
    (240, ')')
    (241, '')
    (242, 'tads = (')
    (243, '    expand(')
    (244, '        f"{tad_folder}/TADs_{{sampleTADs}}_{{tad_res_win}}.bed",')
    (245, '        sampleTADs=config["call_TADs"]["samples"],')
    (246, '        tad_res_win=tad_res_win,')
    (247, '    )')
    (248, '    if config["call_TADs"]["do"]')
    (249, '    else []')
    (250, ')')
    (251, '')
    (252, 'tads_pileups = (')
    (253, '    expand(')
    (254, '        f"{pileups_folder}/{tad_folder_name}/{{sample}}-{{resolution}}_over_TADs_{{sampleTADs}}_{{tad_res_win}}_{{norm}}_local_rescaled.clpy",')
    (255, '        sample=samples,')
    (256, '        resolution=pileup_resolutions,')
    (257, '        sampleTADs=config["call_TADs"]["samples"],')
    (258, '        tad_res_win=tad_res_win,')
    (259, '        norm=pileup_norms,')
    (260, '    )')
    (261, '    if config["call_TADs"]["do"] and config["pileups"]["do"]')
    (262, '    else []')
    (263, ')')
    (264, 'dot_methods = [')
    (265, '    m for m in config["call_dots"]["methods"] if config["call_dots"]["methods"][m]["do"]')
    (266, ']')
    (267, 'if dot_methods:')
    (268, '    loops = expand(')
    (269, '        f"{loop_folder}/merged_resolutions/Loops_{{method}}_{{sampleLoops}}.bedpe",')
    (270, '        method=dot_methods,')
    (271, '        sampleLoops=config["call_dots"]["samples"],')
    (272, '    )')
    (273, '')
    (274, '    loops_pileups = (')
    (275, '        expand(')
    (276, '            f"{pileups_folder}/{loop_folder_name}/{{sample}}-{{resolution}}_over_Loops_{{method}}_{{sampleLoops}}_{{norm}}_{{mode}}.clpy",')
    (277, '            sample=samples,')
    (278, '            resolution=pileup_resolutions,')
    (279, '            method=dot_methods,')
    (280, '            sampleLoops=config["call_dots"]["samples"],')
    (281, '            norm=pileup_norms,')
    (282, '            mode=["distal", "by_distance"],')
    (283, '        )')
    (284, '        if config["pileups"]["do"]')
    (285, '        else []')
    (286, '    )')
    (287, 'else:')
    (288, '    loops = []')
    (289, '    loops_pileups = []')
    (290, '')
    (291, 'for file in loops:')
    (292, '    name = path.splitext(path.basename(file))[0]')
    (293, '    bedfiles_dict[name] = file')
    (294, '    bedtype_dict[name] = "bedpe"')
    (295, 'for file in tads + diff_boundaries:')
    (296, '    name = path.splitext(path.basename(file))[0]')
    (297, '    bedfiles_dict[name] = file')
    (298, '    bedtype_dict[name] = "bed"')
    (299, '')
    (300, 'beds_pileups = []')
    (301, 'if config["pileups"]["do"]:')
    (302, '    for bedname, row in bed_df.iterrows():')
    (303, '        modes = []')
    (304, '        for mode in pileup_params.keys():')
    (305, '            if row[mode]:')
    (306, '                modes += [mode]')
    (307, '')
    (308, '        for sample in samples:')
    (309, '            if sample not in samples_annotations.index:')
    (310, '                continue')
    (311, '            if (')
    (312, '                bedname in samples_annotations.columns')
    (313, '                and not samples_annotations.loc[sample, bedname]')
    (314, '            ):')
    (315, '                continue')
    (316, '            beds_pileups += expand(')
    (317, '                f"{pileups_folder}/{beds_folder_name}/{sample}-{{resolution}}_over_{bedname}_{{norm}}_{{mode}}.clpy",')
    (318, '                resolution=pileup_resolutions,')
    (319, '                norm=pileup_norms,')
    (320, '                mode=modes,')
    (321, '            )')
    (322, '')
    (323, 'saddles = (')
    (324, '    expand(')
    (325, '        f"{saddles_folder}/{{sample}}_{{resolution}}_{{bins}}{{dist}}.{{ending}}",')
    (326, '        sample=samples,')
    (327, '        resolution=eigenvector_resolutions,')
    (328, '        bins=config["saddle"]["bins"],')
    (329, '        dist=saddle_separations + [""],')
    (330, '        ending=["saddledump.npz", "digitized.tsv"],')
    (331, '    )')
    (332, '    if config["saddle"]["do"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AliSaadatV/Variant_Calling_Snakemake, file=workflow/Snakefile
context_key: ['if config[\\\'PED\\\'] != ""']
    (24, 'def get_pedigree_command(resource):')
    (25, '    command = ""')
    (26, '    if config[\\\'PED\\\'] != "":')
    (27, '        command = "-ped " + config[\\\'PED\\\']')
    (28, '    return command')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AliSaadatV/Variant_Calling_Snakemake, file=workflow/Snakefile
context_key: ['if config[\\\'DATA\\\'] == "WES"']
    (30, 'def get_vqsr_DP_option(resource):')
    (31, '    if config[\\\'DATA\\\'] == "WES":')
    (32, '        command = ""')
    (33, '    if config[\\\'DATA\\\'] == "WGS":')
    (34, '        command = "-an DP"')
    (35, '    return command')
    (36, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AliSaadatV/Variant_Calling_Snakemake, file=workflow/Snakefile
context_key: ['if config[\\\'INBREED_COEFF_FILTER\\\'] == "EXCLUDE"']
    (37, 'def get_vqsr_InbreedingCoefficient_option(resource):')
    (38, '    if config[\\\'INBREED_COEFF_FILTER\\\'] == "EXCLUDE":')
    (39, '        command = ""   ')
    (40, '    if config[\\\'INBREED_COEFF_FILTER\\\'] == "INCLUDE":')
    (41, '        command = "-an InbreedingCoeff"')
    (42, '    return command')
    (43, '')
    (44, '#def get_wes_intervals_command(resource):    ')
    (45, '#    command = ""')
    (46, '#    command = "-L " + config[\\\'WES\\\'][\\\'INTERVALS\\\'] + " "')
    (47, '#    if config[\\\'WES\\\'][\\\'INTERVALS\\\'] == "":')
    (48, '#        command = ""')
    (49, '#    return command')
    (50, '')
    (51, '#def get_wes_padding_command(resource):')
    (52, '#    command = ""')
    (53, '#    command = "--ip " + config[\\\'WES\\\'][\\\'PADDING\\\'] + " " ')
    (54, '#    if config[\\\'WES\\\'][\\\'PADDING\\\'] == "":')
    (55, '#        command = ""')
    (56, '#    return command')
    (57, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AliSaadatV/Variant_Calling_Snakemake, file=workflow/Snakefile
context_key: ["if config[\\'DATA\\'] == \\'WES\\'", 'if config[\\\'WES\\\'][\\\'PADDING\\\'] == ""']
    (58, 'def get_intervals_command(resource):')
    (59, "    if config[\\'DATA\\'] == \\'WES\\':")
    (60, '        command = "-L " + config[\\\'WES\\\'][\\\'INTERVALS\\\'] + " " + "--ip " + config[\\\'WES\\\'][\\\'PADDING\\\'] + " " ')
    (61, '        if config[\\\'WES\\\'][\\\'PADDING\\\'] == "":')
    (62, '            command = "-L " + config[\\\'WES\\\'][\\\'INTERVALS\\\'] + " "')
    (63, '        if config[\\\'WES\\\'][\\\'INTERVALS\\\'] == "":')
    (64, '            command = ""')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AliSaadatV/Variant_Calling_Snakemake, file=workflow/Snakefile
context_key: ["if config[\\'DATA\\'] == \\'WES\\'", "if config[\\'DATA\\'] == \\'WGS\\'"]
    (58, 'def get_intervals_command(resource):')
    (59, "    if config[\\'DATA\\'] == \\'WES\\':")
    (60, '        command = "-L " + config[\\\'WES\\\'][\\\'INTERVALS\\\'] + " " + "--ip " + config[\\\'WES\\\'][\\\'PADDING\\\'] + " " ')
    (61, '        if config[\\\'WES\\\'][\\\'PADDING\\\'] == "":')
    (62, '            command = "-L " + config[\\\'WES\\\'][\\\'INTERVALS\\\'] + " "')
    (63, '        if config[\\\'WES\\\'][\\\'INTERVALS\\\'] == "":')
    (64, '            command = ""')
    (65, "    if config[\\'DATA\\'] == \\'WGS\\':")
    (66, '        chroms = [str(c) for c in range(1, 23)] + ["X", "Y", "M"]')
    (67, '        command = ""')
    (68, '        for l in chroms:')
    (69, '            command = command + "-L chr" + l + " "')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AliSaadatV/Variant_Calling_Snakemake, file=workflow/Snakefile
context_key: ["if config[\\'DATA\\'] == \\'WES\\'"]
    (58, 'def get_intervals_command(resource):')
    (59, "    if config[\\'DATA\\'] == \\'WES\\':")
    (60, '        command = "-L " + config[\\\'WES\\\'][\\\'INTERVALS\\\'] + " " + "--ip " + config[\\\'WES\\\'][\\\'PADDING\\\'] + " " ')
    (61, '        if config[\\\'WES\\\'][\\\'PADDING\\\'] == "":')
    (62, '            command = "-L " + config[\\\'WES\\\'][\\\'INTERVALS\\\'] + " "')
    (63, '        if config[\\\'WES\\\'][\\\'INTERVALS\\\'] == "":')
    (64, '            command = ""')
    (65, "    if config[\\'DATA\\'] == \\'WGS\\':")
    (66, '        chroms = [str(c) for c in range(1, 23)] + ["X", "Y", "M"]')
    (67, '        command = ""')
    (68, '        for l in chroms:')
    (69, '            command = command + "-L chr" + l + " "')
    (70, '    return command')
    (71, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AliSaadatV/Variant_Calling_Snakemake, file=workflow/Snakefile
context_key: ['if config[\\\'DATA\\\'] == "WES"']
    (72, 'def get_bwa_memory(resource):')
    (73, '    if config[\\\'DATA\\\'] == "WES":')
    (74, '        return 15000')
    (75, '    if config[\\\'DATA\\\'] == "WGS":')
    (76, '        return 50000')
    (77, '    else:')
    (78, '        return 15000')
    (79, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AliSaadatV/Variant_Calling_Snakemake, file=workflow/Snakefile
context_key: ['if config[\\\'DATA\\\'] == "WES"']
    (80, 'def get_mkdup_memory(resource):')
    (81, '    if config[\\\'DATA\\\'] == "WES":')
    (82, '        return 40000')
    (83, '    if config[\\\'DATA\\\'] == "WGS":')
    (84, '        return 80000')
    (85, '    else:')
    (86, '        return 40000')
    (87, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AliSaadatV/Variant_Calling_Snakemake, file=workflow/Snakefile
context_key: ['if config[\\\'DATA\\\'] == "WES"']
    (88, 'def get_mkdup_xmx(resource):')
    (89, '    if config[\\\'DATA\\\'] == "WES":')
    (90, '        return expand(\\\'"-Xmx{maxmemory}"\\\', maxmemory = config[\\\'MAXMEMORY\\\'][\\\'MARK_DUP_WES\\\'])')
    (91, '    if config[\\\'DATA\\\'] == "WGS":')
    (92, '        return expand(\\\'"-Xmx{maxmemory}"\\\', maxmemory = config[\\\'MAXMEMORY\\\'][\\\'MARK_DUP_WGS\\\'])')
    (93, '    else:')
    (94, '        return expand(\\\'"-Xmx{maxmemory}"\\\', maxmemory = config[\\\'MAXMEMORY\\\'][\\\'MARK_DUP_WES\\\'])')
    (95, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AliSaadatV/Variant_Calling_Snakemake, file=workflow/Snakefile
context_key: ['if config[\\\'DATA\\\'] == "WES"']
    (96, 'def get_HC_memory(resource):')
    (97, '    if config[\\\'DATA\\\'] == "WES":')
    (98, '        return 10000')
    (99, '    if config[\\\'DATA\\\'] == "WGS":')
    (100, '        return 20000')
    (101, '    else:')
    (102, '        return 10000')
    (103, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AliSaadatV/Variant_Calling_Snakemake, file=workflow/Snakefile
context_key: ['if config[\\\'DATA\\\'] == "WES"']
    (104, 'def get_HC_xmx(resource):')
    (105, '    if config[\\\'DATA\\\'] == "WES":')
    (106, '        return expand(\\\'"-Xmx{maxmemory}"\\\', maxmemory = config[\\\'MAXMEMORY\\\'][\\\'HC_WES\\\'])')
    (107, '    if config[\\\'DATA\\\'] == "WGS":')
    (108, '        return expand(\\\'"-Xmx{maxmemory}"\\\', maxmemory = config[\\\'MAXMEMORY\\\'][\\\'HC_WGS\\\'])')
    (109, '    else:')
    (110, '        return expand(\\\'"-Xmx{maxmemory}"\\\', maxmemory = config[\\\'MAXMEMORY\\\'][\\\'HC_WES\\\'])')
    (111, '')
    (112, '#### Set up report #####')
    (113, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AliSaadatV/Variant_Calling_Snakemake, file=workflow/Snakefile
context_key: ['if config[\\\'PED\\\'] == ""', 'rule all', 'input']
    (104, 'def get_HC_xmx(resource):')
    (105, '    if config[\\\'DATA\\\'] == "WES":')
    (106, '        return expand(\\\'"-Xmx{maxmemory}"\\\', maxmemory = config[\\\'MAXMEMORY\\\'][\\\'HC_WES\\\'])')
    (107, '    if config[\\\'DATA\\\'] == "WGS":')
    (108, '        return expand(\\\'"-Xmx{maxmemory}"\\\', maxmemory = config[\\\'MAXMEMORY\\\'][\\\'HC_WGS\\\'])')
    (109, '    else:')
    (110, '        return expand(\\\'"-Xmx{maxmemory}"\\\', maxmemory = config[\\\'MAXMEMORY\\\'][\\\'HC_WES\\\'])')
    (111, '')
    (112, '#### Set up report #####')
    (113, '')
    (114, 'report: "report/workflow.rst"')
    (115, '')
    (116, '##### Target rules depnding on the existense of pedigree file #####')
    (117, 'localrules: all ')
    (118, '')
    (119, 'if config[\\\'PED\\\'] == "":')
    (120, '    rule all:')
    (121, '        input:')
    (122, '            expand("../results/qc/fastqc/{sample}_R{read}_fastqc.html", sample = SAMPLES, read = [1, 2]),')
    (123, '            "../results/vcf/refined_GQ.vcf.gz"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=KevinLYW366/TBSeqPipe, file=workflow/rules/common.smk
context_key: ['if config["data_cleaning"]']
    (54, 'def get_fastq_reads(sample):')
    (55, '    """')
    (56, '    If data cleaning step is chosen to skip, just use the cleaned fastq files as the input of bwa_map')
    (57, '    """')
    (58, '    if config["data_cleaning"]:')
    (59, '        return ["results/00.data_cleaning/%s/%s_1.clean.fastq.gz" % (sample, sample),')
    (60, '                "results/00.data_cleaning/%s/%s_2.clean.fastq.gz" % (sample, sample)]')
    (61, '    else:')
    (62, '        if config["data_dir_format"]:')
    (63, '            return [config["data_dir"] + "/%s/%s_%s1.%s" % (sample, sample, FASTQREAD, FASTQSUFFIX),')
    (64, '                    config["data_dir"] + "/%s/%s_%s2.%s" % (sample, sample, FASTQREAD, FASTQSUFFIX)]')
    (65, '        else:')
    (66, '            return [config["data_dir"] + "/%s_%s1.%s" % (sample, FASTQREAD, FASTQSUFFIX),')
    (67, '                    config["data_dir"] + "/%s_%s2.%s" % (sample, FASTQREAD, FASTQSUFFIX)]')
    (68, '')
    (69, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=KevinLYW366/TBSeqPipe, file=workflow/rules/common.smk
context_key: ['if config["kraken_filter"]']
    (70, 'def get_qualified_results(path, wildcards):')
    (71, '    """')
    (72, '    Input a file path with sample wildcards such as "results/01.Alignment/{sample}/{sample}.sorted.bam".')
    (73, '    Return required file path of qualified samples based on kraken qc result.')
    (74, '    This function will be used to generate an output file list, which is helpful to')
    (75, '      prevent unqualified samples from running specific analysis steps of the workflow.')
    (76, '    """')
    (77, '    thres = config["kraken_threshold"]')
    (78, '    if config["kraken_filter"]:')
    (79, '        qc = pd.read_csv(checkpoints.kraken_qc.get().output[0], sep="\\\\t", header=None, comment=\\\'#\\\')')
    (80, '        qc.columns = ["sample", "percent", "category"]')
    (81, '        return expand(path, sample=qc[qc["percent"] > thres]["sample"])')
    (82, '    else:')
    (83, '        return expand(path, sample=SAMPLES)')
    (84, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/Genrich_compare, file=Snakefile
context_key: ['if config["from_fastq"]', 'if config["paired_end"]']
    (79, 'def get_R1_files(wildcards):')
    (80, '    sample_name = "_".join(wildcards.sample.split("_")[0:-1])')
    (81, '    mark = wildcards.sample.split("_")[-1]')
    (82, '    if config["from_fastq"]:')
    (83, '        if config["paired_end"]:')
    (84, "            return  FILES[sample_name][mark][\\'R1\\']")
    (85, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/Genrich_compare, file=Snakefile
context_key: ['if config["from_fastq"]', 'if config["paired_end"]']
    (86, 'def get_R2_files(wildcards):')
    (87, '    sample_name = "_".join(wildcards.sample.split("_")[0:-1])')
    (88, '    mark = wildcards.sample.split("_")[-1]')
    (89, '    if config["from_fastq"]:')
    (90, '        if config["paired_end"]:')
    (91, "            return  FILES[sample_name][mark][\\'R2\\']")
    (92, '')
    (93, '## when there are multiple fastq.gz for the same sample, same mark, merge them.')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/Genrich_compare, file=Snakefile
context_key: ['if config["paired_end"]', 'rule merge_fastqs', 'input']
    (86, 'def get_R2_files(wildcards):')
    (87, '    sample_name = "_".join(wildcards.sample.split("_")[0:-1])')
    (88, '    mark = wildcards.sample.split("_")[-1]')
    (89, '    if config["from_fastq"]:')
    (90, '        if config["paired_end"]:')
    (91, "            return  FILES[sample_name][mark][\\'R2\\']")
    (92, '')
    (93, '## when there are multiple fastq.gz for the same sample, same mark, merge them.')
    (94, 'if config["paired_end"]:')
    (95, '    rule merge_fastqs:')
    (96, '        input:')
    (97, '            r1 = get_R1_files,')
    (98, '            r2 = get_R2_files')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jts/ncov-tools, file=workflow/rules/common.smk
context_key: ['if config.get("platform") == "illumina"']
    (199, 'def get_qc_reports(wildcards):')
    (200, '    out = [ get_qc_summary(wildcards) ]')
    (201, '')
    (202, '    prefix = get_run_name()')
    (203, '    out.append("lineages/%s_pangolin_version.txt" % (prefix))')
    (204, '')
    (205, '    # currently these reports are only generated for illumina data')
    (206, '    if config.get("platform") == "illumina":')
    (207, '        out.append(get_mixture_report(wildcards))')
    (208, '        out.append(get_ambiguous_report(wildcards))')
    (209, '')
    (210, '    # only try to make negative control report if NC samples have been defined')
    (211, '    if len(get_valid_negative_control_samples()) > 0:')
    (212, '        out.append(get_negative_control_report(wildcards))')
    (213, '    return out')
    (214, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=jaleezyy/covid-19-signal, file=Snakefile
context_key: ["if config[\\'pangolin_fast\\']"]
    (84, 'def get_pooled_fastq_files(sample_name, r):')
    (85, "    sample_fastqs = samples[samples[\\'sample\\'] == sample_name]")
    (86, "    if r == \\'1\\':")
    (87, "        relpath = sample_fastqs[\\'r1_path\\'].values")
    (88, "    elif r == \\'2\\':")
    (89, "        relpath = sample_fastqs[\\'r2_path\\'].values")
    (90, '')
    (91, '    return [ os.path.abspath(os.path.join(exec_dir, r)) for r in relpath ]')
    (92, '')
    (93, '# determine raw FASTQ handling')
    (94, '# if duplicate sample names in table, run legacy concat_and_sort')
    (95, "if samples[\\'sample\\'].duplicated().any():")
    (96, '    print("Duplicate sample names in sample table. Assuming multi-lane samples exist")')
    (97, '    ruleorder: concat_and_sort > link_raw_data')
    (98, 'else:')
    (99, '    ruleorder: link_raw_data > concat_and_sort')
    (100, '')
    (101, '# Determine Pangolin analysis mode')
    (102, "if config[\\'pangolin_fast\\']:")
    (103, "    pango_speed = \\'fast\\'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=BeeCSI-Microbiome/taxonomic_profiling_pipeline, file=rules/common.smk
context_key: ["if config[\\'filter_targets\\']"]
    (10, 'def get_filter_string():')
    (11, '    """Creates the filter string based on targets specified in config file"""')
    (12, '    # Base string to which filter targets will be appended')
    (13, '    F_STRING="unclassified\\\\|cellular organism"')
    (14, "    if config[\\'filter_targets\\']:")
    (15, '        print("Found filter targets in the config file")')
    (16, '        F_STRING = \\\'{0}\\\\|{1}\\\'.format(F_STRING, "\\\\|".join(config["filter_targets"]))')
    (17, "        print(\\'Filtering with the following filter string:\\")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=maehler/gwas-workflow, file=Snakefile
context_key: ["if config['symlink_data']"]
    (10, 'def get_variants():')
    (11, "    if config['symlink_data']:")
    (12, '        return os.path.join(')
    (13, "            config['symlink_dir'],")
    (14, "            os.path.basename(config['variants']))")
    (15, "    return config['variants']")
    (16, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/Struo, file=Snakefile
context_key: ["if config['keep_intermediate'] == True", "if config['use_ancient'] == True"]
    (67, 'def all_which_input(wildcards):')
    (68, '    input_files = []')
    (69, '    # kraken2')
    (70, "    if not config['databases']['kraken2'].startswith('Skip'):")
    (71, "        if config['keep_intermediate'] == True:")
    (72, "            x = expand(kraken2_dir + 'added/{sample}.done',")
    (73, "                       sample = config['samples_unique'])")
    (74, '            input_files += x\\t    ')
    (75, "        input_files.append(os.path.join(kraken2_dir, 'hash.k2d'))")
    (76, "        input_files.append(os.path.join(kraken2_dir, 'opts.k2d'))")
    (77, "        input_files.append(os.path.join(kraken2_dir, 'taxo.k2d'))")
    (78, "        input_files.append(os.path.join(kraken2_dir, 'seqid2taxid.map'))")
    (79, '')
    (80, '    # bracken')
    (81, "    if (not config['databases']['kraken2'].startswith('Skip') and")
    (82, "        not config['databases']['bracken'].startswith('Skip')):")
    (83, "    \\tx = expand(os.path.join(kraken2_dir, 'database{read_len}mers.kraken'),")
    (84, "\\t           read_len = config['params']['bracken_build_read_lens'])")
    (85, '        input_files += x')
    (86, '')
    (87, '    # humann2')
    (88, "    if not config['databases']['humann2_bowtie2'].startswith('Skip') and \\\\")
    (89, "       not config['databases']['humann2_diamond'].startswith('Skip'):")
    (90, "        if config['keep_intermediate'] == True:")
    (91, "            if config['use_ancient'] == True:                ")
    (92, "                x = ancient(expand(annot_dir + 'prodigal/{sample}/annot.fna.gz',")
    (93, "                                   sample = config['samples_unique']))")
    (94, '            else:\\t\\t\\t   ')
    (95, "                x = expand(annot_dir + 'prodigal/{sample}/annot.fna.gz',")
    (96, "                           sample = config['samples_unique'])")
    (97, '            input_files += x')
    (98, "            if config['use_ancient'] == True:                \\t    ")
    (99, "                x = ancient(expand(annot_dir + 'prodigal/{sample}/annot.faa.gz',")
    (100, "                                   sample = config['samples_unique']))")
    (101, '            else:')
    (102, "                x = expand(annot_dir + 'prodigal/{sample}/annot.faa.gz',")
    (103, "                           sample = config['samples_unique'])")
    (104, '            input_files += x')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/Struo, file=Snakefile
context_key: ["if config['keep_intermediate'] == True", "if not config['databases']['humann2_bowtie2'].startswith('Skip')"]
    (67, 'def all_which_input(wildcards):')
    (68, '    input_files = []')
    (69, '    # kraken2')
    (70, "    if not config['databases']['kraken2'].startswith('Skip'):")
    (71, "        if config['keep_intermediate'] == True:")
    (72, "            x = expand(kraken2_dir + 'added/{sample}.done',")
    (73, "                       sample = config['samples_unique'])")
    (74, '            input_files += x\\t    ')
    (75, "        input_files.append(os.path.join(kraken2_dir, 'hash.k2d'))")
    (76, "        input_files.append(os.path.join(kraken2_dir, 'opts.k2d'))")
    (77, "        input_files.append(os.path.join(kraken2_dir, 'taxo.k2d'))")
    (78, "        input_files.append(os.path.join(kraken2_dir, 'seqid2taxid.map'))")
    (79, '')
    (80, '    # bracken')
    (81, "    if (not config['databases']['kraken2'].startswith('Skip') and")
    (82, "        not config['databases']['bracken'].startswith('Skip')):")
    (83, "    \\tx = expand(os.path.join(kraken2_dir, 'database{read_len}mers.kraken'),")
    (84, "\\t           read_len = config['params']['bracken_build_read_lens'])")
    (85, '        input_files += x')
    (86, '')
    (87, '    # humann2')
    (88, "    if not config['databases']['humann2_bowtie2'].startswith('Skip') and \\\\")
    (89, "       not config['databases']['humann2_diamond'].startswith('Skip'):")
    (90, "        if config['keep_intermediate'] == True:")
    (91, "            if config['use_ancient'] == True:                ")
    (92, "                x = ancient(expand(annot_dir + 'prodigal/{sample}/annot.fna.gz',")
    (93, "                                   sample = config['samples_unique']))")
    (94, '            else:\\t\\t\\t   ')
    (95, "                x = expand(annot_dir + 'prodigal/{sample}/annot.fna.gz',")
    (96, "                           sample = config['samples_unique'])")
    (97, '            input_files += x')
    (98, "            if config['use_ancient'] == True:                \\t    ")
    (99, "                x = ancient(expand(annot_dir + 'prodigal/{sample}/annot.faa.gz',")
    (100, "                                   sample = config['samples_unique']))")
    (101, '            else:')
    (102, "                x = expand(annot_dir + 'prodigal/{sample}/annot.faa.gz',")
    (103, "                           sample = config['samples_unique'])")
    (104, '            input_files += x')
    (105, "    if not config['databases']['humann2_bowtie2'].startswith('Skip'):")
    (106, "        input_files.append(os.path.join(humann2_dir, 'bowtie2_build.done'))")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/Struo, file=Snakefile
context_key: ["if config['keep_intermediate'] == True", "if not config['databases']['humann2_diamond'].startswith('Skip')"]
    (67, 'def all_which_input(wildcards):')
    (68, '    input_files = []')
    (69, '    # kraken2')
    (70, "    if not config['databases']['kraken2'].startswith('Skip'):")
    (71, "        if config['keep_intermediate'] == True:")
    (72, "            x = expand(kraken2_dir + 'added/{sample}.done',")
    (73, "                       sample = config['samples_unique'])")
    (74, '            input_files += x\\t    ')
    (75, "        input_files.append(os.path.join(kraken2_dir, 'hash.k2d'))")
    (76, "        input_files.append(os.path.join(kraken2_dir, 'opts.k2d'))")
    (77, "        input_files.append(os.path.join(kraken2_dir, 'taxo.k2d'))")
    (78, "        input_files.append(os.path.join(kraken2_dir, 'seqid2taxid.map'))")
    (79, '')
    (80, '    # bracken')
    (81, "    if (not config['databases']['kraken2'].startswith('Skip') and")
    (82, "        not config['databases']['bracken'].startswith('Skip')):")
    (83, "    \\tx = expand(os.path.join(kraken2_dir, 'database{read_len}mers.kraken'),")
    (84, "\\t           read_len = config['params']['bracken_build_read_lens'])")
    (85, '        input_files += x')
    (86, '')
    (87, '    # humann2')
    (88, "    if not config['databases']['humann2_bowtie2'].startswith('Skip') and \\\\")
    (89, "       not config['databases']['humann2_diamond'].startswith('Skip'):")
    (90, "        if config['keep_intermediate'] == True:")
    (91, "            if config['use_ancient'] == True:                ")
    (92, "                x = ancient(expand(annot_dir + 'prodigal/{sample}/annot.fna.gz',")
    (93, "                                   sample = config['samples_unique']))")
    (94, '            else:\\t\\t\\t   ')
    (95, "                x = expand(annot_dir + 'prodigal/{sample}/annot.fna.gz',")
    (96, "                           sample = config['samples_unique'])")
    (97, '            input_files += x')
    (98, "            if config['use_ancient'] == True:                \\t    ")
    (99, "                x = ancient(expand(annot_dir + 'prodigal/{sample}/annot.faa.gz',")
    (100, "                                   sample = config['samples_unique']))")
    (101, '            else:')
    (102, "                x = expand(annot_dir + 'prodigal/{sample}/annot.faa.gz',")
    (103, "                           sample = config['samples_unique'])")
    (104, '            input_files += x')
    (105, "    if not config['databases']['humann2_bowtie2'].startswith('Skip'):")
    (106, "        input_files.append(os.path.join(humann2_dir, 'bowtie2_build.done'))")
    (107, "    if not config['databases']['humann2_diamond'].startswith('Skip'):\\t")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=leylabmpi/Struo, file=Snakefile
context_key: ["if config['keep_intermediate'] == True"]
    (67, 'def all_which_input(wildcards):')
    (68, '    input_files = []')
    (69, '    # kraken2')
    (70, "    if not config['databases']['kraken2'].startswith('Skip'):")
    (71, "        if config['keep_intermediate'] == True:")
    (72, "            x = expand(kraken2_dir + 'added/{sample}.done',")
    (73, "                       sample = config['samples_unique'])")
    (74, '            input_files += x\\t    ')
    (75, "        input_files.append(os.path.join(kraken2_dir, 'hash.k2d'))")
    (76, "        input_files.append(os.path.join(kraken2_dir, 'opts.k2d'))")
    (77, "        input_files.append(os.path.join(kraken2_dir, 'taxo.k2d'))")
    (78, "        input_files.append(os.path.join(kraken2_dir, 'seqid2taxid.map'))")
    (79, '')
    (80, '    # bracken')
    (81, "    if (not config['databases']['kraken2'].startswith('Skip') and")
    (82, "        not config['databases']['bracken'].startswith('Skip')):")
    (83, "    \\tx = expand(os.path.join(kraken2_dir, 'database{read_len}mers.kraken'),")
    (84, "\\t           read_len = config['params']['bracken_build_read_lens'])")
    (85, '        input_files += x')
    (86, '')
    (87, '    # humann2')
    (88, "    if not config['databases']['humann2_bowtie2'].startswith('Skip') and \\\\")
    (89, "       not config['databases']['humann2_diamond'].startswith('Skip'):")
    (90, "        if config['keep_intermediate'] == True:")
    (91, "            if config['use_ancient'] == True:                ")
    (92, "                x = ancient(expand(annot_dir + 'prodigal/{sample}/annot.fna.gz',")
    (93, "                                   sample = config['samples_unique']))")
    (94, '            else:\\t\\t\\t   ')
    (95, "                x = expand(annot_dir + 'prodigal/{sample}/annot.fna.gz',")
    (96, "                           sample = config['samples_unique'])")
    (97, '            input_files += x')
    (98, "            if config['use_ancient'] == True:                \\t    ")
    (99, "                x = ancient(expand(annot_dir + 'prodigal/{sample}/annot.faa.gz',")
    (100, "                                   sample = config['samples_unique']))")
    (101, '            else:')
    (102, "                x = expand(annot_dir + 'prodigal/{sample}/annot.faa.gz',")
    (103, "                           sample = config['samples_unique'])")
    (104, '            input_files += x')
    (105, "    if not config['databases']['humann2_bowtie2'].startswith('Skip'):")
    (106, "        input_files.append(os.path.join(humann2_dir, 'bowtie2_build.done'))")
    (107, "    if not config['databases']['humann2_diamond'].startswith('Skip'):\\t")
    (108, "        input_files.append(os.path.join(humann2_dir,'all_genes_annot.dmnd'))")
    (109, '    ')
    (110, '    # ret')
    (111, '    return input_files')
    (112, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dorinemerlat/EXOGAP, file=workflow/rules/prepare_data.smk
context_key: ['if config["protein_main_sets"]["small_uniprot_set"] != None']
    (143, 'def chose_main_protein_data():')
    (144, '    files = list()')
    (145, '    if config["protein_main_sets"]["small_uniprot_set"] != None:')
    (146, '        files.append("resources/proteins/small_uniprot_protein_set.fa")')
    (147, '    if config["protein_main_sets"]["large_uniprot_set"] != None:')
    (148, '        files.append("resources/proteins/large_uniprot_protein_set.fa")')
    (149, '    if config["protein_main_sets"]["local_set"] is True:')
    (150, '        files = files + get_data("resources/proteins/proteins_for_main_protein_set", True)')
    (151, '    print(files)')
    (152, '    return files')
    (153, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=dorinemerlat/EXOGAP, file=workflow/rules/prepare_data.smk
context_key: ['if config["protein_parallele_set"]["uniprot_parallele_set"] != None']
    (191, 'def chose_parallele_protein_data():')
    (192, '    files = list()')
    (193, '    if config["protein_parallele_set"]["uniprot_parallele_set"] != None:')
    (194, '        files.append("resources/proteins/parallele_uniprot_protein_set.fa")')
    (195, '    if config["protein_parallele_set"]["local_set"] is True:')
    (196, '        files = files + get_data("resources/proteins/proteins_for_parallele_protein_set", True)')
    (197, '    print(files)')
    (198, '    return files')
    (199, '')
    (200, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/kraken2_classification, file=Snakefile
context_key: ["if config[\\'run_bracken\\']"]
    (46, 'def get_sample_reads(sample_file):')
    (47, '    sample_reads = {}')
    (48, "    paired_end = \\'\\'")
    (49, '    with open(sample_file) as sf:')
    (50, '        for l in sf.readlines():')
    (51, '            s = l.strip().split("\\\\t")')
    (52, "            if len(s) == 1 or s[0] == \\'Sample\\' or s[0] == \\'#Sample\\' or s[0].startswith(\\'#\\'):")
    (53, '                continue')
    (54, '            sample = s[0]')
    (55, '            # paired end specified')
    (56, '            if (len(s)==3):')
    (57, '                reads = [s[1],s[2]]')
    (58, "                if paired_end != \\'\\' and not paired_end:")
    (59, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (60, '                paired_end = True')
    (61, '            # single end specified')
    (62, '            elif len(s)==2:')
    (63, '                reads=s[1]')
    (64, "                if paired_end != \\'\\' and paired_end:")
    (65, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (66, '                paired_end = False')
    (67, '            if sample in sample_reads:')
    (68, '                raise ValueError("Non-unique sample encountered!")')
    (69, '            sample_reads[sample] = reads')
    (70, '    return (sample_reads, paired_end)')
    (71, '')
    (72, '')
    (73, '# read in sample info and reads from the sample_file')
    (74, "sample_reads, paired_end = get_sample_reads(config[\\'sample_file\\'])")
    (75, 'if paired_end:')
    (76, "    paired_string = \\'--paired\\'")
    (77, 'else:')
    (78, "    paired_string = \\'\\'")
    (79, 'sample_names = sample_reads.keys()')
    (80, '')
    (81, '# also read in desired confidence threshold for Kraken')
    (82, "if not \\'confidence_threshold\\' in config:")
    (83, "    config[\\'confidence_threshold\\'] = 0.0")
    (84, "confidence_threshold = config[\\'confidence_threshold\\']")
    (85, '')
    (86, '# extra specified files to generate from the config file')
    (87, 'extra_run_list =[]')
    (88, '# add bracken to extra files if running it')
    (89, "if config[\\'run_bracken\\']:")
    (90, "    extra_run_list.append(\\'bracken\\')")
    (91, "    extra_run_list.append(\\'krakenonly_processed\\')")
    (92, '    downstream_processing_input = expand(join(outdir, "classification/{samp}.krak_bracken_species.report"), samp=sample_names)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/kraken2_classification, file=Snakefile
context_key: ["if config[\\'extract_unmapped\\']", 'if paired_end']
    (46, 'def get_sample_reads(sample_file):')
    (47, '    sample_reads = {}')
    (48, "    paired_end = \\'\\'")
    (49, '    with open(sample_file) as sf:')
    (50, '        for l in sf.readlines():')
    (51, '            s = l.strip().split("\\\\t")')
    (52, "            if len(s) == 1 or s[0] == \\'Sample\\' or s[0] == \\'#Sample\\' or s[0].startswith(\\'#\\'):")
    (53, '                continue')
    (54, '            sample = s[0]')
    (55, '            # paired end specified')
    (56, '            if (len(s)==3):')
    (57, '                reads = [s[1],s[2]]')
    (58, "                if paired_end != \\'\\' and not paired_end:")
    (59, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (60, '                paired_end = True')
    (61, '            # single end specified')
    (62, '            elif len(s)==2:')
    (63, '                reads=s[1]')
    (64, "                if paired_end != \\'\\' and paired_end:")
    (65, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (66, '                paired_end = False')
    (67, '            if sample in sample_reads:')
    (68, '                raise ValueError("Non-unique sample encountered!")')
    (69, '            sample_reads[sample] = reads')
    (70, '    return (sample_reads, paired_end)')
    (71, '')
    (72, '')
    (73, '# read in sample info and reads from the sample_file')
    (74, "sample_reads, paired_end = get_sample_reads(config[\\'sample_file\\'])")
    (75, 'if paired_end:')
    (76, "    paired_string = \\'--paired\\'")
    (77, 'else:')
    (78, "    paired_string = \\'\\'")
    (79, 'sample_names = sample_reads.keys()')
    (80, '')
    (81, '# also read in desired confidence threshold for Kraken')
    (82, "if not \\'confidence_threshold\\' in config:")
    (83, "    config[\\'confidence_threshold\\'] = 0.0")
    (84, "confidence_threshold = config[\\'confidence_threshold\\']")
    (85, '')
    (86, '# extra specified files to generate from the config file')
    (87, 'extra_run_list =[]')
    (88, '# add bracken to extra files if running it')
    (89, "if config[\\'run_bracken\\']:")
    (90, "    extra_run_list.append(\\'bracken\\')")
    (91, "    extra_run_list.append(\\'krakenonly_processed\\')")
    (92, '    downstream_processing_input = expand(join(outdir, "classification/{samp}.krak_bracken_species.report"), samp=sample_names)')
    (93, 'else:')
    (94, '    downstream_processing_input = expand(join(outdir, "classification/{samp}.krak.report"), samp=sample_names)')
    (95, '')
    (96, '# do we want to extract unmapped reads?')
    (97, "if config[\\'extract_unmapped\\']:")
    (98, '    if paired_end:')
    (99, "        extra_run_list.append(\\'unmapped_paired\\')")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/kraken2_classification, file=Snakefile
context_key: ["if config[\\'extract_unmapped\\']", 'else']
    (46, 'def get_sample_reads(sample_file):')
    (47, '    sample_reads = {}')
    (48, "    paired_end = \\'\\'")
    (49, '    with open(sample_file) as sf:')
    (50, '        for l in sf.readlines():')
    (51, '            s = l.strip().split("\\\\t")')
    (52, "            if len(s) == 1 or s[0] == \\'Sample\\' or s[0] == \\'#Sample\\' or s[0].startswith(\\'#\\'):")
    (53, '                continue')
    (54, '            sample = s[0]')
    (55, '            # paired end specified')
    (56, '            if (len(s)==3):')
    (57, '                reads = [s[1],s[2]]')
    (58, "                if paired_end != \\'\\' and not paired_end:")
    (59, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (60, '                paired_end = True')
    (61, '            # single end specified')
    (62, '            elif len(s)==2:')
    (63, '                reads=s[1]')
    (64, "                if paired_end != \\'\\' and paired_end:")
    (65, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (66, '                paired_end = False')
    (67, '            if sample in sample_reads:')
    (68, '                raise ValueError("Non-unique sample encountered!")')
    (69, '            sample_reads[sample] = reads')
    (70, '    return (sample_reads, paired_end)')
    (71, '')
    (72, '')
    (73, '# read in sample info and reads from the sample_file')
    (74, "sample_reads, paired_end = get_sample_reads(config[\\'sample_file\\'])")
    (75, 'if paired_end:')
    (76, "    paired_string = \\'--paired\\'")
    (77, 'else:')
    (78, "    paired_string = \\'\\'")
    (79, 'sample_names = sample_reads.keys()')
    (80, '')
    (81, '# also read in desired confidence threshold for Kraken')
    (82, "if not \\'confidence_threshold\\' in config:")
    (83, "    config[\\'confidence_threshold\\'] = 0.0")
    (84, "confidence_threshold = config[\\'confidence_threshold\\']")
    (85, '')
    (86, '# extra specified files to generate from the config file')
    (87, 'extra_run_list =[]')
    (88, '# add bracken to extra files if running it')
    (89, "if config[\\'run_bracken\\']:")
    (90, "    extra_run_list.append(\\'bracken\\')")
    (91, "    extra_run_list.append(\\'krakenonly_processed\\')")
    (92, '    downstream_processing_input = expand(join(outdir, "classification/{samp}.krak_bracken_species.report"), samp=sample_names)')
    (93, 'else:')
    (94, '    downstream_processing_input = expand(join(outdir, "classification/{samp}.krak.report"), samp=sample_names)')
    (95, '')
    (96, '# do we want to extract unmapped reads?')
    (97, "if config[\\'extract_unmapped\\']:")
    (98, '    if paired_end:')
    (99, "        extra_run_list.append(\\'unmapped_paired\\')")
    (100, '    else:')
    (101, "        extra_run_list.append(\\'unmapped_single\\')")
    (102, '')
    (103, '# additional outputs determined by whats specified in the readme')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/kraken2_classification, file=Snakefile
context_key: ["if config[\\'database\\'] in [\\'/labs/asbhatt/data/program_indices/kraken2/kraken_custom_feb2019/genbank_genome_chromosome_scaffold\\'"]
    (46, 'def get_sample_reads(sample_file):')
    (47, '    sample_reads = {}')
    (48, "    paired_end = \\'\\'")
    (49, '    with open(sample_file) as sf:')
    (50, '        for l in sf.readlines():')
    (51, '            s = l.strip().split("\\\\t")')
    (52, "            if len(s) == 1 or s[0] == \\'Sample\\' or s[0] == \\'#Sample\\' or s[0].startswith(\\'#\\'):")
    (53, '                continue')
    (54, '            sample = s[0]')
    (55, '            # paired end specified')
    (56, '            if (len(s)==3):')
    (57, '                reads = [s[1],s[2]]')
    (58, "                if paired_end != \\'\\' and not paired_end:")
    (59, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (60, '                paired_end = True')
    (61, '            # single end specified')
    (62, '            elif len(s)==2:')
    (63, '                reads=s[1]')
    (64, "                if paired_end != \\'\\' and paired_end:")
    (65, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (66, '                paired_end = False')
    (67, '            if sample in sample_reads:')
    (68, '                raise ValueError("Non-unique sample encountered!")')
    (69, '            sample_reads[sample] = reads')
    (70, '    return (sample_reads, paired_end)')
    (71, '')
    (72, '')
    (73, '# read in sample info and reads from the sample_file')
    (74, "sample_reads, paired_end = get_sample_reads(config[\\'sample_file\\'])")
    (75, 'if paired_end:')
    (76, "    paired_string = \\'--paired\\'")
    (77, 'else:')
    (78, "    paired_string = \\'\\'")
    (79, 'sample_names = sample_reads.keys()')
    (80, '')
    (81, '# also read in desired confidence threshold for Kraken')
    (82, "if not \\'confidence_threshold\\' in config:")
    (83, "    config[\\'confidence_threshold\\'] = 0.0")
    (84, "confidence_threshold = config[\\'confidence_threshold\\']")
    (85, '')
    (86, '# extra specified files to generate from the config file')
    (87, 'extra_run_list =[]')
    (88, '# add bracken to extra files if running it')
    (89, "if config[\\'run_bracken\\']:")
    (90, "    extra_run_list.append(\\'bracken\\')")
    (91, "    extra_run_list.append(\\'krakenonly_processed\\')")
    (92, '    downstream_processing_input = expand(join(outdir, "classification/{samp}.krak_bracken_species.report"), samp=sample_names)')
    (93, 'else:')
    (94, '    downstream_processing_input = expand(join(outdir, "classification/{samp}.krak.report"), samp=sample_names)')
    (95, '')
    (96, '# do we want to extract unmapped reads?')
    (97, "if config[\\'extract_unmapped\\']:")
    (98, '    if paired_end:')
    (99, "        extra_run_list.append(\\'unmapped_paired\\')")
    (100, '    else:')
    (101, "        extra_run_list.append(\\'unmapped_single\\')")
    (102, '')
    (103, '# additional outputs determined by whats specified in the readme')
    (104, 'extra_files = {')
    (105, '    "bracken": expand(join(outdir, "classification/{samp}.krak_bracken_species.report"), samp=sample_names),')
    (106, '    "krakenonly_processed": join(outdir, \\\'processed_results_krakenonly/plots/classified_taxonomy_barplot_species.pdf\\\'),')
    (107, '    "unmapped_paired": expand(join(outdir, "unmapped_reads/{samp}_unmapped_1.fq"), samp=sample_names),')
    (108, '    "unmapped_single": expand(join(outdir, "unmapped_reads/{samp}_unmapped.fq"), samp=sample_names),')
    (109, '    "barplot": join(outdir, "plots/taxonomic_composition.pdf"),')
    (110, '    "krona": expand(join(outdir, "krona/{samp}.html"), samp = sample_names),')
    (111, '    "mpa_heatmap": join(outdir, "mpa_reports/merge_metaphlan_heatmap.png"),')
    (112, '    "biom_file": join(outdir, "table.biom"),')
    (113, '}')
    (114, 'run_extra_all_outputs = [extra_files[f] for f in extra_run_list]')
    (115, '# print("run Extra files: " + str(run_extra_all_outputs))')
    (116, '')
    (117, '# set some resource requirements')
    (118, "if config[\\'database\\'] in [\\'/labs/asbhatt/data/program_indices/kraken2/kraken_custom_feb2019/genbank_genome_chromosome_scaffold\\',")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/kraken2_classification, file=Snakefile
context_key: ["if config[\\'taxonomic_level\\'] != \\'S\\'"]
    (46, 'def get_sample_reads(sample_file):')
    (47, '    sample_reads = {}')
    (48, "    paired_end = \\'\\'")
    (49, '    with open(sample_file) as sf:')
    (50, '        for l in sf.readlines():')
    (51, '            s = l.strip().split("\\\\t")')
    (52, "            if len(s) == 1 or s[0] == \\'Sample\\' or s[0] == \\'#Sample\\' or s[0].startswith(\\'#\\'):")
    (53, '                continue')
    (54, '            sample = s[0]')
    (55, '            # paired end specified')
    (56, '            if (len(s)==3):')
    (57, '                reads = [s[1],s[2]]')
    (58, "                if paired_end != \\'\\' and not paired_end:")
    (59, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (60, '                paired_end = True')
    (61, '            # single end specified')
    (62, '            elif len(s)==2:')
    (63, '                reads=s[1]')
    (64, "                if paired_end != \\'\\' and paired_end:")
    (65, "                    sys.exit(\\'All samples must be paired or single ended.\\')")
    (66, '                paired_end = False')
    (67, '            if sample in sample_reads:')
    (68, '                raise ValueError("Non-unique sample encountered!")')
    (69, '            sample_reads[sample] = reads')
    (70, '    return (sample_reads, paired_end)')
    (71, '')
    (72, '')
    (73, '# read in sample info and reads from the sample_file')
    (74, "sample_reads, paired_end = get_sample_reads(config[\\'sample_file\\'])")
    (75, 'if paired_end:')
    (76, "    paired_string = \\'--paired\\'")
    (77, 'else:')
    (78, "    paired_string = \\'\\'")
    (79, 'sample_names = sample_reads.keys()')
    (80, '')
    (81, '# also read in desired confidence threshold for Kraken')
    (82, "if not \\'confidence_threshold\\' in config:")
    (83, "    config[\\'confidence_threshold\\'] = 0.0")
    (84, "confidence_threshold = config[\\'confidence_threshold\\']")
    (85, '')
    (86, '# extra specified files to generate from the config file')
    (87, 'extra_run_list =[]')
    (88, '# add bracken to extra files if running it')
    (89, "if config[\\'run_bracken\\']:")
    (90, "    extra_run_list.append(\\'bracken\\')")
    (91, "    extra_run_list.append(\\'krakenonly_processed\\')")
    (92, '    downstream_processing_input = expand(join(outdir, "classification/{samp}.krak_bracken_species.report"), samp=sample_names)')
    (93, 'else:')
    (94, '    downstream_processing_input = expand(join(outdir, "classification/{samp}.krak.report"), samp=sample_names)')
    (95, '')
    (96, '# do we want to extract unmapped reads?')
    (97, "if config[\\'extract_unmapped\\']:")
    (98, '    if paired_end:')
    (99, "        extra_run_list.append(\\'unmapped_paired\\')")
    (100, '    else:')
    (101, "        extra_run_list.append(\\'unmapped_single\\')")
    (102, '')
    (103, '# additional outputs determined by whats specified in the readme')
    (104, 'extra_files = {')
    (105, '    "bracken": expand(join(outdir, "classification/{samp}.krak_bracken_species.report"), samp=sample_names),')
    (106, '    "krakenonly_processed": join(outdir, \\\'processed_results_krakenonly/plots/classified_taxonomy_barplot_species.pdf\\\'),')
    (107, '    "unmapped_paired": expand(join(outdir, "unmapped_reads/{samp}_unmapped_1.fq"), samp=sample_names),')
    (108, '    "unmapped_single": expand(join(outdir, "unmapped_reads/{samp}_unmapped.fq"), samp=sample_names),')
    (109, '    "barplot": join(outdir, "plots/taxonomic_composition.pdf"),')
    (110, '    "krona": expand(join(outdir, "krona/{samp}.html"), samp = sample_names),')
    (111, '    "mpa_heatmap": join(outdir, "mpa_reports/merge_metaphlan_heatmap.png"),')
    (112, '    "biom_file": join(outdir, "table.biom"),')
    (113, '}')
    (114, 'run_extra_all_outputs = [extra_files[f] for f in extra_run_list]')
    (115, '# print("run Extra files: " + str(run_extra_all_outputs))')
    (116, '')
    (117, '# set some resource requirements')
    (118, "if config[\\'database\\'] in [\\'/labs/asbhatt/data/program_indices/kraken2/kraken_custom_feb2019/genbank_genome_chromosome_scaffold\\',")
    (119, "                          \\'/labs/asbhatt/data/program_indices/kraken2/kraken_custom_jan2020/genbank_genome_chromosome_scaffold\\',")
    (120, "                          \\'/labs/asbhatt/data/program_indices/kraken2/kraken_custom_dec2021/genbank_genome_chromosome_scaffold\\',")
    (121, "                          \\'/oak/stanford/scg/lab_asbhatt/data/program_indices/kraken2/kraken_custom_feb2019/genbank_genome_chromosome_scaffold\\',")
    (122, "                          \\'/oak/stanford/scg/lab_asbhatt/data/program_indices/kraken2/kraken_custom_jan2020/genbank_genome_chromosome_scaffold\\',")
    (123, "                          \\'/oak/stanford/scg/lab_asbhatt/data/program_indices/kraken2/kraken_custom_dec2021/genbank_genome_chromosome_scaffold\\']:")
    (124, '    kraken_memory = 256')
    (125, '    kraken_threads = 8')
    (126, 'else:')
    (127, '    kraken_memory = 64')
    (128, '    kraken_threads = 4')
    (129, '')
    (130, '# Taxonomic level can only be species right now. A future fix could look at the ')
    (131, "# output file name of Bracken and adjust based on taxonomic level. But I don\\'t think")
    (132, '# anyone uses anything other than species')
    (133, "if config[\\'taxonomic_level\\'] != \\'S\\':")
    (134, "    sys.exit(\\'taxonomic_level setting can only be S\\')")
    (135, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khayer/rna_seq_standard_pipeline, file=workflow/rules/common.smk
context_key: ['if config["single_end"]']
    (9, 'def get_raw_reads(wildcards):')
    (10, '    run_ids = samples[samples["Run"] == wildcards.sample]["Run"].tolist()')
    (11, '    out = []')
    (12, '    if config["single_end"]:')
    (13, '        for r in run_ids:')
    (14, '            out.extend ( ')
    (15, '                expand ( [')
    (16, '                    "reads/{sample}.fastq.gz" ')
    (17, '                ], sample = r')
    (18, '                )')
    (19, '            )')
    (20, '    else:')
    (21, '        for r in run_ids:')
    (22, '            out.extend ( ')
    (23, '                expand ( [')
    (24, '                    "reads/{sample}_1.fastq.gz" , "reads/{sample}_2.fastq.gz"')
    (25, '                ], sample = r')
    (26, '                )')
    (27, '            )')
    (28, '    return out')
    (29, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khayer/rna_seq_standard_pipeline, file=workflow/rules/common.smk
context_key: ['if config["single_end"]']
    (30, 'def get_trimmed_reads(wildcards):')
    (31, '    run_ids = samples[samples["sample_name"] == wildcards.sample_name]["Run"].tolist()')
    (32, '    out = []')
    (33, '    if config["single_end"]:')
    (34, '        for r in run_ids:')
    (35, '            out.extend ( ')
    (36, '                expand ( [')
    (37, '                    "results/trimmed/{sample}_trim.fastq.gz" ')
    (38, '                ], sample = r')
    (39, '                )')
    (40, '            )')
    (41, '    else:')
    (42, '        for r in run_ids:')
    (43, '            out.extend ( ')
    (44, '                expand ( [')
    (45, '                    "results/trimmed/{sample}_trim_1.fastq.gz", "results/trimmed/{sample}_trim_2.fastq.gz"')
    (46, '                ], sample = r')
    (47, '                )')
    (48, '            )')
    (49, '    return out')
    (50, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khayer/rna_seq_standard_pipeline, file=workflow/rules/common.smk
context_key: ['if config["single_end"]']
    (51, 'def get_trimmed_reads2(wildcards):')
    (52, '    run_ids = samples[samples["Run"] == wildcards.sample]["Run"].tolist()')
    (53, '    out = []')
    (54, '    if config["single_end"]:')
    (55, '        for r in run_ids:')
    (56, '            out.extend ( ')
    (57, '                expand ( [')
    (58, '                    "results/trimmed/{sample}_trim.fastq.gz" ')
    (59, '                ], sample = r')
    (60, '                )')
    (61, '            )')
    (62, '    else:')
    (63, '        for r in run_ids:')
    (64, '            out.extend ( ')
    (65, '                expand ( [')
    (66, '                    "results/trimmed/{sample}_trim_1.fastq.gz", "results/trimmed/{sample}_trim_2.fastq.gz"')
    (67, '                ], sample = r')
    (68, '                )')
    (69, '            )')
    (70, '    return out')
    (71, '')
    (72, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=khayer/rna_seq_standard_pipeline, file=workflow/rules/common.smk
context_key: ['if config["single_end"]']
    (112, 'def all_input(wildcards):')
    (113, '')
    (114, '    wanted_input = []')
    (115, '')
    (116, '    ## QC with fastQC and multiQC')
    (117, '    #wanted_input.extend([')
    (118, '    #    "results/qc/multiqc/multiqc.html"')
    (119, '    #])')
    (120, '')
    (121, '    # trimming')
    (122, '    for sample in samples.index:')
    (123, '        if config["single_end"]:')
    (124, '            wanted_input.extend(')
    (125, '                expand (')
    (126, '                        [')
    (127, '                            "results/trimmed/{sample}_trim.fastq.gz",')
    (128, '                            "results/fastqc/{sample}_trim_fastqc.zip"')
    (129, '                        ],')
    (130, '                        sample = sample')
    (131, '                    )')
    (132, '                )')
    (133, '        else:')
    (134, '            for read in ["1","2"]:')
    (135, '                wanted_input.extend(')
    (136, '                        expand (')
    (137, '                            [')
    (138, '                                "results/trimmed/{sample}_trim_{read}.fastq.gz",')
    (139, '                                "results/fastqc/{sample}_trim_{read}_fastqc.zip"')
    (140, '                            ],')
    (141, '                            sample = sample, read = read')
    (142, '                        )')
    (143, '                    )')
    (144, '')
    (145, '    for sample in samples.sample_name:')
    (146, '        if config["single_end"]:')
    (147, '            wanted_input.extend(')
    (148, '                expand (')
    (149, '                        [')
    (150, '                            "results/coverage/{sample}_CPM.bw"')
    (151, '                        ],')
    (152, '                        sample = sample')
    (153, '                    )')
    (154, '                )')
    (155, '        else:')
    (156, '            for read in ["1","2"]:')
    (157, '                wanted_input.extend(')
    (158, '                        expand (')
    (159, '                            [')
    (160, '                                "results/coverage/{sample}_fwd_CPM.bw", ')
    (161, '                                "results/coverage/{sample}_rev_CPM.bw"')
    (162, '                            ],')
    (163, '                            sample = sample, read = read')
    (164, '                        )')
    (165, '                    )')
    (166, '')
    (167, '    for sn in samples.sample_name:')
    (168, '        wanted_input.extend(')
    (169, '                    expand (')
    (170, '                        [')
    (171, '                            ')
    (172, '                            "results/quant/salmon_quant_{sample_name}/quant.sf",')
    (173, '                            #"results/splicing/majiq/majiq_{sample_name}/build_{sample_name}/{sample_name}.sj",')
    (174, '                            "results/mapped/{sample_name}_Aligned.sortedByCoord.out_junc.bed",')
    (175, '                            "results/mapped/{sample_name}_Aligned.sortedByCoord.out_genes.ent",')
    (176, '                            "results/mapped/{sample_name}_selected_genes.bam"')
    (177, '                            ')
    (178, '                        ],')
    (179, '                        sample_name = sn')
    (180, '                    )')
    (181, '                )')
    (182, '    ## get merged files')
    (183, '    wanted_input.extend(["results/quant/all_star_junctions.csv"])')
    (184, '    wanted_input.extend(["results/quant/all_gene_tmps.csv"])')
    (185, '    return wanted_input')
    (186, '    ')
    (187, '')
    (188, '')
    (189, '')
    (190, '')
    (191, '### run multiqc at the very end')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=pughlab/cfMeDIP-seq-analysis-pipeline, file=Snakefile
context_key: ["if config[\\'data\\'][\\'cohorts\\'][cohort_name][\\'active\\'"]
    (66, 'def get_all_samples(cohort=None):')
    (67, '    """Retrieves all samples to be processed.')
    (68, '    Does so by calling get_cohort_data, and therefore filters out excluded_cases.')
    (69, '')
    (70, '    Keyword arguments:')
    (71, '        cohort -- Name of a cohort, OPTIONAL. If not specified, returns all samples')
    (72, '                  across all cohorts.')
    (73, '    """')
    (74, '    all_samples = pd.concat([')
    (75, '        get_cohort_data(cohort_name).assign(cohort_name = cohort_name)')
    (76, '        for cohort_name')
    (77, "        in config[\\'data\\'][\\'cohorts\\']")
    (78, "        if config[\\'data\\'][\\'cohorts\\'][cohort_name][\\'active\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bwinnacott/SNP-Pipeline, file=workflow/rules/utilities.smk
context_key: ["if config[\\'use_haplotypecaller\\']"]
    (41, 'def get_callers():')
    (42, '    callers_set = []')
    (43, "    if config[\\'use_haplotypecaller\\']:")
    (44, "        callers_set.append(\\'HaplotypeCaller\\')")
    (45, "    if config[\\'use_freebayes\\']:")
    (46, "        callers_set.append(\\'Freebayes\\')")
    (47, "    if config[\\'use_bcftools\\']:")
    (48, "        callers_set.append(\\'Bcftools\\')")
    (49, '    ')
    (50, '    return callers_set')
    (51, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sabiqali/strline, file=Snakefile
context_key: ['if config[wildcards.sample][\\\'fastq\\\'] == "none"']
    (2, 'def get_fastq_for_sample(wildcards):')
    (3, '    if config[wildcards.sample][\\\'fastq\\\'] == "none":')
    (4, '        return "fastq/" + wildcards.sample + "." + wildcards.basecall_config + ".fastq"')
    (5, '    else:')
    (6, "        return config[wildcards.sample][\\'fastq\\']")
    (7, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sabiqali/strline, file=Snakefile
context_key: ['if config[wildcards.sample][\\\'fastq\\\'] == "none"']
    (70, 'def get_sequencing_summary_for_sample(wildcards):')
    (71, '    if config[wildcards.sample][\\\'fastq\\\'] == "none":')
    (72, '        return get_basecalled_dir(wildcards) + "sequencing_summary.txt"')
    (73, '    else:')
    (74, "        return config[wildcards.sample][\\'summary\\']")
    (75, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aidanfoo96/MINUUR, file=workflow/rules/MapCoverageBin.smk
context_key: ['if config["MetagenomeAssm"]["UseKrakenExtracted"]']
    (7, 'def megahit_input_1(wildcards): ')
    (8, '    if config["MetagenomeAssm"]["UseKrakenExtracted"]:             ')
    (9, '        return("../results/kraken_taxon_extract/{sample}_extract_1.fastq")')
    (10, '    elif config["input_type"]["fastq"]:')
    (11, '        return("../results/unmapped_fastq_ffq/{sample}_unmapped_1.fastq")')
    (12, '    elif config["input_type"]["bam"]: ')
    (13, '        return("../results/unmapped_fastq/{sample}_unmapped_1.fastq")')
    (14, '    ')
    (15, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aidanfoo96/MINUUR, file=workflow/rules/MapCoverageBin.smk
context_key: ['if config["MetagenomeAssm"]["UseKrakenExtracted"]']
    (16, 'def megahit_input_2(wildcards): ')
    (17, '    if config["MetagenomeAssm"]["UseKrakenExtracted"]: ')
    (18, '        return("../results/kraken_taxon_extract/{sample}_extract_2.fastq")')
    (19, '    elif config["input_type"]["fastq"]:')
    (20, '        return("../results/unmapped_fastq_ffq/{sample}_unmapped_2.fastq")')
    (21, '    elif config["input_type"]["bam"]: ')
    (22, '        return("../results/unmapped_fastq/{sample}_unmapped_2.fastq")')
    (23, '')
    (24, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aidanfoo96/MINUUR, file=workflow/rules/03_ReadClassification.smk
context_key: ['if config["ProcessBam"]["FromFastq"]']
    (7, 'def classification_input_r1(wildcards): ')
    (8, '    if config["ProcessBam"]["FromFastq"]:')
    (9, '        return("../results/unmapped_fastq_ffq/{sample}_unmapped_1.fastq")')
    (10, '            ')
    (11, '    else: ')
    (12, '        return("../results/unmapped_fastq/{sample}_unmapped_1.fastq")')
    (13, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aidanfoo96/MINUUR, file=workflow/rules/03_ReadClassification.smk
context_key: ['if config["ProcessBam"]["FromFastq"]']
    (14, 'def classification_input_r2(wildcards): ')
    (15, '    if config["ProcessBam"]["FromFastq"]:')
    (16, '        return("../results/unmapped_fastq_ffq/{sample}_unmapped_2.fastq")')
    (17, '            ')
    (18, '    else: ')
    (19, '        return("../results/unmapped_fastq/{sample}_unmapped_2.fastq")')
    (20, '                ')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aidanfoo96/MINUUR, file=workflow/rules/03_ReadClassification.smk
context_key: ['if config["ProcessBam"]["FromFastq"]']
    (21, 'def classification_sum_input(wildcards): ')
    (22, '    input_list = []')
    (23, '    if config["ProcessBam"]["FromFastq"]: ')
    (24, '        input_list.extend(')
    (25, '            expand(')
    (26, '                [')
    (27, '                    "../results/alignment_stats_ffq/concatenated_alignment_statistics.txt",')
    (28, '')
    (29, '                ]')
    (30, '            )')
    (31, '        )')
    (32, '    else: ')
    (33, '        input_list.extend(')
    (34, '            expand(')
    (35, '                [')
    (36, '                    "../results/alignment_stats/concatenated_alignment_statistics.txt",')
    (37, '                ],')
    (38, '')
    (39, '            )')
    (40, '        )')
    (41, '    ')
    (42, '    return(input_list)')
    (43, '')
    (44, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aidanfoo96/MINUUR, file=workflow/rules/ReadFunction.smk
context_key: ['if config["input_type"]["fastq"]']
    (5, 'def get_unmap_fastq_1(wildcards): ')
    (6, '    if config["input_type"]["fastq"]: ')
    (7, '        return("../results/unmapped_fastq_ffq/{sample}_unmapped_1.fastq")')
    (8, '    elif config["input_type"]["bam"]: ')
    (9, '        return("../results/unmapped_fastq/{sample}_unmapped_1.fastq")')
    (10, '')
    (11, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aidanfoo96/MINUUR, file=workflow/rules/ReadFunction.smk
context_key: ['if config["input_type"]["fastq"]']
    (12, 'def get_unmap_fastq_2(wildcards): ')
    (13, '    if config["input_type"]["fastq"]: ')
    (14, '        return("../results/unmapped_fastq_ffq/{sample}_unmapped_1.fastq")')
    (15, '    elif config["input_type"]["bam"]: ')
    (16, '        return("../results/unmapped_fastq/{sample}_unmapped_1.fastq")')
    (17, '')
    (18, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aidanfoo96/MINUUR, file=workflow/rules/Functions.smk
context_key: ['if config["QC"]["Activate"]']
    (5, 'def RunPipeline(wildcards):')
    (6, '    ')
    (7, '    final_input = []')
    (8, '    ')
    (9, '    if config["QC"]["Activate"]: ')
    (10, '        final_input.extend(')
    (11, '            expand(')
    (12, '                [')
    (13, '                    "../results/.input.check",')
    (14, '                    "../results/qc/fastqc/{sample}_{num}_fastqc.zip",')
    (15, '                    "../results/qc/trimmed_fastq/{sample}_trimmed_1.fastq",')
    (16, '                    "../results/qc/trimmed_fastq/{sample}_trimmed_2.fastq",')
    (17, '                    "../results/qc/fastqc_trimmed/{sample}_{num}.html", ')
    (18, '                    "../results/aligned_bam/{sample}_sorted.bam",')
    (19, '                ],')
    (20, '                sample = samples,')
    (21, '                num = [1, 2],')
    (22, '            )')
    (23, '        )')
    (24, '')
    (25, '    if config["ProcessBam"]["Activate"]: ')
    (26, '        if config["ProcessBam"]["FromFastq"]:')
    (27, '            final_input.extend(')
    (28, '                expand(')
    (29, '                    [')
    (30, '                        "../results/bam_stats_ffq/{sample}_stats.txt",')
    (31, '                        "../results/unmapped_fastq_ffq/{sample}_unmapped_1.fastq",')
    (32, '                        "../results/alignment_stats_ffq/{sample}_align_stats.txt",')
    (33, '                        "../results/alignment_stats_ffq/concatenated_alignment_statistics.txt",')
    (34, '                        "../results/unmapped_bam_ffq/{sample}_unmapped.bam",')
    (35, '                        "../results/unmapped_fastq_ffq/{sample}_unmapped_1.fastq.gz",')
    (36, '')
    (37, '                    ],')
    (38, '                    sample = samples')
    (39, '                )')
    (40, '            )')
    (41, '')
    (42, '        else:')
    (43, '            final_input.extend(')
    (44, '                expand(')
    (45, '                    [')
    (46, '                        "../results/bam_stats/{sample}_stats.txt",')
    (47, '                        "../results/alignment_stats/{sample}_align_stats.txt",')
    (48, '                        "../results/alignment_stats/concatenated_alignment_statistics.txt",')
    (49, '                        "../results/unmapped_bam/{sample}_unmapped.bam",')
    (50, '                        "../results/unmapped_fastq/{sample}_unmapped_1.fastq",')
    (51, '                    ],')
    (52, '                    sample = samples')
    (53, '                )')
    (54, '            )')
    (55, '        ')
    (56, '    ')
    (57, '    if config["KrakenClassification"]["Activate"]:')
    (58, '        final_input.extend(')
    (59, '            expand(')
    (60, '                [')
    (61, '                    "../results/kraken_out/mpa_report/{sample}_report.txt",')
    (62, '                    "../results/kraken_out/report/{sample}_kraken_report",')
    (63, '                ],')
    (64, '                sample = samples')
    (65, '            )')
    (66, '        )')
    (67, '                ')
    (68, '    if config["KrakenSummaries"]["Activate"]:')
    (69, '        final_input.extend(')
    (70, '            expand(')
    (71, '                [')
    (72, '                    "../results/kraken_out/mpa_out/{sample}_mpa_conv_report.txt",')
    (73, '                    "../results/kraken_results/tables/kingdom_table_tidy.txt",')
    (74, '                    "../results/kraken_results/tables/genus_table_tidy.txt",')
    (75, '                    "../results/kraken_results/tables/species_table_tidy.txt",')
    (76, '                    "../results/kraken_results/tables/classified_reads_table.txt",')
    (77, '                    "../results/kraken_results/plots/classified_reads_plot.pdf",')
    (78, '                    "../results/alignment_stats_ffq/tables/aligned_reads_wide.txt",')
    (79, '                    "../results/alignment_stats_ffq/tables/aligned_reads_long.txt",')
    (80, '                    "../results/kraken_results/plots/classified_proportions.pdf",')
    (81, '                    "../results/alignment_stats_ffq/plots/alignment_stat_plot.pdf",')
    (82, '                    "../results/kraken_results/plots/genus_heatmap.pdf", ')
    (83, '                    "../results/kraken_results/classified_summary/{sample}_classified_summary.txt", ')
    (84, '                    "../results/kraken_results/concatenated_kraken_summary.txt",')
    (85, '                    "../results/kraken_results/plots/genus_spatial_plot.pdf", ')
    (86, '                    "../results/kraken_results/plots/species_spatial_plot.pdf",')
    (87, '                ],')
    (88, '                sample = samples')
    (89, '            )')
    (90, '        )')
    (91, '')
    (92, '    if config["ExtractKrakenTaxa"]["Activate"]: ')
    (93, '        final_input.extend(')
    (94, '            expand(')
    (95, '                [')
    (96, '                    "../results/kraken_taxon_extract/{sample}_extract_1.fastq",')
    (97, '                    "../results/kraken_taxon_extract/{sample}_extract_2.fastq",')
    (98, '                ],')
    (99, '                sample = samples')
    (100, '            )')
    (101, '        )')
    (102, '')
    (103, '    if config["BrackenReestimation"]["Activate"]:')
    (104, '        final_input.extend(')
    (105, '            expand(')
    (106, '                [')
    (107, '                    "../results/bracken_reestimation/bracken_out/{sample}_bracken.txt", ')
    (108, '                    "../results/bracken_reestimation/concat_bracken_out/concatenated_bracken_report.txt",')
    (109, '                    "../results/bracken_reestimation/plots/stratified_species_heatmaps.pdf",')
    (110, '                ],')
    (111, '                sample = samples')
    (112, '            )')
    (113, '        )')
    (114, '')
    (115, '    if config["MetaphlanClassification"]["Activate"]:')
    (116, '        final_input.extend(')
    (117, '            expand(')
    (118, '                [')
    (119, '                    "../results/metaphlan_out/taxa_profile/{sample}_taxa_prof.txt",')
    (120, '                    "../results/metaphlan_out/bowtie2_aln/{sample}.bowtie2.bz2",')
    (121, '                ],')
    (122, '                sample = samples')
    (123, '            )')
    (124, '        )')
    (125, '')
    (126, '    if config["MetaphlanClassification"]["CleanMetaphlanReport"]: ')
    (127, '        final_input.extend(')
    (128, '            expand(')
    (129, '                [')
    (130, '                    "../results/metaphlan_out/taxa_profile_clean/{sample}_taxa_prof_clean.txt",')
    (131, '                    "../results/metaphlan_out/taxa_profile_clean/merged_tbl/merged_taxa_prof_clean.txt",')
    (132, '                    "../results/metaphlan_out/clean_summaries/kingdom_table_tidy.txt",')
    (133, '                    "../results/metaphlan_out/clean_summaries/genus_table_tidy.txt",')
    (134, '                    "../results/metaphlan_out/clean_summaries/species_table_tidy.txt"')
    (135, '                ],')
    (136, '                sample = samples')
    (137, '            )')
    (138, '        )')
    (139, '')
    (140, '    if config["HumannAnalysis"]["Activate"]: ')
    (141, '        final_input.extend(')
    (142, '            expand(')
    (143, '                [')
    (144, '                    "../results/concatenated_fastq/{sample}_unmapped_conc.fastq",')
    (145, '                    "../results/humann_out/{sample}_humann3_profile/{sample}_unmapped_conc_genefamilies.tsv",')
    (146, '                    "../results/humann_out/{sample}_humann3_profile/{sample}_unmapped_conc_pathabundance.tsv",')
    (147, '                    "../results/humann_out/{sample}_humann3_profile/{sample}_unmapped_conc_pathcoverage.tsv",')
    (148, '                    "../results/humann_out/sorted_abundance_profiles/{sample}_unmapped_conc_genefamilies.tsv", ')
    (149, '                    "../results/humann_out/sorted_abundance_profiles/{sample}_unmapped_conc_pathabundance.tsv", ')
    (150, '                    "../results/humann_out/sorted_abundance_profiles/{sample}_unmapped_conc_pathcoverage.tsv",')
    (151, '                    "../results/humann_out/concatenated_humann_files/all_samples_genesfamilies_humann3.tsv",')
    (152, '                    "../results/humann_out/concatenated_humann_files/all_samples_pathabundance_humann3.tsv", ')
    (153, '                    "../results/humann_out/concatenated_humann_files/all_samples_pathcoverage_humann3.tsv",')
    (154, '                    "../results/humann_out/concatenated_humann_files/relative_abund/all_samples_genefamilies_humann3_relab.tsv",')
    (155, '                    "../results/humann_out/concatenated_humann_files/relative_abund/all_samples_pathabundance_humann3_relab.tsv",')
    (156, '                    "../results/humann_out/concatenated_humann_files/relative_abund/all_samples_pathcoverage_humann3_relab.tsv",')
    (157, '                    "../results/humann_out/concatenated_humann_files/relab_strat/all_samples_genefamilies_humann3_relab_stratified.tsv",')
    (158, '                    "../results/humann_out/concatenated_humann_files/relab_strat/all_samples_genefamilies_humann3_relab_unstratified.tsv",')
    (159, '                    "../results/humann_out/concatenated_humann_files/relab_strat/all_samples_pathabundance_humann3_relab_stratified.tsv",')
    (160, '                    "../results/humann_out/concatenated_humann_files/relab_strat/all_samples_pathabundance_humann3_relab_unstratified.tsv",')
    (161, '                    "../results/humann_out/concatenated_humann_files/relab_strat/all_samples_pathcoverage_humann3_relab_stratified.tsv",')
    (162, '                    "../results/humann_out/concatenated_humann_files/relab_strat/all_samples_pathcoverage_humann3_relab_unstratified.tsv",')
    (163, '                    "../results/humann_out/summarised_bowtie2_stats/{sample}_bowtie2_alignment_summarised_gene_number.tsv",')
    (164, '                    "../results/humann_out/summarised_bowtie2_stats/concatenated_bowtie2_alignment_summarised_gene_number.tsv",')
    (165, '                    "../results/humann_out/{sample}_humann3_profile/{sample}_unmapped_conc_humann_temp/{sample}_unmapped_conc_bowtie2_aligned.tsv",')
    (166, '                    "../results/humann_out/plots/gene_hits.pdf",')
    (167, '                    "../results/humann_out/plots/gene_and_path_hits_per_sample.pdf",')
    (168, '                    "../results/humann_out/plots/gene_and_path_hits_per_species.pdf", ')
    (169, '')
    (170, '                ],')
    (171, '                sample = samples')
    (172, '            )')
    (173, '        )')
    (174, '')
    (175, '    if config["HumannAnalysis"]["Activate"]:')
    (176, '        final_input.extend(')
    (177, '            expand(')
    (178, '                [')
    (179, '                    "../results/humann_out/concatenated_humann_files/renamed_abund_tbl/all_samples_genesfamilies_humann3_relab_stratified_uniref90.tsv",')
    (180, '                ]')
    (181, '                ')
    (182, '            )')
    (183, '        )')
    (184, '')
    (185, '    if config["GetBiologicalProcess"]["Activate"]: ')
    (186, '        final_input.extend(')
    (187, '            expand(')
    (188, '                [')
    (189, '                    "../results/humann_out/extracted_function/{role}_extracted_function_bar.pdf",')
    (190, '                    "../results/humann_out/extracted_function/{role}_extracted_function_heatmap.pdf",')
    (191, '                ],')
    (192, '                role = config["GetBiologicalProcess"]["Process"]')
    (193, '            )')
    (194, '        )')
    (195, '    ')
    (196, '    if config["MetagenomeAssm"]["Activate"]: ')
    (197, '        final_input.extend(')
    (198, '            expand(')
    (199, '                [')
    (200, '                    "../results/megahit_assm/{sample}_assm/final.contigs.fa",')
    (201, '                    "../results/quast_out/{sample}_quast_output/report.txt",')
    (202, '                    "../results/quast_out/concat_transposed_report.tsv",')
    (203, '                    ')
    (204, '')
    (205, '')
    (206, '                ],')
    (207, '                sample = samples')
    (208, '')
    (209, '            )')
    (210, '        )')
    (211, '')
    (212, '')
    (213, '    if config["MetagenomeBinning"]["Activate"]: ')
    (214, '        final_input.extend(')
    (215, '            expand(')
    (216, '                [')
    (217, '')
    (218, '                    "../results/binning/{sample}/contig_index/{sample}_sorted.bam",')
    (219, '                    "../results/binning/{sample}/contig_depth/{sample}_sorted.depth",')
    (220, '                    "../results/binning/metabat_out/{sample}_bins/{sample}",')
    (221, '                    "../results/megahit_assm/{sample}_assm/{sample}.amb",')
    (222, '                    "../results/megahit_assm/{sample}_assm/{sample}.ann",')
    (223, '                    "../results/megahit_assm/{sample}_assm/{sample}.bwt",')
    (224, '                    "../results/megahit_assm/{sample}_assm/{sample}.pac",')
    (225, '                    "../results/megahit_assm/{sample}_assm/{sample}.sa",')
    (226, '                    ')
    (227, '                ],')
    (228, '                ')
    (229, '                sample = samples')
    (230, '')
    (231, '            )')
    (232, '        )   ')
    (233, '')
    (234, '    if config["CheckmBinQA"]["Activate"]: ')
    (235, '        final_input.extend(')
    (236, '            expand(')
    (237, '                [')
    (238, '                    "../results/binning/checkm_out/{sample}_checkm/lineage.ms",')
    (239, '                    "../results/binning/checkm_out/QA_out/{sample}_checkm_out.tsv",')
    (240, '                    "../results/binning/checkm_out/concatenated_bin_stats.tsv",')
    (241, '                    "../results/binning/plots/CompletenessVsContam.pdf",')
    (242, '                    "../results/binning/plots/NumContigsVsCompleteness.pdf",')
    (243, '                    "../results/binning/plots/CompletenessVsContam_Per_Sample.pdf",')
    (244, '                    "../results/binning/plots/BarChartCompletenessContamination.pdf", ')
    (245, '')
    (246, '                ],')
    (247, '                sample = samples')
    (248, '            )')
    (249, '        )')
    (250, '')
    (251, '    return(final_input)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aidanfoo96/MINUUR, file=workflow/rules/01_ReadQc.smk
context_key: ['if config["input_config"]["automatic"]']
    (11, 'def GetInput(wildcards): ')
    (12, '    """')
    (13, '        Function to determine if input is raw fastq or BAM')
    (14, '    """')
    (15, '    if config["input_config"]["automatic"]:')
    (16, '        fastqID = pd.read_csv(config["samples"], sep = "\\\\t")')
    (17, '        fastqID = (')
    (18, '            fastqID.assign(fastq1Path=f"../resources/" + fastqID["sampleID"] + "_1.fastq.gz")')
    (19, '            .assign(fastq2Path=f"../resources/" + fastqID["sampleID"] + "_2.fastq.gz")')
    (20, '            .set_index("sampleID")')
    (21, '        )')
    (22, '    else:')
    (23, '        assert os.path.isfile(')
    (24, '            config["input_config"]["manual"]')
    (25, '        ), f"You haven\\\'t specified a samples table. Please create one or use the \\\'automatic option\\\' with the correct naming scheme specified in MINUUR\\\'s WIKI page"')
    (26, '        fastqID = pd.read_csv(config["input_config"]["manual"], sep = "\\\\t", index_col="sampleID")')
    (27, '')
    (28, '    final = fastqID.loc[wildcards.sample, ["fastq1Path", "fastq2Path"]].dropna()')
    (29, '')
    (30, '    return [f"{final.fastq1Path}", f"{final.fastq2Path}"]')
    (31, '')
    (32, '')
    (33, '#-------------------------------------------------------------------#')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aidanfoo96/MINUUR, file=workflow/rules/02b_BamProcessRaw.smk
context_key: ['if config["input_config"]["automatic"]']
    (7, 'def GetInput(wildcards): ')
    (8, '    """')
    (9, '        Function to determine if input is raw fastq or BAM')
    (10, '    """')
    (11, '    if config["input_config"]["automatic"]:')
    (12, '        units = pd.read_csv(config["samples"], sep = "\\\\t")')
    (13, '        units = (')
    (14, '            units.assign(fastq1Path=f"../resources/" + units["sampleID"] + "_1.fastq.gz")')
    (15, '            .assign(fastq2Path=f"../resources/" + units["sampleID"] + "_2.fastq.gz")')
    (16, '            .set_index("sampleID")')
    (17, '        )')
    (18, '    else:')
    (19, '        assert os.path.isfile(')
    (20, '            config["input_config"]["manual"]')
    (21, '        ), f"config[\\\'input_config\\\'][\\\'manual\\\'] doesn\\\'t exist. Please create one or use the \\\'automatic option\\\' with the correct naming scheme specified in the WIKI page"')
    (22, '        units = pd.read_csv(config["input_config"]["manual"], sep = "\\\\t", index_col="sampleID")')
    (23, '')
    (24, '    u = units.loc[wildcards.sample, ["fastq1Path", "fastq2Path"]].dropna()')
    (25, '')
    (26, '    return [f"{u.fastq1Path}", f"{u.fastq2Path}"]')
    (27, ' ')
    (28, '')
    (29, '#-------------------------------------------------------------------#')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aidanfoo96/MINUUR, file=workflow/rules/Assembly.smk
context_key: ['if config["MetagenomeAssm"]["UseKrakenExtracted"]']
    (7, 'def megahit_input_1(wildcards): ')
    (8, '    if config["MetagenomeAssm"]["UseKrakenExtracted"]:             ')
    (9, '        return("../results/kraken_taxon_extract/{sample}_extract_1.fastq")')
    (10, '    elif config["input_type"]["fastq"]:')
    (11, '        return("../results/unmapped_fastq_ffq/{sample}_unmapped_1.fastq")')
    (12, '    elif config["input_type"]["bam"]: ')
    (13, '        return("../results/unmapped_fastq/{sample}_unmapped_1.fastq")')
    (14, '    ')
    (15, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=aidanfoo96/MINUUR, file=workflow/rules/Assembly.smk
context_key: ['if config["MetagenomeAssm"]["UseKrakenExtracted"]']
    (16, 'def megahit_input_2(wildcards): ')
    (17, '    if config["MetagenomeAssm"]["UseKrakenExtracted"]: ')
    (18, '        return("../results/kraken_taxon_extract/{sample}_extract_2.fastq")')
    (19, '    elif config["input_type"]["fastq"]:')
    (20, '        return("../results/unmapped_fastq_ffq/{sample}_unmapped_2.fastq")')
    (21, '    elif config["input_type"]["bam"]: ')
    (22, '        return("../results/unmapped_fastq/{sample}_unmapped_2.fastq")')
    (23, '')
    (24, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ["if config[\\'admixMapping\\'][\\'skip\\'] != \\'true\\'"]
    (90, 'def get_all_inputs(wildcards): #Primary rule whose input values determine which outputs (and corresponding rules) will be run by snakemake')
    (91, '    input_list = ["accessory/.sHWE_pass.txt", f"sHWE/.optimum_popnum.txt", f"{BASE}-rulegraph.png",')
    (92, '                  f"plink/{BASE}_LDp_sHWE.bed", f"{QUERY}.bed", f"plink/{BASE}.eigenvec",')
    (93, '                  f"accessory/samples.txt", f"input/{BASE}_CtlMat.bed", f"plink/{BASE}.genome.gz",')
    (94, '                  f"{BASE}.genesis.txt", f"{BASE}-Mapping-report.pdf"] #, f"{BASE}.genesis.sig.annt.txt"')
    (95, '    #input_list += [f"{BASE}.blink.txt", f"{BASE}.blink.sig.annt.txt"] #Uncomment to enable GWAS with BLINK.  Newer version of BLINK was causing issues with singularity.')
    (96, "    if config[\\'admixMapping\\'][\\'skip\\'] != \\'true\\': #Do not include admixture mapping in desired results, unless requested.")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ["if config[\\'match_controls\\'] == \\'true\\'"]
    (101, 'def get_cM_inputs(wildcards): #Conditionally determine control-matching input')
    (102, '    input_list = [f"plink/{BASE}_LDp_sHWE_IBDflt.bed"]')
    (103, "    if config[\\'match_controls\\'] == \\'true\\': #IBD results are needed if control matching is requested.  This function will request these results to be produced if match_controls is set in config.")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ["if config[\\'admixMapping\\'][\\'skip\\'] != \\'true\\'"]
    (107, 'def get_dataprep_input(wildcards): #conditionally determine dataprep input for admixture mapping')
    (108, '    input_list = ["accessory/samples.txt"]')
    (109, '    inFile=[]')
    (110, "    if config[\\'admixMapping\\'][\\'skip\\'] != \\'true\\':")
    (111, '        inFile = [f"{RFMIX.rstrip(\\\'/\\\')}/{x}" for x in os.listdir(RFMIX) if x.endswith(f"chr{wildcards.CHROM}.msp.tsv")]')
    (112, '        input_list.append(f"{BASE}.globalancestry.txt")')
    (113, '        assert len(inFile) == 1')
    (114, '    return(input_list + inFile + ["accessory/excluded_related_samples.txt"])')
    (115, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admixMap, file=workflow/Snakefile
context_key: ["if config[\\'admixMapping\\'][\\'skip\\'] != \\'true\\'"]
    (116, 'def get_report_input(wildcards):')
    (117, '    input_list = [f"{BASE}-rulegraph.png", f"{BASE}.genesis.txt"]')
    (118, '    if config[\\\'admixMapping\\\'][\\\'skip\\\'] != \\\'true\\\': input_list += [f"{BASE}.admixmap.txt", f"{BASE}.globalancestry.txt"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sanjaynagi/impute-Ag, file=workflow/rules/common.smk
context_key: ["if config[\\'subsample\\'][\\'activate\\']"]
    (2, 'def whichBams():')
    (3, "    if config[\\'subsample\\'][\\'activate\\']:")
    (4, '        bam = "results/alignments/downSampled/{sample}.bam"')
    (5, '    else:')
    (6, '        bam = "results/alignments/{sample}.bam"')
    (7, '')
    (8, '    return bam')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=tiramisutes/Snakemake_Gene_Family, file=rules/Identify_Gene_Family.smk
context_key: ['if config["module"]["InterProScan"]', 'rule InterProScan_genome', 'input']
    (20, 'def genome_pep_name():')
    (21, '    import os')
    (22, '    genome_pep = config["genome_pep"]')
    (23, '    return os.path.basename(genome_pep)')
    (24, '')
    (25, 'if config["module"]["InterProScan"]:')
    (26, '    rule InterProScan_genome:')
    (27, '        input:')
    (28, '            genome_pep = config["genome_pep"] if config["genome_pep"] else "Resources/{}_pep.fa".format(config["prefix"]),')
    (29, '            inst = "bin/{}/InterProScan_install.log".format(config["InterProScan_name"])')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=eriqande/make-ancestral-fasta-snakeflow, file=workflow/rules/common.smk
context_key: ['if config["homolog_sets_csv"] != "NULL"']
    (42, 'def genome_url_from_torq(wildcards):')
    (43, '\\tif wildcards.torq == "target":')
    (44, '\\t\\treturn config["target_url"]')
    (45, '\\telif wildcards.torq == "query":')
    (46, '\\t\\treturn config["query_url"]')
    (47, '\\telse:')
    (48, '\\t\\treturn "UNEXPECTED TORQ VALUE"')
    (49, '')
    (50, '')
    (51, '')
    (52, '')
    (53, '# For the second mapping')
    (54, 'if config["homolog_sets_csv"] != "NULL":')
    (55, '\\thomologs = pd.read_csv(config["homolog_sets_csv"]).set_index("target", drop=False)')
    (56, '')
    (57, '')
    (58, '# and here is a function to return the space separated')
    (59, '# string of homologs given a target')
    (60, 'def homologs_from_tchrom(wildcards):')
    (61, '\\treturn  homologs.loc[wildcards.tchrom, "homologs"]')
    (62, "'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab, file=workflow/rules/common.smk
context_key: ["if config[\\'Units\\'] is not None"]
    (89, 'def fetch_sample_dir(samples, config):')
    (90, "    # If table is units, target=\\'Library\\'; else if table is samples, target=\\'Sample\\'")
    (91, "    # target = \\'Library\\' if table_type==\\'units\\' else \\'Sample\\' if table_type==\\'samples\\' else None")
    (92, "    target = \\'Library\\'")
    (93, "    fq_r1_extensions = config[\\'r1_extensions\\']")
    (94, "    fq_r2_extensions = config[\\'r2_extensions\\']")
    (95, "    # src_base = \\'raw_fastqs\\' if table_type==\\'units\\' else \\'fastqs\\' if table_type==\\'samples\\' else None")
    (96, "    src_dir = config[\\'src_fq_dir\\']")
    (97, '    for index, row in samples.iterrows():')
    (98, "        samples.at[index,\\'path_to_fq\\'] = rget_path_by_lib(src_dir, row[target], fq_r1_extensions)")
    (99, '')
    (100, "if config[\\'Units\\'] is not None:")
    (101, '    # Case 1: some sample gets re-sequenced to gain deeper depth')
    (102, '    units = (')
    (103, "        pd.read_csv(config[\\'Units\\'], dtype={\\'User\\': str, \\'Project\\': str, \\'Sample\\': str, \\'Library\\': str, \\'File_R1\\': str, \\'File_R2\\': str})")
    (104, '        .set_index("Library", drop=False)')
    (105, '        .sort_index()')
    (106, '    )')
    (107, '')
    (108, '    ### Dirs to source data')
    (109, "    if Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'raw_fastqs\\').is_symlink():")
    (110, "        os.remove(str(Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'raw_fastqs\\')))")
    (111, "    if not Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'raw_fastqs\\').exists():")
    (112, "        if config[\\'src_fq_dir\\'] is None:")
    (113, "            raise ValueError(\\'Missing input fastq files.\\')")
    (114, '        else:')
    (115, "            Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'raw_fastqs\\').symlink_to(config[\\'src_fq_dir\\'])")
    (116, "    elif not Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'raw_fastqs\\').is_symlink():")
    (117, "        config[\\'src_fq_dir\\'] = Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'raw_fastqs\\')")
    (118, '')
    (119, '    # units.Library must be unique')
    (120, '    # if not units["Library"].is_unique:')
    (121, '    if units["Library"].duplicated().any():')
    (122, '        raise ValueError("Duplicated values found! The Library column in units_table.csv mustn\\\'t contain duplicated values.")')
    (123, '')
    (124, '    ## Dir to sample-level fastqs (aggregated units)')
    (125, "    Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'fastqs\\').mkdir(parents=True, exist_ok=True)")
    (126, '')
    (127, '    ## Glob source R1 R2 fq files, complete units table')
    (128, '    fetch_units_fq(units, config)')
    (129, '')
    (130, "    ## Allocate space for aggregation of fq\\'s")
    (131, "    if units[\\'Sample\\'].duplicated().any():")
    (132, "        if config[\\'tmp_fq_dir\\'] is None:")
    (133, "            raise ValueError(\\'\\\\\\'tmp_fq_dir\\\\\\' not specified in config.yaml. Please specify a location to store aggregated fastq units. i.e. \\' +")
    (134, "            str(Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'tmp\\')))")
    (135, '        else:')
    (136, "            Path(config[\\'tmp_fq_dir\\']).mkdir(parents=True, exist_ok=True)")
    (137, '')
    (138, '    ## Prepare for aggregation (#Step 0 rule aggr_fqs)')
    (139, "    for sample in set(units[\\'Sample\\']):")
    (140, "        u_df = units[units[\\'Sample\\']==sample]")
    (141, "        if Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'fastqs\\', sample).exists():")
    (142, "            os.remove(str(Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'fastqs\\', sample)))")
    (143, '        if len(u_df)==1:')
    (144, "            path_to_sample = sorted(Path(config[\\'src_fq_dir\\']).resolve().rglob(u_df[\\'Library\\'].tolist()[0]+\\'/\\'))")
    (145, '            if len(path_to_sample) > 1:')
    (146, "                raise ValueError(\\'More than one library observed under sample:\\' + sample + \\'. Execution halted.\\')")
    (147, "            Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'fastqs\\', sample).symlink_to(path_to_sample[0])")
    (148, '            samples.at[sample, \\\'File_R1\\\'] = units[units[\\\'Sample\\\']==sample]["File_R1"][0]')
    (149, '            samples.at[sample, \\\'File_R2\\\'] = units[units[\\\'Sample\\\']==sample]["File_R2"][0]')
    (150, '        else:')
    (151, "            aggr_dir = Path(config[\\'tmp_fq_dir\\'], config[\\'User\\'], config[\\'Project\\'], sample)")
    (152, '            aggr_dir.mkdir(parents=True, exist_ok=True)')
    (153, "            Path(\\'workflow\\', \\'data\\', config[\\'User\\'], config[\\'Project\\'], \\'fastqs\\', sample).symlink_to(aggr_dir)")
    (154, "            samples.at[sample, \\'File_R1\\'] = \\'_\\'.join([sample,\\'R1.fq.gz\\'])")
    (155, "            samples.at[sample, \\'File_R2\\'] = \\'_\\'.join([sample,\\'R2.fq.gz\\'])")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab, file=workflow/rules/common.smk
context_key: ["if config[\\'repeat_mask\\'] is None"]
    (253, 'def get_mask():')
    (254, "    if config[\\'repeat_mask\\'] is None:")
    (255, "        return \\'\\'")
    (256, '    else:')
    (257, '        return "-m " + config[\\\'repeat_mask\\\'] + " "')
    (258, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=RuiyuRayWang/ScRNAseq_smkpipe_at_Luolab, file=workflow/rules/common.smk
context_key: ["if config[\\'RNA_velocity\\']"]
    (270, 'def get_final_output():')
    (271, "    if config[\\'RNA_velocity\\']:")
    (272, '        return list(set(expand("workflow/data/{user}/{project}/outs/{project}_{suffix}", ')
    (273, "        zip, user=samples.User.to_list(), project=samples.Project.to_list(), suffix=[\\'counts_all.tsv.gz\\',\\'velocyto_all.loom\\'])))")
    (274, '    else:')
    (275, '        return list(set(expand("workflow/data/{user}/{project}/outs/{project}_counts_all.tsv.gz", ')
    (276, '        zip, user=samples.User.to_list(), project=samples.Project.to_list())))')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Vyoming/Immune_Tumor_Bulk_RNA, file=rules/functions.smk
context_key: ['if config["trimming"]["skip"]']
    (6, 'def get_fq(wildcards):')
    (7, '    if config["trimming"]["skip"]:')
    (8, '        # no trimming, use raw reads')
    (9, '        return {"fq1":samples.loc[(wildcards.sample, wildcards.unit), ["fq1"]].dropna(),')
    (10, '                "fq2":samples.loc[(wildcards.sample, wildcards.unit), ["fq2"]].dropna()}')
    (11, '    else:')
    (12, '        # yes trimming, use trimmed data')
    (13, '        if not is_single_end(**wildcards):')
    (14, '            # paired-end sample')
    (15, '            return {"fq1": expand("trimmed/{sample}-{unit}.{group}.fastq.gz",')
    (16, '                          group=1, **wildcards),')
    (17, '                    "fq2": expand("trimmed/{sample}-{unit}.{group}.fastq.gz",')
    (18, '                          group=2, **wildcards)}')
    (19, '        # single end sample')
    (20, '        return {"fq1":"trimmed/{sample}-{unit}.fastq.gz".format(**wildcards)}')
    (21, '')
    (22, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Vyoming/Immune_Tumor_Bulk_RNA, file=rules/functions.smk
context_key: ['if config["trimming"]["skip"]']
    (23, 'def get_fileend(wildcards):')
    (24, '    if config["trimming"]["skip"]:')
    (25, '        fq1=samples.loc[(wildcards.sample, wildcards.unit), ["fq1"]].dropna()')
    (26, '    else:')
    (27, '        if not is_single_end(**wildcards):')
    (28, '            fq1=expand("trimmed/{sample}-{unit}.{group}.fastq.gz",')
    (29, '                          group=1, **wildcards)')
    (30, '        else:')
    (31, '            fq1=expand("trimmed/{sample}-{unit}.fastq.gz", **wildcards),')
    (32, '    if fq1[0].endswith(".gz"):')
    (33, '        readcmd = "--readFilesCommand zcat"')
    (34, '    else:')
    (35, '        readcmd = ""')
    (36, '    return readcmd')
    (37, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MLKaufman/rna-seq-star-deseq2, file=rules/common.smk
context_key: ['if config["trimming"]["skip"]']
    (8, 'def get_fq(wildcards):')
    (9, '    if config["trimming"]["skip"]:')
    (10, '        # no trimming, use raw reads')
    (11, '        return units.loc[(wildcards.sample, wildcards.unit), ["fq1", "fq2"]].dropna()')
    (12, '    else:')
    (13, '        # yes trimming, use trimmed data')
    (14, '        if not is_single_end(**wildcards):')
    (15, '            # paired-end sample')
    (16, '            return expand(')
    (17, '                "trimmed/{sample}-{unit}.{group}.fastq.gz", group=[1, 2], **wildcards')
    (18, '            )')
    (19, '        # single end sample')
    (20, '        return "trimmed/{sample}-{unit}.fastq.gz".format(**wildcards)')
    (21, '')
    (22, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=snakemake-workflows/chipseq, file=workflow/rules/common.smk
context_key: ['if config["single_end"]']
    (61, 'def get_se_pe_branches_input(wildcards):')
    (62, '    if config["single_end"]:')
    (63, '        return "results/bamtools_filtered/{sample}.sorted.bam".format(sample=wildcards.sample)')
    (64, '    else:')
    (65, '        return "results/orph_rm_pe/{sample}.sorted.bam".format(sample=wildcards.sample)')
    (66, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PyPSA/pypsa-eur-mga, file=Snakefile
context_key: ["if config[\\'include_groups\\']"]
    (69, 'def get_wildcard_sets(config):')
    (70, '    wildcard_sets = [')
    (71, "        {**config[\\'scenario-totals\\'], **config[\\'alternative-totals\\']}")
    (72, '    ]')
    (73, "    if config[\\'include_groups\\']:")
    (74, '        wildcard_sets.append(')
    (75, "            {**config[\\'scenario-groups\\'], **config[\\'alternative-groups\\']}")
    (76, '        )')
    (77, "    if config[\\'include_hypercube\\']:")
    (78, '        wildcard_sets.append(')
    (79, "            {**config[\\'scenario-hypercube\\'], **config[\\'alternative-hypercube\\']}")
    (80, '        )')
    (81, '    return wildcard_sets')
    (82, '')
    (83, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PyPSA/pypsa-eur-mga, file=Snakefile
context_key: ['if config["include_groups"]']
    (113, 'def input_generate_all_alternatives(w):')
    (114, '    categories = ["totals"]')
    (115, '    if config["include_groups"]: categories.append("groups")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=PyPSA/pypsa-eur-mga, file=Snakefile
context_key: ['if config["include_hypercube"]']
    (113, 'def input_generate_all_alternatives(w):')
    (114, '    categories = ["totals"]')
    (115, '    if config["include_groups"]: categories.append("groups")')
    (116, '    if config["include_hypercube"]: categories.append("hypercube")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=huishenlab/multiscale_methylation_plot_pipeline, file=workflow/Snakefile
context_key: ["if config[\\'output_directory\\'] == \\'\\'"]
    (42, 'def set_output_directory():')
    (43, "    if config[\\'output_directory\\'] == \\'\\':")
    (44, '        return os.getcwd()')
    (45, '    else:')
    (46, "        return config[\\'output_directory\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=csoneson/WagnerEMT2020, file=Snakefile
context_key: ['if config["useCondaR"] == True', 'else', 'rule all', 'rule setup', 'rule pkginstall', 'rule runfastqc', 'rule runtrimming', 'rule runsalmonquant', 'rule runstar', 'rule listpackages', 'rule softwareversions', 'rule chrnames', 'rule salmonindex', 'rule starindex', 'rule fastqc', 'rule fastqctrimmed', 'rule multiqc', 'rule qcsummary', 'rule trimgaloreSE', 'rule salmonSE', 'rule starSE', 'rule bamindex', 'rule bigwig', 'rule tximeta']
    (41, 'def getpath(str):')
    (42, "\\tif str in [\\'\\', \\'.\\', \\'./\\']:")
    (43, "\\t\\treturn \\'\\'")
    (44, "\\tif str.startswith(\\'./\\'):")
    (45, "\\t\\tregex = re.compile(\\'^\\\\./?\\')")
    (46, "\\t\\tstr = regex.sub(\\'\\', str)")
    (47, "\\tif not str.endswith(\\'/\\'):")
    (48, "\\t\\tstr += \\'/\\'")
    (49, '\\treturn str')
    (50, '')
    (51, 'outputdir = getpath(config["output"])')
    (52, 'FASTQdir = getpath(config["FASTQ"])')
    (53, '')
    (54, '## Define the conda environment for all rules using R')
    (55, 'if config["useCondaR"] == True:')
    (56, '\\tRenv = "envs/environment_R.yaml"')
    (57, 'else:')
    (58, '\\tRenv = "envs/environment.yaml"')
    (59, '')
    (60, '## Define the R binary')
    (61, 'Rbin = config["Rbin"]')
    (62, '')
    (63, '## ------------------------------------------------------------------------------------ ##')
    (64, '## Target definitions')
    (65, '## ------------------------------------------------------------------------------------ ##')
    (66, '## Run all analyses')
    (67, 'rule all:')
    (68, '\\tinput:')
    (69, '\\t\\toutputdir + "MultiQC/multiqc_report.html",')
    (70, '\\t\\toutputdir + "outputR/edgeR_dge.rds",')
    (71, '\\t\\toutputdir + "outputR/qc_summary.rds"')
    (72, '')
    (73, 'rule setup:')
    (74, '\\tinput:')
    (75, '\\t\\toutputdir + "Rout/pkginstall_state.txt",')
    (76, '\\t\\toutputdir + "Rout/softwareversions.done"')
    (77, '')
    (78, '## Install R packages')
    (79, 'rule pkginstall:')
    (80, '\\tinput:')
    (81, '\\t\\tscript = "scripts/install_pkgs.R"')
    (82, '\\toutput:')
    (83, '\\t\\toutputdir + "Rout/pkginstall_state.txt"')
    (84, '\\tparams:')
    (85, '\\t\\tflag = config["annotation"],')
    (86, '\\t\\tncores = config["ncores"],')
    (87, '\\t\\torganism = config["organism"]')
    (88, '\\tpriority:')
    (89, '\\t\\t50')
    (90, '\\tconda:')
    (91, '\\t\\tRenv')
    (92, '\\tlog:')
    (93, '\\t\\toutputdir + "Rout/install_pkgs.Rout"')
    (94, '\\tbenchmark:')
    (95, '\\t\\toutputdir + "benchmarks/install_pkgs.txt"')
    (96, '\\tshell:')
    (97, '\\t\\t\\\'\\\'\\\'{Rbin} CMD BATCH --no-restore --no-save "--args outtxt=\\\'{output}\\\' ncores=\\\'{params.ncores}\\\' annotation=\\\'{params.flag}\\\' organism=\\\'{params.organism}\\\'" {input.script} {log}\\\'\\\'\\\'')
    (98, '')
    (99, '## FastQC on original (untrimmed) files')
    (100, 'rule runfastqc:')
    (101, '\\tinput:')
    (102, '\\t\\texpand(outputdir + "FastQC/{sample}_fastqc.zip", sample = samples.names[samples.type == \\\'SE\\\'].values.tolist())')
    (103, '')
    (104, '## Trimming and FastQC on trimmed files')
    (105, 'rule runtrimming:')
    (106, '\\tinput:')
    (107, '\\t\\texpand(outputdir + "FastQC/{sample}_trimmed_fastqc.zip", sample = samples.names[samples.type == \\\'SE\\\'].values.tolist())')
    (108, '')
    (109, '## Salmon quantification')
    (110, 'rule runsalmonquant:')
    (111, '\\tinput:')
    (112, '\\t\\texpand(outputdir + "salmon/{sample}/quant.sf", sample = samples.names.values.tolist())')
    (113, '')
    (114, '## STAR alignment')
    (115, 'rule runstar:')
    (116, '\\tinput:')
    (117, '\\t\\texpand(outputdir + "STAR/{sample}/{sample}_Aligned.sortedByCoord.out.bam.bai", sample = samples.names.values.tolist()),')
    (118, '\\t\\texpand(outputdir + "STARbigwig/{sample}_Aligned.sortedByCoord.out.bw", sample = samples.names.values.tolist())')
    (119, '')
    (120, '## List all the packages that were used by the R analyses')
    (121, 'rule listpackages:')
    (122, '\\tlog:')
    (123, '\\t\\toutputdir + "Rout/list_packages.Rout"')
    (124, '\\tparams:')
    (125, '\\t\\tRoutdir = outputdir + "Rout",')
    (126, '\\t\\touttxt = outputdir + "R_package_versions.txt",')
    (127, '\\t\\tscript = "scripts/list_packages.R"')
    (128, '\\tconda:')
    (129, '\\t\\tRenv')
    (130, '\\tshell:')
    (131, '\\t\\t\\\'\\\'\\\'{Rbin} CMD BATCH --no-restore --no-save "--args Routdir=\\\'{params.Routdir}\\\' outtxt=\\\'{params.outtxt}\\\'" {params.script} {log}\\\'\\\'\\\'')
    (132, '')
    (133, '## Print the versions of all software packages')
    (134, 'rule softwareversions:')
    (135, '\\toutput:')
    (136, '\\t\\ttouch(outputdir + "Rout/softwareversions.done")')
    (137, '\\tconda:')
    (138, '\\t\\t"envs/environment.yaml"')
    (139, '\\tshell:')
    (140, '\\t\\t"salmon --version; trim_galore --version; "')
    (141, '\\t\\t"echo -n \\\'cutadapt \\\' && cutadapt --version; "')
    (142, '\\t\\t"fastqc --version; STAR --version; samtools --version; multiqc --version; "')
    (143, '\\t\\t"bedtools --version"')
    (144, '')
    (145, '## ------------------------------------------------------------------------------------ ##')
    (146, '## Reference preparation')
    (147, '## ------------------------------------------------------------------------------------ ##')
    (148, '## Extract chromosome names')
    (149, 'rule chrnames:')
    (150, '\\tinput:')
    (151, '\\t\\tgenome = config["genome"]')
    (152, '\\toutput:')
    (153, '\\t\\tconfig["chrnames"]')
    (154, '\\tlog:')
    (155, '\\t\\toutputdir + "logs/get_chrnames.log"')
    (156, '\\tbenchmark:')
    (157, '\\t\\toutputdir + "benchmarks/get_chrnames.txt"')
    (158, '\\tconda:')
    (159, '\\t\\t"envs/environment.yaml"')
    (160, '\\tshell:')
    (161, '\\t\\t"""')
    (162, '\\t\\tgrep ">" {input.genome} | cut -d ">" -f 2 | cut -d " " -f 1 > {output}')
    (163, '\\t\\t"""')
    (164, '')
    (165, '## Generate Salmon index from transcriptome')
    (166, 'rule salmonindex:')
    (167, '\\tinput:')
    (168, '\\t\\ttxome = config["txome"],')
    (169, '\\t\\tgenome = config["genome"],')
    (170, '\\t\\tchrnames = config["chrnames"]')
    (171, '\\toutput:')
    (172, '\\t\\tconfig["salmonindex"] + "/versionInfo.json"')
    (173, '\\tlog:')
    (174, '\\t\\toutputdir + "logs/salmon_index.log"')
    (175, '\\tbenchmark:')
    (176, '\\t\\toutputdir + "benchmarks/salmon_index.txt"')
    (177, '\\tparams:')
    (178, '\\t\\tsalmonoutdir = config["salmonindex"],')
    (179, '\\t\\tanno = config["annotation"],')
    (180, '\\t\\tsalmonextraparams = config["additional_salmon_index"]')
    (181, '\\tconda:')
    (182, '\\t\\t"envs/environment.yaml"')
    (183, '\\tthreads:')
    (184, '\\t\\tconfig["ncores"]')
    (185, '\\tshell:')
    (186, '\\t\\t"""')
    (187, "\\t\\techo \\'Salmon version:\\")
    (188, "\\' > {log}; salmon --version >> {log};")
    (189, '\\t\\tsalmon index -t <(cat {input.txome} {input.genome}) -i {params.salmonoutdir} \\\\')
    (190, '\\t\\t--gencode {params.salmonextraparams} -d {input.chrnames} -p {threads}')
    (191, '\\t\\t"""')
    (192, '')
    (193, '## Generate STAR index')
    (194, 'rule starindex:')
    (195, '\\tinput:')
    (196, '\\t\\tgenome = config["genome"],')
    (197, '\\t\\tgtf = config["gtf"]')
    (198, '\\toutput:')
    (199, '\\t\\tconfig["STARindex"] + "/SA",')
    (200, '\\t\\tconfig["STARindex"] + "/chrNameLength.txt"')
    (201, '\\tlog:')
    (202, '\\t\\toutputdir + "logs/STAR_index.log"')
    (203, '\\tbenchmark:')
    (204, '\\t\\toutputdir + "benchmarks/STAR_index.txt"')
    (205, '\\tparams:')
    (206, '\\t\\tSTARindex = config["STARindex"],')
    (207, '\\t\\treadlength = config["readlength"],')
    (208, '\\t\\tstarextraparams = config["additional_star_index"]')
    (209, '\\tconda:')
    (210, '\\t\\t"envs/environment.yaml"')
    (211, '\\tthreads:')
    (212, '\\t\\tconfig["ncores"]')
    (213, '\\tshell:')
    (214, '\\t\\t"echo \\\'STAR version:\\')
    (215, '\\\' > {log}; STAR --version >> {log}; "')
    (216, '\\t\\t"STAR --runMode genomeGenerate --runThreadN {threads} --genomeDir {params.STARindex} "')
    (217, '\\t\\t"--genomeFastaFiles {input.genome} --sjdbGTFfile {input.gtf} --sjdbOverhang {params.readlength} "')
    (218, '\\t\\t"{params.starextraparams}"')
    (219, '')
    (220, '## ------------------------------------------------------------------------------------ ##')
    (221, '## Quality control')
    (222, '## ------------------------------------------------------------------------------------ ##')
    (223, '## FastQC, original reads')
    (224, 'rule fastqc:')
    (225, '\\tinput:')
    (226, '\\t\\tfastq = FASTQdir + "{sample}." + str(config["fqsuffix"]) + ".gz"')
    (227, '\\toutput:')
    (228, '\\t\\toutputdir + "FastQC/{sample}_fastqc.zip"')
    (229, '\\tparams:')
    (230, '\\t\\tFastQC = outputdir + "FastQC"')
    (231, '\\tlog:')
    (232, '\\t\\toutputdir + "logs/fastqc_{sample}.log"')
    (233, '\\tbenchmark:')
    (234, '\\t\\toutputdir + "benchmarks/fastqc_{sample}.txt"')
    (235, '\\tconda:')
    (236, '\\t\\t"envs/environment.yaml"')
    (237, '\\tthreads:')
    (238, '\\t\\tconfig["ncores"]')
    (239, '\\tshell:')
    (240, '\\t\\t"echo \\\'FastQC version:\\')
    (241, '\\\' > {log}; fastqc --version >> {log}; "')
    (242, '\\t\\t"fastqc -o {params.FastQC} -t {threads} {input.fastq}"')
    (243, '')
    (244, '## FastQC, trimmed reads')
    (245, 'rule fastqctrimmed:')
    (246, '\\tinput:')
    (247, '\\t\\tfastq = outputdir + "FASTQtrimmed/{sample}.fq.gz"')
    (248, '\\toutput:')
    (249, '\\t\\toutputdir + "FastQC/{sample}_fastqc.zip"')
    (250, '\\tparams:')
    (251, '\\t\\tFastQC = outputdir + "FastQC"')
    (252, '\\tlog:')
    (253, '\\t\\toutputdir + "logs/fastqc_trimmed_{sample}.log"')
    (254, '\\tbenchmark:')
    (255, '\\t\\toutputdir + "benchmarks/fastqc_trimmed_{sample}.txt"')
    (256, '\\tconda:')
    (257, '\\t\\t"envs/environment.yaml"')
    (258, '\\tthreads:')
    (259, '\\t\\tconfig["ncores"]')
    (260, '\\tshell:')
    (261, '\\t\\t"echo \\\'FastQC version:\\')
    (262, '\\\' > {log}; fastqc --version >> {log}; "')
    (263, '\\t\\t"fastqc -o {params.FastQC} -t {threads} {input.fastq}"')
    (264, '')
    (265, '')
    (266, '# The config.yaml files determines which steps should be performed')
    (267, 'def multiqc_input(wildcards):')
    (268, '\\tinput = []')
    (269, '\\tinput.extend(expand(outputdir + "FastQC/{sample}_fastqc.zip", sample = samples.names[samples.type == \\\'SE\\\'].values.tolist()))')
    (270, '\\tinput.extend(expand(outputdir + "salmon/{sample}/quant.sf", sample = samples.names.values.tolist()))')
    (271, '\\tif config["run_trimming"]:')
    (272, '\\t\\tinput.extend(expand(outputdir + "FASTQtrimmed/{sample}_trimmed.fq.gz", sample = samples.names[samples.type == \\\'SE\\\'].values.tolist()))')
    (273, '\\t\\tinput.extend(expand(outputdir + "FastQC/{sample}_trimmed_fastqc.zip", sample = samples.names[samples.type == \\\'SE\\\'].values.tolist()))')
    (274, '\\tif config["run_STAR"]:')
    (275, '\\t\\tinput.extend(expand(outputdir + "STAR/{sample}/{sample}_Aligned.sortedByCoord.out.bam.bai", sample = samples.names.values.tolist()))')
    (276, '\\treturn input')
    (277, '')
    (278, '## Determine the input directories for MultiQC depending on the config file')
    (279, 'def multiqc_params(wildcards):')
    (280, '\\tparam = [outputdir + "FastQC",')
    (281, '\\toutputdir + "salmon"]')
    (282, '\\tif config["run_trimming"]:')
    (283, '\\t\\tparam.append(outputdir + "FASTQtrimmed")')
    (284, '\\tif config["run_STAR"]:')
    (285, '\\t\\tparam.append(outputdir + "STAR")')
    (286, '\\treturn param')
    (287, '')
    (288, '## MultiQC')
    (289, 'rule multiqc:')
    (290, '\\tinput:')
    (291, '\\t\\tmultiqc_input')
    (292, '\\toutput:')
    (293, '\\t\\toutputdir + "MultiQC/multiqc_report.html"')
    (294, '\\tparams:')
    (295, '\\t\\tinputdirs = multiqc_params,')
    (296, '\\t\\tMultiQCdir = outputdir + "MultiQC"')
    (297, '\\tlog:')
    (298, '\\t\\toutputdir + "logs/multiqc.log"')
    (299, '\\tbenchmark:')
    (300, '\\t\\toutputdir + "benchmarks/multiqc.txt"')
    (301, '\\tconda:')
    (302, '\\t\\t"envs/environment.yaml"')
    (303, '\\tshell:')
    (304, '\\t\\t"echo \\\'MultiQC version:\\')
    (305, '\\\' > {log}; multiqc --version >> {log}; "')
    (306, '\\t\\t"multiqc {params.inputdirs} -f -o {params.MultiQCdir}"')
    (307, '')
    (308, '# Summary QC plots')
    (309, 'rule qcsummary:')
    (310, '\\tinput:')
    (311, '\\t\\toutputdir + "MultiQC/multiqc_report.html",')
    (312, '\\t\\tscript = "scripts/summarize_qc.R",')
    (313, '\\t\\tmetatxt = config["metatxt"]')
    (314, '\\toutput:')
    (315, '\\t\\toutputdir + "outputR/qc_summary.rds"')
    (316, '\\tlog:')
    (317, '\\t\\toutputdir + "Rout/summarize_qc.Rout"')
    (318, '\\tbenchmark:')
    (319, '\\t\\toutputdir + "benchmarks/summarize_qc.txt"')
    (320, '\\tparams:')
    (321, '\\t\\tmultiqcdir = outputdir + "MultiQC",')
    (322, '\\t\\tfastqcdir = outputdir + "FastQC"')
    (323, '\\tconda:')
    (324, '\\t\\tRenv')
    (325, '\\tshell:')
    (326, '\\t\\t\\\'\\\'\\\'{Rbin} CMD BATCH --no-restore --no-save "--args metafile=\\\'{input.metatxt}\\\' multiqcdir=\\\'{params.multiqcdir}\\\' fastqcdir=\\\'{params.fastqcdir}\\\' outrds=\\\'{output}\\\'" {input.script} {log}\\\'\\\'\\\'')
    (327, '')
    (328, '## ------------------------------------------------------------------------------------ ##')
    (329, '## Adapter trimming')
    (330, '## ------------------------------------------------------------------------------------ ##')
    (331, '# TrimGalore!')
    (332, 'rule trimgaloreSE:')
    (333, '\\tinput:')
    (334, '\\t\\tfastq = FASTQdir + "{sample}." + str(config["fqsuffix"]) + ".gz"')
    (335, '\\toutput:')
    (336, '\\t\\toutputdir + "FASTQtrimmed/{sample}_trimmed.fq.gz"')
    (337, '\\tparams:')
    (338, '\\t\\tFASTQtrimmeddir = outputdir + "FASTQtrimmed"')
    (339, '\\tlog:')
    (340, '\\t\\toutputdir + "logs/trimgalore_{sample}.log"')
    (341, '\\tbenchmark:')
    (342, '\\t\\toutputdir + "benchmarks/trimgalore_{sample}.txt"')
    (343, '\\tconda:')
    (344, '\\t\\t"envs/environment.yaml"')
    (345, '\\tshell:')
    (346, '\\t\\t"echo \\\'TrimGalore! version:\\')
    (347, '\\\' > {log}; trim_galore --version >> {log}; "')
    (348, '\\t\\t"trim_galore -q 20 --phred33 --length 20 -o {params.FASTQtrimmeddir} --path_to_cutadapt cutadapt {input.fastq}"')
    (349, '')
    (350, '## ------------------------------------------------------------------------------------ ##')
    (351, '## Salmon abundance estimation')
    (352, '## ------------------------------------------------------------------------------------ ##')
    (353, '# Estimate abundances with Salmon')
    (354, 'rule salmonSE:')
    (355, '\\tinput:')
    (356, '\\t\\tindex = config["salmonindex"] + "/versionInfo.json",')
    (357, '\\t\\tfastq = outputdir + "FASTQtrimmed/{sample}_trimmed.fq.gz" if config["run_trimming"] else FASTQdir + "{sample}." + str(config["fqsuffix"]) + ".gz"')
    (358, '\\toutput:')
    (359, '\\t\\toutputdir + "salmon/{sample}/quant.sf"')
    (360, '\\tlog:')
    (361, '\\t\\toutputdir + "logs/salmon_{sample}.log"')
    (362, '\\tbenchmark:')
    (363, '\\t\\toutputdir + "benchmarks/salmon_{sample}.txt"')
    (364, '\\tthreads:')
    (365, '\\t\\tconfig["ncores"]')
    (366, '\\tparams:')
    (367, '\\t\\tsalmonindex = config["salmonindex"],')
    (368, '\\t\\tsalmondir = outputdir + "salmon",')
    (369, '\\t\\tsalmonextraparams = config["additional_salmon_quant"]')
    (370, '\\tconda:')
    (371, '\\t\\t"envs/environment.yaml"')
    (372, '\\tshell:')
    (373, '\\t\\t"echo \\\'Salmon version:\\')
    (374, '\\\' > {log}; salmon --version >> {log}; "')
    (375, '\\t\\t"salmon quant -i {params.salmonindex} -l A -r {input.fastq} "')
    (376, '\\t\\t"-o {params.salmondir}/{wildcards.sample} -p {threads} {params.salmonextraparams}"')
    (377, '')
    (378, '## ------------------------------------------------------------------------------------ ##')
    (379, '## STAR mapping')
    (380, '## ------------------------------------------------------------------------------------ ##')
    (381, '## Genome mapping with STAR')
    (382, 'rule starSE:')
    (383, '\\tinput:')
    (384, '\\t\\tindex = config["STARindex"] + "/SA",')
    (385, '\\t\\tfastq = outputdir + "FASTQtrimmed/{sample}_trimmed.fq.gz" if config["run_trimming"] else FASTQdir + "{sample}." + str(config["fqsuffix"]) + ".gz"')
    (386, '\\toutput:')
    (387, '\\t\\toutputdir + "STAR/{sample}/{sample}_Aligned.sortedByCoord.out.bam"')
    (388, '\\tthreads:')
    (389, '\\t\\tconfig["ncores"]')
    (390, '\\tlog:')
    (391, '\\t\\toutputdir + "logs/STAR_{sample}.log"')
    (392, '\\tbenchmark:')
    (393, '\\t\\toutputdir + "benchmarks/STAR_{sample}.txt"')
    (394, '\\tparams:')
    (395, '\\t\\tSTARindex = config["STARindex"],')
    (396, '\\t\\tSTARdir = outputdir + "STAR",')
    (397, '\\t\\tstarextraparams = config["additional_star_align"]')
    (398, '\\tconda:')
    (399, '\\t\\t"envs/environment.yaml"')
    (400, '\\tshell:')
    (401, '\\t\\t"echo \\\'STAR version:\\')
    (402, '\\\' > {log}; STAR --version >> {log}; "')
    (403, '\\t\\t"STAR --genomeDir {params.STARindex} --readFilesIn {input.fastq} "')
    (404, '\\t\\t"--runThreadN {threads} --outFileNamePrefix {params.STARdir}/{wildcards.sample}/{wildcards.sample}_ "')
    (405, '\\t\\t"--outSAMtype BAM SortedByCoordinate --readFilesCommand gunzip -c "')
    (406, '\\t\\t"{params.starextraparams}"')
    (407, '')
    (408, '## Index bam files')
    (409, 'rule bamindex:')
    (410, '\\tinput:')
    (411, '\\t\\tbam = outputdir + "STAR/{sample}/{sample}_Aligned.sortedByCoord.out.bam"')
    (412, '\\toutput:')
    (413, '\\t\\toutputdir + "STAR/{sample}/{sample}_Aligned.sortedByCoord.out.bam.bai"')
    (414, '\\tlog:')
    (415, '\\t\\toutputdir + "logs/samtools_index_{sample}.log"')
    (416, '\\tbenchmark:')
    (417, '\\t\\toutputdir + "benchmarks/samtools_index_{sample}.txt"')
    (418, '\\tconda:')
    (419, '\\t\\t"envs/environment.yaml"')
    (420, '\\tshell:')
    (421, '\\t\\t"echo \\\'samtools version:\\')
    (422, '\\\' > {log}; samtools --version >> {log}; "')
    (423, '\\t\\t"samtools index {input.bam}"')
    (424, '')
    (425, '## Convert BAM files to bigWig')
    (426, 'rule bigwig:')
    (427, '\\tinput:')
    (428, '\\t\\tbam = outputdir + "STAR/{sample}/{sample}_Aligned.sortedByCoord.out.bam",')
    (429, '\\t\\tchrl = config["STARindex"] + "/chrNameLength.txt"')
    (430, '\\toutput:')
    (431, '\\t\\toutputdir + "STARbigwig/{sample}_Aligned.sortedByCoord.out.bw"')
    (432, '\\tparams:')
    (433, '\\t\\tSTARbigwigdir = outputdir + "STARbigwig"')
    (434, '\\tlog:')
    (435, '\\t\\toutputdir + "logs/bigwig_{sample}.log"')
    (436, '\\tbenchmark:')
    (437, '\\t\\toutputdir + "benchmarks/bigwig_{sample}.txt"')
    (438, '\\tconda:')
    (439, '\\t\\t"envs/environment.yaml"')
    (440, '\\tshell:')
    (441, '\\t\\t"echo \\\'bedtools version:\\')
    (442, '\\\' > {log}; bedtools --version >> {log}; "')
    (443, '\\t\\t"bedtools genomecov -split -ibam {input.bam} -bg | LC_COLLATE=C sort -k1,1 -k2,2n > "')
    (444, '\\t\\t"{params.STARbigwigdir}/{wildcards.sample}_Aligned.sortedByCoord.out.bedGraph; "')
    (445, '\\t\\t"bedGraphToBigWig {params.STARbigwigdir}/{wildcards.sample}_Aligned.sortedByCoord.out.bedGraph "')
    (446, '\\t\\t"{input.chrl} {output}; rm -f {params.STARbigwigdir}/{wildcards.sample}_Aligned.sortedByCoord.out.bedGraph"')
    (447, '')
    (448, '## ------------------------------------------------------------------------------------ ##')
    (449, '## Transcript quantification')
    (450, '## ------------------------------------------------------------------------------------ ##')
    (451, '## tximeta')
    (452, 'rule tximeta:')
    (453, '\\tinput:')
    (454, '\\t    outputdir + "Rout/pkginstall_state.txt",')
    (455, '\\t\\texpand(outputdir + "salmon/{sample}/quant.sf", sample = samples.names.values.tolist()),')
    (456, '\\t\\tmetatxt = config["metatxt"],')
    (457, '\\t\\tsalmonidx = config["salmonindex"] + "/versionInfo.json",')
    (458, '\\t\\tscript = "scripts/run_tximeta.R"')
    (459, '\\toutput:')
    (460, '\\t\\toutputdir + "outputR/tximeta_se.rds"')
    (461, '\\tlog:')
    (462, '\\t\\toutputdir + "Rout/tximeta_se.Rout"')
    (463, '\\tbenchmark:')
    (464, '\\t\\toutputdir + "benchmarks/tximeta_se.txt"')
    (465, '\\tparams:')
    (466, '\\t\\tsalmondir = outputdir + "salmon",')
    (467, '\\t\\tflag = config["annotation"],')
    (468, '\\t\\torganism = config["organism"]')
    (469, '\\tconda:')
    (470, '\\t\\tRenv')
    (471, '\\tshell:')
    (472, '\\t\\t\\\'\\\'\\\'{Rbin} CMD BATCH --no-restore --no-save "--args salmondir=\\\'{params.salmondir}\\\' metafile=\\\'{input.metatxt}\\\' outrds=\\\'{output}\\\' annotation=\\\'{params.flag}\\\' organism=\\\'{params.organism}\\\'" {input.script} {log}\\\'\\\'\\\'')
    (473, '')
    (474, '## ------------------------------------------------------------------------------------ ##')
    (475, '## Input variable check')
    (476, '## ------------------------------------------------------------------------------------ ##')
    (477, 'def geneset_param(wildcards):')
    (478, '\\tif config["run_camera"]:')
    (479, '                gs = config["genesets"].replace(" ", "") if config["genesets"] is not None else "NOTDEFINED"')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=MW55/Natrix, file=Snakefile
context_key: ['if config["merge"]["paired_End"]']
    (10, 'def is_single_end(sample, unit):')
    (11, '    return pd.isnull(units.loc[(sample,unit), "fq2"])')
    (12, '')
    (13, 'if config["merge"]["paired_End"]:')
    (14, '    reads = [1,2]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=quevedor2/meta-qiime2-snakemake, file=workflow/rules/common.smk
context_key: ["if config[\\'common\\'][\\'build\\'] == \\'hg19\\'"]
    (93, 'def get_ichorPath(rlib_path):')
    (94, '    #print(str(rlib_path))')
    (95, '    file=open(str(rlib_path), mode=\\\'r\\\',newline="\\')
    (96, '")')
    (97, '    rlib_path = file.read()')
    (98, '    #print(str(rlib_path))')
    (99, '    extdata  = str(rlib_path).rstrip() + "/ichorCNA/extdata/"')
    (100, '    ')
    (101, '    # Setup centromere file name (e.g. GRCh37 instead of hg19)')
    (102, "    if config[\\'common\\'][\\'build\\'] == \\'hg19\\':")
    (103, "        cen_file = \\'GRCh37.p13_centromere_UCSC-gapTable.txt\\'")
    (104, "    elif config[\\'common\\'][\\'build\\'] == \\'hg38\\':")
    (105, "        cen_file = \\'GRCh38.GCA_000001405.2_centromere_acen.txt\\'")
    (106, "        #cen_file = \\'cytoBand_hg38\\'")
    (107, '    ')
    (108, '    # Get the 500kb or 1Mb window annotation')
    (109, "    window_size = config[\\'params\\'][\\'readcounter\\'][\\'window\\']")
    (110, '    window_size = int(window_size / 1000)       # size in kb')
    (111, '    if window_size >= 1000:')
    (112, '        window_size_simple = str(int(window_size / 1000)) + "Mb"')
    (113, '    else:')
    (114, '        window_size_simple = str(window_size) + "kb"')
    (115, '    ')
    (116, '    # assemble wig file prefix')
    (117, '    # Expected Format: [gc/map]_[hg19/hg38]_[window_size]kb.wig')
    (118, '    wig_file = "_" + config[\\\'common\\\'][\\\'build\\\'] + "_" + str(window_size) + "kb.wig"')
    (119, '    normal_file = "HD_ULP_PoN_" + window_size_simple + "_median_normAutosome_mapScoreFiltered_median.rds"')
    (120, '    map_path    = extdata + "map" + wig_file')
    (121, '    gc_path     = extdata + "gc" + wig_file')
    (122, '    cen_path    = extdata + cen_file')
    (123, '    normal_path = extdata + normal_file')
    (124, '    return { "map":map_path, "gc":gc_path, "cen":cen_path, "norm":normal_path }')
    (125, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=crazyhottommy/pyflow-scATACseq, file=Snakefile
context_key: ["if config[\\'recount_all\\']"]
    (15, 'def get_cluster_id(csv_file):')
    (16, '    cluster_ids = []')
    (17, '    with open(csv_file) as ifile:')
    (18, "        csv_reader = csv.reader(ifile, delimiter=\\',\\')")
    (19, '        # if there is a header, skip it by')
    (20, '        #skip header')
    (21, '        header = next(csv_reader)')
    (22, '        for row in csv_reader:')
    (23, '            cluster_ids.append(row[1])')
    (24, '        cluster_ids = sorted(set(cluster_ids))')
    (25, '    return(cluster_ids)')
    (26, '')
    (27, '')
    (28, 'TARGET= []')
    (29, 'CLUSTERS_BAMS = expand("01split_bam/{sample}.split.touch", sample = SAMPLES)')
    (30, 'CLUSTERS_BAIS = []')
    (31, 'CLUSTERS_BIGWIGS = []')
    (32, 'PEAKS = []')
    (33, 'EXTEND_SUMMIT = []')
    (34, 'for sample in SAMPLES:')
    (35, '    CLUSTERS_BAIS.append(expand("01split_bam/{sample}/{sample}_{{cluster_id}}.bam.bai". \\\\')
    (36, '        format(sample = sample), cluster_id = get_cluster_id(FILES[sample][1])))')
    (37, '    CLUSTERS_BIGWIGS.extend(expand("02bigwigs/{sample}/{sample}_{{cluster_id}}.bw". \\\\')
    (38, '        format(sample = sample), cluster_id = get_cluster_id(FILES[sample][1])))')
    (39, '    PEAKS.extend(expand("05peak_filter/{sample}/{sample}_{{cluster_id}}_blacklist_removed.bed". \\\\')
    (40, '        format(sample = sample), cluster_id = get_cluster_id(FILES[sample][1])))')
    (41, '    EXTEND_SUMMIT.extend(expand("06extend_summit/{sample}/{sample}_{{cluster_id}}_extend_summit.bed". \\\\')
    (42, '        format(sample = sample), cluster_id = get_cluster_id(FILES[sample][1])))')
    (43, '')
    (44, '### for merged bam files')
    (45, 'map_sample_to_cluster = defaultdict(list)')
    (46, '')
    (47, 'for sample in SAMPLES:')
    (48, '    map_sample_to_cluster[sample].extend(get_cluster_id(FILES[sample][1]))')
    (49, '')
    (50, 'CLUSTERS = []')
    (51, 'for sample in map_sample_to_cluster:')
    (52, '    CLUSTERS.extend(map_sample_to_cluster[sample])')
    (53, 'CLUSTERS = sorted(set(CLUSTERS))')
    (54, '')
    (55, '')
    (56, '## some clusters only present in certain samples, when merge the same cluster from different ')
    (57, '## samples, this needs to be taken care of.')
    (58, '')
    (59, '## e.g.')
    (60, "## {\\'A\\':[\\'1\\',\\'2\\',\\'3\\'], \\'B\\':[\\'1\\',\\'2\\'], \\'C\\':[\\'1\\']}")
    (61, "## {\\'1\\':[\\'A\\', \\'B\\', \\'C\\'], \\'2\\':[\\'A\\',\\'B\\'], \\'3\\':[1]}")
    (62, '')
    (63, '## for cluster 1, merge bam from A B C samples')
    (64, '## for cluster 2 merge bam from A B samples')
    (65, '## for cluster 3 merge bam only from A sample.')
    (66, '')
    (67, 'map_cluster_to_sample = defaultdict(list)')
    (68, 'for k,v in map_sample_to_cluster.items():')
    (69, '    for x in v:')
    (70, '        map_cluster_to_sample[x].append(k)')
    (71, '')
    (72, ' ')
    (73, '')
    (74, 'MERGED_BIGWIGS = expand("02bigwigs_merge/{cluster_id}.bw", cluster_id = CLUSTERS)')
    (75, 'MERGED_EXTEND_SUMMIT = expand("06extend_summit_merge/{cluster_id}_extend_summit.bed", cluster_id = CLUSTERS )')
    (76, 'RECOUNT_ALL = expand("07recount_all/{sample}/{sample}.mtx", sample = SAMPLES)')
    (77, 'RECOUNT = expand("07recount/{sample}/{sample}.mtx", sample = SAMPLES)')
    (78, 'DIFFPEAKS = expand("08diff_peaks/{sample}/{sample}_differential_accessible_peaks.txt", sample =SAMPLES)')
    (79, '')
    (80, '')
    (81, 'TARGET.extend(CLUSTERS_BAMS)')
    (82, 'TARGET.extend(CLUSTERS_BAIS)')
    (83, 'TARGET.extend(CLUSTERS_BIGWIGS)')
    (84, 'TARGET.extend(PEAKS)')
    (85, 'TARGET.extend(EXTEND_SUMMIT)')
    (86, 'TARGET.extend(MERGED_BIGWIGS)')
    (87, 'TARGET.extend(MERGED_EXTEND_SUMMIT)')
    (88, 'TARGET.extend(DIFFPEAKS)')
    (89, '')
    (90, "if config[\\'recount_all\\']:")
    (91, '    TARGET.extend(RECOUNT_ALL)')
    (92, '    TARGET.append("08diff_peaks_all/all_sample_differential_accessible_peaks.txt")')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AnzeLovse/mag, file=workflow/rules/common.smk
context_key: ['if config["trimming"]["skip"]']
    (8, 'def get_fastq(wildcards):')
    (9, '    """Get file names of reads for FASTQC."""')
    (10, '    if config["trimming"]["skip"]:')
    (11, '        return replicates.loc[(wildcards.sample, wildcards.rep, wildcards.time), ["fq1", "fq2"]].dropna()')
    (12, '    else:')
    (13, '        if is_paired:')
    (14, '            return expand(')
    (15, '                    "trimmed/{sample}-t{time}-{rep}_{pair}.fastq.gz", pair=[1, 2], **wildcards')
    (16, '                )')
    (17, '        else:')
    (18, '            return ["trimmed/{sample}-t{time}-{rep}_1.fastq.gz"]')
    (19, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AnzeLovse/mag, file=workflow/rules/common.smk
context_key: ['if config["trimming"]["skip"]', 'if is_paired']
    (20, 'def get_fastqc_outputs(wildcards):')
    (21, '    """Get all FASTQC outputs for a sample."""')
    (22, '    if config["trimming"]["skip"]:')
    (23, '        if is_paired:')
    (24, '            return expand(')
    (25, '                    "qc/fastqc/{sample}-t{time}-{rep}_R{pair}_fastqc.zip", pair=[1, 2],')
    (26, '                    sample=replicates["sample"], rep=replicates["replicate"], time=replicates["time"],')
    (27, '                )')
    (28, '        else:')
    (29, '            return expand(')
    (30, '                "qc/fastqc/{sample}-t{time}-{rep}_R1_fastqc.zip",')
    (31, '                sample=replicates["sample"], rep=replicates["replicate"], time=replicates["time"],)')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AnzeLovse/mag, file=workflow/rules/common.smk
context_key: ['if config["trimming"]["skip"]', 'else', 'if is_paired']
    (20, 'def get_fastqc_outputs(wildcards):')
    (21, '    """Get all FASTQC outputs for a sample."""')
    (22, '    if config["trimming"]["skip"]:')
    (23, '        if is_paired:')
    (24, '            return expand(')
    (25, '                    "qc/fastqc/{sample}-t{time}-{rep}_R{pair}_fastqc.zip", pair=[1, 2],')
    (26, '                    sample=replicates["sample"], rep=replicates["replicate"], time=replicates["time"],')
    (27, '                )')
    (28, '        else:')
    (29, '            return expand(')
    (30, '                "qc/fastqc/{sample}-t{time}-{rep}_R1_fastqc.zip",')
    (31, '                sample=replicates["sample"], rep=replicates["replicate"], time=replicates["time"],)')
    (32, '    else:')
    (33, '        if is_paired:')
    (34, '            samples = [')
    (35, '                f"qc/{{step}}/{sample}-t{time}-{rep}_R{{pair}}_fastqc.zip"')
    (36, '                for sample, rep, time in replicates.index.to_list()')
    (37, '            ]')
    (38, '            return expand(')
    (39, '                    samples, step=["fastqc", "fastqc_posttrim"], pair=[1, 2]')
    (40, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AnzeLovse/mag, file=workflow/rules/common.smk
context_key: ['if config["trimming"]["skip"]', 'else', 'else']
    (20, 'def get_fastqc_outputs(wildcards):')
    (21, '    """Get all FASTQC outputs for a sample."""')
    (22, '    if config["trimming"]["skip"]:')
    (23, '        if is_paired:')
    (24, '            return expand(')
    (25, '                    "qc/fastqc/{sample}-t{time}-{rep}_R{pair}_fastqc.zip", pair=[1, 2],')
    (26, '                    sample=replicates["sample"], rep=replicates["replicate"], time=replicates["time"],')
    (27, '                )')
    (28, '        else:')
    (29, '            return expand(')
    (30, '                "qc/fastqc/{sample}-t{time}-{rep}_R1_fastqc.zip",')
    (31, '                sample=replicates["sample"], rep=replicates["replicate"], time=replicates["time"],)')
    (32, '    else:')
    (33, '        if is_paired:')
    (34, '            samples = [')
    (35, '                f"qc/{{step}}/{sample}-t{time}-{rep}_R{{pair}}_fastqc.zip"')
    (36, '                for sample, rep, time in replicates.index.to_list()')
    (37, '            ]')
    (38, '            return expand(')
    (39, '                    samples, step=["fastqc", "fastqc_posttrim"], pair=[1, 2]')
    (40, '                )')
    (41, '        else:')
    (42, '            return expand(')
    (43, '                    "qc/{step}/{sample}-t{time}-{rep}_R1_fastqc.zip", step=["fastqc", "fastqc_posttrim"],')
    (44, '                     sample=replicates["sample"], rep=replicates["replicate"], time=replicates["time"],')
    (45, '                )')
    (46, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AnzeLovse/mag, file=workflow/rules/common.smk
context_key: ['if config["trimming"]["adapter_1"]']
    (74, 'def get_trim_params(wildcards):')
    (75, '    """Get a string with Cutadapt parameters."""')
    (76, '    params=[')
    (77, '        "--quality-cutoff", config["trimming"]["min_qual"],')
    (78, '        "--minimum-length", config["trimming"]["min_len"]]')
    (79, '    if config["trimming"]["adapter_1"]:')
    (80, '        params.extend(["-a", config["trimming"]["adapter_1"]])')
    (81, '    if config["trimming"]["adapter_2"]:')
    (82, '        params.extend(["-A", config["trimming"]["adapter_2"]])')
    (83, '    if config["trimming"]["additional"]:')
    (84, '        params.append(config["trimming"]["additional"])')
    (85, '')
    (86, '    return " ".join(params)')
    (87, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sorjuela/age_lesions_females, file=Snakefile
context_key: ['if config["useCondaR"] == True', 'else', 'rule all', 'rule setup', 'rule runfastqc', 'rule runtrimming', 'rule runbismark', 'rule softwareversions', 'rule bismarkindex', 'rule fastqc', 'rule fastqctrimmed', 'rule multiqc', 'rule trimgalorePE', 'rule bismarkPE', 'rule extractMethylation', 'onsuccess', 'onerror']
    (36, 'def getpath(str):')
    (37, "\\tif str in [\\'\\', \\'.\\', \\'./\\']:")
    (38, "\\t\\treturn \\'\\'")
    (39, "\\tif str.startswith(\\'./\\'):")
    (40, "\\t\\tregex = re.compile(\\'^\\\\./?\\')")
    (41, "\\t\\tstr = regex.sub(\\'\\', str)")
    (42, "\\tif not str.endswith(\\'/\\'):")
    (43, "\\t\\tstr += \\'/\\'")
    (44, '\\treturn str')
    (45, '')
    (46, 'outputdir = getpath(config["output"])')
    (47, 'FASTQdir = getpath(config["FASTQ"])')
    (48, '')
    (49, '## Define the conda environment for all rules using R')
    (50, 'if config["useCondaR"] == True:')
    (51, '\\tRenv = "envs/environment_R.yaml"')
    (52, 'else:')
    (53, '\\tRenv = "envs/environment.yaml"')
    (54, '')
    (55, '## ------------------------------------------------------------------------------------ ##')
    (56, '## Target definitions')
    (57, '## ------------------------------------------------------------------------------------ ##')
    (58, '## Run all analyses')
    (59, 'rule all:')
    (60, '\\tinput:')
    (61, '\\t\\toutputdir + "MultiQC/multiqc_report.html",')
    (62, '\\t\\texpand(outputdir + "methextract/{sample}_pe.bismark.cov.gz", sample = samples.names.values.tolist())')
    (63, '')
    (64, 'rule setup:')
    (65, '\\tinput:')
    (66, '\\t\\toutputdir + "Rout/pkginstall_state.txt",')
    (67, '\\t\\toutputdir + "Rout/softwareversions.done"')
    (68, '')
    (69, '')
    (70, '## FastQC on original (untrimmed) files')
    (71, 'rule runfastqc:')
    (72, '\\tinput:')
    (73, '\\t\\texpand(outputdir + "FastQC/{sample}_" + str(config["fqext1"]) + "_fastqc.zip", sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (74, '\\t\\texpand(outputdir + "FastQC/{sample}_" + str(config["fqext2"]) + "_fastqc.zip", sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (75, '\\t\\texpand(outputdir + "FastQC/{sample}_fastqc.zip", sample = samples.names[samples.type == \\\'SE\\\'].values.tolist())')
    (76, '')
    (77, '## Trimming and FastQC on trimmed files')
    (78, 'rule runtrimming:')
    (79, '\\tinput:')
    (80, '\\t\\texpand(outputdir + "FastQC/{sample}_" + str(config["fqext1"]) + "_val_1_fastqc.zip", sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (81, '\\t\\texpand(outputdir + "FastQC/{sample}_" + str(config["fqext2"]) + "_val_2_fastqc.zip", sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()),')
    (82, '\\t\\texpand(outputdir + "FastQC/{sample}_trimmed_fastqc.zip", sample = samples.names[samples.type == \\\'SE\\\'].values.tolist())')
    (83, '')
    (84, 'rule runbismark:')
    (85, '\\tinput:')
    (86, '\\t\\texpand(outputdir + "bismark/{sample}_pe.bam", sample = samples.names.values.tolist())')
    (87, '')
    (88, '')
    (89, '## Print the versions of all software packages')
    (90, 'rule softwareversions:')
    (91, '\\toutput:')
    (92, '\\t\\ttouch(outputdir + "Rout/softwareversions.done")')
    (93, '\\tconda:')
    (94, '\\t\\t"envs/environment.yaml"')
    (95, '\\tshell:')
    (96, '\\t\\t"echo -n \\\'cutadapt \\\' && cutadapt --version; "')
    (97, '\\t\\t"fastqc --version; samtools --version; multiqc --version; "')
    (98, '\\t\\t"bedtools --version"')
    (99, '')
    (100, '## ------------------------------------------------------------------------------------ ##')
    (101, '## Reference preparation')
    (102, '## ------------------------------------------------------------------------------------ ##')
    (103, '## Generate Bismark index')
    (104, 'rule bismarkindex:')
    (105, '\\tinput:')
    (106, '\\t\\tgenome = config["genome"]')
    (107, '\\toutput:')
    (108, '\\t\\tconfig["genome"] + "Bisulfite_Genome/CT_conversion/genome_mfa.CT_conversion.fa",')
    (109, '\\t\\tconfig["genome"] + "Bisulfite_Genome/GA_conversion/genome_mfa.GA_conversion.fa"')
    (110, '\\tlog:')
    (111, '\\t\\toutputdir + "logs/bismark_index.log"')
    (112, '\\tbenchmark:')
    (113, '\\t\\toutputdir + "benchmarks/bismark_index.txt"')
    (114, '\\tconda:')
    (115, '\\t\\t"envs/environment.yaml"')
    (116, '\\tthreads:')
    (117, '\\t\\tconfig["ncores"]')
    (118, '\\tshell:')
    (119, '\\t\\t"bismark --version >> {log}; "')
    (120, '\\t\\t"bismark_genome_preparation --verbose {input.genome}"')
    (121, '')
    (122, '## ------------------------------------------------------------------------------------ ##')
    (123, '## Quality control')
    (124, '## ------------------------------------------------------------------------------------ ##')
    (125, '## FastQC, original reads')
    (126, 'rule fastqc:')
    (127, '\\tinput:')
    (128, '\\t\\tfastq = FASTQdir + "{sample}." + str(config["fqsuffix"]) + ".gz"')
    (129, '\\toutput:')
    (130, '\\t\\toutputdir + "FastQC/{sample}_fastqc.zip"')
    (131, '\\tparams:')
    (132, '\\t\\tFastQC = outputdir + "FastQC"')
    (133, '\\tlog:')
    (134, '\\t\\toutputdir + "logs/fastqc_{sample}.log"')
    (135, '\\tbenchmark:')
    (136, '\\t\\toutputdir + "benchmarks/fastqc_{sample}.txt"')
    (137, '\\tconda:')
    (138, '\\t\\t"envs/environment.yaml"')
    (139, '\\tthreads:')
    (140, '\\t\\tconfig["ncores"]')
    (141, '\\tshell:')
    (142, '\\t\\t"echo \\\'FastQC version:\\')
    (143, '\\\' > {log}; fastqc --version >> {log}; "')
    (144, '\\t\\t"fastqc -o {params.FastQC} -t {threads} {input.fastq}"')
    (145, '')
    (146, '## FastQC, trimmed reads')
    (147, 'rule fastqctrimmed:')
    (148, '\\tinput:')
    (149, '\\t\\tfastq = outputdir + "FASTQtrimmed/{sample}.fq.gz"')
    (150, '\\toutput:')
    (151, '\\t\\toutputdir + "FastQC/{sample}_fastqc.zip"')
    (152, '\\tparams:')
    (153, '\\t\\tFastQC = outputdir + "FastQC"')
    (154, '\\tlog:')
    (155, '\\t\\toutputdir + "logs/fastqc_trimmed_{sample}.log"')
    (156, '\\tbenchmark:')
    (157, '\\t\\toutputdir + "benchmarks/fastqc_trimmed_{sample}.txt"')
    (158, '\\tconda:')
    (159, '\\t\\t"envs/environment.yaml"')
    (160, '\\tthreads:')
    (161, '\\t\\tconfig["ncores"]')
    (162, '\\tshell:')
    (163, '\\t\\t"echo \\\'FastQC version:\\')
    (164, '\\\' > {log}; fastqc --version >> {log}; "')
    (165, '\\t\\t"fastqc -o {params.FastQC} -t {threads} {input.fastq}"')
    (166, '')
    (167, '')
    (168, '')
    (169, '# The config.yaml files determines which steps should be performed')
    (170, 'def multiqc_input(wildcards):')
    (171, '\\tinput = []')
    (172, '\\tinput.extend(expand(outputdir + "FastQC/{sample}_" + str(config["fqext1"]) + "_fastqc.zip", sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (173, '\\tinput.extend(expand(outputdir + "FastQC/{sample}_" + str(config["fqext2"]) + "_fastqc.zip", sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (174, '\\tinput.extend(expand(outputdir + "bismark/{sample}_pe.bam", sample = samples.names.values.tolist()))')
    (175, '\\t')
    (176, '\\tif config["run_trimming"]:')
    (177, '\\t\\tinput.extend(expand(outputdir + "FASTQtrimmed/{sample}_" + str(config["fqext1"]) + "_val_1.fq.gz", sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (178, '\\t\\tinput.extend(expand(outputdir + "FASTQtrimmed/{sample}_" + str(config["fqext2"]) + "_val_2.fq.gz", sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (179, '\\t\\tinput.extend(expand(outputdir + "FastQC/{sample}_" + str(config["fqext1"]) + "_val_1_fastqc.zip", sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (180, '\\t\\tinput.extend(expand(outputdir + "FastQC/{sample}_" + str(config["fqext2"]) + "_val_2_fastqc.zip", sample = samples.names[samples.type == \\\'PE\\\'].values.tolist()))')
    (181, '\\treturn input')
    (182, '')
    (183, '## Determine the input directories for MultiQC depending on the config file')
    (184, 'def multiqc_params(wildcards):')
    (185, '\\tparam = [outputdir + "FastQC",')
    (186, '\\toutputdir + "bismark"]')
    (187, '\\tif config["run_trimming"]:')
    (188, '\\t\\tparam.append(outputdir + "FASTQtrimmed")')
    (189, '\\treturn param')
    (190, '')
    (191, '## MultiQC')
    (192, 'rule multiqc:')
    (193, '\\tinput:')
    (194, '\\t\\tmultiqc_input')
    (195, '\\toutput:')
    (196, '\\t\\toutputdir + "MultiQC/multiqc_report.html"')
    (197, '\\tparams:')
    (198, '\\t\\tinputdirs = multiqc_params,')
    (199, '\\t\\tMultiQCdir = outputdir + "MultiQC"')
    (200, '\\tlog:')
    (201, '\\t\\toutputdir + "logs/multiqc.log"')
    (202, '\\tbenchmark:')
    (203, '\\t\\toutputdir + "benchmarks/multiqc.txt"')
    (204, '\\tconda:')
    (205, '\\t\\t"envs/environment.yaml"')
    (206, '\\tshell:')
    (207, '\\t\\t"echo \\\'MultiQC version:\\')
    (208, '\\\' > {log}; multiqc --version >> {log}; "')
    (209, '\\t\\t"multiqc {params.inputdirs} -f -o {params.MultiQCdir}"')
    (210, '\\t\\t')
    (211, '')
    (212, '## ------------------------------------------------------------------------------------ ##')
    (213, '## Adapter trimming')
    (214, '## ------------------------------------------------------------------------------------ ##')
    (215, '# TrimGalore!')
    (216, 'rule trimgalorePE:')
    (217, '\\tinput:')
    (218, '\\t\\tfastq1 = FASTQdir + "{sample}_" + str(config["fqext1"]) + "." + str(config["fqsuffix"]) + ".gz",')
    (219, '\\t\\tfastq2 = FASTQdir + "{sample}_" + str(config["fqext2"]) + "." + str(config["fqsuffix"]) + ".gz"')
    (220, '\\toutput:')
    (221, '\\t\\toutputdir + "FASTQtrimmed/{sample}_" + str(config["fqext1"]) + "_val_1.fq.gz",')
    (222, '\\t\\toutputdir + "FASTQtrimmed/{sample}_" + str(config["fqext2"]) + "_val_2.fq.gz"')
    (223, '\\tparams:')
    (224, '\\t\\tFASTQtrimmeddir = outputdir + "FASTQtrimmed"')
    (225, '\\tlog:')
    (226, '\\t\\toutputdir + "logs/trimgalore_{sample}.log"')
    (227, '\\tbenchmark:')
    (228, '\\t\\toutputdir + "benchmarks/trimgalore_{sample}.txt"')
    (229, '\\tconda:')
    (230, '\\t\\t"envs/environment.yaml"')
    (231, '\\tshell:')
    (232, '\\t\\t"echo \\\'TrimGalore! version:\\')
    (233, '\\\' > {log}; trim_galore --version >> {log}; "')
    (234, '\\t\\t"trim_galore --clip_R1 5 --clip_R2 5 --three_prime_clip_R1 5 --three_prime_clip_R2 5 "')
    (235, '\\t\\t"-o {params.FASTQtrimmeddir} --path_to_cutadapt cutadapt --paired {input.fastq1} {input.fastq2}"')
    (236, '')
    (237, '## ------------------------------------------------------------------------------------ ##')
    (238, '## Bismark mapping')
    (239, '## ------------------------------------------------------------------------------------ ##')
    (240, '## Genome mapping with bismark')
    (241, '')
    (242, 'rule bismarkPE:')
    (243, '\\tinput:')
    (244, '\\t\\tconfig["genome"] + "Bisulfite_Genome/CT_conversion/genome_mfa.CT_conversion.fa",')
    (245, '\\t\\tconfig["genome"] + "Bisulfite_Genome/GA_conversion/genome_mfa.GA_conversion.fa",')
    (246, '\\t\\tfastq1 = outputdir + "FASTQtrimmed/{sample}_" + str(config["fqext1"]) + "_val_1.fq.gz" if config["run_trimming"] else FASTQdir + "{sample}_" + str(config["fqext1"]) + "." + str(config["fqsuffix"]) + ".gz",')
    (247, '\\t\\tfastq2 = outputdir + "FASTQtrimmed/{sample}_" + str(config["fqext2"]) + "_val_2.fq.gz" if config["run_trimming"] else FASTQdir + "{sample}_" + str(config["fqext2"]) + "." + str(config["fqsuffix"]) + ".gz"')
    (248, '\\toutput:')
    (249, '\\t\\toutputdir + "bismark/{sample}_pe.bam"')
    (250, '\\tthreads:')
    (251, '\\t\\tconfig["ncores"]')
    (252, '\\tlog:')
    (253, '\\t\\toutputdir + "logs/bismark_{sample}.log"')
    (254, '\\tbenchmark:')
    (255, '\\t\\toutputdir + "benchmarks/bismark_{sample}.txt"')
    (256, '\\tparams:')
    (257, '\\t\\tgenome = config["genome"],')
    (258, '\\t\\tbismarkdir = outputdir + "bismark",')
    (259, '\\t\\tsamplename = "{sample}"')
    (260, '\\tconda:')
    (261, '\\t\\t"envs/environment.yaml"')
    (262, '\\tshell:')
    (263, '\\t\\t"bismark --version >> {log}; "')
    (264, '\\t\\t"bismark -p {threads} -B {params.samplename} -o {params.bismarkdir} --genome {params.genome} "')
    (265, '\\t\\t"-1 {input.fastq1} -2 {input.fastq2}"')
    (266, '\\t\\t')
    (267, '')
    (268, 'rule extractMethylation:')
    (269, '\\tinput:')
    (270, '\\t\\toutputdir + "bismark/{sample}_pe.bam"')
    (271, '\\toutput:')
    (272, '\\t\\toutputdir + "methextract/{sample}_pe.bismark.cov.gz"')
    (273, '\\tthreads:')
    (274, '\\t\\tconfig["ncores"]')
    (275, '\\tlog:')
    (276, '\\t\\toutputdir + "logs/methextract_{sample}.log"')
    (277, '\\tbenchmark:')
    (278, '\\t\\toutputdir + "benchmarks/methextract_{sample}.txt"')
    (279, '\\tparams:')
    (280, '\\t\\tgenome = config["genome"],')
    (281, '\\t\\tmethdir = outputdir + "methextract"')
    (282, '\\tconda:')
    (283, '\\t\\t"envs/environment.yaml"')
    (284, '\\tshell:')
    (285, '\\t\\t"bismark --version >> {log}; "')
    (286, '\\t\\t"bismark_methylation_extractor -p --cytosine_report --comprehensive "')
    (287, '\\t\\t" --genome_folder {params.genome} --no_overlap --multicore {threads} --buffer_size 2G "')
    (288, '\\t\\t" -o {params.methdir} {input}"')
    (289, '\\t')
    (290, '')
    (291, '## ------------------------------------------------------------------------------------ ##')
    (292, '## Success and failure messages')
    (293, '## ------------------------------------------------------------------------------------ ##')
    (294, 'onsuccess:')
    (295, '\\tprint("Success! The Snakemake workflow is completed.")')
    (296, '')
    (297, 'onerror:')
    (298, '\\tprint("Error! The Snakemake workflow aborted.")')
    (299, "'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ["if config[\\'admixMapping\\'][\\'skip\\'] != \\'true\\'"]
    (90, 'def get_all_inputs(wildcards): #Primary rule whose input values determine which outputs (and corresponding rules) will be run by snakemake')
    (91, '    input_list = ["accessory/.sHWE_pass.txt", f"sHWE/.optimum_popnum.txt", f"{BASE}-rulegraph.png",')
    (92, '                  f"plink/{BASE}_LDp_sHWE.bed", f"{QUERY}.bed", f"plink/{BASE}.eigenvec",')
    (93, '                  f"accessory/samples.txt", f"input/{BASE}_CtlMat.bed", f"plink/{BASE}.genome.gz",')
    (94, '                  f"{BASE}.genesis.txt", f"{BASE}-Mapping-report.pdf"] #, f"{BASE}.genesis.sig.annt.txt"')
    (95, '    #input_list += [f"{BASE}.blink.txt", f"{BASE}.blink.sig.annt.txt"] #Uncomment to enable GWAS with BLINK.  Newer version of BLINK was causing issues with singularity.')
    (96, "    if config[\\'admixMapping\\'][\\'skip\\'] != \\'true\\': #Do not include admixture mapping in desired results, unless requested.")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ["if config[\\'match_controls\\'] == \\'true\\'"]
    (101, 'def get_cM_inputs(wildcards): #Conditionally determine control-matching input')
    (102, '    input_list = [f"plink/{BASE}_LDp_sHWE_IBDflt.bed"]')
    (103, "    if config[\\'match_controls\\'] == \\'true\\': #IBD results are needed if control matching is requested.  This function will request these results to be produced if match_controls is set in config.")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ["if config[\\'admixMapping\\'][\\'skip\\'] != \\'true\\'"]
    (107, 'def get_dataprep_input(wildcards): #conditionally determine dataprep input for admixture mapping')
    (108, '    input_list = ["accessory/samples.txt"]')
    (109, '    inFile=[]')
    (110, "    if config[\\'admixMapping\\'][\\'skip\\'] != \\'true\\':")
    (111, '        inFile = [f"{RFMIX.rstrip(\\\'/\\\')}/{x}" for x in os.listdir(RFMIX) if x.endswith(f"chr{wildcards.CHROM}.msp.tsv")]')
    (112, '        input_list.append(f"{BASE}.globalancestry.txt")')
    (113, '        assert len(inFile) == 1')
    (114, '    return(input_list + inFile + ["accessory/excluded_related_samples.txt"])')
    (115, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=Monia234/admix-map, file=workflow/Snakefile
context_key: ["if config[\\'admixMapping\\'][\\'skip\\'] != \\'true\\'"]
    (116, 'def get_report_input(wildcards):')
    (117, '    input_list = [f"{BASE}-rulegraph.png", f"{BASE}.genesis.txt"]')
    (118, '    if config[\\\'admixMapping\\\'][\\\'skip\\\'] != \\\'true\\\': input_list += [f"{BASE}.admixmap.txt", f"{BASE}.globalancestry.txt"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=momo54/sage-orderby-experiment, file=Snakefile
context_key: ['if config["experiments"][xp]["check"]']
    (61, 'def xp_files(wcs):')
    (62, '    name = config["name"]')
    (63, '    output = config["output"]')
    (64, '    for xp in config["experiments"]:')
    (65, '        if config["experiments"][xp]["check"]:')
    (66, '            return [f"{output}/{name}/run.csv", f"{output}/{name}/check.csv"]')
    (67, '    return [f"{output}/{name}/run.csv"]')
    (68, '')
    (69, '')
    (70, '# def xp_archive(wcs):')
    (71, '#     name = config["name"]')
    (72, '#     output = config["output"]')
    (73, '#     return ancient(f"{output}/{name}/xp.tar.gz")')
    (74, '')
    (75, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=gustaveroussy/rna-count-kallisto, file=rules/common.smk
context_key: ['if config["workflow"]["fastqc"] is True']
    (209, 'def get_targets() -> Dict[str, Any]:\\r')
    (210, '    """\\r')
    (211, '    This function returns the targets of Snakemake\\r')
    (212, '    following the requests from the user.\\r')
    (213, '    """\\r')
    (214, '    targets = {}\\r')
    (215, '    if config["workflow"]["fastqc"] is True:\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=gustaveroussy/rna-count-kallisto, file=rules/common.smk
context_key: ['if config["workflow"]["multiqc"] is True']
    (209, 'def get_targets() -> Dict[str, Any]:\\r')
    (210, '    """\\r')
    (211, '    This function returns the targets of Snakemake\\r')
    (212, '    following the requests from the user.\\r')
    (213, '    """\\r')
    (214, '    targets = {}\\r')
    (215, '    if config["workflow"]["fastqc"] is True:\\r')
    (216, '        targets["fastqc"] = expand(\\r')
    (217, '            "qc/fastqc/{samples}_fastqc.{ext}",\\r')
    (218, '            samples=fq_root_dict.keys(),\\r')
    (219, '            ext=["html", "zip"]\\r')
    (220, '        )\\r')
    (221, '    if config["workflow"]["multiqc"] is True:\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=gustaveroussy/rna-count-kallisto, file=rules/common.smk
context_key: ['if config["workflow"]["aggregate"] is True']
    (209, 'def get_targets() -> Dict[str, Any]:\\r')
    (210, '    """\\r')
    (211, '    This function returns the targets of Snakemake\\r')
    (212, '    following the requests from the user.\\r')
    (213, '    """\\r')
    (214, '    targets = {}\\r')
    (215, '    if config["workflow"]["fastqc"] is True:\\r')
    (216, '        targets["fastqc"] = expand(\\r')
    (217, '            "qc/fastqc/{samples}_fastqc.{ext}",\\r')
    (218, '            samples=fq_root_dict.keys(),\\r')
    (219, '            ext=["html", "zip"]\\r')
    (220, '        )\\r')
    (221, '    if config["workflow"]["multiqc"] is True:\\r')
    (222, '        targets["multiqc"] = "qc/multiqc_report.html"\\r')
    (223, '    if config["workflow"]["aggregate"] is True:\\r')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cshlwyang/RNAseq, file=rules/align.smk
context_key: ['if config["trimming"]["skip"]']
    (16, 'def get_fileend(wildcards):')
    (17, '    if config["trimming"]["skip"]:')
    (18, '        fq1=units.loc[(wildcards.sample, wildcards.unit), ["fq1"]].dropna()')
    (19, '    else:')
    (20, '        if not is_single_end(**wildcards):')
    (21, '            fq1=expand("trimmed/{sample}-{unit}.{group}.fastq.gz",')
    (22, '                          group=1, **wildcards)')
    (23, '        else:')
    (24, '            fq1=expand("trimmed/{sample}-{unit}.fastq.gz", **wildcards),')
    (25, '    if fq1[0].endswith(".gz"):')
    (26, '        readcmd = "--readFilesCommand zcat"')
    (27, '    else:')
    (28, '        readcmd = ""')
    (29, '    return readcmd')
    (30, '')
    (31, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yzeng-lol/tcge-cfmedip-seq-pipeline, file=workflow/rules/common.smk
context_key: ['if config["aggreate"] and config["paired-end"] and config["spike_in"]']
    (43, 'def get_rule_all_input():')
    (44, '    ## ensure extra env installed')
    (45, '    extra_env = "extra_env/all_extra_env_installed",')
    (46, '')
    (47, '    ## fixed outputs')
    (48, '    meth_qc = "aggregated/meth_qc.txt",')
    (49, '    meta_quant = "aggregated/meth_count.txt.gz",')
    (50, '    meth_filt = "autos_bfilt/meth_count_autos_bfilt.txt.gz",')
    (51, '')
    (52, '    ######################################')
    (53, '    ## aggregated outputs for SAMPLES_aggr')
    (54, '    ## paired-end and spike-in')
    (55, '    if config["aggreate"] and config["paired-end"] and config["spike_in"]:')
    (56, '        mult_qc = "aggregated/QC_pe/multiqc_report.html",')
    (57, '')
    (58, '        ## spike-ins')
    (59, '        spikein_mult_qc = "aggregated_spikein/QC_pe/multiqc_report.html",')
    (60, '        spikein_meth_qc = "aggregated_spikein/meth_qc.txt",')
    (61, '        spikein_meta_quant = "aggregated_spikein/meth_count.txt.gz",')
    (62, '')
    (63, '        #fragment profiles')
    (64, '        fp_gc = "aggregated/fragment_profile_GC_corrected_1mb.tsv",        ## GC corrected fragment profile')
    (65, '')
    (66, '        return  extra_env + mult_qc + meth_qc + meta_quant + meth_filt + spikein_mult_qc + spikein_meth_qc + spikein_meta_quant + fp_gc')
    (67, '')
    (68, '    ## paired-end and no spike-in')
    (69, '    elif config["aggreate"] and config["paired-end"] and config["spike_in"] == False:')
    (70, '        mult_qc = "aggregated/QC_pe/multiqc_report.html",')
    (71, '        fp_gc = "aggregated/fragment_profile_GC_corrected_1mb.tsv",        ## GC corrected fragment profile')
    (72, '')
    (73, '        return  extra_env + mult_qc + meth_qc + meta_quant + meth_filt + fp_gc')
    (74, '')
    (75, '    ## single-end')
    (76, '    elif config["aggreate"] and config["paired-end"] == False:')
    (77, '        mult_qc = "aggregated/QC_se/multiqc_report.html",')
    (78, '        return extra_env + mult_qc + meth_qc + meta_quant + meth_filt')
    (79, '')
    (80, '')
    (81, '    #####################################')
    (82, '    ## outputs for each individual sample')
    (83, '    ## paired-end and spike-in')
    (84, '    elif config["aggreate"] == False and config["paired-end"] and config["spike_in"]:')
    (85, '        fq_qc = expand("fastqc_pe/{samples}_R2_fastqc.zip", samples = SAMPLES["sample_id"]),')
    (86, '        meth_out = expand("meth_qc_quant/{samples}_count.txt", samples = SAMPLES["sample_id"]),')
    (87, '        meth_spikein = expand("meth_qc_quant_spikein/{samples}_count.txt", samples = SAMPLES["sample_id"]),')
    (88, '        frag_size = expand("fragment_size/{samples}_insert_size_metrics.txt", samples = SAMPLES["sample_id"]),')
    (89, '        fp_gc = expand("fragment_profile/{samples}_50_Granges.bed", samples = SAMPLES["sample_id"]),')
    (90, '')
    (91, '        return extra_env + fq_qc + frag_size + meth_out + meth_spikein + fp_gc')
    (92, '')
    (93, '    ## paired-end without spike-in')
    (94, '    elif config["aggreate"] == False and config["paired-end"] and config["spike_in"] == False:')
    (95, '        fq_qc = expand("fastqc_pe/{samples}_R2_fastqc.zip", samples = SAMPLES["sample_id"]),')
    (96, '        meth_out = expand("meth_qc_quant/{samples}_count.txt", samples = SAMPLES["sample_id"]),')
    (97, '        frag_size = expand("fragment_size/{samples}_insert_size_metrics.txt", samples = SAMPLES["sample_id"]),')
    (98, '        fp_gc = expand("fragment_profile/{samples}_50_Granges.bed", samples = SAMPLES["sample_id"]),')
    (99, '')
    (100, '        return extra_env + fq_qc + frag_size + meth_out + fp_gc')
    (101, '')
    (102, '    ## single-end')
    (103, '    elif config["aggreate"] == False and config["paired-end"] == False:')
    (104, '         fq_qc = expand("fastqc_se/{samples}_R2_fastqc.zip", samples = SAMPLES["sample_id"]),')
    (105, '         meth_out = expand("meth_qc_quant/{samples}_count.txt", samples = SAMPLES["sample_id"]),')
    (106, '')
    (107, '         return extra_env + fq_qc + meth_out')
    (108, '')
    (109, '')
    (110, '')
    (111, '')
    (112, '############################')
    (113, '## other functions for input')
    (114, '#############################')
    (115, '')
    (116, '###############################')
    (117, '##  get corresponding bwa_index')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yzeng-lol/tcge-cfmedip-seq-pipeline, file=workflow/rules/common.smk
context_key: ['if config["spike_in"]']
    (118, 'def get_bwa_index():')
    (119, '    if config["spike_in"]:')
    (120, '        #return REF.loc["bwa_idx_spikein"][1]')
    (121, '        return config["spike_idx"]')
    (122, '    else:')
    (123, '        return REF.loc["bwa_index"][1]')
    (124, '')
    (125, '')
    (126, '################################################################')
    (127, '##  get raw fastq files for rename to consistant wildcard.sample')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yzeng-lol/tcge-cfmedip-seq-pipeline, file=workflow/rules/common.smk
context_key: ['if config["paired-end"] == False']
    (128, 'def get_raw_fastq_se(wildcards):')
    (129, '    if config["paired-end"] == False:')
    (130, '        return SAMPLES.loc[wildcards.sample]["R1"].split(",")')
    (131, '    else:')
    (132, '        return ""')
    (133, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yzeng-lol/tcge-cfmedip-seq-pipeline, file=workflow/rules/common.smk
context_key: ['if config["paired-end"]']
    (134, 'def get_raw_fastq_pe_R1(wildcards):')
    (135, '    if config["paired-end"]:')
    (136, '        return SAMPLES.loc[wildcards.sample]["R1"].split(",")')
    (137, '    else:')
    (138, '        return ""')
    (139, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yzeng-lol/tcge-cfmedip-seq-pipeline, file=workflow/rules/common.smk
context_key: ['if config["paired-end"]']
    (140, 'def get_raw_fastq_pe_R2(wildcards):')
    (141, '    if config["paired-end"]:')
    (142, '        return SAMPLES.loc[wildcards.sample]["R2"].split(",")')
    (143, '    else:')
    (144, '        return ""')
    (145, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yzeng-lol/tcge-cfmedip-seq-pipeline, file=workflow/rules/common.smk
context_key: ['if config["paired-end"]']
    (148, 'def get_raw_fastq(wildcards):')
    (149, '    if config["paired-end"]:')
    (150, '        R1 = SAMPLES.loc[wildcards.sample]["R1"],')
    (151, '        R2 = SAMPLES.loc[wildcards.sample]["R2"],')
    (152, '        return R1 + R2')
    (153, '    else:')
    (154, '        return SAMPLES.loc[wildcards.sample]["R1"]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yzeng-lol/tcge-cfmedip-seq-pipeline, file=workflow/rules/common.smk
context_key: ['if config["paired-end"]']
    (159, 'def get_renamed_fastq(wildcards):')
    (160, '    if config["paired-end"]:')
    (161, '        R1 = "renamed_fq/{}_R1.fastq.gz".format(wildcards.sample),')
    (162, '        R2 = "renamed_fq/{}_R2.fastq.gz".format(wildcards.sample),')
    (163, '        return R1 + R2')
    (164, '    else:')
    (165, '        return "renamed_fq/{}.fastq.gz".format(wildcards.sample)')
    (166, '')
    (167, '')
    (168, '################################################')
    (169, '## get fastq for TRIM GALORE')
    (170, '## UMI extracted for paired-end reads, if exist')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yzeng-lol/tcge-cfmedip-seq-pipeline, file=workflow/rules/common.smk
context_key: ['if config["paired-end"] and config["add_umi"]']
    (171, 'def get_fastq_4trim(wildcards):')
    (172, '    if config["paired-end"] and config["add_umi"]:')
    (173, '        R1 = "barcoded_fq_pe/{}_R1.fastq.gz".format(wildcards.sample),')
    (174, '        R2 = "barcoded_fq_pe/{}_R2.fastq.gz".format(wildcards.sample),')
    (175, '        return R1 + R2')
    (176, '    elif config["paired-end"] and config["add_umi"] == False:')
    (177, '        R1 = "renamed_fq/{}_R1.fastq.gz".format(wildcards.sample),')
    (178, '        R2 = "renamed_fq/{}_R2.fastq.gz".format(wildcards.sample),')
    (179, '        return R1 + R2')
    (180, '    elif config["paired-end"] == False and config["add_umi"]:')
    (181, '        return "barcoded_fq_se/{}.fastq.gz".format(wildcards.sample)')
    (182, '    else:')
    (183, '        return "renamed_fq/{}.fastq.gz".format(wildcards.sample)')
    (184, '')
    (185, '##################################')
    (186, '## get trimmed fastq files for BWA')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yzeng-lol/tcge-cfmedip-seq-pipeline, file=workflow/rules/common.smk
context_key: ['if config["paired-end"]']
    (187, 'def get_trimmed_fastq(wildcards):')
    (188, '    if config["paired-end"]:')
    (189, '        R1 = "trimmed_fq/{}_R1_val_1.fq.gz".format(wildcards.sample),')
    (190, '        R2 = "trimmed_fq/{}_R2_val_2.fq.gz".format(wildcards.sample),')
    (191, '        return R1 + R2')
    (192, '    else:')
    (193, '        return "trimmed_fq/{}_trimmed.fq.gz".format(wildcards.sample)')
    (194, '')
    (195, '########################################')
    (196, '## get dedup bam files for meth_qc_quant')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yzeng-lol/tcge-cfmedip-seq-pipeline, file=workflow/rules/common.smk
context_key: ['if config["paired-end"] and config["add_umi"]']
    (197, 'def get_dedup_bam(wildcards):')
    (198, '    if config["paired-end"] and config["add_umi"]:')
    (199, '        return "dedup_bam_umi_pe/{}_dedup.bam".format(wildcards.sample)')
    (200, '    elif config["paired-end"] and config["add_umi"] == False:')
    (201, '        return "dedup_bam_pe/{}_dedup.bam".format(wildcards.sample)')
    (202, '    elif config["paired-end"] == False and config["add_umi"]:')
    (203, '        return "dedup_bam_umi_se/{}_dedup.bam".format(wildcards.sample)')
    (204, '    else:')
    (205, '        return "dedup_bam_se/{}_dedup.bam".format(wildcards.sample)')
    (206, '')
    (207, '')
    (208, '## spike-ins')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yzeng-lol/tcge-cfmedip-seq-pipeline, file=workflow/rules/common.smk
context_key: ['if config["paired-end"] and config["spike_in"]']
    (209, 'def get_dedup_bam_spikein(wildcards):')
    (210, '    if config["paired-end"] and config["spike_in"]:')
    (211, '        return "dedup_bam_spikein/{}_spikein.bam".format(wildcards.sample)')
    (212, '')
    (213, '')
    (214, '#####################')
    (215, '## aggregaton #######')
    (216, '#####################')
    (217, '')
    (218, '####################')
    (219, '## get FASTQC stats')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yzeng-lol/tcge-cfmedip-seq-pipeline, file=workflow/rules/common.smk
context_key: ['if config["paired-end"]']
    (220, 'def get_fastqc_stats():')
    (221, '    if config["paired-end"]:')
    (222, '        r1_raw  = expand("fastqc_pe/{samples}_R1_fastqc.zip", samples = SAMPLES_AGGR["sample_id"]),')
    (223, '        r2_raw  = expand("fastqc_pe/{samples}_R2_fastqc.zip", samples = SAMPLES_AGGR["sample_id"]),')
    (224, '        r1_trim = expand("fastqc_pe/{samples}_R1_val_1_fastqc.zip", samples = SAMPLES_AGGR["sample_id"]),')
    (225, '        r2_trim = expand("fastqc_pe/{samples}_R2_val_2_fastqc.zip", samples = SAMPLES_AGGR["sample_id"]),')
    (226, '        return r1_raw + r2_raw + r1_trim + r2_trim')
    (227, '    else:')
    (228, '         r1_raw  = expand("fastqc_se/{samples}_fastqc.zip", samples = SAMPLES_AGGR["sample_id"]),')
    (229, '         r1_trim = expand("fastqc_se/{samples}_trimmed_fastqc.zip", samples = SAMPLES_AGGR["sample_id"]),')
    (230, '         return r1_raw + r1_trim')
    (231, '')
    (232, '')
    (233, '######################')
    (234, '## get dedup bam stats')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yzeng-lol/tcge-cfmedip-seq-pipeline, file=workflow/rules/common.smk
context_key: ['if config["paired-end"] and config["add_umi"]']
    (235, 'def get_dedup_bam_stats():')
    (236, '    if config["paired-end"] and config["add_umi"]:')
    (237, '        return expand("dedup_bam_umi_pe/{samples}_dedup.bam.stats.txt", samples = SAMPLES_AGGR["sample_id"])')
    (238, '    elif config["paired-end"] and config["add_umi"] == False:')
    (239, '        return expand("dedup_bam_pe/{samples}_dedup.bam.stats.txt", samples = SAMPLES_AGGR["sample_id"])')
    (240, '    elif config["paired-end"] == False and config["add_umi"]:')
    (241, '        return expand("dedup_bam_umi_se/{samples}_dedup.bam.stats.txt", samples = SAMPLES_AGGR["sample_id"])')
    (242, '    else:')
    (243, '        return expand("dedup_bam_se/{samples}_dedup.bam.stats.txt", samples = SAMPLES_AGGR["sample_id"])')
    (244, '')
    (245, '')
    (246, '')
    (247, '#########################')
    (248, '## get spikeins bam stats')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yzeng-lol/tcge-cfmedip-seq-pipeline, file=workflow/rules/common.smk
context_key: ['if config["spike_in"] and config["paired-end"]']
    (249, 'def get_spikein_stats():')
    (250, '    if config["spike_in"] and config["paired-end"]:')
    (251, '        bam_stats = expand("dedup_bam_spikein/{samples}_spikein.bam.stats.txt", samples = SAMPLES_AGGR["sample_id"]),')
    (252, '        frag_stats = expand("fragment_size_spikein/{samples}_insert_size_metrics.txt", samples = SAMPLES_AGGR["sample_id"]),')
    (253, '        return bam_stats + frag_stats')
    (254, '    else:')
    (255, '        return ""')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=ejongepier/NanoClass, file=rules/preprocess.smk
context_key: ['if config["subsample"]["skip"] is True']
    (3, 'def get_seqfiletype(wildcards):')
    (4, '    if config["subsample"]["skip"] is True:')
    (5, '        return "data/{run}/nanofilt/{sample}.filtered.fastq.gz"')
    (6, '    else:')
    (7, '        return "data/{run}/nanofilt/{sample}.subsampled.fastq.gz"')
    (8, '')
    (9, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/lathe, file=Snakefile
context_key: ["if config[\\'assembler\\'] == \\'canu\\'"]
    (245, 'def choose_assembler():')
    (246, "    if config[\\'assembler\\'] == \\'canu\\':")
    (247, '        return(expand(\\\'{{sample}}/1.assemble/assemble_{genome_size}/{{sample}}_{genome_size}.contigs.fasta\\\', genome_size = config[\\\'genome_size\\\'].split(",")))')
    (248, "    elif config[\\'assembler\\'] == \\'flye\\':")
    (249, '        return(expand("{{sample}}/1.assemble/assemble_{g}/assembly.corrected.fasta", g = config[\\\'genome_size\\\'].split(",")))')
    (250, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=bhattlab/lathe, file=Snakefile
context_key: ["if config[\\'skip_circularization\\'] == \\'True\\' or config[\\'skip_circularization\\'] == True"]
    (793, 'def skip_circularization_or_not():')
    (794, '    #Allows all circularization to be skipped when unneeded')
    (795, "    if config[\\'skip_circularization\\'] == \\'True\\' or config[\\'skip_circularization\\'] == True:")
    (796, '        return(rules.polish_final.output)')
    (797, '    else:')
    (798, '        return(rules.circularize_final.output)')
    (799, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=sraorao/snakemake_code_clinic_2, file=Snakefile_inputfunc.smk
context_key: ['if config["REF_VERSION"] == 37']
    (29, 'def get_ref(wildcards):')
    (30, '    """')
    (31, '    Return the correct genome file based on REF_VERSION value set in config.yaml')
    (32, '    :param wildcards:')
    (33, '    :return:')
    (34, '    """')
    (35, '    if config["REF_VERSION"] == 37:')
    (36, '        return [config["REF37"]]')
    (37, '    elif config["REF_VERSION"] == 38:')
    (38, '        return [config["REF38"]]')
    (39, '    else:')
    (40, '        print("incorrect value for reference!")')
    (41, '#-----------------------------------------------------------------------------------------------------------------------')
    (42, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["CONVERSION_CHECK"]', 'if config["IS_PAIRED"]', 'if config["RUN_TRIMMING"]']
    (9, 'def multiqc_input(wildcards):')
    (10, '    input = []')
    (11, '    if config["CONVERSION_CHECK"]:')
    (12, '        if config["IS_PAIRED"]:')
    (13, '            if config["RUN_TRIMMING"]:')
    (14, '                ## paired and trimmed')
    (15, '                input.extend(')
    (16, '                    expand(')
    (17, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (18, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (19, '                    )')
    (20, '                )')
    (21, '            else:')
    (22, '                ## paired and not trimmed')
    (23, '                input.extend(')
    (24, '                    expand(')
    (25, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (26, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (27, '                    )')
    (28, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["CONVERSION_CHECK"]', 'if config["IS_PAIRED"]', 'else', 'if config["RUN_TRIMMING"]']
    (9, 'def multiqc_input(wildcards):')
    (10, '    input = []')
    (11, '    if config["CONVERSION_CHECK"]:')
    (12, '        if config["IS_PAIRED"]:')
    (13, '            if config["RUN_TRIMMING"]:')
    (14, '                ## paired and trimmed')
    (15, '                input.extend(')
    (16, '                    expand(')
    (17, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (18, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (19, '                    )')
    (20, '                )')
    (21, '            else:')
    (22, '                ## paired and not trimmed')
    (23, '                input.extend(')
    (24, '                    expand(')
    (25, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (26, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (27, '                    )')
    (28, '                )')
    (29, '        else:')
    (30, '            if config["RUN_TRIMMING"]:')
    (31, '                ## not paired and trimmed')
    (32, '                input.extend(')
    (33, '                    expand(')
    (34, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (35, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (36, '                    )')
    (37, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["CONVERSION_CHECK"]', 'if config["IS_PAIRED"]', 'else', 'else']
    (9, 'def multiqc_input(wildcards):')
    (10, '    input = []')
    (11, '    if config["CONVERSION_CHECK"]:')
    (12, '        if config["IS_PAIRED"]:')
    (13, '            if config["RUN_TRIMMING"]:')
    (14, '                ## paired and trimmed')
    (15, '                input.extend(')
    (16, '                    expand(')
    (17, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (18, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (19, '                    )')
    (20, '                )')
    (21, '            else:')
    (22, '                ## paired and not trimmed')
    (23, '                input.extend(')
    (24, '                    expand(')
    (25, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (26, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (27, '                    )')
    (28, '                )')
    (29, '        else:')
    (30, '            if config["RUN_TRIMMING"]:')
    (31, '                ## not paired and trimmed')
    (32, '                input.extend(')
    (33, '                    expand(')
    (34, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (35, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (36, '                    )')
    (37, '                )')
    (38, '            else:')
    (39, '                ## not paired and not trimmed')
    (40, '                input.extend(')
    (41, '                    expand(')
    (42, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (43, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (44, '                    )')
    (45, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["CONVERSION_CHECK"]', 'if config["IS_PAIRED"]']
    (9, 'def multiqc_input(wildcards):')
    (10, '    input = []')
    (11, '    if config["CONVERSION_CHECK"]:')
    (12, '        if config["IS_PAIRED"]:')
    (13, '            if config["RUN_TRIMMING"]:')
    (14, '                ## paired and trimmed')
    (15, '                input.extend(')
    (16, '                    expand(')
    (17, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (18, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (19, '                    )')
    (20, '                )')
    (21, '            else:')
    (22, '                ## paired and not trimmed')
    (23, '                input.extend(')
    (24, '                    expand(')
    (25, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (26, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (27, '                    )')
    (28, '                )')
    (29, '        else:')
    (30, '            if config["RUN_TRIMMING"]:')
    (31, '                ## not paired and trimmed')
    (32, '                input.extend(')
    (33, '                    expand(')
    (34, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (35, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (36, '                    )')
    (37, '                )')
    (38, '            else:')
    (39, '                ## not paired and not trimmed')
    (40, '                input.extend(')
    (41, '                    expand(')
    (42, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (43, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (44, '                    )')
    (45, '                )')
    (46, '    if config["IS_PAIRED"]:')
    (47, '        input.extend(')
    (48, '            expand(')
    (49, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_fastqc.zip",')
    (50, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (51, '            )')
    (52, '        )')
    (53, '        input.extend(')
    (54, '            expand(')
    (55, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_fastqc.zip",')
    (56, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (57, '            )')
    (58, '        )')
    (59, '        if config["CONVERSION_CHECK"]:')
    (60, '            if config["RUN_TRIMMING"]:')
    (61, '                input.extend(')
    (62, '                    expand(')
    (63, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (64, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (65, '                    )')
    (66, '                )')
    (67, '            else:')
    (68, '                input.extend(')
    (69, '                    expand(')
    (70, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (71, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (72, '                    )')
    (73, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["CONVERSION_CHECK"]', 'else']
    (9, 'def multiqc_input(wildcards):')
    (10, '    input = []')
    (11, '    if config["CONVERSION_CHECK"]:')
    (12, '        if config["IS_PAIRED"]:')
    (13, '            if config["RUN_TRIMMING"]:')
    (14, '                ## paired and trimmed')
    (15, '                input.extend(')
    (16, '                    expand(')
    (17, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (18, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (19, '                    )')
    (20, '                )')
    (21, '            else:')
    (22, '                ## paired and not trimmed')
    (23, '                input.extend(')
    (24, '                    expand(')
    (25, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (26, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (27, '                    )')
    (28, '                )')
    (29, '        else:')
    (30, '            if config["RUN_TRIMMING"]:')
    (31, '                ## not paired and trimmed')
    (32, '                input.extend(')
    (33, '                    expand(')
    (34, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (35, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (36, '                    )')
    (37, '                )')
    (38, '            else:')
    (39, '                ## not paired and not trimmed')
    (40, '                input.extend(')
    (41, '                    expand(')
    (42, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (43, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (44, '                    )')
    (45, '                )')
    (46, '    if config["IS_PAIRED"]:')
    (47, '        input.extend(')
    (48, '            expand(')
    (49, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_fastqc.zip",')
    (50, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (51, '            )')
    (52, '        )')
    (53, '        input.extend(')
    (54, '            expand(')
    (55, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_fastqc.zip",')
    (56, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (57, '            )')
    (58, '        )')
    (59, '        if config["CONVERSION_CHECK"]:')
    (60, '            if config["RUN_TRIMMING"]:')
    (61, '                input.extend(')
    (62, '                    expand(')
    (63, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (64, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (65, '                    )')
    (66, '                )')
    (67, '            else:')
    (68, '                input.extend(')
    (69, '                    expand(')
    (70, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (71, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (72, '                    )')
    (73, '                )')
    (74, '    else:')
    (75, '        input.extend(')
    (76, '            expand(')
    (77, '                f"{OUTPUT_DIR}FastQC/{{sample}}_fastqc.zip",')
    (78, '                sample=samples.name[samples.type == "SE"].values.tolist(),')
    (79, '            )')
    (80, '        )')
    (81, '        if config["CONVERSION_CHECK"]:')
    (82, '            if config["RUN_TRIMMING"]:')
    (83, '                input.extend(')
    (84, '                    expand(')
    (85, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (86, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (87, '                    )')
    (88, '                )')
    (89, '            else:')
    (90, '                input.extend(')
    (91, '                    expand(')
    (92, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (93, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (94, '                    )')
    (95, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["CONVERSION_CHECK"]', 'if config["RUN_TRIMMING"]', 'if config["IS_PAIRED"]']
    (9, 'def multiqc_input(wildcards):')
    (10, '    input = []')
    (11, '    if config["CONVERSION_CHECK"]:')
    (12, '        if config["IS_PAIRED"]:')
    (13, '            if config["RUN_TRIMMING"]:')
    (14, '                ## paired and trimmed')
    (15, '                input.extend(')
    (16, '                    expand(')
    (17, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (18, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (19, '                    )')
    (20, '                )')
    (21, '            else:')
    (22, '                ## paired and not trimmed')
    (23, '                input.extend(')
    (24, '                    expand(')
    (25, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (26, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (27, '                    )')
    (28, '                )')
    (29, '        else:')
    (30, '            if config["RUN_TRIMMING"]:')
    (31, '                ## not paired and trimmed')
    (32, '                input.extend(')
    (33, '                    expand(')
    (34, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (35, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (36, '                    )')
    (37, '                )')
    (38, '            else:')
    (39, '                ## not paired and not trimmed')
    (40, '                input.extend(')
    (41, '                    expand(')
    (42, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (43, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (44, '                    )')
    (45, '                )')
    (46, '    if config["IS_PAIRED"]:')
    (47, '        input.extend(')
    (48, '            expand(')
    (49, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_fastqc.zip",')
    (50, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (51, '            )')
    (52, '        )')
    (53, '        input.extend(')
    (54, '            expand(')
    (55, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_fastqc.zip",')
    (56, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (57, '            )')
    (58, '        )')
    (59, '        if config["CONVERSION_CHECK"]:')
    (60, '            if config["RUN_TRIMMING"]:')
    (61, '                input.extend(')
    (62, '                    expand(')
    (63, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (64, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (65, '                    )')
    (66, '                )')
    (67, '            else:')
    (68, '                input.extend(')
    (69, '                    expand(')
    (70, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (71, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (72, '                    )')
    (73, '                )')
    (74, '    else:')
    (75, '        input.extend(')
    (76, '            expand(')
    (77, '                f"{OUTPUT_DIR}FastQC/{{sample}}_fastqc.zip",')
    (78, '                sample=samples.name[samples.type == "SE"].values.tolist(),')
    (79, '            )')
    (80, '        )')
    (81, '        if config["CONVERSION_CHECK"]:')
    (82, '            if config["RUN_TRIMMING"]:')
    (83, '                input.extend(')
    (84, '                    expand(')
    (85, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (86, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (87, '                    )')
    (88, '                )')
    (89, '            else:')
    (90, '                input.extend(')
    (91, '                    expand(')
    (92, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (93, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (94, '                    )')
    (95, '                )')
    (96, '    if config["RUN_TRIMMING"]:')
    (97, '        if config["IS_PAIRED"]:')
    (98, '            input.extend(')
    (99, '                expand(')
    (100, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1.fq.gz",')
    (101, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (102, '                )')
    (103, '            )')
    (104, '            input.extend(')
    (105, '                expand(')
    (106, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2.fq.gz",')
    (107, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (108, '                )')
    (109, '            )')
    (110, '            input.extend(')
    (111, '                expand(')
    (112, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_fastqc.zip",')
    (113, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (114, '                )')
    (115, '            )')
    (116, '            input.extend(')
    (117, '                expand(')
    (118, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2_fastqc.zip",')
    (119, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (120, '                )')
    (121, '            )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["CONVERSION_CHECK"]', 'if config["RUN_TRIMMING"]', 'else']
    (9, 'def multiqc_input(wildcards):')
    (10, '    input = []')
    (11, '    if config["CONVERSION_CHECK"]:')
    (12, '        if config["IS_PAIRED"]:')
    (13, '            if config["RUN_TRIMMING"]:')
    (14, '                ## paired and trimmed')
    (15, '                input.extend(')
    (16, '                    expand(')
    (17, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (18, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (19, '                    )')
    (20, '                )')
    (21, '            else:')
    (22, '                ## paired and not trimmed')
    (23, '                input.extend(')
    (24, '                    expand(')
    (25, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (26, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (27, '                    )')
    (28, '                )')
    (29, '        else:')
    (30, '            if config["RUN_TRIMMING"]:')
    (31, '                ## not paired and trimmed')
    (32, '                input.extend(')
    (33, '                    expand(')
    (34, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (35, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (36, '                    )')
    (37, '                )')
    (38, '            else:')
    (39, '                ## not paired and not trimmed')
    (40, '                input.extend(')
    (41, '                    expand(')
    (42, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (43, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (44, '                    )')
    (45, '                )')
    (46, '    if config["IS_PAIRED"]:')
    (47, '        input.extend(')
    (48, '            expand(')
    (49, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_fastqc.zip",')
    (50, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (51, '            )')
    (52, '        )')
    (53, '        input.extend(')
    (54, '            expand(')
    (55, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_fastqc.zip",')
    (56, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (57, '            )')
    (58, '        )')
    (59, '        if config["CONVERSION_CHECK"]:')
    (60, '            if config["RUN_TRIMMING"]:')
    (61, '                input.extend(')
    (62, '                    expand(')
    (63, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (64, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (65, '                    )')
    (66, '                )')
    (67, '            else:')
    (68, '                input.extend(')
    (69, '                    expand(')
    (70, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (71, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (72, '                    )')
    (73, '                )')
    (74, '    else:')
    (75, '        input.extend(')
    (76, '            expand(')
    (77, '                f"{OUTPUT_DIR}FastQC/{{sample}}_fastqc.zip",')
    (78, '                sample=samples.name[samples.type == "SE"].values.tolist(),')
    (79, '            )')
    (80, '        )')
    (81, '        if config["CONVERSION_CHECK"]:')
    (82, '            if config["RUN_TRIMMING"]:')
    (83, '                input.extend(')
    (84, '                    expand(')
    (85, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (86, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (87, '                    )')
    (88, '                )')
    (89, '            else:')
    (90, '                input.extend(')
    (91, '                    expand(')
    (92, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (93, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (94, '                    )')
    (95, '                )')
    (96, '    if config["RUN_TRIMMING"]:')
    (97, '        if config["IS_PAIRED"]:')
    (98, '            input.extend(')
    (99, '                expand(')
    (100, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1.fq.gz",')
    (101, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (102, '                )')
    (103, '            )')
    (104, '            input.extend(')
    (105, '                expand(')
    (106, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2.fq.gz",')
    (107, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (108, '                )')
    (109, '            )')
    (110, '            input.extend(')
    (111, '                expand(')
    (112, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_fastqc.zip",')
    (113, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (114, '                )')
    (115, '            )')
    (116, '            input.extend(')
    (117, '                expand(')
    (118, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2_fastqc.zip",')
    (119, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (120, '                )')
    (121, '            )')
    (122, '        else:')
    (123, '            input.extend(')
    (124, '                expand(')
    (125, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed.fq.gz",')
    (126, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (127, '                )')
    (128, '            )')
    (129, '            input.extend(')
    (130, '                expand(')
    (131, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed_fastqc.zip",')
    (132, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (133, '                )')
    (134, '            )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["CONVERSION_CHECK"]', 'if config["RUN_BISMARK"]', 'if config["IS_PAIRED"]', 'if config["RUN_TRIMMING"]']
    (9, 'def multiqc_input(wildcards):')
    (10, '    input = []')
    (11, '    if config["CONVERSION_CHECK"]:')
    (12, '        if config["IS_PAIRED"]:')
    (13, '            if config["RUN_TRIMMING"]:')
    (14, '                ## paired and trimmed')
    (15, '                input.extend(')
    (16, '                    expand(')
    (17, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (18, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (19, '                    )')
    (20, '                )')
    (21, '            else:')
    (22, '                ## paired and not trimmed')
    (23, '                input.extend(')
    (24, '                    expand(')
    (25, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (26, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (27, '                    )')
    (28, '                )')
    (29, '        else:')
    (30, '            if config["RUN_TRIMMING"]:')
    (31, '                ## not paired and trimmed')
    (32, '                input.extend(')
    (33, '                    expand(')
    (34, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (35, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (36, '                    )')
    (37, '                )')
    (38, '            else:')
    (39, '                ## not paired and not trimmed')
    (40, '                input.extend(')
    (41, '                    expand(')
    (42, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (43, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (44, '                    )')
    (45, '                )')
    (46, '    if config["IS_PAIRED"]:')
    (47, '        input.extend(')
    (48, '            expand(')
    (49, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_fastqc.zip",')
    (50, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (51, '            )')
    (52, '        )')
    (53, '        input.extend(')
    (54, '            expand(')
    (55, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_fastqc.zip",')
    (56, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (57, '            )')
    (58, '        )')
    (59, '        if config["CONVERSION_CHECK"]:')
    (60, '            if config["RUN_TRIMMING"]:')
    (61, '                input.extend(')
    (62, '                    expand(')
    (63, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (64, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (65, '                    )')
    (66, '                )')
    (67, '            else:')
    (68, '                input.extend(')
    (69, '                    expand(')
    (70, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (71, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (72, '                    )')
    (73, '                )')
    (74, '    else:')
    (75, '        input.extend(')
    (76, '            expand(')
    (77, '                f"{OUTPUT_DIR}FastQC/{{sample}}_fastqc.zip",')
    (78, '                sample=samples.name[samples.type == "SE"].values.tolist(),')
    (79, '            )')
    (80, '        )')
    (81, '        if config["CONVERSION_CHECK"]:')
    (82, '            if config["RUN_TRIMMING"]:')
    (83, '                input.extend(')
    (84, '                    expand(')
    (85, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (86, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (87, '                    )')
    (88, '                )')
    (89, '            else:')
    (90, '                input.extend(')
    (91, '                    expand(')
    (92, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (93, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (94, '                    )')
    (95, '                )')
    (96, '    if config["RUN_TRIMMING"]:')
    (97, '        if config["IS_PAIRED"]:')
    (98, '            input.extend(')
    (99, '                expand(')
    (100, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1.fq.gz",')
    (101, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (102, '                )')
    (103, '            )')
    (104, '            input.extend(')
    (105, '                expand(')
    (106, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2.fq.gz",')
    (107, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (108, '                )')
    (109, '            )')
    (110, '            input.extend(')
    (111, '                expand(')
    (112, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_fastqc.zip",')
    (113, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (114, '                )')
    (115, '            )')
    (116, '            input.extend(')
    (117, '                expand(')
    (118, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2_fastqc.zip",')
    (119, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (120, '                )')
    (121, '            )')
    (122, '        else:')
    (123, '            input.extend(')
    (124, '                expand(')
    (125, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed.fq.gz",')
    (126, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (127, '                )')
    (128, '            )')
    (129, '            input.extend(')
    (130, '                expand(')
    (131, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed_fastqc.zip",')
    (132, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (133, '                )')
    (134, '            )')
    (135, '    if config["RUN_BISMARK"]:')
    (136, '        if config["IS_PAIRED"]:')
    (137, '            if config["RUN_TRIMMING"]:')
    (138, '                ## paired and trimmed')
    (139, '                ## alignment')
    (140, '                input.extend(')
    (141, '                    expand(')
    (142, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (143, '                        sample=samples.name[')
    (144, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (145, '                        ].values.tolist(),')
    (146, '                    )')
    (147, '                )')
    (148, '                input.extend(')
    (149, '                    expand(')
    (150, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (151, '                        sample=samples.name[')
    (152, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (153, '                        ].values.tolist(),')
    (154, '                    )')
    (155, '                )')
    (156, '                ## deduplication')
    (157, '                input.extend(')
    (158, '                    expand(')
    (159, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.deduplicated.bam",')
    (160, '                        sample=samples.name[')
    (161, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (162, '                        ].values.tolist(),')
    (163, '                    )')
    (164, '                )')
    (165, '                input.extend(')
    (166, '                    expand(')
    (167, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.deduplicated.bam",')
    (168, '                        sample=samples.name[')
    (169, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (170, '                        ].values.tolist(),')
    (171, '                    )')
    (172, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["CONVERSION_CHECK"]', 'if config["RUN_BISMARK"]', 'if config["IS_PAIRED"]', 'else']
    (9, 'def multiqc_input(wildcards):')
    (10, '    input = []')
    (11, '    if config["CONVERSION_CHECK"]:')
    (12, '        if config["IS_PAIRED"]:')
    (13, '            if config["RUN_TRIMMING"]:')
    (14, '                ## paired and trimmed')
    (15, '                input.extend(')
    (16, '                    expand(')
    (17, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (18, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (19, '                    )')
    (20, '                )')
    (21, '            else:')
    (22, '                ## paired and not trimmed')
    (23, '                input.extend(')
    (24, '                    expand(')
    (25, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (26, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (27, '                    )')
    (28, '                )')
    (29, '        else:')
    (30, '            if config["RUN_TRIMMING"]:')
    (31, '                ## not paired and trimmed')
    (32, '                input.extend(')
    (33, '                    expand(')
    (34, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (35, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (36, '                    )')
    (37, '                )')
    (38, '            else:')
    (39, '                ## not paired and not trimmed')
    (40, '                input.extend(')
    (41, '                    expand(')
    (42, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (43, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (44, '                    )')
    (45, '                )')
    (46, '    if config["IS_PAIRED"]:')
    (47, '        input.extend(')
    (48, '            expand(')
    (49, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_fastqc.zip",')
    (50, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (51, '            )')
    (52, '        )')
    (53, '        input.extend(')
    (54, '            expand(')
    (55, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_fastqc.zip",')
    (56, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (57, '            )')
    (58, '        )')
    (59, '        if config["CONVERSION_CHECK"]:')
    (60, '            if config["RUN_TRIMMING"]:')
    (61, '                input.extend(')
    (62, '                    expand(')
    (63, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (64, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (65, '                    )')
    (66, '                )')
    (67, '            else:')
    (68, '                input.extend(')
    (69, '                    expand(')
    (70, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (71, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (72, '                    )')
    (73, '                )')
    (74, '    else:')
    (75, '        input.extend(')
    (76, '            expand(')
    (77, '                f"{OUTPUT_DIR}FastQC/{{sample}}_fastqc.zip",')
    (78, '                sample=samples.name[samples.type == "SE"].values.tolist(),')
    (79, '            )')
    (80, '        )')
    (81, '        if config["CONVERSION_CHECK"]:')
    (82, '            if config["RUN_TRIMMING"]:')
    (83, '                input.extend(')
    (84, '                    expand(')
    (85, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (86, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (87, '                    )')
    (88, '                )')
    (89, '            else:')
    (90, '                input.extend(')
    (91, '                    expand(')
    (92, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (93, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (94, '                    )')
    (95, '                )')
    (96, '    if config["RUN_TRIMMING"]:')
    (97, '        if config["IS_PAIRED"]:')
    (98, '            input.extend(')
    (99, '                expand(')
    (100, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1.fq.gz",')
    (101, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (102, '                )')
    (103, '            )')
    (104, '            input.extend(')
    (105, '                expand(')
    (106, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2.fq.gz",')
    (107, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (108, '                )')
    (109, '            )')
    (110, '            input.extend(')
    (111, '                expand(')
    (112, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_fastqc.zip",')
    (113, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (114, '                )')
    (115, '            )')
    (116, '            input.extend(')
    (117, '                expand(')
    (118, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2_fastqc.zip",')
    (119, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (120, '                )')
    (121, '            )')
    (122, '        else:')
    (123, '            input.extend(')
    (124, '                expand(')
    (125, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed.fq.gz",')
    (126, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (127, '                )')
    (128, '            )')
    (129, '            input.extend(')
    (130, '                expand(')
    (131, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed_fastqc.zip",')
    (132, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (133, '                )')
    (134, '            )')
    (135, '    if config["RUN_BISMARK"]:')
    (136, '        if config["IS_PAIRED"]:')
    (137, '            if config["RUN_TRIMMING"]:')
    (138, '                ## paired and trimmed')
    (139, '                ## alignment')
    (140, '                input.extend(')
    (141, '                    expand(')
    (142, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (143, '                        sample=samples.name[')
    (144, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (145, '                        ].values.tolist(),')
    (146, '                    )')
    (147, '                )')
    (148, '                input.extend(')
    (149, '                    expand(')
    (150, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (151, '                        sample=samples.name[')
    (152, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (153, '                        ].values.tolist(),')
    (154, '                    )')
    (155, '                )')
    (156, '                ## deduplication')
    (157, '                input.extend(')
    (158, '                    expand(')
    (159, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.deduplicated.bam",')
    (160, '                        sample=samples.name[')
    (161, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (162, '                        ].values.tolist(),')
    (163, '                    )')
    (164, '                )')
    (165, '                input.extend(')
    (166, '                    expand(')
    (167, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.deduplicated.bam",')
    (168, '                        sample=samples.name[')
    (169, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (170, '                        ].values.tolist(),')
    (171, '                    )')
    (172, '                )')
    (173, '            else:')
    (174, '                ## paired and not trimmed')
    (175, '                ## alignment')
    (176, '                input.extend(')
    (177, '                    expand(')
    (178, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (179, '                        sample=samples.name[')
    (180, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (181, '                        ].values.tolist(),')
    (182, '                    )')
    (183, '                )')
    (184, '                input.extend(')
    (185, '                    expand(')
    (186, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (187, '                        sample=samples.name[')
    (188, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (189, '                        ].values.tolist(),')
    (190, '                    )')
    (191, '                )')
    (192, '                ## deduplication')
    (193, '                input.extend(')
    (194, '                    expand(')
    (195, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.deduplicated.bam",')
    (196, '                        sample=samples.name[')
    (197, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (198, '                        ].values.tolist(),')
    (199, '                    )')
    (200, '                )')
    (201, '                input.extend(')
    (202, '                    expand(')
    (203, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.deduplicated.bam",')
    (204, '                        sample=samples.name[')
    (205, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (206, '                        ].values.tolist(),')
    (207, '                    )')
    (208, '                )')
    (209, '            ## qualimap')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["CONVERSION_CHECK"]', 'if config["RUN_BISMARK"]', 'if config["IS_PAIRED"]']
    (9, 'def multiqc_input(wildcards):')
    (10, '    input = []')
    (11, '    if config["CONVERSION_CHECK"]:')
    (12, '        if config["IS_PAIRED"]:')
    (13, '            if config["RUN_TRIMMING"]:')
    (14, '                ## paired and trimmed')
    (15, '                input.extend(')
    (16, '                    expand(')
    (17, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (18, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (19, '                    )')
    (20, '                )')
    (21, '            else:')
    (22, '                ## paired and not trimmed')
    (23, '                input.extend(')
    (24, '                    expand(')
    (25, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (26, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (27, '                    )')
    (28, '                )')
    (29, '        else:')
    (30, '            if config["RUN_TRIMMING"]:')
    (31, '                ## not paired and trimmed')
    (32, '                input.extend(')
    (33, '                    expand(')
    (34, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (35, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (36, '                    )')
    (37, '                )')
    (38, '            else:')
    (39, '                ## not paired and not trimmed')
    (40, '                input.extend(')
    (41, '                    expand(')
    (42, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (43, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (44, '                    )')
    (45, '                )')
    (46, '    if config["IS_PAIRED"]:')
    (47, '        input.extend(')
    (48, '            expand(')
    (49, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_fastqc.zip",')
    (50, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (51, '            )')
    (52, '        )')
    (53, '        input.extend(')
    (54, '            expand(')
    (55, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_fastqc.zip",')
    (56, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (57, '            )')
    (58, '        )')
    (59, '        if config["CONVERSION_CHECK"]:')
    (60, '            if config["RUN_TRIMMING"]:')
    (61, '                input.extend(')
    (62, '                    expand(')
    (63, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (64, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (65, '                    )')
    (66, '                )')
    (67, '            else:')
    (68, '                input.extend(')
    (69, '                    expand(')
    (70, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (71, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (72, '                    )')
    (73, '                )')
    (74, '    else:')
    (75, '        input.extend(')
    (76, '            expand(')
    (77, '                f"{OUTPUT_DIR}FastQC/{{sample}}_fastqc.zip",')
    (78, '                sample=samples.name[samples.type == "SE"].values.tolist(),')
    (79, '            )')
    (80, '        )')
    (81, '        if config["CONVERSION_CHECK"]:')
    (82, '            if config["RUN_TRIMMING"]:')
    (83, '                input.extend(')
    (84, '                    expand(')
    (85, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (86, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (87, '                    )')
    (88, '                )')
    (89, '            else:')
    (90, '                input.extend(')
    (91, '                    expand(')
    (92, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (93, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (94, '                    )')
    (95, '                )')
    (96, '    if config["RUN_TRIMMING"]:')
    (97, '        if config["IS_PAIRED"]:')
    (98, '            input.extend(')
    (99, '                expand(')
    (100, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1.fq.gz",')
    (101, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (102, '                )')
    (103, '            )')
    (104, '            input.extend(')
    (105, '                expand(')
    (106, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2.fq.gz",')
    (107, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (108, '                )')
    (109, '            )')
    (110, '            input.extend(')
    (111, '                expand(')
    (112, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_fastqc.zip",')
    (113, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (114, '                )')
    (115, '            )')
    (116, '            input.extend(')
    (117, '                expand(')
    (118, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2_fastqc.zip",')
    (119, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (120, '                )')
    (121, '            )')
    (122, '        else:')
    (123, '            input.extend(')
    (124, '                expand(')
    (125, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed.fq.gz",')
    (126, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (127, '                )')
    (128, '            )')
    (129, '            input.extend(')
    (130, '                expand(')
    (131, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed_fastqc.zip",')
    (132, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (133, '                )')
    (134, '            )')
    (135, '    if config["RUN_BISMARK"]:')
    (136, '        if config["IS_PAIRED"]:')
    (137, '            if config["RUN_TRIMMING"]:')
    (138, '                ## paired and trimmed')
    (139, '                ## alignment')
    (140, '                input.extend(')
    (141, '                    expand(')
    (142, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (143, '                        sample=samples.name[')
    (144, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (145, '                        ].values.tolist(),')
    (146, '                    )')
    (147, '                )')
    (148, '                input.extend(')
    (149, '                    expand(')
    (150, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (151, '                        sample=samples.name[')
    (152, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (153, '                        ].values.tolist(),')
    (154, '                    )')
    (155, '                )')
    (156, '                ## deduplication')
    (157, '                input.extend(')
    (158, '                    expand(')
    (159, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.deduplicated.bam",')
    (160, '                        sample=samples.name[')
    (161, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (162, '                        ].values.tolist(),')
    (163, '                    )')
    (164, '                )')
    (165, '                input.extend(')
    (166, '                    expand(')
    (167, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.deduplicated.bam",')
    (168, '                        sample=samples.name[')
    (169, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (170, '                        ].values.tolist(),')
    (171, '                    )')
    (172, '                )')
    (173, '            else:')
    (174, '                ## paired and not trimmed')
    (175, '                ## alignment')
    (176, '                input.extend(')
    (177, '                    expand(')
    (178, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (179, '                        sample=samples.name[')
    (180, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (181, '                        ].values.tolist(),')
    (182, '                    )')
    (183, '                )')
    (184, '                input.extend(')
    (185, '                    expand(')
    (186, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (187, '                        sample=samples.name[')
    (188, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (189, '                        ].values.tolist(),')
    (190, '                    )')
    (191, '                )')
    (192, '                ## deduplication')
    (193, '                input.extend(')
    (194, '                    expand(')
    (195, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.deduplicated.bam",')
    (196, '                        sample=samples.name[')
    (197, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (198, '                        ].values.tolist(),')
    (199, '                    )')
    (200, '                )')
    (201, '                input.extend(')
    (202, '                    expand(')
    (203, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.deduplicated.bam",')
    (204, '                        sample=samples.name[')
    (205, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (206, '                        ].values.tolist(),')
    (207, '                    )')
    (208, '                )')
    (209, '            ## qualimap')
    (210, '            input.extend(')
    (211, '                expand(')
    (212, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_p1/qualimapReport.html",')
    (213, '                    sample=samples.name[samples.origin == "parent1"].values.tolist(),')
    (214, '                )')
    (215, '            )')
    (216, '            input.extend(')
    (217, '                expand(')
    (218, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_p2/qualimapReport.html",')
    (219, '                    sample=samples.name[samples.origin == "parent2"].values.tolist(),')
    (220, '                )')
    (221, '            )')
    (222, '            input.extend(')
    (223, '                expand(')
    (224, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_allo_pe_1/qualimapReport.html",')
    (225, '                    sample=samples.name[')
    (226, '                        (samples.type == "PE") & (samples.origin == "allopolyploid")')
    (227, '                    ].values.tolist(),')
    (228, '                )')
    (229, '            )')
    (230, '            input.extend(')
    (231, '                expand(')
    (232, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_allo_pe_2/qualimapReport.html",')
    (233, '                    sample=samples.name[')
    (234, '                        (samples.type == "PE") & (samples.origin == "allopolyploid")')
    (235, '                    ].values.tolist(),')
    (236, '                )')
    (237, '            )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["CONVERSION_CHECK"]', 'if config["RUN_BISMARK"]', 'else', 'if config["RUN_TRIMMING"]']
    (9, 'def multiqc_input(wildcards):')
    (10, '    input = []')
    (11, '    if config["CONVERSION_CHECK"]:')
    (12, '        if config["IS_PAIRED"]:')
    (13, '            if config["RUN_TRIMMING"]:')
    (14, '                ## paired and trimmed')
    (15, '                input.extend(')
    (16, '                    expand(')
    (17, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (18, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (19, '                    )')
    (20, '                )')
    (21, '            else:')
    (22, '                ## paired and not trimmed')
    (23, '                input.extend(')
    (24, '                    expand(')
    (25, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (26, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (27, '                    )')
    (28, '                )')
    (29, '        else:')
    (30, '            if config["RUN_TRIMMING"]:')
    (31, '                ## not paired and trimmed')
    (32, '                input.extend(')
    (33, '                    expand(')
    (34, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (35, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (36, '                    )')
    (37, '                )')
    (38, '            else:')
    (39, '                ## not paired and not trimmed')
    (40, '                input.extend(')
    (41, '                    expand(')
    (42, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (43, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (44, '                    )')
    (45, '                )')
    (46, '    if config["IS_PAIRED"]:')
    (47, '        input.extend(')
    (48, '            expand(')
    (49, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_fastqc.zip",')
    (50, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (51, '            )')
    (52, '        )')
    (53, '        input.extend(')
    (54, '            expand(')
    (55, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_fastqc.zip",')
    (56, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (57, '            )')
    (58, '        )')
    (59, '        if config["CONVERSION_CHECK"]:')
    (60, '            if config["RUN_TRIMMING"]:')
    (61, '                input.extend(')
    (62, '                    expand(')
    (63, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (64, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (65, '                    )')
    (66, '                )')
    (67, '            else:')
    (68, '                input.extend(')
    (69, '                    expand(')
    (70, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (71, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (72, '                    )')
    (73, '                )')
    (74, '    else:')
    (75, '        input.extend(')
    (76, '            expand(')
    (77, '                f"{OUTPUT_DIR}FastQC/{{sample}}_fastqc.zip",')
    (78, '                sample=samples.name[samples.type == "SE"].values.tolist(),')
    (79, '            )')
    (80, '        )')
    (81, '        if config["CONVERSION_CHECK"]:')
    (82, '            if config["RUN_TRIMMING"]:')
    (83, '                input.extend(')
    (84, '                    expand(')
    (85, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (86, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (87, '                    )')
    (88, '                )')
    (89, '            else:')
    (90, '                input.extend(')
    (91, '                    expand(')
    (92, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (93, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (94, '                    )')
    (95, '                )')
    (96, '    if config["RUN_TRIMMING"]:')
    (97, '        if config["IS_PAIRED"]:')
    (98, '            input.extend(')
    (99, '                expand(')
    (100, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1.fq.gz",')
    (101, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (102, '                )')
    (103, '            )')
    (104, '            input.extend(')
    (105, '                expand(')
    (106, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2.fq.gz",')
    (107, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (108, '                )')
    (109, '            )')
    (110, '            input.extend(')
    (111, '                expand(')
    (112, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_fastqc.zip",')
    (113, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (114, '                )')
    (115, '            )')
    (116, '            input.extend(')
    (117, '                expand(')
    (118, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2_fastqc.zip",')
    (119, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (120, '                )')
    (121, '            )')
    (122, '        else:')
    (123, '            input.extend(')
    (124, '                expand(')
    (125, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed.fq.gz",')
    (126, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (127, '                )')
    (128, '            )')
    (129, '            input.extend(')
    (130, '                expand(')
    (131, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed_fastqc.zip",')
    (132, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (133, '                )')
    (134, '            )')
    (135, '    if config["RUN_BISMARK"]:')
    (136, '        if config["IS_PAIRED"]:')
    (137, '            if config["RUN_TRIMMING"]:')
    (138, '                ## paired and trimmed')
    (139, '                ## alignment')
    (140, '                input.extend(')
    (141, '                    expand(')
    (142, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (143, '                        sample=samples.name[')
    (144, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (145, '                        ].values.tolist(),')
    (146, '                    )')
    (147, '                )')
    (148, '                input.extend(')
    (149, '                    expand(')
    (150, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (151, '                        sample=samples.name[')
    (152, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (153, '                        ].values.tolist(),')
    (154, '                    )')
    (155, '                )')
    (156, '                ## deduplication')
    (157, '                input.extend(')
    (158, '                    expand(')
    (159, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.deduplicated.bam",')
    (160, '                        sample=samples.name[')
    (161, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (162, '                        ].values.tolist(),')
    (163, '                    )')
    (164, '                )')
    (165, '                input.extend(')
    (166, '                    expand(')
    (167, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.deduplicated.bam",')
    (168, '                        sample=samples.name[')
    (169, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (170, '                        ].values.tolist(),')
    (171, '                    )')
    (172, '                )')
    (173, '            else:')
    (174, '                ## paired and not trimmed')
    (175, '                ## alignment')
    (176, '                input.extend(')
    (177, '                    expand(')
    (178, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (179, '                        sample=samples.name[')
    (180, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (181, '                        ].values.tolist(),')
    (182, '                    )')
    (183, '                )')
    (184, '                input.extend(')
    (185, '                    expand(')
    (186, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (187, '                        sample=samples.name[')
    (188, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (189, '                        ].values.tolist(),')
    (190, '                    )')
    (191, '                )')
    (192, '                ## deduplication')
    (193, '                input.extend(')
    (194, '                    expand(')
    (195, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.deduplicated.bam",')
    (196, '                        sample=samples.name[')
    (197, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (198, '                        ].values.tolist(),')
    (199, '                    )')
    (200, '                )')
    (201, '                input.extend(')
    (202, '                    expand(')
    (203, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.deduplicated.bam",')
    (204, '                        sample=samples.name[')
    (205, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (206, '                        ].values.tolist(),')
    (207, '                    )')
    (208, '                )')
    (209, '            ## qualimap')
    (210, '            input.extend(')
    (211, '                expand(')
    (212, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_p1/qualimapReport.html",')
    (213, '                    sample=samples.name[samples.origin == "parent1"].values.tolist(),')
    (214, '                )')
    (215, '            )')
    (216, '            input.extend(')
    (217, '                expand(')
    (218, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_p2/qualimapReport.html",')
    (219, '                    sample=samples.name[samples.origin == "parent2"].values.tolist(),')
    (220, '                )')
    (221, '            )')
    (222, '            input.extend(')
    (223, '                expand(')
    (224, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_allo_pe_1/qualimapReport.html",')
    (225, '                    sample=samples.name[')
    (226, '                        (samples.type == "PE") & (samples.origin == "allopolyploid")')
    (227, '                    ].values.tolist(),')
    (228, '                )')
    (229, '            )')
    (230, '            input.extend(')
    (231, '                expand(')
    (232, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_allo_pe_2/qualimapReport.html",')
    (233, '                    sample=samples.name[')
    (234, '                        (samples.type == "PE") & (samples.origin == "allopolyploid")')
    (235, '                    ].values.tolist(),')
    (236, '                )')
    (237, '            )')
    (238, '        else:')
    (239, '            if config["RUN_TRIMMING"]:')
    (240, '                ## not paired, trimmed')
    (241, '                ## alignment')
    (242, '                input.extend(')
    (243, '                    expand(')
    (244, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_trimmed_bismark_bt2.bam",')
    (245, '                        sample=samples.name[')
    (246, '                            (samples.type == "SE") & (samples.origin != "parent2")')
    (247, '                        ].values.tolist(),')
    (248, '                    )')
    (249, '                )')
    (250, '                input.extend(')
    (251, '                    expand(')
    (252, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_trimmed_bismark_bt2.bam",')
    (253, '                        sample=samples.name[')
    (254, '                            (samples.type == "SE") & (samples.origin != "parent1")')
    (255, '                        ].values.tolist(),')
    (256, '                    )')
    (257, '                )')
    (258, '                ## deduplication')
    (259, '                input.extend(')
    (260, '                    expand(')
    (261, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_trimmed_bismark_bt2.deduplicated.bam",')
    (262, '                        sample=samples.name[')
    (263, '                            (samples.type == "SE") & (samples.origin != "parent2")')
    (264, '                        ].values.tolist(),')
    (265, '                    )')
    (266, '                )')
    (267, '                input.extend(')
    (268, '                    expand(')
    (269, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_trimmed_bismark_bt2.deduplicated.bam",')
    (270, '                        sample=samples.name[')
    (271, '                            (samples.type == "SE") & (samples.origin != "parent1")')
    (272, '                        ].values.tolist(),')
    (273, '                    )')
    (274, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["CONVERSION_CHECK"]', 'if config["RUN_BISMARK"]', 'else', 'else']
    (9, 'def multiqc_input(wildcards):')
    (10, '    input = []')
    (11, '    if config["CONVERSION_CHECK"]:')
    (12, '        if config["IS_PAIRED"]:')
    (13, '            if config["RUN_TRIMMING"]:')
    (14, '                ## paired and trimmed')
    (15, '                input.extend(')
    (16, '                    expand(')
    (17, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (18, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (19, '                    )')
    (20, '                )')
    (21, '            else:')
    (22, '                ## paired and not trimmed')
    (23, '                input.extend(')
    (24, '                    expand(')
    (25, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (26, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (27, '                    )')
    (28, '                )')
    (29, '        else:')
    (30, '            if config["RUN_TRIMMING"]:')
    (31, '                ## not paired and trimmed')
    (32, '                input.extend(')
    (33, '                    expand(')
    (34, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (35, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (36, '                    )')
    (37, '                )')
    (38, '            else:')
    (39, '                ## not paired and not trimmed')
    (40, '                input.extend(')
    (41, '                    expand(')
    (42, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (43, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (44, '                    )')
    (45, '                )')
    (46, '    if config["IS_PAIRED"]:')
    (47, '        input.extend(')
    (48, '            expand(')
    (49, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_fastqc.zip",')
    (50, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (51, '            )')
    (52, '        )')
    (53, '        input.extend(')
    (54, '            expand(')
    (55, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_fastqc.zip",')
    (56, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (57, '            )')
    (58, '        )')
    (59, '        if config["CONVERSION_CHECK"]:')
    (60, '            if config["RUN_TRIMMING"]:')
    (61, '                input.extend(')
    (62, '                    expand(')
    (63, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (64, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (65, '                    )')
    (66, '                )')
    (67, '            else:')
    (68, '                input.extend(')
    (69, '                    expand(')
    (70, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (71, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (72, '                    )')
    (73, '                )')
    (74, '    else:')
    (75, '        input.extend(')
    (76, '            expand(')
    (77, '                f"{OUTPUT_DIR}FastQC/{{sample}}_fastqc.zip",')
    (78, '                sample=samples.name[samples.type == "SE"].values.tolist(),')
    (79, '            )')
    (80, '        )')
    (81, '        if config["CONVERSION_CHECK"]:')
    (82, '            if config["RUN_TRIMMING"]:')
    (83, '                input.extend(')
    (84, '                    expand(')
    (85, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (86, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (87, '                    )')
    (88, '                )')
    (89, '            else:')
    (90, '                input.extend(')
    (91, '                    expand(')
    (92, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (93, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (94, '                    )')
    (95, '                )')
    (96, '    if config["RUN_TRIMMING"]:')
    (97, '        if config["IS_PAIRED"]:')
    (98, '            input.extend(')
    (99, '                expand(')
    (100, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1.fq.gz",')
    (101, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (102, '                )')
    (103, '            )')
    (104, '            input.extend(')
    (105, '                expand(')
    (106, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2.fq.gz",')
    (107, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (108, '                )')
    (109, '            )')
    (110, '            input.extend(')
    (111, '                expand(')
    (112, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_fastqc.zip",')
    (113, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (114, '                )')
    (115, '            )')
    (116, '            input.extend(')
    (117, '                expand(')
    (118, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2_fastqc.zip",')
    (119, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (120, '                )')
    (121, '            )')
    (122, '        else:')
    (123, '            input.extend(')
    (124, '                expand(')
    (125, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed.fq.gz",')
    (126, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (127, '                )')
    (128, '            )')
    (129, '            input.extend(')
    (130, '                expand(')
    (131, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed_fastqc.zip",')
    (132, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (133, '                )')
    (134, '            )')
    (135, '    if config["RUN_BISMARK"]:')
    (136, '        if config["IS_PAIRED"]:')
    (137, '            if config["RUN_TRIMMING"]:')
    (138, '                ## paired and trimmed')
    (139, '                ## alignment')
    (140, '                input.extend(')
    (141, '                    expand(')
    (142, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (143, '                        sample=samples.name[')
    (144, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (145, '                        ].values.tolist(),')
    (146, '                    )')
    (147, '                )')
    (148, '                input.extend(')
    (149, '                    expand(')
    (150, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (151, '                        sample=samples.name[')
    (152, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (153, '                        ].values.tolist(),')
    (154, '                    )')
    (155, '                )')
    (156, '                ## deduplication')
    (157, '                input.extend(')
    (158, '                    expand(')
    (159, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.deduplicated.bam",')
    (160, '                        sample=samples.name[')
    (161, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (162, '                        ].values.tolist(),')
    (163, '                    )')
    (164, '                )')
    (165, '                input.extend(')
    (166, '                    expand(')
    (167, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.deduplicated.bam",')
    (168, '                        sample=samples.name[')
    (169, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (170, '                        ].values.tolist(),')
    (171, '                    )')
    (172, '                )')
    (173, '            else:')
    (174, '                ## paired and not trimmed')
    (175, '                ## alignment')
    (176, '                input.extend(')
    (177, '                    expand(')
    (178, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (179, '                        sample=samples.name[')
    (180, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (181, '                        ].values.tolist(),')
    (182, '                    )')
    (183, '                )')
    (184, '                input.extend(')
    (185, '                    expand(')
    (186, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (187, '                        sample=samples.name[')
    (188, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (189, '                        ].values.tolist(),')
    (190, '                    )')
    (191, '                )')
    (192, '                ## deduplication')
    (193, '                input.extend(')
    (194, '                    expand(')
    (195, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.deduplicated.bam",')
    (196, '                        sample=samples.name[')
    (197, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (198, '                        ].values.tolist(),')
    (199, '                    )')
    (200, '                )')
    (201, '                input.extend(')
    (202, '                    expand(')
    (203, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.deduplicated.bam",')
    (204, '                        sample=samples.name[')
    (205, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (206, '                        ].values.tolist(),')
    (207, '                    )')
    (208, '                )')
    (209, '            ## qualimap')
    (210, '            input.extend(')
    (211, '                expand(')
    (212, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_p1/qualimapReport.html",')
    (213, '                    sample=samples.name[samples.origin == "parent1"].values.tolist(),')
    (214, '                )')
    (215, '            )')
    (216, '            input.extend(')
    (217, '                expand(')
    (218, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_p2/qualimapReport.html",')
    (219, '                    sample=samples.name[samples.origin == "parent2"].values.tolist(),')
    (220, '                )')
    (221, '            )')
    (222, '            input.extend(')
    (223, '                expand(')
    (224, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_allo_pe_1/qualimapReport.html",')
    (225, '                    sample=samples.name[')
    (226, '                        (samples.type == "PE") & (samples.origin == "allopolyploid")')
    (227, '                    ].values.tolist(),')
    (228, '                )')
    (229, '            )')
    (230, '            input.extend(')
    (231, '                expand(')
    (232, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_allo_pe_2/qualimapReport.html",')
    (233, '                    sample=samples.name[')
    (234, '                        (samples.type == "PE") & (samples.origin == "allopolyploid")')
    (235, '                    ].values.tolist(),')
    (236, '                )')
    (237, '            )')
    (238, '        else:')
    (239, '            if config["RUN_TRIMMING"]:')
    (240, '                ## not paired, trimmed')
    (241, '                ## alignment')
    (242, '                input.extend(')
    (243, '                    expand(')
    (244, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_trimmed_bismark_bt2.bam",')
    (245, '                        sample=samples.name[')
    (246, '                            (samples.type == "SE") & (samples.origin != "parent2")')
    (247, '                        ].values.tolist(),')
    (248, '                    )')
    (249, '                )')
    (250, '                input.extend(')
    (251, '                    expand(')
    (252, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_trimmed_bismark_bt2.bam",')
    (253, '                        sample=samples.name[')
    (254, '                            (samples.type == "SE") & (samples.origin != "parent1")')
    (255, '                        ].values.tolist(),')
    (256, '                    )')
    (257, '                )')
    (258, '                ## deduplication')
    (259, '                input.extend(')
    (260, '                    expand(')
    (261, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_trimmed_bismark_bt2.deduplicated.bam",')
    (262, '                        sample=samples.name[')
    (263, '                            (samples.type == "SE") & (samples.origin != "parent2")')
    (264, '                        ].values.tolist(),')
    (265, '                    )')
    (266, '                )')
    (267, '                input.extend(')
    (268, '                    expand(')
    (269, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_trimmed_bismark_bt2.deduplicated.bam",')
    (270, '                        sample=samples.name[')
    (271, '                            (samples.type == "SE") & (samples.origin != "parent1")')
    (272, '                        ].values.tolist(),')
    (273, '                    )')
    (274, '                )')
    (275, '            else:')
    (276, '                ## not paired, not trimmed')
    (277, '                ## alignment')
    (278, '                input.extend(')
    (279, '                    expand(')
    (280, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_bismark_bt2.bam",')
    (281, '                        sample=samples.name[')
    (282, '                            (samples.type == "SE") & (samples.origin != "parent2")')
    (283, '                        ].values.tolist(),')
    (284, '                    )')
    (285, '                )')
    (286, '                input.extend(')
    (287, '                    expand(')
    (288, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_bismark_bt2.bam",')
    (289, '                        sample=samples.name[')
    (290, '                            (samples.type == "SE") & (samples.origin != "parent1")')
    (291, '                        ].values.tolist(),')
    (292, '                    )')
    (293, '                )')
    (294, '                ## deduplication')
    (295, '                input.extend(')
    (296, '                    expand(')
    (297, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_bismark_bt2.deduplicated.bam",')
    (298, '                        sample=samples.name[')
    (299, '                            (samples.type == "SE") & (samples.origin != "parent2")')
    (300, '                        ].values.tolist(),')
    (301, '                    )')
    (302, '                )')
    (303, '                input.extend(')
    (304, '                    expand(')
    (305, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_bismark_bt2.deduplicated.bam",')
    (306, '                        sample=samples.name[')
    (307, '                            (samples.type == "SE") & (samples.origin != "parent1")')
    (308, '                        ].values.tolist(),')
    (309, '                    )')
    (310, '                )')
    (311, '            ## qualimap')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["CONVERSION_CHECK"]', 'if config["RUN_BISMARK"]', 'else']
    (9, 'def multiqc_input(wildcards):')
    (10, '    input = []')
    (11, '    if config["CONVERSION_CHECK"]:')
    (12, '        if config["IS_PAIRED"]:')
    (13, '            if config["RUN_TRIMMING"]:')
    (14, '                ## paired and trimmed')
    (15, '                input.extend(')
    (16, '                    expand(')
    (17, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (18, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (19, '                    )')
    (20, '                )')
    (21, '            else:')
    (22, '                ## paired and not trimmed')
    (23, '                input.extend(')
    (24, '                    expand(')
    (25, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (26, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (27, '                    )')
    (28, '                )')
    (29, '        else:')
    (30, '            if config["RUN_TRIMMING"]:')
    (31, '                ## not paired and trimmed')
    (32, '                input.extend(')
    (33, '                    expand(')
    (34, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (35, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (36, '                    )')
    (37, '                )')
    (38, '            else:')
    (39, '                ## not paired and not trimmed')
    (40, '                input.extend(')
    (41, '                    expand(')
    (42, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (43, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (44, '                    )')
    (45, '                )')
    (46, '    if config["IS_PAIRED"]:')
    (47, '        input.extend(')
    (48, '            expand(')
    (49, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_fastqc.zip",')
    (50, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (51, '            )')
    (52, '        )')
    (53, '        input.extend(')
    (54, '            expand(')
    (55, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_fastqc.zip",')
    (56, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (57, '            )')
    (58, '        )')
    (59, '        if config["CONVERSION_CHECK"]:')
    (60, '            if config["RUN_TRIMMING"]:')
    (61, '                input.extend(')
    (62, '                    expand(')
    (63, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (64, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (65, '                    )')
    (66, '                )')
    (67, '            else:')
    (68, '                input.extend(')
    (69, '                    expand(')
    (70, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (71, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (72, '                    )')
    (73, '                )')
    (74, '    else:')
    (75, '        input.extend(')
    (76, '            expand(')
    (77, '                f"{OUTPUT_DIR}FastQC/{{sample}}_fastqc.zip",')
    (78, '                sample=samples.name[samples.type == "SE"].values.tolist(),')
    (79, '            )')
    (80, '        )')
    (81, '        if config["CONVERSION_CHECK"]:')
    (82, '            if config["RUN_TRIMMING"]:')
    (83, '                input.extend(')
    (84, '                    expand(')
    (85, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (86, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (87, '                    )')
    (88, '                )')
    (89, '            else:')
    (90, '                input.extend(')
    (91, '                    expand(')
    (92, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (93, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (94, '                    )')
    (95, '                )')
    (96, '    if config["RUN_TRIMMING"]:')
    (97, '        if config["IS_PAIRED"]:')
    (98, '            input.extend(')
    (99, '                expand(')
    (100, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1.fq.gz",')
    (101, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (102, '                )')
    (103, '            )')
    (104, '            input.extend(')
    (105, '                expand(')
    (106, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2.fq.gz",')
    (107, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (108, '                )')
    (109, '            )')
    (110, '            input.extend(')
    (111, '                expand(')
    (112, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_fastqc.zip",')
    (113, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (114, '                )')
    (115, '            )')
    (116, '            input.extend(')
    (117, '                expand(')
    (118, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2_fastqc.zip",')
    (119, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (120, '                )')
    (121, '            )')
    (122, '        else:')
    (123, '            input.extend(')
    (124, '                expand(')
    (125, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed.fq.gz",')
    (126, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (127, '                )')
    (128, '            )')
    (129, '            input.extend(')
    (130, '                expand(')
    (131, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed_fastqc.zip",')
    (132, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (133, '                )')
    (134, '            )')
    (135, '    if config["RUN_BISMARK"]:')
    (136, '        if config["IS_PAIRED"]:')
    (137, '            if config["RUN_TRIMMING"]:')
    (138, '                ## paired and trimmed')
    (139, '                ## alignment')
    (140, '                input.extend(')
    (141, '                    expand(')
    (142, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (143, '                        sample=samples.name[')
    (144, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (145, '                        ].values.tolist(),')
    (146, '                    )')
    (147, '                )')
    (148, '                input.extend(')
    (149, '                    expand(')
    (150, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (151, '                        sample=samples.name[')
    (152, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (153, '                        ].values.tolist(),')
    (154, '                    )')
    (155, '                )')
    (156, '                ## deduplication')
    (157, '                input.extend(')
    (158, '                    expand(')
    (159, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.deduplicated.bam",')
    (160, '                        sample=samples.name[')
    (161, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (162, '                        ].values.tolist(),')
    (163, '                    )')
    (164, '                )')
    (165, '                input.extend(')
    (166, '                    expand(')
    (167, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.deduplicated.bam",')
    (168, '                        sample=samples.name[')
    (169, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (170, '                        ].values.tolist(),')
    (171, '                    )')
    (172, '                )')
    (173, '            else:')
    (174, '                ## paired and not trimmed')
    (175, '                ## alignment')
    (176, '                input.extend(')
    (177, '                    expand(')
    (178, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (179, '                        sample=samples.name[')
    (180, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (181, '                        ].values.tolist(),')
    (182, '                    )')
    (183, '                )')
    (184, '                input.extend(')
    (185, '                    expand(')
    (186, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (187, '                        sample=samples.name[')
    (188, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (189, '                        ].values.tolist(),')
    (190, '                    )')
    (191, '                )')
    (192, '                ## deduplication')
    (193, '                input.extend(')
    (194, '                    expand(')
    (195, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.deduplicated.bam",')
    (196, '                        sample=samples.name[')
    (197, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (198, '                        ].values.tolist(),')
    (199, '                    )')
    (200, '                )')
    (201, '                input.extend(')
    (202, '                    expand(')
    (203, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.deduplicated.bam",')
    (204, '                        sample=samples.name[')
    (205, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (206, '                        ].values.tolist(),')
    (207, '                    )')
    (208, '                )')
    (209, '            ## qualimap')
    (210, '            input.extend(')
    (211, '                expand(')
    (212, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_p1/qualimapReport.html",')
    (213, '                    sample=samples.name[samples.origin == "parent1"].values.tolist(),')
    (214, '                )')
    (215, '            )')
    (216, '            input.extend(')
    (217, '                expand(')
    (218, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_p2/qualimapReport.html",')
    (219, '                    sample=samples.name[samples.origin == "parent2"].values.tolist(),')
    (220, '                )')
    (221, '            )')
    (222, '            input.extend(')
    (223, '                expand(')
    (224, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_allo_pe_1/qualimapReport.html",')
    (225, '                    sample=samples.name[')
    (226, '                        (samples.type == "PE") & (samples.origin == "allopolyploid")')
    (227, '                    ].values.tolist(),')
    (228, '                )')
    (229, '            )')
    (230, '            input.extend(')
    (231, '                expand(')
    (232, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_allo_pe_2/qualimapReport.html",')
    (233, '                    sample=samples.name[')
    (234, '                        (samples.type == "PE") & (samples.origin == "allopolyploid")')
    (235, '                    ].values.tolist(),')
    (236, '                )')
    (237, '            )')
    (238, '        else:')
    (239, '            if config["RUN_TRIMMING"]:')
    (240, '                ## not paired, trimmed')
    (241, '                ## alignment')
    (242, '                input.extend(')
    (243, '                    expand(')
    (244, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_trimmed_bismark_bt2.bam",')
    (245, '                        sample=samples.name[')
    (246, '                            (samples.type == "SE") & (samples.origin != "parent2")')
    (247, '                        ].values.tolist(),')
    (248, '                    )')
    (249, '                )')
    (250, '                input.extend(')
    (251, '                    expand(')
    (252, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_trimmed_bismark_bt2.bam",')
    (253, '                        sample=samples.name[')
    (254, '                            (samples.type == "SE") & (samples.origin != "parent1")')
    (255, '                        ].values.tolist(),')
    (256, '                    )')
    (257, '                )')
    (258, '                ## deduplication')
    (259, '                input.extend(')
    (260, '                    expand(')
    (261, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_trimmed_bismark_bt2.deduplicated.bam",')
    (262, '                        sample=samples.name[')
    (263, '                            (samples.type == "SE") & (samples.origin != "parent2")')
    (264, '                        ].values.tolist(),')
    (265, '                    )')
    (266, '                )')
    (267, '                input.extend(')
    (268, '                    expand(')
    (269, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_trimmed_bismark_bt2.deduplicated.bam",')
    (270, '                        sample=samples.name[')
    (271, '                            (samples.type == "SE") & (samples.origin != "parent1")')
    (272, '                        ].values.tolist(),')
    (273, '                    )')
    (274, '                )')
    (275, '            else:')
    (276, '                ## not paired, not trimmed')
    (277, '                ## alignment')
    (278, '                input.extend(')
    (279, '                    expand(')
    (280, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_bismark_bt2.bam",')
    (281, '                        sample=samples.name[')
    (282, '                            (samples.type == "SE") & (samples.origin != "parent2")')
    (283, '                        ].values.tolist(),')
    (284, '                    )')
    (285, '                )')
    (286, '                input.extend(')
    (287, '                    expand(')
    (288, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_bismark_bt2.bam",')
    (289, '                        sample=samples.name[')
    (290, '                            (samples.type == "SE") & (samples.origin != "parent1")')
    (291, '                        ].values.tolist(),')
    (292, '                    )')
    (293, '                )')
    (294, '                ## deduplication')
    (295, '                input.extend(')
    (296, '                    expand(')
    (297, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_bismark_bt2.deduplicated.bam",')
    (298, '                        sample=samples.name[')
    (299, '                            (samples.type == "SE") & (samples.origin != "parent2")')
    (300, '                        ].values.tolist(),')
    (301, '                    )')
    (302, '                )')
    (303, '                input.extend(')
    (304, '                    expand(')
    (305, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_bismark_bt2.deduplicated.bam",')
    (306, '                        sample=samples.name[')
    (307, '                            (samples.type == "SE") & (samples.origin != "parent1")')
    (308, '                        ].values.tolist(),')
    (309, '                    )')
    (310, '                )')
    (311, '            ## qualimap')
    (312, '            input.extend(')
    (313, '                expand(')
    (314, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_p1/qualimapReport.html",')
    (315, '                    sample=samples.name[samples.origin == "parent1"].values.tolist(),')
    (316, '                )')
    (317, '            )')
    (318, '            input.extend(')
    (319, '                expand(')
    (320, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_p2/qualimapReport.html",')
    (321, '                    sample=samples.name[samples.origin == "parent2"].values.tolist(),')
    (322, '                )')
    (323, '            )')
    (324, '            input.extend(')
    (325, '                expand(')
    (326, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_allo_se_1/qualimapReport.html",')
    (327, '                    sample=samples.name[')
    (328, '                        (samples.type == "SE") & (samples.origin == "allopolyploid")')
    (329, '                    ].values.tolist(),')
    (330, '                )')
    (331, '            )')
    (332, '            input.extend(')
    (333, '                expand(')
    (334, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_allo_se_2/qualimapReport.html",')
    (335, '                    sample=samples.name[')
    (336, '                        (samples.type == "SE") & (samples.origin == "allopolyploid")')
    (337, '                    ].values.tolist(),')
    (338, '                )')
    (339, '            )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["CONVERSION_CHECK"]']
    (9, 'def multiqc_input(wildcards):')
    (10, '    input = []')
    (11, '    if config["CONVERSION_CHECK"]:')
    (12, '        if config["IS_PAIRED"]:')
    (13, '            if config["RUN_TRIMMING"]:')
    (14, '                ## paired and trimmed')
    (15, '                input.extend(')
    (16, '                    expand(')
    (17, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (18, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (19, '                    )')
    (20, '                )')
    (21, '            else:')
    (22, '                ## paired and not trimmed')
    (23, '                input.extend(')
    (24, '                    expand(')
    (25, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (26, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (27, '                    )')
    (28, '                )')
    (29, '        else:')
    (30, '            if config["RUN_TRIMMING"]:')
    (31, '                ## not paired and trimmed')
    (32, '                input.extend(')
    (33, '                    expand(')
    (34, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (35, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (36, '                    )')
    (37, '                )')
    (38, '            else:')
    (39, '                ## not paired and not trimmed')
    (40, '                input.extend(')
    (41, '                    expand(')
    (42, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (43, '                        sample=samples.name[samples.type == "SE"].values.tolist(),')
    (44, '                    )')
    (45, '                )')
    (46, '    if config["IS_PAIRED"]:')
    (47, '        input.extend(')
    (48, '            expand(')
    (49, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_fastqc.zip",')
    (50, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (51, '            )')
    (52, '        )')
    (53, '        input.extend(')
    (54, '            expand(')
    (55, '                f"{OUTPUT_DIR}FastQC/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_fastqc.zip",')
    (56, '                sample=samples.name[samples.type == "PE"].values.tolist(),')
    (57, '            )')
    (58, '        )')
    (59, '        if config["CONVERSION_CHECK"]:')
    (60, '            if config["RUN_TRIMMING"]:')
    (61, '                input.extend(')
    (62, '                    expand(')
    (63, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (64, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (65, '                    )')
    (66, '                )')
    (67, '            else:')
    (68, '                input.extend(')
    (69, '                    expand(')
    (70, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (71, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (72, '                    )')
    (73, '                )')
    (74, '    else:')
    (75, '        input.extend(')
    (76, '            expand(')
    (77, '                f"{OUTPUT_DIR}FastQC/{{sample}}_fastqc.zip",')
    (78, '                sample=samples.name[samples.type == "SE"].values.tolist(),')
    (79, '            )')
    (80, '        )')
    (81, '        if config["CONVERSION_CHECK"]:')
    (82, '            if config["RUN_TRIMMING"]:')
    (83, '                input.extend(')
    (84, '                    expand(')
    (85, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_trimmed_bismark_bt2.bam",')
    (86, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (87, '                    )')
    (88, '                )')
    (89, '            else:')
    (90, '                input.extend(')
    (91, '                    expand(')
    (92, '                        f"{OUTPUT_DIR}Conversion_efficiency/{{sample}}/cc.{{sample}}_bismark_bt2.bam",')
    (93, '                        sample=samples.name[samples.type == "PE"].values.tolist(),')
    (94, '                    )')
    (95, '                )')
    (96, '    if config["RUN_TRIMMING"]:')
    (97, '        if config["IS_PAIRED"]:')
    (98, '            input.extend(')
    (99, '                expand(')
    (100, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1.fq.gz",')
    (101, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (102, '                )')
    (103, '            )')
    (104, '            input.extend(')
    (105, '                expand(')
    (106, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2.fq.gz",')
    (107, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (108, '                )')
    (109, '            )')
    (110, '            input.extend(')
    (111, '                expand(')
    (112, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_fastqc.zip",')
    (113, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (114, '                )')
    (115, '            )')
    (116, '            input.extend(')
    (117, '                expand(')
    (118, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_{str(config[\\\'PAIR_2\\\'])}_val_2_fastqc.zip",')
    (119, '                    sample=samples.name[samples.type == "PE"].values.tolist(),')
    (120, '                )')
    (121, '            )')
    (122, '        else:')
    (123, '            input.extend(')
    (124, '                expand(')
    (125, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed.fq.gz",')
    (126, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (127, '                )')
    (128, '            )')
    (129, '            input.extend(')
    (130, '                expand(')
    (131, '                    f"{OUTPUT_DIR}FASTQtrimmed/{{sample}}_trimmed_fastqc.zip",')
    (132, '                    sample=samples.name[samples.type == "SE"].values.tolist(),')
    (133, '                )')
    (134, '            )')
    (135, '    if config["RUN_BISMARK"]:')
    (136, '        if config["IS_PAIRED"]:')
    (137, '            if config["RUN_TRIMMING"]:')
    (138, '                ## paired and trimmed')
    (139, '                ## alignment')
    (140, '                input.extend(')
    (141, '                    expand(')
    (142, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (143, '                        sample=samples.name[')
    (144, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (145, '                        ].values.tolist(),')
    (146, '                    )')
    (147, '                )')
    (148, '                input.extend(')
    (149, '                    expand(')
    (150, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.bam",')
    (151, '                        sample=samples.name[')
    (152, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (153, '                        ].values.tolist(),')
    (154, '                    )')
    (155, '                )')
    (156, '                ## deduplication')
    (157, '                input.extend(')
    (158, '                    expand(')
    (159, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.deduplicated.bam",')
    (160, '                        sample=samples.name[')
    (161, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (162, '                        ].values.tolist(),')
    (163, '                    )')
    (164, '                )')
    (165, '                input.extend(')
    (166, '                    expand(')
    (167, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_val_1_bismark_bt2_pe.deduplicated.bam",')
    (168, '                        sample=samples.name[')
    (169, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (170, '                        ].values.tolist(),')
    (171, '                    )')
    (172, '                )')
    (173, '            else:')
    (174, '                ## paired and not trimmed')
    (175, '                ## alignment')
    (176, '                input.extend(')
    (177, '                    expand(')
    (178, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (179, '                        sample=samples.name[')
    (180, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (181, '                        ].values.tolist(),')
    (182, '                    )')
    (183, '                )')
    (184, '                input.extend(')
    (185, '                    expand(')
    (186, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.bam",')
    (187, '                        sample=samples.name[')
    (188, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (189, '                        ].values.tolist(),')
    (190, '                    )')
    (191, '                )')
    (192, '                ## deduplication')
    (193, '                input.extend(')
    (194, '                    expand(')
    (195, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.deduplicated.bam",')
    (196, '                        sample=samples.name[')
    (197, '                            (samples.type == "PE") & (samples.origin != "parent2")')
    (198, '                        ].values.tolist(),')
    (199, '                    )')
    (200, '                )')
    (201, '                input.extend(')
    (202, '                    expand(')
    (203, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_{str(config[\\\'PAIR_1\\\'])}_bismark_bt2_pe.deduplicated.bam",')
    (204, '                        sample=samples.name[')
    (205, '                            (samples.type == "PE") & (samples.origin != "parent1")')
    (206, '                        ].values.tolist(),')
    (207, '                    )')
    (208, '                )')
    (209, '            ## qualimap')
    (210, '            input.extend(')
    (211, '                expand(')
    (212, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_p1/qualimapReport.html",')
    (213, '                    sample=samples.name[samples.origin == "parent1"].values.tolist(),')
    (214, '                )')
    (215, '            )')
    (216, '            input.extend(')
    (217, '                expand(')
    (218, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_p2/qualimapReport.html",')
    (219, '                    sample=samples.name[samples.origin == "parent2"].values.tolist(),')
    (220, '                )')
    (221, '            )')
    (222, '            input.extend(')
    (223, '                expand(')
    (224, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_allo_pe_1/qualimapReport.html",')
    (225, '                    sample=samples.name[')
    (226, '                        (samples.type == "PE") & (samples.origin == "allopolyploid")')
    (227, '                    ].values.tolist(),')
    (228, '                )')
    (229, '            )')
    (230, '            input.extend(')
    (231, '                expand(')
    (232, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_allo_pe_2/qualimapReport.html",')
    (233, '                    sample=samples.name[')
    (234, '                        (samples.type == "PE") & (samples.origin == "allopolyploid")')
    (235, '                    ].values.tolist(),')
    (236, '                )')
    (237, '            )')
    (238, '        else:')
    (239, '            if config["RUN_TRIMMING"]:')
    (240, '                ## not paired, trimmed')
    (241, '                ## alignment')
    (242, '                input.extend(')
    (243, '                    expand(')
    (244, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_trimmed_bismark_bt2.bam",')
    (245, '                        sample=samples.name[')
    (246, '                            (samples.type == "SE") & (samples.origin != "parent2")')
    (247, '                        ].values.tolist(),')
    (248, '                    )')
    (249, '                )')
    (250, '                input.extend(')
    (251, '                    expand(')
    (252, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_trimmed_bismark_bt2.bam",')
    (253, '                        sample=samples.name[')
    (254, '                            (samples.type == "SE") & (samples.origin != "parent1")')
    (255, '                        ].values.tolist(),')
    (256, '                    )')
    (257, '                )')
    (258, '                ## deduplication')
    (259, '                input.extend(')
    (260, '                    expand(')
    (261, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_trimmed_bismark_bt2.deduplicated.bam",')
    (262, '                        sample=samples.name[')
    (263, '                            (samples.type == "SE") & (samples.origin != "parent2")')
    (264, '                        ].values.tolist(),')
    (265, '                    )')
    (266, '                )')
    (267, '                input.extend(')
    (268, '                    expand(')
    (269, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_trimmed_bismark_bt2.deduplicated.bam",')
    (270, '                        sample=samples.name[')
    (271, '                            (samples.type == "SE") & (samples.origin != "parent1")')
    (272, '                        ].values.tolist(),')
    (273, '                    )')
    (274, '                )')
    (275, '            else:')
    (276, '                ## not paired, not trimmed')
    (277, '                ## alignment')
    (278, '                input.extend(')
    (279, '                    expand(')
    (280, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_1/1.{{sample}}_bismark_bt2.bam",')
    (281, '                        sample=samples.name[')
    (282, '                            (samples.type == "SE") & (samples.origin != "parent2")')
    (283, '                        ].values.tolist(),')
    (284, '                    )')
    (285, '                )')
    (286, '                input.extend(')
    (287, '                    expand(')
    (288, '                        f"{OUTPUT_DIR}Bismark/{{sample}}_2/2.{{sample}}_bismark_bt2.bam",')
    (289, '                        sample=samples.name[')
    (290, '                            (samples.type == "SE") & (samples.origin != "parent1")')
    (291, '                        ].values.tolist(),')
    (292, '                    )')
    (293, '                )')
    (294, '                ## deduplication')
    (295, '                input.extend(')
    (296, '                    expand(')
    (297, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_1/1.{{sample}}_bismark_bt2.deduplicated.bam",')
    (298, '                        sample=samples.name[')
    (299, '                            (samples.type == "SE") & (samples.origin != "parent2")')
    (300, '                        ].values.tolist(),')
    (301, '                    )')
    (302, '                )')
    (303, '                input.extend(')
    (304, '                    expand(')
    (305, '                        f"{OUTPUT_DIR}Bismark/deduplication/{{sample}}_2/2.{{sample}}_bismark_bt2.deduplicated.bam",')
    (306, '                        sample=samples.name[')
    (307, '                            (samples.type == "SE") & (samples.origin != "parent1")')
    (308, '                        ].values.tolist(),')
    (309, '                    )')
    (310, '                )')
    (311, '            ## qualimap')
    (312, '            input.extend(')
    (313, '                expand(')
    (314, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_p1/qualimapReport.html",')
    (315, '                    sample=samples.name[samples.origin == "parent1"].values.tolist(),')
    (316, '                )')
    (317, '            )')
    (318, '            input.extend(')
    (319, '                expand(')
    (320, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_p2/qualimapReport.html",')
    (321, '                    sample=samples.name[samples.origin == "parent2"].values.tolist(),')
    (322, '                )')
    (323, '            )')
    (324, '            input.extend(')
    (325, '                expand(')
    (326, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_allo_se_1/qualimapReport.html",')
    (327, '                    sample=samples.name[')
    (328, '                        (samples.type == "SE") & (samples.origin == "allopolyploid")')
    (329, '                    ].values.tolist(),')
    (330, '                )')
    (331, '            )')
    (332, '            input.extend(')
    (333, '                expand(')
    (334, '                    f"{OUTPUT_DIR}qualimap/{{sample}}_allo_se_2/qualimapReport.html",')
    (335, '                    sample=samples.name[')
    (336, '                        (samples.type == "SE") & (samples.origin == "allopolyploid")')
    (337, '                    ].values.tolist(),')
    (338, '                )')
    (339, '            )')
    (340, '    return input')
    (341, '')
    (342, '')
    (343, '## Define a function to create parameter directories for multiqc based on the settings in the config file')
    (344, '')
    (345, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["CONVERSION_CHECK"]']
    (346, 'def multiqc_params(wildcards):')
    (347, '    param = [f"{OUTPUT_DIR}FastQC"]')
    (348, '    if config["CONVERSION_CHECK"]:')
    (349, '        param.append(f"{OUTPUT_DIR}Conversion_efficiency")')
    (350, '    if config["RUN_TRIMMING"]:')
    (351, '        param.append(f"{OUTPUT_DIR}FASTQtrimmed")')
    (352, '    if config["RUN_BISMARK"]:')
    (353, '        param.append(f"{OUTPUT_DIR}Bismark")')
    (354, '        param.append(f"{OUTPUT_DIR}qualimap")')
    (355, '    return param')
    (356, '')
    (357, '')
    (358, '# General rule to run all analyses depending on the config settings')
    (359, '')
    (360, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["RUN_DOWNSTREAM"]', 'if config["ONLY_CG_CONTEXT"]', 'if config["POLYPLOID_ONLY"]']
    (361, 'def dmr_input(wildcards):')
    (362, '    input = []')
    (363, '    if config["RUN_DOWNSTREAM"]:')
    (364, '        if config["ONLY_CG_CONTEXT"]:')
    (365, '            if config["POLYPLOID_ONLY"]:')
    (366, '                input.extend(')
    (367, '                    expand(')
    (368, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (369, '                        context=["CG_context"],')
    (370, '                    )')
    (371, '                )')
    (372, '                input.extend(')
    (373, '                    expand(')
    (374, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (375, '                        context=["CG_context"],')
    (376, '                    )')
    (377, '                )')
    (378, '            elif config["DIPLOID_ONLY"]:')
    (379, '                input.extend(')
    (380, '                    expand(')
    (381, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (382, '                        context=["CG_context"],')
    (383, '                    )')
    (384, '                )')
    (385, '            else:')
    (386, '                input.extend(')
    (387, '                    expand(')
    (388, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (389, '                        context=["CG_context"],')
    (390, '                    )')
    (391, '                )')
    (392, '                input.extend(')
    (393, '                    expand(')
    (394, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (395, '                        context=["CG_context"],')
    (396, '                    )')
    (397, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["RUN_DOWNSTREAM"]', 'if config["ONLY_CG_CONTEXT"]', 'else', 'if config["POLYPLOID_ONLY"]']
    (361, 'def dmr_input(wildcards):')
    (362, '    input = []')
    (363, '    if config["RUN_DOWNSTREAM"]:')
    (364, '        if config["ONLY_CG_CONTEXT"]:')
    (365, '            if config["POLYPLOID_ONLY"]:')
    (366, '                input.extend(')
    (367, '                    expand(')
    (368, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (369, '                        context=["CG_context"],')
    (370, '                    )')
    (371, '                )')
    (372, '                input.extend(')
    (373, '                    expand(')
    (374, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (375, '                        context=["CG_context"],')
    (376, '                    )')
    (377, '                )')
    (378, '            elif config["DIPLOID_ONLY"]:')
    (379, '                input.extend(')
    (380, '                    expand(')
    (381, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (382, '                        context=["CG_context"],')
    (383, '                    )')
    (384, '                )')
    (385, '            else:')
    (386, '                input.extend(')
    (387, '                    expand(')
    (388, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (389, '                        context=["CG_context"],')
    (390, '                    )')
    (391, '                )')
    (392, '                input.extend(')
    (393, '                    expand(')
    (394, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (395, '                        context=["CG_context"],')
    (396, '                    )')
    (397, '                )')
    (398, '        else:')
    (399, '            if config["POLYPLOID_ONLY"]:')
    (400, '                input.extend(')
    (401, '                    expand(')
    (402, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (403, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (404, '                    )')
    (405, '                )')
    (406, '                input.extend(')
    (407, '                    expand(')
    (408, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (409, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (410, '                    )')
    (411, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["RUN_DOWNSTREAM"]', 'if config["ONLY_CG_CONTEXT"]', 'else', 'elif config["DIPLOID_ONLY"]']
    (361, 'def dmr_input(wildcards):')
    (362, '    input = []')
    (363, '    if config["RUN_DOWNSTREAM"]:')
    (364, '        if config["ONLY_CG_CONTEXT"]:')
    (365, '            if config["POLYPLOID_ONLY"]:')
    (366, '                input.extend(')
    (367, '                    expand(')
    (368, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (369, '                        context=["CG_context"],')
    (370, '                    )')
    (371, '                )')
    (372, '                input.extend(')
    (373, '                    expand(')
    (374, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (375, '                        context=["CG_context"],')
    (376, '                    )')
    (377, '                )')
    (378, '            elif config["DIPLOID_ONLY"]:')
    (379, '                input.extend(')
    (380, '                    expand(')
    (381, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (382, '                        context=["CG_context"],')
    (383, '                    )')
    (384, '                )')
    (385, '            else:')
    (386, '                input.extend(')
    (387, '                    expand(')
    (388, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (389, '                        context=["CG_context"],')
    (390, '                    )')
    (391, '                )')
    (392, '                input.extend(')
    (393, '                    expand(')
    (394, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (395, '                        context=["CG_context"],')
    (396, '                    )')
    (397, '                )')
    (398, '        else:')
    (399, '            if config["POLYPLOID_ONLY"]:')
    (400, '                input.extend(')
    (401, '                    expand(')
    (402, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (403, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (404, '                    )')
    (405, '                )')
    (406, '                input.extend(')
    (407, '                    expand(')
    (408, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (409, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (410, '                    )')
    (411, '                )')
    (412, '            elif config["DIPLOID_ONLY"]:')
    (413, '                input.extend(')
    (414, '                    expand(')
    (415, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (416, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (417, '                    )')
    (418, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["RUN_DOWNSTREAM"]', 'if config["ONLY_CG_CONTEXT"]', 'else', 'else']
    (361, 'def dmr_input(wildcards):')
    (362, '    input = []')
    (363, '    if config["RUN_DOWNSTREAM"]:')
    (364, '        if config["ONLY_CG_CONTEXT"]:')
    (365, '            if config["POLYPLOID_ONLY"]:')
    (366, '                input.extend(')
    (367, '                    expand(')
    (368, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (369, '                        context=["CG_context"],')
    (370, '                    )')
    (371, '                )')
    (372, '                input.extend(')
    (373, '                    expand(')
    (374, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (375, '                        context=["CG_context"],')
    (376, '                    )')
    (377, '                )')
    (378, '            elif config["DIPLOID_ONLY"]:')
    (379, '                input.extend(')
    (380, '                    expand(')
    (381, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (382, '                        context=["CG_context"],')
    (383, '                    )')
    (384, '                )')
    (385, '            else:')
    (386, '                input.extend(')
    (387, '                    expand(')
    (388, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (389, '                        context=["CG_context"],')
    (390, '                    )')
    (391, '                )')
    (392, '                input.extend(')
    (393, '                    expand(')
    (394, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (395, '                        context=["CG_context"],')
    (396, '                    )')
    (397, '                )')
    (398, '        else:')
    (399, '            if config["POLYPLOID_ONLY"]:')
    (400, '                input.extend(')
    (401, '                    expand(')
    (402, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (403, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (404, '                    )')
    (405, '                )')
    (406, '                input.extend(')
    (407, '                    expand(')
    (408, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (409, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (410, '                    )')
    (411, '                )')
    (412, '            elif config["DIPLOID_ONLY"]:')
    (413, '                input.extend(')
    (414, '                    expand(')
    (415, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (416, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (417, '                    )')
    (418, '                )')
    (419, '            else:')
    (420, '                input.extend(')
    (421, '                    expand(')
    (422, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (423, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (424, '                    )')
    (425, '                )')
    (426, '                input.extend(')
    (427, '                    expand(')
    (428, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (429, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (430, '                    )')
    (431, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["RUN_DOWNSTREAM"]', 'if config["RUN_DMR_ANALYSIS"]', 'if config["ONLY_CG_CONTEXT"]', 'if config["POLYPLOID_ONLY"]']
    (361, 'def dmr_input(wildcards):')
    (362, '    input = []')
    (363, '    if config["RUN_DOWNSTREAM"]:')
    (364, '        if config["ONLY_CG_CONTEXT"]:')
    (365, '            if config["POLYPLOID_ONLY"]:')
    (366, '                input.extend(')
    (367, '                    expand(')
    (368, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (369, '                        context=["CG_context"],')
    (370, '                    )')
    (371, '                )')
    (372, '                input.extend(')
    (373, '                    expand(')
    (374, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (375, '                        context=["CG_context"],')
    (376, '                    )')
    (377, '                )')
    (378, '            elif config["DIPLOID_ONLY"]:')
    (379, '                input.extend(')
    (380, '                    expand(')
    (381, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (382, '                        context=["CG_context"],')
    (383, '                    )')
    (384, '                )')
    (385, '            else:')
    (386, '                input.extend(')
    (387, '                    expand(')
    (388, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (389, '                        context=["CG_context"],')
    (390, '                    )')
    (391, '                )')
    (392, '                input.extend(')
    (393, '                    expand(')
    (394, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (395, '                        context=["CG_context"],')
    (396, '                    )')
    (397, '                )')
    (398, '        else:')
    (399, '            if config["POLYPLOID_ONLY"]:')
    (400, '                input.extend(')
    (401, '                    expand(')
    (402, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (403, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (404, '                    )')
    (405, '                )')
    (406, '                input.extend(')
    (407, '                    expand(')
    (408, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (409, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (410, '                    )')
    (411, '                )')
    (412, '            elif config["DIPLOID_ONLY"]:')
    (413, '                input.extend(')
    (414, '                    expand(')
    (415, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (416, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (417, '                    )')
    (418, '                )')
    (419, '            else:')
    (420, '                input.extend(')
    (421, '                    expand(')
    (422, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (423, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (424, '                    )')
    (425, '                )')
    (426, '                input.extend(')
    (427, '                    expand(')
    (428, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (429, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (430, '                    )')
    (431, '                )')
    (432, '    if config["RUN_DMR_ANALYSIS"]:')
    (433, '        if config["ONLY_CG_CONTEXT"]:')
    (434, '            if config["POLYPLOID_ONLY"]:')
    (435, '                input.extend(')
    (436, '                    expand(')
    (437, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (438, '                        context=["CG_context"],')
    (439, '                    )')
    (440, '                )')
    (441, '                input.extend(')
    (442, '                    expand(')
    (443, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (444, '                        context=["CG_context"],')
    (445, '                    )')
    (446, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["RUN_DOWNSTREAM"]', 'if config["RUN_DMR_ANALYSIS"]', 'if config["ONLY_CG_CONTEXT"]', 'elif config["DIPLOID_ONLY"]']
    (361, 'def dmr_input(wildcards):')
    (362, '    input = []')
    (363, '    if config["RUN_DOWNSTREAM"]:')
    (364, '        if config["ONLY_CG_CONTEXT"]:')
    (365, '            if config["POLYPLOID_ONLY"]:')
    (366, '                input.extend(')
    (367, '                    expand(')
    (368, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (369, '                        context=["CG_context"],')
    (370, '                    )')
    (371, '                )')
    (372, '                input.extend(')
    (373, '                    expand(')
    (374, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (375, '                        context=["CG_context"],')
    (376, '                    )')
    (377, '                )')
    (378, '            elif config["DIPLOID_ONLY"]:')
    (379, '                input.extend(')
    (380, '                    expand(')
    (381, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (382, '                        context=["CG_context"],')
    (383, '                    )')
    (384, '                )')
    (385, '            else:')
    (386, '                input.extend(')
    (387, '                    expand(')
    (388, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (389, '                        context=["CG_context"],')
    (390, '                    )')
    (391, '                )')
    (392, '                input.extend(')
    (393, '                    expand(')
    (394, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (395, '                        context=["CG_context"],')
    (396, '                    )')
    (397, '                )')
    (398, '        else:')
    (399, '            if config["POLYPLOID_ONLY"]:')
    (400, '                input.extend(')
    (401, '                    expand(')
    (402, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (403, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (404, '                    )')
    (405, '                )')
    (406, '                input.extend(')
    (407, '                    expand(')
    (408, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (409, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (410, '                    )')
    (411, '                )')
    (412, '            elif config["DIPLOID_ONLY"]:')
    (413, '                input.extend(')
    (414, '                    expand(')
    (415, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (416, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (417, '                    )')
    (418, '                )')
    (419, '            else:')
    (420, '                input.extend(')
    (421, '                    expand(')
    (422, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (423, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (424, '                    )')
    (425, '                )')
    (426, '                input.extend(')
    (427, '                    expand(')
    (428, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (429, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (430, '                    )')
    (431, '                )')
    (432, '    if config["RUN_DMR_ANALYSIS"]:')
    (433, '        if config["ONLY_CG_CONTEXT"]:')
    (434, '            if config["POLYPLOID_ONLY"]:')
    (435, '                input.extend(')
    (436, '                    expand(')
    (437, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (438, '                        context=["CG_context"],')
    (439, '                    )')
    (440, '                )')
    (441, '                input.extend(')
    (442, '                    expand(')
    (443, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (444, '                        context=["CG_context"],')
    (445, '                    )')
    (446, '                )')
    (447, '            elif config["DIPLOID_ONLY"]:')
    (448, '                input.extend(')
    (449, '                    expand(')
    (450, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_diploid.txt",')
    (451, '                        context=["CG_context"],')
    (452, '                    )')
    (453, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["RUN_DOWNSTREAM"]', 'if config["RUN_DMR_ANALYSIS"]', 'if config["ONLY_CG_CONTEXT"]', 'else']
    (361, 'def dmr_input(wildcards):')
    (362, '    input = []')
    (363, '    if config["RUN_DOWNSTREAM"]:')
    (364, '        if config["ONLY_CG_CONTEXT"]:')
    (365, '            if config["POLYPLOID_ONLY"]:')
    (366, '                input.extend(')
    (367, '                    expand(')
    (368, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (369, '                        context=["CG_context"],')
    (370, '                    )')
    (371, '                )')
    (372, '                input.extend(')
    (373, '                    expand(')
    (374, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (375, '                        context=["CG_context"],')
    (376, '                    )')
    (377, '                )')
    (378, '            elif config["DIPLOID_ONLY"]:')
    (379, '                input.extend(')
    (380, '                    expand(')
    (381, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (382, '                        context=["CG_context"],')
    (383, '                    )')
    (384, '                )')
    (385, '            else:')
    (386, '                input.extend(')
    (387, '                    expand(')
    (388, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (389, '                        context=["CG_context"],')
    (390, '                    )')
    (391, '                )')
    (392, '                input.extend(')
    (393, '                    expand(')
    (394, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (395, '                        context=["CG_context"],')
    (396, '                    )')
    (397, '                )')
    (398, '        else:')
    (399, '            if config["POLYPLOID_ONLY"]:')
    (400, '                input.extend(')
    (401, '                    expand(')
    (402, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (403, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (404, '                    )')
    (405, '                )')
    (406, '                input.extend(')
    (407, '                    expand(')
    (408, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (409, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (410, '                    )')
    (411, '                )')
    (412, '            elif config["DIPLOID_ONLY"]:')
    (413, '                input.extend(')
    (414, '                    expand(')
    (415, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (416, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (417, '                    )')
    (418, '                )')
    (419, '            else:')
    (420, '                input.extend(')
    (421, '                    expand(')
    (422, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (423, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (424, '                    )')
    (425, '                )')
    (426, '                input.extend(')
    (427, '                    expand(')
    (428, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (429, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (430, '                    )')
    (431, '                )')
    (432, '    if config["RUN_DMR_ANALYSIS"]:')
    (433, '        if config["ONLY_CG_CONTEXT"]:')
    (434, '            if config["POLYPLOID_ONLY"]:')
    (435, '                input.extend(')
    (436, '                    expand(')
    (437, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (438, '                        context=["CG_context"],')
    (439, '                    )')
    (440, '                )')
    (441, '                input.extend(')
    (442, '                    expand(')
    (443, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (444, '                        context=["CG_context"],')
    (445, '                    )')
    (446, '                )')
    (447, '            elif config["DIPLOID_ONLY"]:')
    (448, '                input.extend(')
    (449, '                    expand(')
    (450, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_diploid.txt",')
    (451, '                        context=["CG_context"],')
    (452, '                    )')
    (453, '                )')
    (454, '            else:')
    (455, '                input.extend(')
    (456, '                    expand(')
    (457, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/parent1_v_allo.txt",')
    (458, '                        context=["CG_context"],')
    (459, '                    )')
    (460, '                )')
    (461, '                input.extend(')
    (462, '                    expand(')
    (463, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/parent2_v_allo.txt",')
    (464, '                        context=["CG_context"],')
    (465, '                    )')
    (466, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["RUN_DOWNSTREAM"]', 'if config["RUN_DMR_ANALYSIS"]', 'else', 'if config["POLYPLOID_ONLY"]']
    (361, 'def dmr_input(wildcards):')
    (362, '    input = []')
    (363, '    if config["RUN_DOWNSTREAM"]:')
    (364, '        if config["ONLY_CG_CONTEXT"]:')
    (365, '            if config["POLYPLOID_ONLY"]:')
    (366, '                input.extend(')
    (367, '                    expand(')
    (368, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (369, '                        context=["CG_context"],')
    (370, '                    )')
    (371, '                )')
    (372, '                input.extend(')
    (373, '                    expand(')
    (374, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (375, '                        context=["CG_context"],')
    (376, '                    )')
    (377, '                )')
    (378, '            elif config["DIPLOID_ONLY"]:')
    (379, '                input.extend(')
    (380, '                    expand(')
    (381, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (382, '                        context=["CG_context"],')
    (383, '                    )')
    (384, '                )')
    (385, '            else:')
    (386, '                input.extend(')
    (387, '                    expand(')
    (388, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (389, '                        context=["CG_context"],')
    (390, '                    )')
    (391, '                )')
    (392, '                input.extend(')
    (393, '                    expand(')
    (394, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (395, '                        context=["CG_context"],')
    (396, '                    )')
    (397, '                )')
    (398, '        else:')
    (399, '            if config["POLYPLOID_ONLY"]:')
    (400, '                input.extend(')
    (401, '                    expand(')
    (402, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (403, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (404, '                    )')
    (405, '                )')
    (406, '                input.extend(')
    (407, '                    expand(')
    (408, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (409, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (410, '                    )')
    (411, '                )')
    (412, '            elif config["DIPLOID_ONLY"]:')
    (413, '                input.extend(')
    (414, '                    expand(')
    (415, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (416, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (417, '                    )')
    (418, '                )')
    (419, '            else:')
    (420, '                input.extend(')
    (421, '                    expand(')
    (422, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (423, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (424, '                    )')
    (425, '                )')
    (426, '                input.extend(')
    (427, '                    expand(')
    (428, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (429, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (430, '                    )')
    (431, '                )')
    (432, '    if config["RUN_DMR_ANALYSIS"]:')
    (433, '        if config["ONLY_CG_CONTEXT"]:')
    (434, '            if config["POLYPLOID_ONLY"]:')
    (435, '                input.extend(')
    (436, '                    expand(')
    (437, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (438, '                        context=["CG_context"],')
    (439, '                    )')
    (440, '                )')
    (441, '                input.extend(')
    (442, '                    expand(')
    (443, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (444, '                        context=["CG_context"],')
    (445, '                    )')
    (446, '                )')
    (447, '            elif config["DIPLOID_ONLY"]:')
    (448, '                input.extend(')
    (449, '                    expand(')
    (450, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_diploid.txt",')
    (451, '                        context=["CG_context"],')
    (452, '                    )')
    (453, '                )')
    (454, '            else:')
    (455, '                input.extend(')
    (456, '                    expand(')
    (457, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/parent1_v_allo.txt",')
    (458, '                        context=["CG_context"],')
    (459, '                    )')
    (460, '                )')
    (461, '                input.extend(')
    (462, '                    expand(')
    (463, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/parent2_v_allo.txt",')
    (464, '                        context=["CG_context"],')
    (465, '                    )')
    (466, '                )')
    (467, '        else:')
    (468, '            if config["POLYPLOID_ONLY"]:')
    (469, '                input.extend(')
    (470, '                    expand(')
    (471, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (472, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (473, '                    )')
    (474, '                )')
    (475, '                input.extend(')
    (476, '                    expand(')
    (477, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (478, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (479, '                    )')
    (480, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["RUN_DOWNSTREAM"]', 'if config["RUN_DMR_ANALYSIS"]', 'else', 'elif config["DIPLOID_ONLY"]']
    (361, 'def dmr_input(wildcards):')
    (362, '    input = []')
    (363, '    if config["RUN_DOWNSTREAM"]:')
    (364, '        if config["ONLY_CG_CONTEXT"]:')
    (365, '            if config["POLYPLOID_ONLY"]:')
    (366, '                input.extend(')
    (367, '                    expand(')
    (368, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (369, '                        context=["CG_context"],')
    (370, '                    )')
    (371, '                )')
    (372, '                input.extend(')
    (373, '                    expand(')
    (374, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (375, '                        context=["CG_context"],')
    (376, '                    )')
    (377, '                )')
    (378, '            elif config["DIPLOID_ONLY"]:')
    (379, '                input.extend(')
    (380, '                    expand(')
    (381, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (382, '                        context=["CG_context"],')
    (383, '                    )')
    (384, '                )')
    (385, '            else:')
    (386, '                input.extend(')
    (387, '                    expand(')
    (388, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (389, '                        context=["CG_context"],')
    (390, '                    )')
    (391, '                )')
    (392, '                input.extend(')
    (393, '                    expand(')
    (394, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (395, '                        context=["CG_context"],')
    (396, '                    )')
    (397, '                )')
    (398, '        else:')
    (399, '            if config["POLYPLOID_ONLY"]:')
    (400, '                input.extend(')
    (401, '                    expand(')
    (402, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (403, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (404, '                    )')
    (405, '                )')
    (406, '                input.extend(')
    (407, '                    expand(')
    (408, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (409, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (410, '                    )')
    (411, '                )')
    (412, '            elif config["DIPLOID_ONLY"]:')
    (413, '                input.extend(')
    (414, '                    expand(')
    (415, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (416, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (417, '                    )')
    (418, '                )')
    (419, '            else:')
    (420, '                input.extend(')
    (421, '                    expand(')
    (422, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (423, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (424, '                    )')
    (425, '                )')
    (426, '                input.extend(')
    (427, '                    expand(')
    (428, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (429, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (430, '                    )')
    (431, '                )')
    (432, '    if config["RUN_DMR_ANALYSIS"]:')
    (433, '        if config["ONLY_CG_CONTEXT"]:')
    (434, '            if config["POLYPLOID_ONLY"]:')
    (435, '                input.extend(')
    (436, '                    expand(')
    (437, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (438, '                        context=["CG_context"],')
    (439, '                    )')
    (440, '                )')
    (441, '                input.extend(')
    (442, '                    expand(')
    (443, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (444, '                        context=["CG_context"],')
    (445, '                    )')
    (446, '                )')
    (447, '            elif config["DIPLOID_ONLY"]:')
    (448, '                input.extend(')
    (449, '                    expand(')
    (450, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_diploid.txt",')
    (451, '                        context=["CG_context"],')
    (452, '                    )')
    (453, '                )')
    (454, '            else:')
    (455, '                input.extend(')
    (456, '                    expand(')
    (457, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/parent1_v_allo.txt",')
    (458, '                        context=["CG_context"],')
    (459, '                    )')
    (460, '                )')
    (461, '                input.extend(')
    (462, '                    expand(')
    (463, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/parent2_v_allo.txt",')
    (464, '                        context=["CG_context"],')
    (465, '                    )')
    (466, '                )')
    (467, '        else:')
    (468, '            if config["POLYPLOID_ONLY"]:')
    (469, '                input.extend(')
    (470, '                    expand(')
    (471, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (472, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (473, '                    )')
    (474, '                )')
    (475, '                input.extend(')
    (476, '                    expand(')
    (477, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (478, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (479, '                    )')
    (480, '                )')
    (481, '            elif config["DIPLOID_ONLY"]:')
    (482, '                input.extend(')
    (483, '                    expand(')
    (484, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_diploid.txt",')
    (485, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (486, '                    )')
    (487, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["RUN_DOWNSTREAM"]', 'if config["RUN_DMR_ANALYSIS"]', 'else', 'else']
    (361, 'def dmr_input(wildcards):')
    (362, '    input = []')
    (363, '    if config["RUN_DOWNSTREAM"]:')
    (364, '        if config["ONLY_CG_CONTEXT"]:')
    (365, '            if config["POLYPLOID_ONLY"]:')
    (366, '                input.extend(')
    (367, '                    expand(')
    (368, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (369, '                        context=["CG_context"],')
    (370, '                    )')
    (371, '                )')
    (372, '                input.extend(')
    (373, '                    expand(')
    (374, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (375, '                        context=["CG_context"],')
    (376, '                    )')
    (377, '                )')
    (378, '            elif config["DIPLOID_ONLY"]:')
    (379, '                input.extend(')
    (380, '                    expand(')
    (381, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (382, '                        context=["CG_context"],')
    (383, '                    )')
    (384, '                )')
    (385, '            else:')
    (386, '                input.extend(')
    (387, '                    expand(')
    (388, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (389, '                        context=["CG_context"],')
    (390, '                    )')
    (391, '                )')
    (392, '                input.extend(')
    (393, '                    expand(')
    (394, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (395, '                        context=["CG_context"],')
    (396, '                    )')
    (397, '                )')
    (398, '        else:')
    (399, '            if config["POLYPLOID_ONLY"]:')
    (400, '                input.extend(')
    (401, '                    expand(')
    (402, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (403, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (404, '                    )')
    (405, '                )')
    (406, '                input.extend(')
    (407, '                    expand(')
    (408, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (409, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (410, '                    )')
    (411, '                )')
    (412, '            elif config["DIPLOID_ONLY"]:')
    (413, '                input.extend(')
    (414, '                    expand(')
    (415, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (416, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (417, '                    )')
    (418, '                )')
    (419, '            else:')
    (420, '                input.extend(')
    (421, '                    expand(')
    (422, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (423, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (424, '                    )')
    (425, '                )')
    (426, '                input.extend(')
    (427, '                    expand(')
    (428, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (429, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (430, '                    )')
    (431, '                )')
    (432, '    if config["RUN_DMR_ANALYSIS"]:')
    (433, '        if config["ONLY_CG_CONTEXT"]:')
    (434, '            if config["POLYPLOID_ONLY"]:')
    (435, '                input.extend(')
    (436, '                    expand(')
    (437, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (438, '                        context=["CG_context"],')
    (439, '                    )')
    (440, '                )')
    (441, '                input.extend(')
    (442, '                    expand(')
    (443, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (444, '                        context=["CG_context"],')
    (445, '                    )')
    (446, '                )')
    (447, '            elif config["DIPLOID_ONLY"]:')
    (448, '                input.extend(')
    (449, '                    expand(')
    (450, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_diploid.txt",')
    (451, '                        context=["CG_context"],')
    (452, '                    )')
    (453, '                )')
    (454, '            else:')
    (455, '                input.extend(')
    (456, '                    expand(')
    (457, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/parent1_v_allo.txt",')
    (458, '                        context=["CG_context"],')
    (459, '                    )')
    (460, '                )')
    (461, '                input.extend(')
    (462, '                    expand(')
    (463, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/parent2_v_allo.txt",')
    (464, '                        context=["CG_context"],')
    (465, '                    )')
    (466, '                )')
    (467, '        else:')
    (468, '            if config["POLYPLOID_ONLY"]:')
    (469, '                input.extend(')
    (470, '                    expand(')
    (471, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (472, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (473, '                    )')
    (474, '                )')
    (475, '                input.extend(')
    (476, '                    expand(')
    (477, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (478, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (479, '                    )')
    (480, '                )')
    (481, '            elif config["DIPLOID_ONLY"]:')
    (482, '                input.extend(')
    (483, '                    expand(')
    (484, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_diploid.txt",')
    (485, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (486, '                    )')
    (487, '                )')
    (488, '            else:')
    (489, '                input.extend(')
    (490, '                    expand(')
    (491, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/parent1_v_allo.txt",')
    (492, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (493, '                    )')
    (494, '                )')
    (495, '                input.extend(')
    (496, '                    expand(')
    (497, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/parent2_v_allo.txt",')
    (498, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (499, '                    )')
    (500, '                )')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["RUN_DOWNSTREAM"]']
    (361, 'def dmr_input(wildcards):')
    (362, '    input = []')
    (363, '    if config["RUN_DOWNSTREAM"]:')
    (364, '        if config["ONLY_CG_CONTEXT"]:')
    (365, '            if config["POLYPLOID_ONLY"]:')
    (366, '                input.extend(')
    (367, '                    expand(')
    (368, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (369, '                        context=["CG_context"],')
    (370, '                    )')
    (371, '                )')
    (372, '                input.extend(')
    (373, '                    expand(')
    (374, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (375, '                        context=["CG_context"],')
    (376, '                    )')
    (377, '                )')
    (378, '            elif config["DIPLOID_ONLY"]:')
    (379, '                input.extend(')
    (380, '                    expand(')
    (381, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (382, '                        context=["CG_context"],')
    (383, '                    )')
    (384, '                )')
    (385, '            else:')
    (386, '                input.extend(')
    (387, '                    expand(')
    (388, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (389, '                        context=["CG_context"],')
    (390, '                    )')
    (391, '                )')
    (392, '                input.extend(')
    (393, '                    expand(')
    (394, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (395, '                        context=["CG_context"],')
    (396, '                    )')
    (397, '                )')
    (398, '        else:')
    (399, '            if config["POLYPLOID_ONLY"]:')
    (400, '                input.extend(')
    (401, '                    expand(')
    (402, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_1.txt",')
    (403, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (404, '                    )')
    (405, '                )')
    (406, '                input.extend(')
    (407, '                    expand(')
    (408, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_polyploid_{{context}}_2.txt",')
    (409, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (410, '                    )')
    (411, '                )')
    (412, '            elif config["DIPLOID_ONLY"]:')
    (413, '                input.extend(')
    (414, '                    expand(')
    (415, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_A_v_B_diploid_{{context}}.txt",')
    (416, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (417, '                    )')
    (418, '                )')
    (419, '            else:')
    (420, '                input.extend(')
    (421, '                    expand(')
    (422, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent1_v_allo_{{context}}.txt",')
    (423, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (424, '                    )')
    (425, '                )')
    (426, '                input.extend(')
    (427, '                    expand(')
    (428, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/DM_genes_parent2_v_allo_{{context}}.txt",')
    (429, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (430, '                    )')
    (431, '                )')
    (432, '    if config["RUN_DMR_ANALYSIS"]:')
    (433, '        if config["ONLY_CG_CONTEXT"]:')
    (434, '            if config["POLYPLOID_ONLY"]:')
    (435, '                input.extend(')
    (436, '                    expand(')
    (437, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (438, '                        context=["CG_context"],')
    (439, '                    )')
    (440, '                )')
    (441, '                input.extend(')
    (442, '                    expand(')
    (443, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (444, '                        context=["CG_context"],')
    (445, '                    )')
    (446, '                )')
    (447, '            elif config["DIPLOID_ONLY"]:')
    (448, '                input.extend(')
    (449, '                    expand(')
    (450, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_diploid.txt",')
    (451, '                        context=["CG_context"],')
    (452, '                    )')
    (453, '                )')
    (454, '            else:')
    (455, '                input.extend(')
    (456, '                    expand(')
    (457, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/parent1_v_allo.txt",')
    (458, '                        context=["CG_context"],')
    (459, '                    )')
    (460, '                )')
    (461, '                input.extend(')
    (462, '                    expand(')
    (463, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/parent2_v_allo.txt",')
    (464, '                        context=["CG_context"],')
    (465, '                    )')
    (466, '                )')
    (467, '        else:')
    (468, '            if config["POLYPLOID_ONLY"]:')
    (469, '                input.extend(')
    (470, '                    expand(')
    (471, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (472, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (473, '                    )')
    (474, '                )')
    (475, '                input.extend(')
    (476, '                    expand(')
    (477, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_polyploid.txt",')
    (478, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (479, '                    )')
    (480, '                )')
    (481, '            elif config["DIPLOID_ONLY"]:')
    (482, '                input.extend(')
    (483, '                    expand(')
    (484, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/A_v_B_diploid.txt",')
    (485, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (486, '                    )')
    (487, '                )')
    (488, '            else:')
    (489, '                input.extend(')
    (490, '                    expand(')
    (491, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/parent1_v_allo.txt",')
    (492, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (493, '                    )')
    (494, '                )')
    (495, '                input.extend(')
    (496, '                    expand(')
    (497, '                        f"{OUTPUT_DIR}DMR_analysis/dmrseq/{{context}}/parent2_v_allo.txt",')
    (498, '                        context=["CG_context", "CHG_context", "CHH_context"],')
    (499, '                    )')
    (500, '                )')
    (501, '    return input')
    (502, '')
    (503, '')
    (504, '# Define a function to create an input for dmrseq_CG to include all the samples from the previous steps')
    (505, '')
    (506, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["POLYPLOID_ONLY"]']
    (612, 'def dmrseq_CG_special_input_A(wildcards):')
    (613, '    input = []')
    (614, '    if config["POLYPLOID_ONLY"]:')
    (615, '        input.extend(')
    (616, '            expand(')
    (617, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/allopolyploid/{{sample}}_CG.cov",')
    (618, '                sample=samples.name[')
    (619, '                    (samples.origin == "allopolyploid") & (samples.condition == "A")')
    (620, '                ].values.tolist(),')
    (621, '            )')
    (622, '        )')
    (623, '    if config["DIPLOID_ONLY"]:')
    (624, '        input.extend(')
    (625, '            expand(')
    (626, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/parent1/{{sample}}_CG.cov",')
    (627, '                sample=samples.name[')
    (628, '                    (samples.origin == "parent1") & (samples.condition == "A")')
    (629, '                ].values.tolist(),')
    (630, '            )')
    (631, '        )')
    (632, '        input.extend(')
    (633, '            expand(')
    (634, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/parent2/{{sample}}_CG.cov",')
    (635, '                sample=samples.name[')
    (636, '                    (samples.origin == "parent2") & (samples.condition == "A")')
    (637, '                ].values.tolist(),')
    (638, '            )')
    (639, '        )')
    (640, '    return input')
    (641, '')
    (642, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["POLYPLOID_ONLY"]']
    (643, 'def dmrseq_CG_special_input_B(wildcards):')
    (644, '    input = []')
    (645, '    if config["POLYPLOID_ONLY"]:')
    (646, '        input.extend(')
    (647, '            expand(')
    (648, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/allopolyploid/{{sample}}_CG.cov",')
    (649, '                sample=samples.name[')
    (650, '                    (samples.origin == "allopolyploid") & (samples.condition == "B")')
    (651, '                ].values.tolist(),')
    (652, '            )')
    (653, '        )')
    (654, '    if config["DIPLOID_ONLY"]:')
    (655, '        input.extend(')
    (656, '            expand(')
    (657, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/parent1/{{sample}}_CG.cov",')
    (658, '                sample=samples.name[')
    (659, '                    (samples.origin == "parent1") & (samples.condition == "B")')
    (660, '                ].values.tolist(),')
    (661, '            )')
    (662, '        )')
    (663, '        input.extend(')
    (664, '            expand(')
    (665, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/parent2/{{sample}}_CG.cov",')
    (666, '                sample=samples.name[')
    (667, '                    (samples.origin == "parent2") & (samples.condition == "B")')
    (668, '                ].values.tolist(),')
    (669, '            )')
    (670, '        )')
    (671, '    return input')
    (672, '')
    (673, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["POLYPLOID_ONLY"]']
    (674, 'def dmrseq_CHG_special_input_A(wildcards):')
    (675, '    input = []')
    (676, '    if config["POLYPLOID_ONLY"]:')
    (677, '        input.extend(')
    (678, '            expand(')
    (679, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/allopolyploid/{{sample}}_CHG.cov",')
    (680, '                sample=samples.name[')
    (681, '                    (samples.origin == "allopolyploid") & (samples.condition == "A")')
    (682, '                ].values.tolist(),')
    (683, '            )')
    (684, '        )')
    (685, '    if config["DIPLOID_ONLY"]:')
    (686, '        input.extend(')
    (687, '            expand(')
    (688, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/parent1/{{sample}}_CHG.cov",')
    (689, '                sample=samples.name[')
    (690, '                    (samples.origin == "parent1") & (samples.condition == "A")')
    (691, '                ].values.tolist(),')
    (692, '            )')
    (693, '        )')
    (694, '        input.extend(')
    (695, '            expand(')
    (696, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/parent2/{{sample}}_CHG.cov",')
    (697, '                sample=samples.name[')
    (698, '                    (samples.origin == "parent2") & (samples.condition == "A")')
    (699, '                ].values.tolist(),')
    (700, '            )')
    (701, '        )')
    (702, '    return input')
    (703, '')
    (704, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["POLYPLOID_ONLY"]']
    (705, 'def dmrseq_CHG_special_input_B(wildcards):')
    (706, '    input = []')
    (707, '    if config["POLYPLOID_ONLY"]:')
    (708, '        input.extend(')
    (709, '            expand(')
    (710, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/allopolyploid/{{sample}}_CHG.cov",')
    (711, '                sample=samples.name[')
    (712, '                    (samples.origin == "allopolyploid") & (samples.condition == "B")')
    (713, '                ].values.tolist(),')
    (714, '            )')
    (715, '        )')
    (716, '    if config["DIPLOID_ONLY"]:')
    (717, '        input.extend(')
    (718, '            expand(')
    (719, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/parent1/{{sample}}_CHG.cov",')
    (720, '                sample=samples.name[')
    (721, '                    (samples.origin == "parent1") & (samples.condition == "B")')
    (722, '                ].values.tolist(),')
    (723, '            )')
    (724, '        )')
    (725, '        input.extend(')
    (726, '            expand(')
    (727, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/parent2/{{sample}}_CHG.cov",')
    (728, '                sample=samples.name[')
    (729, '                    (samples.origin == "parent2") & (samples.condition == "B")')
    (730, '                ].values.tolist(),')
    (731, '            )')
    (732, '        )')
    (733, '    return input')
    (734, '')
    (735, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["POLYPLOID_ONLY"]']
    (736, 'def dmrseq_CHH_special_input_A(wildcards):')
    (737, '    input = []')
    (738, '    if config["POLYPLOID_ONLY"]:')
    (739, '        input.extend(')
    (740, '            expand(')
    (741, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/allopolyploid/{{sample}}_CHH.cov",')
    (742, '                sample=samples.name[')
    (743, '                    (samples.origin == "allopolyploid") & (samples.condition == "A")')
    (744, '                ].values.tolist(),')
    (745, '            )')
    (746, '        )')
    (747, '    if config["DIPLOID_ONLY"]:')
    (748, '        input.extend(')
    (749, '            expand(')
    (750, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/parent1/{{sample}}_CHH.cov",')
    (751, '                sample=samples.name[')
    (752, '                    (samples.origin == "parent1") & (samples.condition == "A")')
    (753, '                ].values.tolist(),')
    (754, '            )')
    (755, '        )')
    (756, '        input.extend(')
    (757, '            expand(')
    (758, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/parent2/{{sample}}_CHH.cov",')
    (759, '                sample=samples.name[')
    (760, '                    (samples.origin == "parent2") & (samples.condition == "A")')
    (761, '                ].values.tolist(),')
    (762, '            )')
    (763, '        )')
    (764, '    return input')
    (765, '')
    (766, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=supermaxiste/ARPEGGIO, file=rules/input_functions.smk
context_key: ['if config["POLYPLOID_ONLY"]']
    (767, 'def dmrseq_CHH_special_input_B(wildcards):')
    (768, '    input = []')
    (769, '    if config["POLYPLOID_ONLY"]:')
    (770, '        input.extend(')
    (771, '            expand(')
    (772, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/allopolyploid/{{sample}}_CHH.cov",')
    (773, '                sample=samples.name[')
    (774, '                    (samples.origin == "allopolyploid") & (samples.condition == "B")')
    (775, '                ].values.tolist(),')
    (776, '            )')
    (777, '        )')
    (778, '    if config["DIPLOID_ONLY"]:')
    (779, '        input.extend(')
    (780, '            expand(')
    (781, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/parent1/{{sample}}_CHH.cov",')
    (782, '                sample=samples.name[')
    (783, '                    (samples.origin == "parent1") & (samples.condition == "B")')
    (784, '                ].values.tolist(),')
    (785, '            )')
    (786, '        )')
    (787, '        input.extend(')
    (788, '            expand(')
    (789, '                f"{OUTPUT_DIR}DMR_analysis/context_separation/parent2/{{sample}}_CHH.cov",')
    (790, '                sample=samples.name[')
    (791, '                    (samples.origin == "parent2") & (samples.condition == "B")')
    (792, '                ].values.tolist(),')
    (793, '            )')
    (794, '        )')
    (795, '    return input')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=singleron-RD/Dna-seq_Analysis, file=workflow/rules/common.smk
context_key: ['if config["split_fq"]']
    (107, '    def adapter_list(self,sample_name,fq1,fq2,fq_outdir,mod,adapter_dict_sorted,index_len):')
    (108, '        global I')
    (109, "        if mod == \\'fq1\\':")
    (110, '            fq=fq1')
    (111, "        if mod == \\'fq2\\':")
    (112, '            fq=fq2')
    (113, '')
    (114, '        frist_adapter = list(adapter_dict_sorted.items())[0][0]')
    (115, '        name_list = xopen(f"{fq_outdir}/{sample_name}_{frist_adapter}.lst",\\\'w\\\')')
    (116, '')
    (117, '        fh =  pysam.FastxFile(fq)')
    (118, '        for record in fh:')
    (119, '            header, seq, = record.name, record.sequence')
    (120, '            if hm_dis(seq[:9],adapter_dict_sorted[frist_adapter]) > 1:')
    (121, '                continue')
    (122, "            name_list.write(f\\'{header}\\")
    (123, "\\')")
    (124, '        name_list.close()')
    (125, '        fh.close()')
    (126, '')
    (127, "        out_fq_R1 = f\\'{fq_outdir}/{sample_name}_{frist_adapter}_R1.fastq\\'")
    (128, "        out_fq_R2 = f\\'{fq_outdir}/{sample_name}_{frist_adapter}_R2.fastq\\'")
    (129, '')
    (130, '        cmd_line = (')
    (131, "                    f\\'seqtk subseq {fq1} {fq_outdir}/{sample_name}_{frist_adapter}.lst > {out_fq_R1};\\'")
    (132, "                    f\\'seqtk subseq {fq2} {fq_outdir}/{sample_name}_{frist_adapter}.lst > {out_fq_R2}\\'")
    (133, '                    )')
    (134, '       ')
    (135, '        subprocess.check_call(cmd_line,shell=True)')
    (136, "        print(f\\'the {I+1} finished\\')")
    (137, '        ')
    (138, '        if I < index_len:')
    (139, '            adapter_dict_sorted.popitem(last=False)')
    (140, '            I += 1')
    (141, '            i = index_len - I ')
    (142, '            return i')
    (143, '')
    (144, '###### Config file and sample sheets #####')
    (145, 'configfile: "config/config.yaml"')
    (146, '')
    (147, 'if config["split_fq"]:')
    (148, "    mapfile = config[\\'mapfile\\']")
    (149, '    runner =  Split_fastq(mapfile)')
    (150, '    runner.run()')
    (151, '')
    (152, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=singleron-RD/Dna-seq_Analysis, file=workflow/rules/common.smk
context_key: ['if config["split_fq"]']
    (182, 'def get_mapfile():')
    (183, '    """')
    (184, '    Get split fastq mapfile')
    (185, '    """')
    (186, '    if config["split_fq"]:')
    (187, '        # case 2: remove duplicates')
    (188, '        mapfile = "config/mapfile"')
    (189, '    return mapfile')
    (190, '')
    (191, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=singleron-RD/Dna-seq_Analysis, file=workflow/rules/common.smk
context_key: ['if config["processing"]["remove-duplicates"]']
    (254, 'def get_recal_input(bai=False):')
    (255, '    # case 1: no duplicate removal')
    (256, '    f = "results/mapped/{sample}-{unit}.sorted.bam"')
    (257, '    if config["processing"]["remove-duplicates"]:')
    (258, '        # case 2: remove duplicates')
    (259, '        f = "results/dedup/{sample}-{unit}.bam"')
    (260, '    if bai:')
    (261, '        if config["processing"].get("restrict-regions"):')
    (262, '            # case 3: need an index because random access is required')
    (263, '            f += ".bai"')
    (264, '            return f')
    (265, '        else:')
    (266, '            # case 4: no index needed')
    (267, '            return []')
    (268, '    else:')
    (269, '        return f')
    (270, '')
    (271, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=rimjhimroy/SnakeGATK, file=rules/common.smk
context_key: ['if config["processing"]["remove-duplicates"]']
    (53, 'def get_recal_input(bai=False):')
    (54, '    # case 1: no duplicate removal')
    (55, '    f = "data/bam/{sample}.RG.bam"')
    (56, '    if config["processing"]["remove-duplicates"]:')
    (57, '        # case 2: remove duplicates')
    (58, '        f = "output/dedup/{sample}.bam"')
    (59, '    if bai:')
    (60, '        if config["processing"].get("regions"):')
    (61, '            # case 3: need an index because random access is required')
    (62, '            f += ".bai"')
    (63, '            return f')
    (64, '        else:')
    (65, '            # case 4: no index needed')
    (66, '            return []')
    (67, '    else:')
    (68, "        return f'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=AlthafSinghawansaUHN/cfmedipseq_pipeline, file=Snakefile
context_key: ["if config[\\'data\\'][\\'cohorts\\'][cohort_name][\\'active\\'"]
    (75, 'def get_all_samples(cohort=None):')
    (76, '    """Retrieves all samples to be processed.')
    (77, '    Does so by calling get_cohort_data, and therefore filters out excluded_cases.')
    (78, '    Keyword arguments:')
    (79, '        cohort -- Name of a cohort, OPTIONAL. If not specified, returns all samples')
    (80, '                  across all cohorts.')
    (81, '    """')
    (82, '    ')
    (83, '    all_samples = pd.concat([')
    (84, '        get_cohort_data(cohort_name).assign(cohort_name = cohort_name)')
    (85, '        for cohort_name')
    (86, "        in config[\\'data\\'][\\'cohorts\\']")
    (87, "        if config[\\'data\\'][\\'cohorts\\'][cohort_name][\\'active\\']")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=LittleFool/snakemake-workflow-gsea, file=rules/star.smk
context_key: ['if config["trimming"]["skip"]']
    (3, 'def get_fq(wildcards):')
    (4, '    if config["trimming"]["skip"]:')
    (5, '        # no trimming, use raw reads')
    (6, '        return units.loc[(wildcards.sample, wildcards.unit), ["fq1", "fq2"]].dropna()')
    (7, '    else:')
    (8, '        # yes trimming, use trimmed data')
    (9, '        if not is_single_end(**wildcards):')
    (10, '            # paired-end sample')
    (11, '            return expand("data/trimmed/{sample}-{unit}.{group}.fastq.gz",')
    (12, '                          group=[1, 2], **wildcards)')
    (13, '        # single end sample')
    (14, '        return "data/trimmed/{sample}-{unit}.fastq.gz".format(**wildcards)')
    (15, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/SnakeRNASeq, file=rules/00_common.smk
context_key: ['if config["trimming"]["skip"]']
    (32, 'def GetReads(num, wildcards):')
    (33, '    trim1="/scratch/ac32082/02.PeanutRNASeq/01.analysis/peanut_rna_seq_analysis/results/02_trim/{smp}_R1_val_1.fq.gz"')
    (34, '    trim2="/scratch/ac32082/02.PeanutRNASeq/01.analysis/peanut_rna_seq_analysis/results/02_trim/{smp}_R2_val_2.fq.gz"')
    (35, '    if config["trimming"]["skip"]:')
    (36, '        return samples.loc[(wildcards.smp), ["r1", "r2"]]')
    (37, '    else:')
    (38, '        sample_list = [trim1, trim2]')
    (39, '        return sample_list[num];')
    (40, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=akcorut/SnakeRNASeq, file=rules/00_common.smk
context_key: ['if config["decontamination"]["skip"]']
    (41, 'def GetClean(num):')
    (42, '    clean1="/scratch/ac32082/02.PeanutRNASeq/01.analysis/peanut_rna_seq_analysis/results/03_decontamination/03b_rrna_cleaned/{smp}_R1_clean.fq"')
    (43, '    clean2="/scratch/ac32082/02.PeanutRNASeq/01.analysis/peanut_rna_seq_analysis/results/03_decontamination/03b_rrna_cleaned/{smp}_R2_clean.fq"')
    (44, '    trim1="/scratch/ac32082/02.PeanutRNASeq/01.analysis/peanut_rna_seq_analysis/results/02_trim/{smp}_R1_val_1.fq.gz"')
    (45, '    trim2="/scratch/ac32082/02.PeanutRNASeq/01.analysis/peanut_rna_seq_analysis/results/02_trim/{smp}_R2_val_2.fq.gz"')
    (46, '    if config["decontamination"]["skip"]:')
    (47, '        sample_list = [trim1,trim2]')
    (48, '    else:')
    (49, '        sample_list = [clean1, clean2]')
    (50, "    return sample_list[num]'")
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/common.smk
context_key: ['if config["processing"]["remove_duplicates"]']
    (81, 'def get_recal_input(bai=False):')
    (82, '    # case 1: no duplicate removal')
    (83, '    f = f"{OUTDIR}/mapped/{{sample}}-{{unit}}.sorted.bam"')
    (84, '    if config["processing"]["remove_duplicates"]:')
    (85, '        # case 2: remove duplicates')
    (86, '        f = f"{OUTDIR}/dedup/{{sample}}-{{unit}}.bam"')
    (87, '    if bai:')
    (88, '        if config["processing"].get("restrict_regions"):')
    (89, '            # case 3: need an index because random access is required')
    (90, '            f = f.replace(".bam",".bai")')
    (91, '            return f')
    (92, '        else:')
    (93, '            # case 4: no index needed')
    (94, '            return []')
    (95, '    else:')
    (96, '        return f')
    (97, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=cnio-bu/varca, file=rules/common.smk
context_key: ['if config["processing"].get("restrict_regions")']
    (98, 'def get_mutect_params(sample):')
    (99, '    if config["processing"].get("restrict_regions"):')
    (100, '        regions_call="-L "+config["processing"].get("restrict_regions")')
    (101, '    else:')
    (102, '        regions_call=""')
    (103, '    if samples.loc[(sample),"control"] != sample:')
    (104, '        normal_call="-I "+get_merged_bam(samples.loc[(sample),"control"])[0]+" -normal "+samples.loc[(sample),"control"]')
    (105, '    else:')
    (106, '        normal_call=""')
    (107, '    return regions_call,normal_call')
    (108, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=BleekerLab/freebayes_snp_calling, file=Snakefile
context_key: ['if config["remove_workdir"]', 'rule all', 'input']
    (52, 'def get_fastq(wildcards):')
    (53, '     """Get fastq files of given sample-unit."""')
    (54, '     return units.loc[(wildcards.sample, wildcards.unit), ["fq1","fq2"]].dropna()')
    (55, '')
    (56, '')
    (57, '#def merge_bams(wildcards):')
    (58, '#    "collects all bams files corresponding to the same library"')
    (59, '#    bam_files = glob(TEMP_DIR + "mapped/" + wildcards.sample + "_" + "L[0-9]+\\\\.sorted.dedup.bam")')
    (60, '#    return bam_files')
    (61, '')
    (62, '')
    (63, '#################')
    (64, '# Desired output')
    (65, '#################')
    (66, 'QC = RESULT_DIR + "multiqc_report.html"')
    (67, '')
    (68, 'BAMS = expand(TEMP_DIR + "mapped/{sample}_{unit}.bam",')
    (69, '    sample=SAMPLES,')
    (70, '    unit=UNITS)')
    (71, '')
    (72, 'STATS = [')
    (73, '  expand(RESULT_DIR + "stats/{sample}.stats.txt", sample = SAMPLES),')
    (74, '  expand(RESULT_DIR + "stats/{sample}.bigwig", sample = SAMPLES),')
    (75, '  RESULT_DIR + "stats/genome_coverage_depth.tsv"')
    (76, ']')
    (77, '')
    (78, 'GLOBAL_VCF  = [')
    (79, '  RESULT_DIR + "freebayes_variants.vcf.gz",')
    (80, '  RESULT_DIR + "samtools_variants.vcf.gz"')
    (81, '  ]')
    (82, '')
    (83, 'if config["remove_workdir"]:')
    (84, '    rule all:')
    (85, '        input:')
    (86, '            QC,')
    (87, '            BAMS,')
    (88, '            GLOBAL_VCF,')
    (89, '            STATS')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if config["experiments"][project]["assignments"][assignment]["type"] == "file"']
    (62, 'def getAssignmentFile(project, assignment):')
    (63, '    if config["experiments"][project]["assignments"][assignment]["type"] == "file":')
    (64, '        return config["experiments"][project]["assignments"][assignment][')
    (65, '            "assignment_file"')
    (66, '        ]')
    (67, '    if config["experiments"][project]["assignments"][assignment]["type"] == "config":')
    (68, '        conf = config["experiments"][project]["assignments"][assignment][')
    (69, '            "assignment_config"')
    (70, '        ]')
    (71, '        name = config["experiments"][project]["assignments"][assignment][')
    (72, '            "assignment_name"')
    (73, '        ]')
    (74, '        return expand(')
    (75, '            "results/assignment/{assignment}/assignment_barcodes.{config}.sorted.tsv.gz",')
    (76, '            assignment=name,')
    (77, '            config=conf,')
    (78, '        )')
    (79, '')
    (80, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if config["experiments"][project][key] == value']
    (199, 'def getOutputConditionReplicateType_helper(file, project, skip={}):')
    (200, '    """')
    (201, '    Inserts {condition}, {replicate} and {type} from config into given file.')
    (202, '    Can skip projects with the given config set by skip.')
    (203, '    """')
    (204, '    output = []')
    (205, '')
    (206, '    for key, value in skip.items():')
    (207, '        if config["experiments"][project][key] == value:')
    (208, '            return []')
    (209, '    conditions = getConditions(project)')
    (210, '    for condition in conditions:')
    (211, '        replicates = getReplicatesOfCondition(project, condition)')
    (212, '        output += expand(')
    (213, '            file,')
    (214, '            project=project,')
    (215, '            condition=condition,')
    (216, '            replicate=replicates,')
    (217, '            type=["RNA", "DNA"],')
    (218, '        )')
    (219, '    return output')
    (220, '')
    (221, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=kircherlab/MPRAsnakeflow, file=workflow/rules/common.smk
context_key: ['if config["experiments"][project]["demultiplex"]']
    (423, 'def getBamFile(project, condition, replicate, type):')
    (424, '    """')
    (425, '    gelper to get the correct BAM file (demultiplexed or not)')
    (426, '    """')
    (427, '    if config["experiments"][project]["demultiplex"]:')
    (428, '        return "results/%s/counts/merged_demultiplex_%s_%s_%s.bam" % (')
    (429, '            project,')
    (430, '            condition,')
    (431, '            replicate,')
    (432, '            type,')
    (433, '        )')
    (434, '    else:')
    (435, '        return "results/experiments/%s/counts/%s_%s_%s.bam" % (')
    (436, '            project,')
    (437, '            condition,')
    (438, '            replicate,')
    (439, '            type,')
    (440, '        )')
    (441, '')
    (442, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=yanhui-k/Natrix, file=Snakefile
context_key: ['if config["merge"]["paired_End"]']
    (10, 'def is_single_end(sample, unit):')
    (11, '    return pd.isnull(units.loc[(sample,unit), "fq2"])')
    (12, '')
    (13, 'if config["merge"]["paired_End"]:')
    (14, '    reads = [1,2]')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=johnne/ASV-clustering, file=workflow/rules/swarm.smk
context_key: ['if config["swarm"][opt]']
    (13, 'def check_swarm_options(opt):')
    (14, '    if config["swarm"][opt]:')
    (15, '        return f"--{opt}"')
    (16, '    return ""')
    (17, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

repo=johnne/ASV-clustering, file=workflow/rules/opticlust.smk
context_key: ['if config["opticlust"]["aligner"] == "vsearch"']
    (32, 'def opticlust_input(wildcards):')
    (33, '    if config["opticlust"]["aligner"] == "vsearch":')
    (34, '        return f"results/vsearch/{wildcards.rundir}/asv_seqs.dist.reformat.gz"')
    (35, '    else:')
    (36, '        return f"results/opticlust/{wildcards.rundir}/asv_seqs.dist.gz"')
    (37, '')
-  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  

